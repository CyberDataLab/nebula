{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p> NEBULA: A Platform for Decentralized Federated Learning <p> nebula-dfl.com |     nebula-dfl.eu |     federatedlearning.inf.um.es </p> </p>"},{"location":"#about-nebula","title":"\ud83c\udf0c About NEBULA","text":"<p>NEBULA (previously known as Fedstellar<sup>1</sup>) is a cutting-edge platform designed to facilitate the training of federated models within both centralized and decentralized architectures. It streamlines the development, deployment, and management of federated applications across physical and virtualized devices.</p> <p>NEBULA is developed by Enrique Tom\u00e1s Mart\u00ednez Beltr\u00e1n, Alberto Huertas Celdr\u00e1n, Alejandro Avil\u00e9s Serrano, and Fernando Torres Vega in collaboration with the University of Murcia, armasuisse, and the University of Zurich.</p> <p> </p>"},{"location":"#key-components","title":"\ud83d\ude80 Key Components","text":"<p>NEBULA boasts a modular architecture that consists of three core elements:</p> <ul> <li>Frontend: A user-friendly interface for setting up experiments and monitoring progress.</li> <li>Controller: An orchestrator that ensures efficient operation management.</li> <li>Core: The fundamental component deployed on each device to handle federated learning processes.</li> </ul>"},{"location":"#main-features","title":"\ud83c\udf1f Main Features","text":"<ul> <li>Decentralized: Train models without a central server, leveraging decentralized federated learning.</li> <li>Privacy-preserving: Maintain data privacy by training on-device and only sharing model updates.</li> <li>Topology-agnostic: Support for various network topologies including star, ring, and mesh.</li> <li>Model-agnostic: Compatible with a wide range of machine learning algorithms, from deep learning to traditional methods.</li> <li>Network communication: Secure and efficient device communication with features like compression, network failure tolerance, and condition simulation.</li> <li>Trustworthiness: Ensure the integrity of the learning process by verifying the reliability of the federation.</li> <li>Security: Implement security mechanisms to protect the learning process from adversarial attacks.</li> <li>Real-time monitoring: Provides live performance metrics and visualizations during the learning process.</li> </ul>"},{"location":"#scenario-applications","title":"\ud83c\udf0d Scenario Applications","text":"<ul> <li>\ud83c\udfe5 Healthcare: Train models on medical devices such as wearables, smartphones, and sensors.</li> <li>\ud83c\udfed Industry 4.0: Implement on industrial devices like robots, drones, and constrained devices.</li> <li>\ud83d\udcf1 Mobile services: Optimize for mobile devices including smartphones, tablets, and laptops.</li> <li>\ud83d\udee1\ufe0f Military: Apply to military equipment such as drones, robots, and sensors.</li> <li>\ud83d\ude97 Vehicular scenarios: Utilize in vehicles including cars, trucks, and drones.</li> </ul>"},{"location":"#get-started","title":"\ud83c\udfaf Get Started","text":"<p>To start using NEBULA, follow our detailed Installation Guide and User Manual. For any queries or contributions, check out our Contribution Guide.</p>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>We welcome contributions from the community to enhance NEBULA. If you are interested in contributing, please follow the next steps:</p> <ol> <li>Fork the repository</li> <li>Create a new branch with your feature or bug fix (<code>git checkout -b feature/your-feature</code>).</li> <li>Commit your changes (<code>git commit -am 'Add new feature'</code>).</li> <li>Push to the branch (<code>git push origin feature/your-feature</code>).</li> <li>Create a new Pull Request.</li> </ol>"},{"location":"#citation","title":"\ud83d\udcda Citation","text":"<p>If you use NEBULA (or Fedstellar) in a scientific publication, we would appreciate using the following citations:</p> <pre><code>@article{MartinezBeltran:DFL:2023,\n    title        = {{Decentralized Federated Learning: Fundamentals, State of the Art, Frameworks, Trends, and Challenges}},\n    author       = {Mart{\\'i}nez Beltr{\\'a}n, Enrique Tom{\\'a}s and Quiles P{\\'e}rez, Mario and S{\\'a}nchez S{\\'a}nchez, Pedro Miguel and L{\\'o}pez Bernal, Sergio and Bovet, G{\\'e}r{\\^o}me and Gil P{\\'e}rez, Manuel and Mart{\\'i}nez P{\\'e}rez, Gregorio and Huertas Celdr{\\'a}n, Alberto},\n    year         = 2023,\n    volume       = {25},\n    number       = {4},\n    pages        = {2983-3013},\n    journal      = {IEEE Communications Surveys &amp; Tutorials},\n    doi          = {10.1109/COMST.2023.3315746},\n    preprint     = {https://arxiv.org/abs/2211.08413}\n}\n</code></pre> <pre><code>@article{MartinezBeltran:fedstellar:2024,\n    title        = {{Fedstellar: A Platform for Decentralized Federated Learning}},\n    author       = {Mart{\\'i}nez Beltr{\\'a}n, Enrique Tom{\\'a}s and Perales G{\\'o}mez, {\\'A}ngel Luis and Feng, Chao and S{\\'a}nchez S{\\'a}nchez, Pedro Miguel and L{\\'o}pez Bernal, Sergio and Bovet, G{\\'e}r{\\^o}me and Gil P{\\'e}rez, Manuel and Mart{\\'i}nez P{\\'e}rez, Gregorio and Huertas Celdr{\\'a}n, Alberto},\n    year         = 2024,\n    volume       = {242},\n    issn         = {0957-4174},\n    pages        = {122861},\n    journal      = {Expert Systems with Applications},\n    doi          = {10.1016/j.eswa.2023.122861},\n    preprint     = {https://arxiv.org/abs/2306.09750}\n}\n</code></pre> <pre><code>@inproceedings{MartinezBeltran:fedstellar_demo:2023,\n    title        = {{Fedstellar: A Platform for Training Models in a Privacy-preserving and Decentralized Fashion}},\n    author       = {Mart{\\'i}nez Beltr{\\'a}n, Enrique Tom{\\'a}s and S{\\'a}nchez S{\\'a}nchez, Pedro Miguel and L{\\'o}pez Bernal, Sergio and Bovet, G{\\'e}r{\\^o}me and Gil P{\\'e}rez, Manuel and Mart{\\'i}nez P{\\'e}rez, Gregorio and Huertas Celdr{\\'a}n, Alberto},\n    year         = 2023,\n    month        = aug,\n    booktitle    = {Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, {IJCAI-23}},\n    publisher    = {International Joint Conferences on Artificial Intelligence Organization},\n    pages        = {7154--7157},\n    doi          = {10.24963/ijcai.2023/838},\n    note         = {Demo Track},\n    editor       = {Edith Elkind}\n}\n</code></pre> <pre><code>@article{MartinezBeltran:DFL_mitigating_threats:2023,\n    title        = {{Mitigating Communications Threats in Decentralized Federated Learning through Moving Target Defense}},\n    author       = {Mart{\\'i}nez Beltr{\\'a}n, Enrique Tom{\\'a}s and S{\\'a}nchez S{\\'a}nchez, Pedro Miguel and L{\\'o}pez Bernal, Sergio and Bovet, G{\\'e}r{\\^o}me and Gil P{\\'e}rez, Manuel and Mart{\\'i}nez P{\\'e}rez, Gregorio and Huertas Celdr{\\'a}n, Alberto},\n    year         = 2024,\n    journal      = {Wireless Networks},\n    doi          = {10.1007/s11276-024-03667-8}\n    preprint     = {https://arxiv.org/abs/2307.11730}\n}\n</code></pre>"},{"location":"#license","title":"\ud83d\udcdd License","text":"<p>Distributed under the GNU AGPLv3 License. See <code>LICENSE</code> for more information.</p> <ul> <li>Community Edition \u2014 released under the GNU Affero GPL v3.0.</li> <li>Enterprise Edition \u2014 proprietary license &amp; premium support available.</li> </ul> <p>Contact enriquetomas@um.es and alberto.huertas@um.es for commercial terms.</p>"},{"location":"#acknowledgements","title":"\ud83d\ude4f Acknowledgements","text":"<p>We would like to thank the following projects for their contributions which have helped shape NEBULA:</p> <ul> <li>PyTorch Lightning for the training loop and model management</li> <li>Tensorboard for the visualization tools and monitoring capabilities</li> <li>Different datasets (nebula/core/datasets) and models (nebula/core/models) for testing and validation purposes</li> <li>FastAPI for the RESTful API</li> <li>Fedstellar platform and p2pfl library</li> <li>Adversarial Robustness Toolbox (ART) for the implementation of adversarial attacks</li> <li>D3.js for the network visualizations</li> </ul> <ol> <li> <p>Fedstellar was our first version of the platform. We have redesigned the previous functionalities and added new capabilities based on our research. The platform is now called NEBULA and is available as an open-source project.\u00a0\u21a9</p> </li> </ol>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p>"},{"location":"commercial-faq/","title":"Commercial FAQ \u2014 NEBULA Enterprise","text":"<p>Q 1. What does the commercial license cover? To be determined.</p> <p>Q 2. Does the commercial edition include extra features? To be determined.</p> <p>Q 3. Pricing model? To be determined.</p> <p>Q 4. Can we contribute back fixes? Absolutely; your patches remain under AGPL in the community edition, and you can keep proprietary extensions private under the commercial agreement.</p>"},{"location":"contributing/","title":"Contributing to NEBULA","text":""},{"location":"contributing/#1-fork-branch-commit","title":"1 \u2022 Fork \u2192 Branch \u2192 Commit","text":"<p>Follow conventional-commit style.</p>"},{"location":"contributing/#2-sign-the-cla","title":"2 \u2022 Sign the CLA","text":"<p>When you open your first Pull Request, CLA-assistant will block the merge until you tick the box confirming you accept the ICLA. Add a Developer-Certificate-of-Origin line in every commit:</p> <pre><code>Signed-off-by: Your Name &lt;your.email@example.com&gt;\n</code></pre>"},{"location":"contributing/#3-open-a-pull-request","title":"3 \u2022 Open a Pull Request","text":"<p>All status checks must pass before merging.</p>"},{"location":"contributing/#4-wait-for-review","title":"4 \u2022 Wait for Review","text":"<p>The pull request will be reviewed by the maintainers.</p>"},{"location":"contributing/#5-get-feedback","title":"5 \u2022 Get Feedback","text":"<p>The maintainers will provide feedback on the pull request.</p>"},{"location":"contributing/#6-merge-the-pull-request","title":"6 \u2022 Merge the Pull Request","text":"<p>The pull request will be merged by the maintainers.</p>"},{"location":"developerguide/","title":"Developer Guide","text":"<p>This guide is designed to help developers understand and contribute to the project. It provides detailed instructions on navigating the codebase, and implementing new features. Whether you're looking to fix bugs, add enhancements, or better understand the architecture, this guide will walk you through the essential processes and best practices for development.</p>"},{"location":"developerguide/#nebula-frontend","title":"NEBULA Frontend","text":"<p>This section explains the structure of the frontend and provides instructions on how to add new parameters or sections.</p>"},{"location":"developerguide/#frontend-structure","title":"Frontend Structure","text":"Structure <pre><code>/nebula/\n  addons/\n  config/\n  core/\n  frontend/\n    config/\n      nebula\n      participant.json.example\n    databases/\n      participants.db\n      notes.db\n      scenarios.db\n      users.db\n    static/\n    templates/\n      401.html\n      403.html\n      404.html\n      405.html\n      413.html\n      admin.html\n      dashboard.html\n      deployment.html\n      index.html\n      layout.html\n      monitor.html\n      private.html\n      statistics.html\n    app.py\n    database.py\n    Dockerfile\n    start_services.sh\n</code></pre> <p>The frontend is organized within the <code>frontend/</code> directory. Key files and folders include:</p> <ul> <li><code>config/</code> \u2192 Contains the participant.json.example, the default structure for the paramteres passed to each participant.</li> <li><code>databases/</code> \u2192 Contains the different databases for NEBULA</li> <li><code>static/</code> \u2192 Holds static assets (CSS, images, JS, etc.).</li> <li><code>templates/</code> \u2192 Contains HTML templates. Focus on deployment.html</li> </ul>"},{"location":"developerguide/#adding-a-new-parameter","title":"Adding a New Parameter","text":"<p>Define the new parameter in the participant.json.example file. Only create a new field if necessary</p> participant.json.example <pre><code>{\n    \"scenario_args\": {\n      \"name\": \"\",\n      \"start_time\": \"\",\n      \"federation\": \"DFL\",\n      \"rounds\": 10,\n      \"deployment\": \"process\",\n      \"controller\": \"127.0.0.1:5000\",\n      \"random_seed\": 42,\n      \"n_participants\": 0,\n      /* New parameter in each setting */\n      \"new_parameter_key\" : \"new_parameter_value\",\n      \"config_version\": \"development\"\n    },\n    /* Add a new_field if necessary */\n    \"new_field\": {\n        \"new_parameter_key\" : \"new_parameter_value\"\n    }\n}\n</code></pre> <p>To implement a new attack type, first locate the section where attacks are defined. Then, add the new attack option along with its corresponding parameter. Below is an example of how to integrate the attack and its associated parameter.</p> deployment.html <pre><code>&lt;div class=\"form-group row container-shadow tiny grey\"&gt;\n    &lt;h5 class=\"step-number\"&gt;Robustness &lt;i class=\"fa fa-shield\"&gt;&lt;/i&gt;\n        &lt;input type=\"checkbox\" tyle=\"display: none;\"&gt;\n        &lt;label for=\"robustness-lock\" class=\"icon-container\" style=\"float: right;\"&gt;\n            &lt;i class=\"fa fa-lock\"&gt;&lt;/i&gt;\n        &lt;/label&gt;\n    &lt;/h5&gt;\n    &lt;h5 class=\"step-title\"&gt;Attack Type&lt;/h5&gt;\n    &lt;div class=\"form-check form-check-inline\"&gt;\n        &lt;select class=\"form-control\" id=\"poisoning-attack-select\" name=\"poisoning-attack\"&gt;\n            &lt;option selected&gt;No Attack&lt;/option&gt;\n            &lt;option&gt;New Attack&lt;/option&gt; &lt;!-- Add this --&gt;\n        &lt;/select&gt;\n        &lt;h5 id=\"poisoned-participant-title\" class=\"step-title\"&gt;\n            % Malicious participants\n        &lt;/h5&gt;\n        &lt;div class=\"form-check form-check-inline\" style=\"display: none;\" id=\"poisoned-participant-percent-container\"&gt;\n            &lt;input type=\"number\" class=\"form-control\" id=\"poisoned-participant-percent\"\n                placeholder=\"% malicious participants\" min=\"0\" value=\"0\"&gt;\n                &lt;select class=\"form-control\" id=\"malicious-participants-select\" name=\"malicious-participants-select\"&gt;\n                &lt;option selected&gt;Percentage&lt;/option&gt;\n                &lt;option&gt;Manual&lt;/option&gt;\n            &lt;/select&gt;\n        &lt;/div&gt;\n        &lt;h5 id=\"poisoned-participant-title\" class=\"step-title\"&gt;\n            % Malicious participants\n        &lt;/h5&gt;\n        &lt;div class=\"form-check form-check-inline\" style=\"display: none;\" id=\"poisoned-participant-percent-container\"&gt;\n            &lt;input type=\"number\" class=\"form-control\" id=\"poisoned-participant-percent\"\n                placeholder=\"% malicious participants\" min=\"0\" value=\"0\"&gt;\n        &lt;/div&gt;\n        &lt;h5 id=\"new-parameter-title\" class=\"step-title\"&gt; &lt;!-- Add this --&gt;\n            New parameter\n        &lt;/h5&gt;\n        &lt;div class=\"form-check form-check-inline\" style=\"display: none;\" id=\"new-parameter-container\"&gt;\n            &lt;input type=\"number\" class=\"form-control\" id=\"new-parameter-value\"\n                placeholder=\"new parameter value\" min=\"0\" value=\"0\"&gt;\n        &lt;/div&gt;\n    &lt;/div&gt;\n&lt;/div&gt;\n</code></pre> <p>To receive the parameter in nebula/scenarios.py, you need to modify the Scenario class to accept the new parameter. This involves updating the Scenario class constructor and possibly the relevant methods to handle the new parameter accordingly.</p> Class Scenario <pre><code>class Scenario:\n    def __init__(\n        self,\n        scenario_title,\n        scenario_description,\n        new_paramater, # &lt;--- Add this\n    ):\n        self.scenario_title = scenario_title\n        self.scenario_description = scenario_description\n        self.new_parameter = new_parameter # &lt;--- Add this\n</code></pre> <p>Now you must save the parameter in the participant configuration.</p> <p>The participant configuration files are located in the /app/config/ directory. Ensure that the new parameter is added to the participant's JSON file, so it can be accessed later when the configuration is loaded.</p> Class ScenarioManagement <pre><code>    class ScenarioManagement:\n    def __init__(self, scenario, user=None):\n        # Save participant settings\n        for participant in self.scenario.participants:\n            participant_config = self.scenario.participants[participant]\n            participant_file = os.path.join(self.config_dir, f\"participant_{participant_config['id']}.json\")\n            os.makedirs(os.path.dirname(participant_file), exist_ok=True)\n            shutil.copy(\n                os.path.join(\n                    os.path.dirname(__file__),\n                    \"./frontend/config/participant.json.example\",\n                ),\n                participant_file,\n            )\n            os.chmod(participant_file, 0o777)\n            with open(participant_file) as f:\n                participant_config = json.load(f)\n\n            participant_config[\"network_args\"][\"ip\"] = participant_config[\"ip\"]\n            participant_config[\"network_args\"][\"port\"] = int(participant_config[\"port\"])\n            # In case you are adding a parameter to a previously defined functionality\n            participant_config[\"data_args\"][\"new_parameter\"] = self.scenario.new_parameter\n            # In case you are creating a new functionality\n            participant_config[\"new_field\"][\"new_parameter\"] = self.scenario.new_parameter\n</code></pre>"},{"location":"developerguide/#nebula-backend","title":"NEBULA Backend","text":"<p>To view the documentation of functions in more detail, you must go to the NEBULA API Reference. This reference will provide you with comprehensive details about the available functions, their parameters, return types, and usage examples, allowing you to understand how to properly implement and interact with them.</p>"},{"location":"developerguide/#backend-structure","title":"Backend Structure","text":"Structure <pre><code>/nebula/\n  addons/\n    attacks/\n    trustworthiness/\n    waf/\n  core/\n    aggregation/\n    datasets/\n    models/\n    network/\n    pb/\n    training/\n    utils/\n    engine.py\n    eventmanager.py\n    role.py\n  controller.py\n  participant.py\n  scenarios.py\n  utils.py\n</code></pre> <p>The backend is organized within the <code>/nebula/</code> directory. Key files and folders include:</p> <p>Addons/</p> <p>The <code>addons/</code> directory contains extended functionalities that can be integrated into the core system.</p> <ul> <li><code>attacks/</code> \u2192 Simulates attacks, primarily for security purposes, including adversarial attacks in machine learning.</li> <li><code>trustworthiness/</code> \u2192 Evaluates the trustworthiness and reliability of participants, focusing on security and ethical considerations.</li> <li><code>waf/</code> \u2192 Implements a Web Application Firewall (WAF) to filter and monitor HTTP traffic for potential threats.</li> </ul> <p>Core/</p> <p>The <code>core/</code> directory contains the essential components for the backend operation.</p> <ul> <li><code>aggregation/</code> \u2192 Manages the aggregation of data from different nodes.</li> <li><code>datasets/</code> \u2192 Handles dataset management, including loading and preprocessing data.</li> <li><code>models/</code> \u2192 Defines machine learning model architectures and related functionalities, such as training and evaluation.</li> <li><code>network/</code> \u2192 Manages communication between participants in a distributed system.</li> <li><code>pb/</code> \u2192 Implements Protocol Buffers (PB) for efficient data serialization and communication.</li> <li><code>training/</code> \u2192 Contains the logic for model training, optimization, and evaluation.</li> <li><code>utils/</code> \u2192 Provides utility functions for file handling, logging, and common tasks.</li> </ul> <p>Files</p> <ul> <li><code>engine.py</code> \u2192 The main engine orchestrating participant communications, training, and overall behavior.</li> <li><code>eventmanager.py</code> \u2192 Handles event management, logging, and notifications within the system.</li> <li><code>role.py</code> \u2192 Defines participant roles and their interactions.</li> </ul> <p>Standalone Scripts</p> <p>These scripts act as entry points or controllers for various backend functionalities.</p> <ul> <li><code>controller.py</code> \u2192 Manages the flow of operations, coordinating tasks and interactions.</li> <li><code>participant.py</code> \u2192 Represents a participant in the decentralized network, handling computations and communication.</li> <li><code>scenarios.py</code> \u2192 Defines different simulation scenarios for testing and running participants under specific conditions.</li> <li><code>utils.py</code> \u2192 Contains helper functions that simplify development and maintenance.</li> </ul>"},{"location":"developerguide/#adding-new-datasets","title":"Adding new Datasets","text":""},{"location":"developerguide/#add-the-dataset-option-in-the-front","title":"Add the Dataset option in the front","text":"<p>First, you must add the Dataset option in the frontend. Adding the Dataset option to the scenario generated by the frontend requires a slightly different approach.</p> Datasets in Deployment.html <pre><code>&lt;script&gt;\n    // Add the dataset with each model here\n    var datasets = {\n        \"MNIST\": [\"MLP\", \"CNN\"],\n        \"FashionMNIST\": [\"MLP\", \"CNN\"],\n        \"EMNIST\": [\"MLP\", \"CNN\"],\n        \"CIFAR10\": [\"CNN\", \"CNNv2\", \"CNNv3\", \"ResNet9\", \"fastermobilenet\", \"simplemobilenet\"],\n        \"CIFAR100\": [\"CNN\"],\n    }\n    var datasetSelect = document.getElementById(\"datasetSelect\");\n    var modelSelect = document.getElementById(\"modelSelect\");\n&lt;/script&gt;\n</code></pre> <p>If you want to add a new Dataset you can implement this in two ways on the folder /nebula/core/datasets/new_dataset/new_dataset.py</p>"},{"location":"developerguide/#import-the-dataset-from-torchvision","title":"Import the Dataset from Torchvision","text":"<p>You can use the MNIST Dataset as a code example to demonstrate how to import the dataset from torchvision, initialize it, and load its configuration.</p> MNIST Code example <pre><code>import os\n\nfrom torchvision import transforms\nfrom torchvision.datasets import MNIST\n\nfrom nebula.core.datasets.nebuladataset import NebulaDataset\n\n\nclass MNISTDataset(NebulaDataset):\n    def __init__(\n        self,\n        num_classes=10,\n        partition_id=0,\n        partitions_number=1,\n        batch_size=32,\n        num_workers=4,\n        iid=True,\n        partition=\"dirichlet\",\n        partition_parameter=0.5,\n        seed=42,\n        config=None,\n    ):\n        super().__init__(\n            num_classes=num_classes,\n            partition_id=partition_id,\n            partitions_number=partitions_number,\n            batch_size=batch_size,\n            num_workers=num_workers,\n            iid=iid,\n            partition=partition,\n            partition_parameter=partition_parameter,\n            seed=seed,\n            config=config,\n        )\n        if partition_id &lt; 0 or partition_id &gt;= partitions_number:\n            raise ValueError(f\"partition_id {partition_id} is out of range for partitions_number {partitions_number}\")\n\n    def initialize_dataset(self):\n        if self.train_set is None:\n            self.train_set = self.load_mnist_dataset(train=True)\n        if self.test_set is None:\n            self.test_set = self.load_mnist_dataset(train=False)\n\n        self.test_indices_map = list(range(len(self.test_set)))\n\n        # Depending on the iid flag, generate a non-iid or iid map of the train set\n        if self.iid:\n            self.train_indices_map = self.generate_iid_map(self.train_set, self.partition, self.partition_parameter)\n            self.local_test_indices_map = self.generate_iid_map(self.test_set, self.partition, self.partition_parameter)\n        else:\n            self.train_indices_map = self.generate_non_iid_map(self.train_set, self.partition, self.partition_parameter)\n            self.local_test_indices_map = self.generate_non_iid_map(\n                self.test_set, self.partition, self.partition_parameter\n            )\n\n        print(f\"Length of train indices map: {len(self.train_indices_map)}\")\n        print(f\"Lenght of test indices map (global): {len(self.test_indices_map)}\")\n        print(f\"Length of test indices map (local): {len(self.local_test_indices_map)}\")\n\n    def load_mnist_dataset(self, train=True):\n        apply_transforms = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.5,), (0.5,), inplace=True),\n        ])\n        data_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"data\")\n        os.makedirs(data_dir, exist_ok=True)\n        return MNIST(\n            data_dir,\n            train=train,\n            download=True,\n            transform=apply_transforms,\n        )\n</code></pre>"},{"location":"developerguide/#import-the-dataset-from-your-own","title":"Import the Dataset from your own","text":"<p>If you want to import a dataset, you must first create a folder named data where you will store the image_list. Then, create a Dataset class similar to the one in the MilitarySAR code example.</p> MilitarySAR Code Example <pre><code>class MilitarySAR(Dataset):\ndef __init__(self, name=\"soc\", is_train=False, transform=None):\n    self.is_train = is_train\n    self.name = name\n\n    self.data = []\n    self.targets = []\n    self.serial_numbers = []\n\n    # Path to data is \"data\" folder in the same directory as this file\n    self.path_to_data = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"data\")\n\n    self.transform = transform\n\n    # self._load_data(self.path_to_data)\n\n    mode = \"train\" if self.is_train else \"test\"\n    self.image_list = glob.glob(os.path.join(self.path_to_data, f\"{self.name}/{mode}/*/*.npy\"))\n    self.label_list = glob.glob(os.path.join(self.path_to_data, f\"{self.name}/{mode}/*/*.json\"))\n    self.image_list = sorted(self.image_list, key=os.path.basename)\n    self.label_list = sorted(self.label_list, key=os.path.basename)\n    assert len(self.image_list) == len(self.label_list)\n\ndef __len__(self):\n\ndef __getitem__(self, idx):\n\ndef _load_metadata(self):\n\ndef get_targets(self):\n</code></pre> <p>Then you must create a MilitarySARDataset class in order to use it, as shown in the example below</p> MilitarySARDataset Code example <pre><code>class MilitarySARDataset(NebulaDataset):\n    def __init__(\n        self,\n        num_classes=10,\n        partition_id=0,\n        partitions_number=1,\n        batch_size=32,\n        num_workers=4,\n        iid=True,\n        partition=\"dirichlet\",\n        partition_parameter=0.5,\n        seed=42,\n        config=None,\n    ):\n        super().__init__(\n            num_classes=num_classes,\n            partition_id=partition_id,\n            partitions_number=partitions_number,\n            batch_size=batch_size,\n            num_workers=num_workers,\n            iid=iid,\n            partition=partition,\n            partition_parameter=partition_parameter,\n            seed=seed,\n            config=config,\n        )\n\n    def initialize_dataset(self):\n\n    def load_militarysar_dataset(self, train=True):\n</code></pre>"},{"location":"developerguide/#define-transforms","title":"Define transforms","text":"<p>You can apply transformations like cropping and normalization using <code>torchvision.transforms</code>.</p> <p>For example, the MilitarySAR dataset uses RandomCrop for training and CenterCrop for testing.</p> MilitarySAR <pre><code>class RandomCrop:\n    def __init__(self, size):\n        if isinstance(size, int):\n            self.size = (size, size)\n        else:\n            assert len(size) == 2\n            self.size = size\n\n    def __call__(self, sample):\n        _input = sample\n\n        if len(_input.shape) &lt; 3:\n            _input = np.expand_dims(_input, axis=2)\n\n        h, w, _ = _input.shape\n        oh, ow = self.size\n\n        dh = h - oh\n        dw = w - ow\n        y = np.random.randint(0, dh) if dh &gt; 0 else 0\n        x = np.random.randint(0, dw) if dw &gt; 0 else 0\n        oh = oh if dh &gt; 0 else h\n        ow = ow if dw &gt; 0 else w\n\n        return _input[y : y + oh, x : x + ow, :]\n\n\nclass CenterCrop:\n    def __init__(self, size):\n        if isinstance(size, int):\n            self.size = (size, size)\n        else:\n            assert len(size) == 2\n            self.size = size\n\n    def __call__(self, sample):\n        _input = sample\n\n        if len(_input.shape) &lt; 3:\n            _input = np.expand_dims(_input, axis=2)\n\n        h, w, _ = _input.shape\n        oh, ow = self.size\n        y = (h - oh) // 2\n        x = (w - ow) // 2\n\n        return _input[y : y + oh, x : x + ow, :]\n\nclass MilitarySARDataset(NebulaDataset):\n    def load_militarysar_dataset(self, train=True):\n        apply_transforms = [CenterCrop(88), transforms.ToTensor()]\n        if train:\n            apply_transforms = [RandomCrop(88), transforms.ToTensor()]\n\n        return MilitarySAR(name=\"soc\", is_train=train, transform=transforms.Compose(apply_transforms))\n</code></pre>"},{"location":"developerguide/#associate-the-model-with-the-new-dataset","title":"Associate the Model with the new Dataset","text":"<p>Now, you need to add the model you want to use with the dataset in the /nebula/core/models/ folder, by creating a file named new_dataset/new_model.py</p> <p>The model must inherit from the NebulaModel class</p> MLP Code example <pre><code>import torch\n\nfrom nebula.core.models.nebulamodel import NebulaModel\n\n\nclass MNISTModelMLP(NebulaModel):\n    def __init__(\n        self,\n        input_channels=1,\n        num_classes=10,\n        learning_rate=1e-3,\n        metrics=None,\n        confusion_matrix=None,\n        seed=None,\n    ):\n        super().__init__(input_channels, num_classes, learning_rate, metrics, confusion_matrix, seed)\n\n        self.example_input_array = torch.zeros(1, 1, 28, 28)\n        self.learning_rate = learning_rate\n        self.criterion = torch.nn.CrossEntropyLoss()\n        self.l1 = torch.nn.Linear(28 * 28, 256)\n        self.l2 = torch.nn.Linear(256, 128)\n        self.l3 = torch.nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        batch_size, channels, width, height = x.size()\n        x = x.view(batch_size, -1)\n        x = self.l1(x)\n        x = torch.relu(x)\n        x = self.l2(x)\n        x = torch.relu(x)\n        x = self.l3(x)\n        return x\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n        return optimizer\n</code></pre>"},{"location":"developerguide/#adding-new-aggregators","title":"Adding new Aggregators","text":""},{"location":"developerguide/#adding-the-aggregator-in-the-frontend","title":"Adding the Aggregator in the frontend","text":"<p>You must add the new aggregator in the deployment.html file and ensure that it is correctly included in the JSON files generated within the /app/config folder. After making the necessary updates in the HTML, verify that the new aggregator is properly reflected in the corresponding configuration files by checking the JSON structure and values.</p> deployment.html <pre><code>    &lt;h5 class=\"step-title\"&gt;Aggregation algorithm&lt;/h5&gt;\n    &lt;div class=\"form-check form-check-inline\"&gt;\n        &lt;select class=\"form-control\" id=\"aggregationSelect\" name=\"aggregation\"\n            style=\"display: inline; width: 50%\"&gt;\n            &lt;option selected&gt;FedAvg&lt;/option&gt;\n            &lt;option&gt;Krum&lt;/option&gt;\n            &lt;option&gt;TrimmedMean&lt;/option&gt;\n            &lt;option&gt;Median&lt;/option&gt;\n            &lt;!--Add this--&gt;\n            &lt;option&gt;new_aggregation&lt;/option&gt;\n        &lt;/select&gt;\n    &lt;/div&gt;\n</code></pre>"},{"location":"developerguide/#adding-the-aggregator-file","title":"Adding the Aggregator file","text":"<p>You need to add the aggregator you want to use into /nebula/core/aggregation/ by creating a file named new_aggregator.py</p> <p>The new aggregator must inherit from the Aggregator class. You can use FedAvg as an example to guide your implementation</p> Aggregator class <pre><code>class Aggregator(ABC):\n    def __init__(self, config=None, engine=None):\n        self.config = config\n        self.engine = engine\n        self._addr = config.participant[\"network_args\"][\"addr\"]\n        logging.info(f\"[{self.__class__.__name__}] Starting Aggregator\")\n        self._federation_nodes = set()\n        self._waiting_global_update = False\n        self._pending_models_to_aggregate = {}\n        self._future_models_to_aggregate = {}\n        self._add_model_lock = Locker(name=\"add_model_lock\", async_lock=True)\n        self._aggregation_done_lock = Locker(name=\"aggregation_done_lock\", async_lock=True)\n\n    def __str__(self):\n        return self.__class__.__name__\n\n    def __repr__(self):\n        return self.__str__()\n\n    @property\n    def cm(self):\n        return self.engine.cm\n\n    @abstractmethod\n    def run_aggregation(self, models):\n        if len(models) == 0:\n            logging.error(\"Trying to aggregate models when there are no models\")\n            return None\n\n    async def update_federation_nodes(self, federation_nodes):\n        if not self._aggregation_done_lock.locked():\n            self._federation_nodes = federation_nodes\n            self._pending_models_to_aggregate.clear()\n            await self._aggregation_done_lock.acquire_async(\n                timeout=self.config.participant[\"aggregator_args\"][\"aggregation_timeout\"]\n            )\n        else:\n            raise Exception(\"It is not possible to set nodes to aggregate when the aggregation is running.\")\n\n    def set_waiting_global_update(self):\n        self._waiting_global_update = True\n\n    async def reset(self):\n        await self._add_model_lock.acquire_async()\n        self._federation_nodes.clear()\n        self._pending_models_to_aggregate.clear()\n        try:\n            await self._aggregation_done_lock.release_async()\n        except:\n            pass\n        await self._add_model_lock.release_async()\n\n    def get_nodes_pending_models_to_aggregate(self):\n        return {node for key in self._pending_models_to_aggregate.keys() for node in key.split()}\n\n    async def _handle_global_update(self, model, source):\n        logging.info(f\"\ud83d\udd04  _handle_global_update | source={source}\")\n        logging.info(\n            f\"\ud83d\udd04  _handle_global_update | Received a model from {source}. Overwriting __models with the aggregated model.\"\n        )\n        self._pending_models_to_aggregate.clear()\n        self._pending_models_to_aggregate = {source: (model, 1)}\n        self._waiting_global_update = False\n        await self._add_model_lock.release_async()\n        await self._aggregation_done_lock.release_async()\n\n    async def _add_pending_model(self, model, weight, source):\n        if len(self._federation_nodes) &lt;= len(self.get_nodes_pending_models_to_aggregate()):\n            logging.info(\"\ud83d\udd04  _add_pending_model | Ignoring model...\")\n            await self._add_model_lock.release_async()\n            return None\n\n        if source not in self._federation_nodes:\n            logging.info(f\"\ud83d\udd04  _add_pending_model | Can't add a model from ({source}), which is not in the federation.\")\n            await self._add_model_lock.release_async()\n            return None\n\n        elif source not in self.get_nodes_pending_models_to_aggregate():\n            logging.info(\n                \"\ud83d\udd04  _add_pending_model | Node is not in the aggregation buffer --&gt; Include model in the aggregation buffer.\"\n            )\n            self._pending_models_to_aggregate.update({source: (model, weight)})\n\n        logging.info(\n            f\"\ud83d\udd04  _add_pending_model | Model added in aggregation buffer ({len(self.get_nodes_pending_models_to_aggregate())!s}/{len(self._federation_nodes)!s}) | Pending nodes: {self._federation_nodes - self.get_nodes_pending_models_to_aggregate()}\"\n        )\n\n        # Check if _future_models_to_aggregate has models in the current round to include in the aggregation buffer\n        if self.engine.get_round() in self._future_models_to_aggregate:\n            logging.info(\n                f\"\ud83d\udd04  _add_pending_model | Including next models in the aggregation buffer for round {self.engine.get_round()}\"\n            )\n            for future_model in self._future_models_to_aggregate[self.engine.get_round()]:\n                if future_model is None:\n                    continue\n                future_model, future_weight, future_source = future_model\n                if (\n                    future_source in self._federation_nodes\n                    and future_source not in self.get_nodes_pending_models_to_aggregate()\n                ):\n                    self._pending_models_to_aggregate.update({future_source: (future_model, future_weight)})\n                    logging.info(\n                        f\"\ud83d\udd04  _add_pending_model | Next model added in aggregation buffer ({len(self.get_nodes_pending_models_to_aggregate())!s}/{len(self._federation_nodes)!s}) | Pending nodes: {self._federation_nodes - self.get_nodes_pending_models_to_aggregate()}\"\n                    )\n            del self._future_models_to_aggregate[self.engine.get_round()]\n\n            for future_round in list(self._future_models_to_aggregate.keys()):\n                if future_round &lt; self.engine.get_round():\n                    del self._future_models_to_aggregate[future_round]\n\n        if len(self.get_nodes_pending_models_to_aggregate()) &gt;= len(self._federation_nodes):\n            logging.info(\"\ud83d\udd04  _add_pending_model | All models were added in the aggregation buffer. Run aggregation...\")\n            await self._aggregation_done_lock.release_async()\n        await self._add_model_lock.release_async()\n        return self.get_nodes_pending_models_to_aggregate()\n\n    async def include_model_in_buffer(self, model, weight, source=None, round=None, local=False):\n        await self._add_model_lock.acquire_async()\n        logging.info(\n            f\"\ud83d\udd04  include_model_in_buffer | source={source} | round={round} | weight={weight} |--| __models={self._pending_models_to_aggregate.keys()} | federation_nodes={self._federation_nodes} | pending_models_to_aggregate={self.get_nodes_pending_models_to_aggregate()}\"\n        )\n        if model is None:\n            logging.info(\"\ud83d\udd04  include_model_in_buffer | Ignoring model bad formed...\")\n            await self._add_model_lock.release_async()\n            return\n\n        if round == -1:\n            # Be sure that the model message is not from the initialization round (round = -1)\n            logging.info(\"\ud83d\udd04  include_model_in_buffer | Ignoring model with round -1\")\n            await self._add_model_lock.release_async()\n            return\n\n        if self._waiting_global_update and not local:\n            await self._handle_global_update(model, source)\n            return\n\n        await self._add_pending_model(model, weight, source)\n\n        if len(self.get_nodes_pending_models_to_aggregate()) &gt;= len(self._federation_nodes):\n            logging.info(\n                f\"\ud83d\udd04  include_model_in_buffer | Broadcasting MODELS_INCLUDED for round {self.engine.get_round()}\"\n            )\n            message = self.cm.create_message(\"federation\", \"federation_models_included\", [str(arg) for arg in [self.engine.get_round()]])\n            await self.cm.send_message_to_neighbors(message)\n\n        return\n\n    async def get_aggregation(self):\n        try:\n            timeout = self.config.participant[\"aggregator_args\"][\"aggregation_timeout\"]\n            await self._aggregation_done_lock.acquire_async(timeout=timeout)\n        except TimeoutError:\n            logging.exception(\"\ud83d\udd04  get_aggregation | Timeout reached for aggregation\")\n        except asyncio.CancelledError:\n            logging.exception(\"\ud83d\udd04  get_aggregation | Lock acquisition was cancelled\")\n        except Exception as e:\n            logging.exception(f\"\ud83d\udd04  get_aggregation | Error acquiring lock: {e}\")\n        finally:\n            await self._aggregation_done_lock.release_async()\n\n        if self._waiting_global_update and len(self._pending_models_to_aggregate) == 1:\n            logging.info(\n                \"\ud83d\udd04  get_aggregation | Received an global model. Overwriting my model with the aggregated model.\"\n            )\n            aggregated_model = next(iter(self._pending_models_to_aggregate.values()))[0]\n            self._pending_models_to_aggregate.clear()\n            return aggregated_model\n\n        unique_nodes_involved = set(node for key in self._pending_models_to_aggregate for node in key.split())\n\n        if len(unique_nodes_involved) != len(self._federation_nodes):\n            missing_nodes = self._federation_nodes - unique_nodes_involved\n            logging.info(f\"\ud83d\udd04  get_aggregation | Aggregation incomplete, missing models from: {missing_nodes}\")\n        else:\n            logging.info(\"\ud83d\udd04  get_aggregation | All models accounted for, proceeding with aggregation.\")\n\n        aggregated_result = self.run_aggregation(self._pending_models_to_aggregate)\n        self._pending_models_to_aggregate.clear()\n        return aggregated_result\n\n    async def include_next_model_in_buffer(self, model, weight, source=None, round=None):\n        logging.info(f\"\ud83d\udd04  include_next_model_in_buffer | source={source} | round={round} | weight={weight}\")\n        if round not in self._future_models_to_aggregate:\n            self._future_models_to_aggregate[round] = []\n        decoded_model = self.engine.trainer.deserialize_model(model)\n        self._future_models_to_aggregate[round].append((decoded_model, weight, source))\n\n    def print_model_size(self, model):\n        total_params = 0\n        total_memory = 0\n\n        for _, param in model.items():\n            num_params = param.numel()\n            total_params += num_params\n\n            memory_usage = param.element_size() * num_params\n            total_memory += memory_usage\n\n        total_memory_in_mb = total_memory / (1024**2)\n        logging.info(f\"print_model_size | Model size: {total_memory_in_mb} MB\")\n</code></pre> FedAvg.py <pre><code>import gc\n\nimport torch\n\nfrom nebula.core.aggregation.aggregator import Aggregator\n\n\nclass FedAvg(Aggregator):\n    \"\"\"\n    Aggregator: Federated Averaging (FedAvg)\n    Authors: McMahan et al.\n    Year: 2016\n    \"\"\"\n\n    def __init__(self, config=None, **kwargs):\n        super().__init__(config, **kwargs)\n\n    def run_aggregation(self, models):\n        super().run_aggregation(models)\n\n        models = list(models.values())\n\n        total_samples = float(sum(weight for _, weight in models))\n\n        if total_samples == 0:\n            raise ValueError(\"Total number of samples must be greater than zero.\")\n\n        last_model_params = models[-1][0]\n        accum = {layer: torch.zeros_like(param, dtype=torch.float32) for layer, param in last_model_params.items()}\n\n        with torch.no_grad():\n            for model_parameters, weight in models:\n                normalized_weight = weight / total_samples\n                for layer in accum:\n                    accum[layer].add_(\n                        model_parameters[layer].to(accum[layer].dtype),\n                        alpha=normalized_weight,\n                    )\n\n        del models\n        gc.collect()\n\n        # self.print_model_size(accum)\n        return accum\n</code></pre>"},{"location":"developerguide/#adding-new-messages","title":"Adding new messages","text":"<p>To add a new message to the application, follow these steps:</p> <p>1. Create your message in the nebula.proto file inside the nebula/core/network/pb. Follow the structure used by other messages. For example:</p> New message example <pre><code>message FederationMessage {\n  enum Action {\n    FEDERATION_START = 0;\n    REPUTATION = 1;\n    FEDERATION_MODELS_INCLUDED = 2;\n    FEDERATION_READY = 3;\n  }\n  Action action = 1;\n  repeated string arguments = 2;\n  int32 round = 3;\n}\n</code></pre> <p>Even if your message only has one \u201ctype,\u201d define it in this manner so that subsequent steps stay consistent.</p> <p>2. Once the message is created in nebula.proto, run the protobuf protocol as indicated in the file to generate the nebula_2pb.py file.</p> <p>3. In the file /core/network/actions.py, add the class that represents your message, following the structure of the others, and include it in the dictionary ACTION_CLASSES as a key-value pair (message_name, created_class). The message_name should be in lowercase.</p> <p>4. In the file /core/network/messages.py, add your message template in the dictionary inside the _define_message_templates() function. Follow the existing structure, indicating in parameters the different parameters of your message. In defaults, you can provide default values for your parameters to simplify message creation. It is important to use the exact same parameter names you defined in nebula.proto.</p> <p>5. In the file core/engine.py, define your callback that will be executed when the message is received. Use the same naming convention used in the other callbacks:</p> Naming convention example <pre><code>&lt;message_name&gt;_&lt;message_action&gt;_callback(self, source, message)\n</code></pre> <p>If you have followed all the previous steps, at runtime, the events and callbacks associated with your message will automatically be registered. The system will run the callback you defined whenever the new message is received. There is nothing else to do\u2014your message is now implemented in the messaging protocol!</p>"},{"location":"developerguide/#event-system","title":"Event System","text":"<p>An Event-Driven Architecture (EDA) is a design model in which system components communicate through events instead of direct calls. In this architecture, producers generate events that are consumed by other services in an asynchronous manner, allowing a high degree of decoupling, scalability, and flexibility.</p>"},{"location":"developerguide/#events","title":"Events","text":"<p>There are currently three different event types defined in the file /core/nebulaevents.py:</p> <ul> <li>NodeEvent: Events associated with the training-aggregation process that comprises FL.</li> <li>MessageEvent: Events associated with Nebula\u2019s communication protocol.</li> <li>AddonEvent: Events associated with additional components that can be added to scenarios.</li> </ul> <p>Each event type has a different internal structure and is managed independently by the event handler, including maintaining a separate queue for each event type. Moreover, for NodeEvent, there is an option to define whether the subscribed listeners should be run concurrently or not.</p> <p>Meanwhile, we have the EventManager, which controls how events are subscribed to and published. We will first look at how to use this functionality with Nebula\u2019s native events and then go through the steps for creating new events.</p> Import <pre><code>from nebula.core.eventmanager import EventManager\n</code></pre> <p>Once it is imported, we can subscribe to and publish events in our .py files:</p> Subscribing to an event <pre><code>await EventManager.get_instance().subscribe(EventType, callback_used_on_trigger)\n</code></pre> <p>Note that EventType is the class that represents the event (not a specific instance) and callback_used_on_trigger is a coroutine (defined with async). To specify EventType, you need to import it from nebulaevents.py</p> Import <pre><code>from nebula.core.nebulaevents import EventType\n</code></pre> <p>Publishing an event:</p> <ol> <li> <p>Import the event type you want to publish.</p> </li> <li> <p>Create an instance of that event, adhering to its definition.</p> </li> <li> <p>Use the corresponding publish function for that event type.</p> </li> </ol> Event example <pre><code>current_time = time.time()\nrse = RoundStartEvent(self.round, current_time)\nawait EventManager.get_instance().publish_node_event(rse)\n</code></pre> <p>When the event is published, all subscribed listeners for that event type will be triggered. As mentioned, there are three different publish functions, each tied to a specific type of event.</p> <p>Finally, to create a new event, go to the file /core/nebulaevents.py. Depending on the type of event you wish to implement, create a class that extends one of the three native event types. After doing this, the usage of your new event is transparent to the rest of the system, and you can use the functions described above without any issues.</p>"},{"location":"git-workflow/","title":"\ud83d\ude80 Git Workflow &amp; Best Practices","text":"<p>A concise, opinionated workflow for the Frontend, Controller, and Core teams working on NEBULA Platform.</p> <p>Following these conventions minimises merge conflicts, preserves history, and accelerates delivery.</p>"},{"location":"git-workflow/#1-branch-model","title":"1. Branch Model","text":"<pre><code>main     \u2500\u2500\u25b6 production (always deployable)\n\u2502\n\u251c\u2500 develop            \u2500\u2500\u25b6 integration branch\n\u2502   \u251c\u2500 feature/&lt;scope&gt;-&lt;desc&gt;\n\u2502   \u251c\u2500 bugfix/&lt;scope&gt;-&lt;desc&gt;\n\u2502   \u251c\u2500 hotfix/&lt;scope&gt;-&lt;desc&gt;\n\u2502   \u2514\u2500 release/&lt;version&gt;\n</code></pre>"},{"location":"git-workflow/#11-branch-types","title":"1.1 Branch Types","text":"Branch Purpose <code>main</code> Stable, released code. Direct commits are blocked. <code>develop</code> Integration of completed work before a release. <code>feature/*</code> New functionality. <code>bugfix/*</code> Non\u2011critical fixes. <code>hotfix/*</code> Critical production patches. <code>release/*</code> Final hardening + version bump before shipping. <p>Rule of thumb: Every change flows through a Pull Request (PR) \u2014 never push to <code>main</code> or <code>develop</code> directly.</p>"},{"location":"git-workflow/#2-naming-conventions","title":"2. Naming Conventions","text":"Type Pattern Example Feature <code>feature/&lt;component&gt;-&lt;desc&gt;</code> <code>feature/frontend-login-page</code> Bugfix <code>bugfix/&lt;component&gt;-&lt;desc&gt;</code> <code>bugfix/controller-null-pointer</code> Hotfix <code>hotfix/&lt;component&gt;-&lt;desc&gt;</code> <code>hotfix/core-memory-leak</code> Release <code>release/&lt;major&gt;.&lt;minor&gt;.&lt;patch&gt;</code> <code>release/1.4.0</code>"},{"location":"git-workflow/#3-conventional-commits","title":"3. Conventional Commits","text":"<p>Format: <code>&lt;type&gt;(&lt;scope&gt;): &lt;short summary&gt;</code></p> Type Use for \u2026 Example <code>feat</code> New feature <code>feat(controller): add /scenarios endpoint</code> <code>fix</code> Bug fix <code>fix(frontend): correct form validation</code> <code>docs</code> Docs only <code>docs(core): explain rule engine config</code> <code>refactor</code> Internal restructuring <code>refactor(core): extract auth helpers</code> <code>test</code> Add/modify tests <code>test(frontend): snapshot for Button</code> <code>chore</code> Build, tooling, release, cleaning <code>chore: removing logs</code> <p>References to issues: add <code>Closes #123</code> in the commit body.</p>"},{"location":"git-workflow/#4-endtoend-workflow","title":"4. End\u2011to\u2011End Workflow","text":"<ol> <li>Create an issue describing scope, acceptance criteria, and dependencies.</li> <li>Branch from <code>develop</code> following the naming rules.</li> <li>Commit early &amp; often using Conventional Commits.</li> <li>Push &amp; open a PR to <code>develop</code>.</li> <li>Assign at least one reviewer.</li> <li>CI must pass (lint, tests, build).</li> <li>Review &amp; merge when approved. Delete the remote branch.</li> </ol>"},{"location":"git-workflow/#41-releasing","title":"4.1 Releasing","text":"<p>For planned releases, follow this workflow to ensure stability:</p> <ul> <li>When to release: Based on sprint cycles or feature readiness</li> <li>Key points:</li> <li>Release branches isolate stabilization work</li> <li>All changes must be regression tested</li> <li> <p>Coordinates deployment across components</p> </li> <li> <p>Branch <code>release/x.y.z</code> from <code>develop</code>.</p> </li> <li>Bump version, update CHANGELOG, run full regression tests.</li> <li>Open PR <code>release/x.y.z</code> \u2192 <code>main</code>; merge when green.</li> <li>Tag <code>vX.Y.Z</code> on <code>main</code> (<code>git tag v1.2.0 &amp;&amp; git push origin v1.2.0</code>).</li> <li>Merge <code>main</code> back into <code>develop</code> to keep history linear.</li> </ul>"},{"location":"git-workflow/#42-hotfixing","title":"4.2 Hotfixing","text":"<p>For critical production issues that need immediate fixes:</p> <ul> <li>When to use: Only for urgent production bugs that can't wait for the next release</li> <li>Key points:</li> <li>Hotfixes bypass <code>develop</code> and go directly to <code>main</code></li> <li>Must be thoroughly tested despite urgency</li> <li>Requires careful coordination with the team</li> <li> <p>Should be small, focused changes targeting only the critical issue</p> </li> <li> <p>Branch <code>hotfix/...</code> from <code>main</code>.</p> </li> <li>Fix, test, and open PR to <code>main</code>.</li> <li>Tag <code>vX.Y.Z-hotfix</code> (or patch bump) after merge.</li> <li>Back\u2011merge <code>main</code> into <code>develop</code>.</li> </ul>"},{"location":"git-workflow/#5-code-review-checklist","title":"5. Code Review Checklist","text":"<p>Use this quick checklist before approving a PR.</p> <ul> <li> Code builds &amp; CI is green.</li> <li> No obvious security, performance, or accessibility issues.</li> <li> Docs, API spec, and configs updated.</li> <li> Commit history is clean (squash/rebase as needed).</li> </ul>"},{"location":"git-workflow/#6-housekeeping-commands","title":"6. House\u2011Keeping Commands","text":"Task Command Delete local branch <code>git branch -d &lt;branch&gt;</code> Delete remote branch <code>git push origin --delete &lt;branch&gt;</code> Sync <code>develop</code> <code>git pull --rebase origin develop</code> Inspect history <code>git log --oneline --graph --decorate --all</code>"},{"location":"git-workflow/#7-additional-tips","title":"7. Additional Tips","text":"<ul> <li>Keep PRs small (&lt;\u202f1000\u202fLOC) to speed up review.</li> <li>Use Draft PRs to gather feedback early.</li> </ul>"},{"location":"git-workflow/#8-resources","title":"8. Resources","text":"<ul> <li>Conventional Commits</li> <li>Git Branching Model</li> </ul>"},{"location":"installation/","title":"Installation","text":"<p>Welcome to the NEBULA platform installation guide. This document explains how to obtain, install, run, and troubleshoot NEBULA.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<p>For the best experience, ensure the following prerequisites are met:</p> <ul> <li>Linux (Ubuntu 20.04 LTS recommended) or macOS (10.15 Catalina or later). Currently, we do not maintain an up-to-date version for Windows.</li> <li>Minimum 8 GB RAM (+32 GB recommended for virtualized devices).</li> <li>Minimum 20 GB disk space for Docker images and containers. Additional space is required for datasets, models, and results.</li> <li>Docker Engine 24.0.4 or higher (24.0.7 recommended, https://docs.docker.com/engine/install/)</li> <li>Docker Compose 2.19.0 or higher (2.19.1 recommended, https://docs.docker.com/compose/install/)</li> </ul>"},{"location":"installation/#obtaining-nebula","title":"Obtaining NEBULA","text":"<p>You can obtain the source code from https://github.com/CyberDataLab/nebula</p> <p>Or clone the repository using git:</p> <pre><code>user@host:~$ git clone https://github.com/CyberDataLab/nebula.git</code></pre> <p>Now, you can move to the source directory:</p> <pre><code>user@host:~$ cd nebula</code></pre>"},{"location":"installation/#installing-nebula","title":"Installing NEBULA","text":"<p>Install required dependencies and set up Docker containers by running:</p> <pre><code>user@host:~$ make install</code></pre> <p>Next, activate the virtual environment:</p> <pre><code>user@host:~$ source .venv/bin/activate</code></pre> <p>If you forget this command, you can type:</p> <pre><code>user@host:~$ make shell</code></pre> <p>Your shell prompt should look similar to:</p> <pre><code>(nebula-dfl) user@host:~$</code></pre>"},{"location":"installation/#using-nvidia-gpu-on-nodes-optional","title":"Using NVIDIA GPU on Nodes (Optional)","text":"<p>For nodes equipped with NVIDIA GPUs, ensure the following prerequisites:</p> <ul> <li>NVIDIA Driver: Version 525.60.13 or later.</li> <li>CUDA: Version 12.1 is required. After installation, verify with <code>nvidia-smi</code>.</li> <li>NVIDIA Container Toolkit: Install to enable GPU access within Docker containers.</li> </ul> <p>Follow these guides for proper installation:</p> <ul> <li>CUDA Installation Guide for Linux</li> <li>NVIDIA Container Toolkit Installation Guide</li> </ul> <p>Note: Ensure that the CUDA toolkit version is compatible with your driver and, if needed, update the Docker runtime to support GPU integration.</p>"},{"location":"installation/#running-nebula","title":"Running NEBULA","text":"<p>Once the installation is finished, you can check if NEBULA is installed properly using:</p> <pre><code>(nebula-dfl) user@host:~$ python app/main.py --version</code></pre> <p>To run NEBULA, you can use the following command line:</p> <pre><code>(nebula-dfl) user@host:~$ python app/main.py</code></pre> <p>Note: The first run may build the nebula-frontend Docker image, which can take a few minutes.</p> <p>Display available parameters:</p> <pre><code>(nebula-dfl) user@host:~$ python app/main.py --help</code></pre> <p>By default, the frontend is available at http://127.0.0.1:6060. If the 6060 port is unavailable, a random port will be assigned automatically and prompted in the console.</p> <p>Also, you can define the specific port using the following command line:</p> <pre><code>(nebula-dfl) user@host:~$ python app/main.py --webport [PORT]</code></pre> <p>and the default port of the statistics endpoint:</p> <pre><code>(nebula-dfl) user@host:~$ python app/main.py --statsport [PORT]</code></pre>"},{"location":"installation/#nebula-frontend-credentials","title":"NEBULA Frontend Credentials","text":"<p>You can log in with the default credentials:</p> <pre><code>- User: admin\n- Password: admin\n</code></pre> <p>If these do not work, please contact Enrique Tom\u00e1s Mart\u00ednez Beltr\u00e1n at enriquetomas@um.es.</p>"},{"location":"installation/#stopping-nebula","title":"Stopping NEBULA","text":"<p>To stop NEBULA, you can use the following command line:</p> <pre><code>(nebula-dfl) user@host:~$ python app/main.py --stop</code></pre> <p>Be careful! This command will stop all the containers related to NEBULA: Frontend, Controller, and Nodes.</p>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":"<p>If frontend is not working, check the logs in app/logs/frontend.log</p> <p>If any of the following errors appear, take a look at the docker logs of the nebula-frontend container:</p> <pre><code>user@host:~$ docker logs user_nebula-frontend</code></pre> <p>Network nebula_X Error failed to create network nebula_X: Error response from daemon: Pool overlaps with other one on this address space</p> <p>Solution: Delete the docker network nebula_X</p> <pre><code>user@host:~$ docker network rm nebula_X</code></pre> <p>Error: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?</p> <p>Solution: Start the docker daemon</p> <pre><code>user@host:~$ sudo dockerdX</code></pre> <p>Solution: Enable the following option in Docker Desktop</p> <p>Settings -&gt; Advanced -&gt; Allow the default Docker socket to be used</p> <p></p> <p>Error: Cannot connect to the Docker daemon at tcp://X.X.X.X:2375. Is the docker daemon running?</p> <p>Solution: Start the docker daemon</p> <pre><code>user@host:~$ sudo dockerd -H tcp://X.X.X.X:2375</code></pre> <p>If frontend is not working, restart docker daemon</p> <pre><code>user@host:~$ sudo systemctl restart docker</code></pre> <p>Error: Too many open files</p> <p>Solution: Increase the number of open files</p> <p>ulimit -n 65536</p> <p>Also, you can add the following lines to the file /etc/security/limits.conf</p> <ul> <li>soft nofile 65536</li> <li>hard nofile 65536</li> </ul>"},{"location":"userguide/","title":"User Guide","text":"<p>In this section, we will explain how to use the NEBULA Platform</p>"},{"location":"userguide/#running-nebula","title":"Running NEBULA","text":"<p>To run NEBULA, you can use the following command line:</p> <pre><code>(nebula-dfl)user@host:~$ python app/main.py [PARAMS]</code></pre> <p>The first time you run the platform, the nebula-frontend docker image will be built. This process can take a few minutes.</p> <p>You can show the PARAMS using:</p> <pre><code>(nebula-dfl)user@host:~$ python app/main.py --help</code></pre> <p>The frontend will be available at http://127.0.0.1:6060 (by default) if the port is available</p> <p>To change the default port of the frontend, you can use the following command line:</p> <pre><code>(nebula-dfl)user@host:~$ python app/main.py --webport [PORT]</code></pre> <p>To change the default port of the statistics endpoint, you can use the following command line:</p> <pre><code>(nebula-dfl)user@host:~$ python app/main.py --statsport [PORT]</code></pre>"},{"location":"userguide/#nebula-frontend","title":"NEBULA Frontend","text":""},{"location":"userguide/#top-navigation-bar-buttons","title":"Top Navigation Bar Buttons","text":"<ul> <li>HOME \u2013 Returns to the main landing page of NEBULA.</li> <li>SOURCE CODE \u2013 Redirects to the NEBULA's repository.</li> <li>DOCUMENTATION \u2013 Provides access to NEBULA's official documentation, including installation, usage, and API references.</li> <li>WEBSITE \u2013 Redirects to NEBULA's official website.</li> <li>ACCOUNT \u2013 Contains user-related options such as login, logout, and account settings.</li> <li>DASHBOARD \u2013 Takes users to the main dashboard where they can interact with NEBULA's features.</li> </ul>"},{"location":"userguide/#feedback-section","title":"Feedback Section","text":"<ul> <li>Send Feedback: You can send us feedback through this button, sharing how you use NEBULA and helping improve the platform through community engagement.</li> </ul>"},{"location":"userguide/#deployment-of-scenarios","title":"Deployment of Scenarios","text":"<p>Here you can define the different parameters used to deploy the federation of nodes</p>"},{"location":"userguide/#nodes-graph","title":"Nodes graph","text":"<p>It represents the topology of the nodes. In this graph, nodes can be selected and modified based on their role. The available options include:</p> <p>Modify node type, specific nodes can be selected and assigned roles such as:</p> <ul> <li>Malicious nodes</li> <li>Aggregator nodes</li> </ul> <p>Modify topology, the connections between nodes can be altered to change the structure of the network. The actions available are:</p> <ul> <li>Add links: New connections can be created between selected nodes by right-clicking.</li> <li>Remove links: Existing connections between nodes can be deleted by clicking on the link.</li> </ul>"},{"location":"userguide/#buttons-and-actions","title":"Buttons and Actions","text":"<p>Add Scenario</p> <p>Allows users to create a new scenario.</p> <p>Functionality:</p> <ul> <li>Opens a form to configure a new scenario.</li> <li>Users can define settings such as title, description, and deployment type.</li> </ul> <p>Advanced Mode</p> <p>Expands additional configuration options for nodes (from section 8 onwards).</p> <p>Functionality:   When enabled, displays advanced settings:</p> <ul> <li>Participants</li> <li>Advanced Topology</li> <li>Advanced Training</li> <li>Robustness</li> <li>Defense</li> <li>Mobility</li> </ul> <p>Generate Scenario List</p> <p>Displays a list of all created scenarios.</p> <p>Functionality:</p> <ul> <li>Retrieves and shows existing scenarios.</li> <li>Allows users to select or modify a scenario.</li> </ul> <p>Load and Save Configuration (JSON) Provides options to import/export scenario configurations using JSON.</p> <p>Load JSON:</p> <ul> <li>Allows users to upload a JSON file to load scenario configurations.</li> </ul> <p>Download Configuration:</p> <ul> <li>Exports the current scenario settings into a JSON file.</li> </ul> <p>Run Federation Starts the federation process with the selected configuration.</p>"},{"location":"userguide/#scenario-configuration-steps","title":"Scenario configuration steps","text":"<p>1. Scenario Information Define general information about the scenario.</p> <ul> <li>Scenario title: Set a name for the scenario.</li> <li>Scenario description: Provide a description for the scenario.</li> </ul> <p>2. Deployment Select how the scenario will be deployed.</p> <ul> <li>Processes</li> <li>Docker containers</li> <li>Physical devices</li> </ul> <p>3. Federation Architecture Configure the architecture of the federation.</p> <ul> <li>Federation approach: Select the approach (e.g., DFL).</li> <li>Number of rounds: Define the number of training rounds.</li> </ul> <p>4. Network Topology Set up the network topology for participants.</p> <ul> <li> <p>Topology generation:</p> <ul> <li>Custom topology</li> <li>Predefined topology (e.g., Fully)</li> </ul> </li> <li> <p>Number of nodes: Specify the number of participants.</p> </li> </ul> <p>5. Dataset Configure dataset-related settings.</p> <ul> <li>Federated dataset: Select the dataset (e.g., MNIST).</li> <li>Dataset type: Choose between IID or Non-IID distribution.</li> <li>Partition methods: Set the partition strategy (e.g., Dirichlet).</li> <li>Parameter setting: Adjust parameters for data partitioning.</li> </ul> <p>6. Training Define the training model.</p> <ul> <li>Model: Choose the model architecture (e.g., MLP).</li> </ul> <p>7. Aggregation Configure the aggregation method.</p> <ul> <li>Aggregation algorithm: Select an aggregation algorithm (e.g., FedAvg).</li> </ul> <p>8. Participants Configure the participants involved in the federation.</p> <ul> <li>Logging: Choose the type of logs to be recorded (e.g., alerts and logs).</li> <li>Reporting: Enable or disable reporting for the participants.</li> <li>Individual participants: View details or start specific participants manually.</li> </ul> <p>9. Advanced Topology Define the spatial distribution of participants.</p> <ul> <li>Distance between participants: Adjust the distance between nodes in the topology.</li> </ul> <p>10. Advanced Training Set additional training parameters.</p> <ul> <li>Number of Epochs: Define the number of training epochs for each participant.</li> </ul> <p>11. Robustness Configure the robustness of the federation by specifying potential attacks.</p> <ul> <li>Attack Type: Choose from different types of attacks or select \"No Attack\" for a standard setup.</li> </ul> <p>12. Defense Enable or disable security mechanisms for the federation.</p> <ul> <li>Reputation System: Choose whether to enable or disable reputation-based security.</li> </ul> <p>13. Mobility Manage the mobility settings for participants.</p> <ul> <li>Default location selection: Set participant locations as random or custom.</li> <li>Mobility configuration: Enable or disable participant mobility.</li> </ul>"},{"location":"userguide/#dashboard","title":"Dashboard","text":"<p>The NEBULA Dashboard provides an overview of the current federation scenarios and allows users to manage and monitor them effectively. Below is an explanation of the key components and buttons visible on the dashboard.</p> <p>Current Scenario Status</p> <ul> <li>Scenario name: Displays the name of the currently running scenario.</li> <li>Scenario title and description: Shows the title and description provided during the scenario creation.</li> <li>Scenario start time: Indicates when the scenario was initiated.</li> </ul> <p>Buttons</p> <ul> <li>Deploy new scenario:   Use this button to create and deploy a new federation scenario. It redirects you to the scenario configuration interface.</li> <li>Compare scenarios:   Allows you to compare the results of completed scenarios. Useful for analyzing performance differences.</li> </ul> <p>Scenarios in the Database This section provides a table summarizing all scenarios in the database.</p> <p>Columns</p> <ul> <li>User: Shows the user who created the scenario.</li> <li>Title: Displays the scenario title.</li> <li>Start time: Indicates when the scenario was started.</li> <li>Model: The model being used for training (e.g., MLP).</li> <li>Dataset: The dataset used in the scenario (e.g., MNIST).</li> <li>Rounds: The number of training rounds configured.</li> <li>Status: Indicates whether the scenario is running, stopped, or completed.</li> </ul> <p>Buttons in the \"Action\" Column</p> <ul> <li>Monitor:   Opens the monitoring interface for the selected scenario, showing metrics like accuracy and loss over time.</li> <li>Real-time metrics:   Displays live updates of training metrics while the scenario is running.</li> <li>Save Note:   Allows you to save custom notes or observations related to the scenario for future reference.</li> <li>Scenario Config:   Opens the configuration details of the selected scenario, allowing you to review the parameters used during its creation.</li> <li>Stop scenario:   Immediately halts the execution of the selected scenario.</li> </ul>"},{"location":"userguide/#monitor","title":"Monitor","text":"<p>Scenario Information This section provides a summary of the scenario's metadata and controls for managing it.</p> <ul> <li>Title: Displays the name of the scenario.</li> <li>Description: A brief description of the scenario's purpose or configuration.</li> <li>Status: Indicates whether the scenario is Running, Completed or Stopped.</li> </ul> <p>Buttons</p> <ul> <li>Stop Scenario: Halts the execution of the currently running scenario.</li> <li>Real-time Metrics: Redirects you to the real-time metrics dashboard to monitor the scenario's performance.</li> <li>Download Logs/Metrics: Downloads logs and metrics data for further analysis.</li> <li>Download Metrics: Allows you to save metric data separately in your local environment.</li> </ul> <p>Nodes in the Database This table summarizes all nodes participating in the scenario.</p> <p>Columns</p> <ul> <li>UID: A unique identifier for each node.</li> <li>IDX: The index of the node in the scenario.</li> <li>IP: The IP address of the node.</li> <li>Role: Indicates the role of the node (e.g., aggregator).</li> <li>Round: Specifies the current round of operation for the node.</li> <li>Status: Displays whether the node is Online or Offline.</li> <li>Logs: A button to access detailed logs for each node.</li> </ul> <p>Map A real-time visualization of the nodes and their interactions displayed on a map.</p> <ul> <li>Purpose: Provides a geographical representation of node distributions and connections.</li> <li>Features:</li> <li>Interactive map with zoom and pan functionality.</li> <li>Visualizes active nodes and their interactions with connecting lines.</li> </ul> <p>Topology This section illustrates the network topology of the scenario.</p> <ul> <li>Purpose: Shows the relationships and interactions between nodes in a structured topology.</li> <li>Download: You can click the \"here\" link to download a detailed representation of the topology.</li> </ul>"},{"location":"userguide/#realtime-metrics","title":"Realtime Metrics","text":"<p>Displays graphs of metrics recorded during the execution.   Each graph is interactive, enabling comparison of metrics across participants or scenarios.</p> <ul> <li> <p>Main options:</p> <ul> <li>Filters: Use regex to select the runs to display.</li> <li>Axes: Configure the horizontal axis (step, relative, wall).</li> <li>Smoothing: Adjust the curve using a slider.</li> </ul> </li> <li> <p>Example metrics:</p> <ul> <li>Test (Global)/Accuracy</li> <li>Test (Global)/F1Score</li> </ul> </li> </ul> <p></p> <p>Similar to the Time Series tab but focused on global scalar values, such as accuracy and loss over iterations.</p> <p>Images (Confusion Matrix): </p> <p>Displays the confusion matrix generated during the execution.   Used to evaluate the classification performance of the models.   Each matrix is specific to a participant or global metrics.</p> <p>Configuration Options:</p> <ul> <li>Smoothing: Adjusts the visualization of the curves.</li> <li>Ignore outliers: Excludes outlier values in the graphs for a clearer representation.</li> <li>Card size: Modifies the size of the graphs.</li> </ul>"},{"location":"api/","title":"Documentation for Nebula Module","text":"<p>This API Reference is designed to help developers understand every part of the code, providing detailed information about functions, parameters, data structures, and interactions within the platform.</p> <p>On the left, you'll find the directory tree of the platform, including folders, functions, code, and documentation.</p>"},{"location":"api/SUMMARY/","title":"SUMMARY","text":"<ul> <li>nebula<ul> <li>addons<ul> <li>attacks<ul> <li>attacks</li> <li>communications<ul> <li>communicationattack</li> <li>delayerattack</li> <li>floodingattack</li> </ul> </li> <li>dataset<ul> <li>datapoison</li> <li>datasetattack</li> <li>labelflipping</li> </ul> </li> <li>model<ul> <li>gllneuroninversion</li> <li>modelattack</li> <li>modelpoison</li> <li>swappingweights</li> </ul> </li> </ul> </li> <li>env</li> <li>functions</li> <li>gps<ul> <li>gpsmodule</li> <li>nebulagps</li> </ul> </li> <li>mobility</li> <li>networksimulation<ul> <li>nebulanetworksimulator</li> <li>networksimulator</li> </ul> </li> <li>reporter</li> <li>reputation<ul> <li>reputation</li> </ul> </li> <li>topologymanager</li> <li>trustworthiness<ul> <li>calculation</li> <li>factsheet</li> <li>graphics</li> <li>metric</li> <li>pillar</li> <li>trustworthiness</li> <li>utils</li> </ul> </li> <li>waf</li> </ul> </li> <li>controller<ul> <li>controller</li> <li>database</li> <li>http_helpers</li> <li>scenarios</li> </ul> </li> <li>core<ul> <li>addonmanager</li> <li>aggregation<ul> <li>aggregator</li> <li>fedavg</li> <li>krum</li> <li>median</li> <li>trimmedmean</li> <li>updatehandlers<ul> <li>cflupdatehandler</li> <li>dflupdatehandler</li> <li>sdflupdatehandler</li> <li>updatehandler</li> </ul> </li> </ul> </li> <li>datasets<ul> <li>changeablesubset</li> <li>cifar10<ul> <li>cifar10</li> </ul> </li> <li>cifar100<ul> <li>cifar100</li> </ul> </li> <li>datamodule</li> <li>emnist<ul> <li>emnist</li> </ul> </li> <li>fashionmnist<ul> <li>fashionmnist</li> </ul> </li> <li>mnist<ul> <li>mnist</li> </ul> </li> <li>nebuladataset</li> </ul> </li> <li>engine</li> <li>eventmanager</li> <li>models<ul> <li>cifar10<ul> <li>cnn</li> <li>cnnV2</li> <li>cnnV3</li> <li>fastermobilenet</li> <li>resnet</li> <li>simplemobilenet</li> </ul> </li> <li>cifar100<ul> <li>cnn</li> </ul> </li> <li>emnist<ul> <li>cnn</li> <li>mlp</li> </ul> </li> <li>fashionmnist<ul> <li>cnn</li> <li>mlp</li> </ul> </li> <li>mnist<ul> <li>cnn</li> <li>mlp</li> </ul> </li> <li>nebulamodel</li> <li>sentiment140<ul> <li>cnn</li> <li>rnn</li> </ul> </li> </ul> </li> <li>nebulaevents</li> <li>network<ul> <li>actions</li> <li>blacklist</li> <li>communications</li> <li>connection</li> <li>discoverer</li> <li>externalconnection<ul> <li>externalconnectionservice</li> <li>nebuladiscoveryservice</li> </ul> </li> <li>forwarder</li> <li>health</li> <li>messages</li> <li>propagator</li> </ul> </li> <li>node</li> <li>noderole</li> <li>pb<ul> <li>nebula_pb2</li> </ul> </li> <li>role</li> <li>situationalawareness<ul> <li>awareness<ul> <li>arbitrationpolicies<ul> <li>arbitrationpolicy</li> <li>staticarbitrationpolicy</li> </ul> </li> <li>sanetwork<ul> <li>neighborpolicies<ul> <li>distanceneighborpolicy</li> <li>fcneighborpolicy</li> <li>idleneighborpolicy</li> <li>neighborpolicy</li> <li>ringneighborpolicy</li> <li>starneighborpolicy</li> </ul> </li> <li>sanetwork</li> </ul> </li> <li>sareasoner</li> <li>satraining<ul> <li>satraining</li> <li>trainingpolicy<ul> <li>bpstrainingpolicy</li> <li>fastreboot</li> <li>htstrainingpolicy</li> <li>qdstrainingpolicy</li> <li>trainingpolicy</li> </ul> </li> </ul> </li> <li>sautils<ul> <li>sacommand</li> <li>samoduleagent</li> <li>sasystemmonitor</li> </ul> </li> <li>suggestionbuffer</li> </ul> </li> <li>discovery<ul> <li>candidateselection<ul> <li>candidateselector</li> <li>distcandidateselector</li> <li>fccandidateselector</li> <li>ringcandidateselector</li> <li>stdcandidateselector</li> </ul> </li> <li>federationconnector</li> <li>modelhandlers<ul> <li>aggmodelhandler</li> <li>defaultmodelhandler</li> <li>modelhandler</li> <li>stdmodelhandler</li> </ul> </li> </ul> </li> <li>situationalawareness</li> </ul> </li> <li>training<ul> <li>lightning</li> <li>scikit</li> <li>siamese</li> </ul> </li> </ul> </li> <li>frontend<ul> <li>app</li> </ul> </li> <li>physical<ul> <li>api</li> </ul> </li> <li>utils</li> </ul> </li> </ul>"},{"location":"api/utils/","title":"Documentation for Utils Module","text":""},{"location":"api/utils/#nebula.utils.DockerUtils","title":"<code>DockerUtils</code>","text":"<p>Utility class for Docker operations such as creating networks, checking containers, and removing networks or containers by name prefix.</p> Source code in <code>nebula/utils.py</code> <pre><code>class DockerUtils:\n    \"\"\"\n    Utility class for Docker operations such as creating networks,\n    checking containers, and removing networks or containers by name prefix.\n    \"\"\"\n\n    @classmethod\n    def create_docker_network(cls, network_name, subnet=None, prefix=24):\n        \"\"\"\n        Creates a Docker bridge network with a given name and optional subnet.\n        If subnet is None or already in use, it finds an available subnet in\n        the 192.168.X.0/24 range starting from 192.168.50.0/24.\n\n        Args:\n            network_name (str): Name of the Docker network to create.\n            subnet (str, optional): Subnet in CIDR notation (e.g. \"192.168.50.0/24\").\n            prefix (int, optional): Network prefix length, default is 24.\n\n        Returns:\n            str or None: The base subnet (e.g. \"192.168.50\") of the created or existing\n                         network, or None if an error occurred.\n        \"\"\"\n        try:\n            # Connect to Docker\n            client = docker.from_env()\n            base_subnet = \"192.168\"\n\n            # Obtain existing docker subnets\n            existing_subnets = []\n            networks = client.networks.list()\n\n            existing_network = next((n for n in networks if n.name == network_name), None)\n\n            if existing_network:\n                ipam_config = existing_network.attrs.get(\"IPAM\", {}).get(\"Config\", [])\n                if ipam_config:\n                    # Assume there's only one subnet per network for simplicity\n                    existing_subnet = ipam_config[0].get(\"Subnet\", \"\")\n                    potential_base = \".\".join(existing_subnet.split(\".\")[:3])  # Extract base from subnet\n                    logging.info(f\"Network '{network_name}' already exists with base {potential_base}\")\n                    return potential_base\n\n            for network in networks:\n                ipam_config = network.attrs.get(\"IPAM\", {}).get(\"Config\", [])\n                if ipam_config:\n                    for config in ipam_config:\n                        if \"Subnet\" in config:\n                            existing_subnets.append(config[\"Subnet\"])\n\n            # If no subnet is provided or it exists, find the next available one\n            if not subnet or subnet in existing_subnets:\n                for i in range(50, 255):  # Iterate over 192.168.50.0 to 192.168.254.0\n                    subnet = f\"{base_subnet}.{i}.0/{prefix}\"\n                    potential_base = f\"{base_subnet}.{i}\"\n                    if subnet not in existing_subnets:\n                        break\n                else:\n                    raise ValueError(\"No available subnets found.\")\n\n            # Create the Docker network\n            gateway = f\"{subnet.split('/')[0].rsplit('.', 1)[0]}.1\"\n            ipam_pool = docker.types.IPAMPool(subnet=subnet, gateway=gateway)\n            ipam_config = docker.types.IPAMConfig(pool_configs=[ipam_pool])\n            network = client.networks.create(name=network_name, driver=\"bridge\", ipam=ipam_config)\n\n            logging.info(f\"Network created: {network.name} with subnet {subnet}\")\n            return potential_base\n\n        except docker.errors.APIError:\n            logging.exception(\"Error interacting with Docker\")\n            return None\n        except Exception:\n            logging.exception(\"Unexpected error\")\n            return None\n        finally:\n            client.close()  # Ensure the Docker client is closed\n\n    @classmethod\n    def check_docker_by_prefix(cls, prefix):\n        \"\"\"\n        Checks if there are any Docker containers whose names start with the given prefix.\n\n        Args:\n            prefix (str): Prefix string to match container names.\n\n        Returns:\n            bool: True if any container matches the prefix, False otherwise.\n        \"\"\"\n        try:\n            # Connect to Docker client\n            client = docker.from_env()\n\n            containers = client.containers.list(all=True)  # `all=True` to include stopped containers\n\n            # Iterate through containers and remove those with the matching prefix\n            for container in containers:\n                if container.name.startswith(prefix):\n                    return True\n\n            return False\n\n        except docker.errors.APIError:\n            logging.exception(\"Error interacting with Docker\")\n        except Exception:\n            logging.exception(\"Unexpected error\")\n\n    @classmethod\n    def remove_docker_network(cls, network_name):\n        \"\"\"\n        Removes a Docker network by name.\n\n        Args:\n            network_name (str): Name of the Docker network to remove.\n\n        Returns:\n            None\n        \"\"\"\n        try:\n            # Connect to Docker\n            client = docker.from_env()\n\n            # Get the network by name\n            network = client.networks.get(network_name)\n\n            # Remove the network\n            network.remove()\n\n            logging.info(f\"Network {network_name} removed successfully.\")\n        except docker.errors.NotFound:\n            logging.exception(f\"Network {network_name} not found.\")\n        except docker.errors.APIError:\n            logging.exception(\"Error interacting with Docker\")\n        except Exception:\n            logging.exception(\"Unexpected error\")\n\n    @classmethod\n    def remove_docker_networks_by_prefix(cls, prefix):\n        \"\"\"\n        Removes all Docker networks whose names start with the given prefix.\n\n        Args:\n            prefix (str): Prefix string to match network names.\n\n        Returns:\n            None\n        \"\"\"\n        try:\n            # Connect to Docker\n            client = docker.from_env()\n\n            # List all networks\n            networks = client.networks.list()\n\n            # Filter and remove networks with names starting with the prefix\n            for network in networks:\n                if network.name.startswith(prefix):\n                    network.remove()\n                    logging.info(f\"Network {network.name} removed successfully.\")\n\n        except docker.errors.NotFound:\n            logging.info(f\"One or more networks with prefix {prefix} not found.\")\n        except docker.errors.APIError:\n            logging.info(\"Error interacting with Docker\")\n        except Exception:\n            logging.info(\"Unexpected error\")\n\n    @classmethod\n    def remove_containers_by_prefix(cls, prefix):\n        \"\"\"\n        Removes all Docker containers whose names start with the given prefix.\n        Containers are forcibly removed even if they are running.\n\n        Args:\n            prefix (str): Prefix string to match container names.\n\n        Returns:\n            None\n        \"\"\"\n        try:\n            # Connect to Docker client\n            client = docker.from_env()\n\n            containers = client.containers.list(all=True)  # `all=True` to include stopped containers\n\n            # Iterate through containers and remove those with the matching prefix\n            for container in containers:\n                if container.name.startswith(prefix):\n                    logging.info(f\"Removing container: {container.name}\")\n                    container.remove(force=True)  # force=True to stop and remove if running\n                    logging.info(f\"Container {container.name} removed successfully.\")\n\n        except docker.errors.APIError:\n            logging.exception(\"Error interacting with Docker\")\n        except Exception:\n            logging.exception(\"Unexpected error\")\n</code></pre>"},{"location":"api/utils/#nebula.utils.DockerUtils.check_docker_by_prefix","title":"<code>check_docker_by_prefix(prefix)</code>  <code>classmethod</code>","text":"<p>Checks if there are any Docker containers whose names start with the given prefix.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>Prefix string to match container names.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if any container matches the prefix, False otherwise.</p> Source code in <code>nebula/utils.py</code> <pre><code>@classmethod\ndef check_docker_by_prefix(cls, prefix):\n    \"\"\"\n    Checks if there are any Docker containers whose names start with the given prefix.\n\n    Args:\n        prefix (str): Prefix string to match container names.\n\n    Returns:\n        bool: True if any container matches the prefix, False otherwise.\n    \"\"\"\n    try:\n        # Connect to Docker client\n        client = docker.from_env()\n\n        containers = client.containers.list(all=True)  # `all=True` to include stopped containers\n\n        # Iterate through containers and remove those with the matching prefix\n        for container in containers:\n            if container.name.startswith(prefix):\n                return True\n\n        return False\n\n    except docker.errors.APIError:\n        logging.exception(\"Error interacting with Docker\")\n    except Exception:\n        logging.exception(\"Unexpected error\")\n</code></pre>"},{"location":"api/utils/#nebula.utils.DockerUtils.create_docker_network","title":"<code>create_docker_network(network_name, subnet=None, prefix=24)</code>  <code>classmethod</code>","text":"<p>Creates a Docker bridge network with a given name and optional subnet. If subnet is None or already in use, it finds an available subnet in the 192.168.X.0/24 range starting from 192.168.50.0/24.</p> <p>Parameters:</p> Name Type Description Default <code>network_name</code> <code>str</code> <p>Name of the Docker network to create.</p> required <code>subnet</code> <code>str</code> <p>Subnet in CIDR notation (e.g. \"192.168.50.0/24\").</p> <code>None</code> <code>prefix</code> <code>int</code> <p>Network prefix length, default is 24.</p> <code>24</code> <p>Returns:</p> Type Description <p>str or None: The base subnet (e.g. \"192.168.50\") of the created or existing          network, or None if an error occurred.</p> Source code in <code>nebula/utils.py</code> <pre><code>@classmethod\ndef create_docker_network(cls, network_name, subnet=None, prefix=24):\n    \"\"\"\n    Creates a Docker bridge network with a given name and optional subnet.\n    If subnet is None or already in use, it finds an available subnet in\n    the 192.168.X.0/24 range starting from 192.168.50.0/24.\n\n    Args:\n        network_name (str): Name of the Docker network to create.\n        subnet (str, optional): Subnet in CIDR notation (e.g. \"192.168.50.0/24\").\n        prefix (int, optional): Network prefix length, default is 24.\n\n    Returns:\n        str or None: The base subnet (e.g. \"192.168.50\") of the created or existing\n                     network, or None if an error occurred.\n    \"\"\"\n    try:\n        # Connect to Docker\n        client = docker.from_env()\n        base_subnet = \"192.168\"\n\n        # Obtain existing docker subnets\n        existing_subnets = []\n        networks = client.networks.list()\n\n        existing_network = next((n for n in networks if n.name == network_name), None)\n\n        if existing_network:\n            ipam_config = existing_network.attrs.get(\"IPAM\", {}).get(\"Config\", [])\n            if ipam_config:\n                # Assume there's only one subnet per network for simplicity\n                existing_subnet = ipam_config[0].get(\"Subnet\", \"\")\n                potential_base = \".\".join(existing_subnet.split(\".\")[:3])  # Extract base from subnet\n                logging.info(f\"Network '{network_name}' already exists with base {potential_base}\")\n                return potential_base\n\n        for network in networks:\n            ipam_config = network.attrs.get(\"IPAM\", {}).get(\"Config\", [])\n            if ipam_config:\n                for config in ipam_config:\n                    if \"Subnet\" in config:\n                        existing_subnets.append(config[\"Subnet\"])\n\n        # If no subnet is provided or it exists, find the next available one\n        if not subnet or subnet in existing_subnets:\n            for i in range(50, 255):  # Iterate over 192.168.50.0 to 192.168.254.0\n                subnet = f\"{base_subnet}.{i}.0/{prefix}\"\n                potential_base = f\"{base_subnet}.{i}\"\n                if subnet not in existing_subnets:\n                    break\n            else:\n                raise ValueError(\"No available subnets found.\")\n\n        # Create the Docker network\n        gateway = f\"{subnet.split('/')[0].rsplit('.', 1)[0]}.1\"\n        ipam_pool = docker.types.IPAMPool(subnet=subnet, gateway=gateway)\n        ipam_config = docker.types.IPAMConfig(pool_configs=[ipam_pool])\n        network = client.networks.create(name=network_name, driver=\"bridge\", ipam=ipam_config)\n\n        logging.info(f\"Network created: {network.name} with subnet {subnet}\")\n        return potential_base\n\n    except docker.errors.APIError:\n        logging.exception(\"Error interacting with Docker\")\n        return None\n    except Exception:\n        logging.exception(\"Unexpected error\")\n        return None\n    finally:\n        client.close()  # Ensure the Docker client is closed\n</code></pre>"},{"location":"api/utils/#nebula.utils.DockerUtils.remove_containers_by_prefix","title":"<code>remove_containers_by_prefix(prefix)</code>  <code>classmethod</code>","text":"<p>Removes all Docker containers whose names start with the given prefix. Containers are forcibly removed even if they are running.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>Prefix string to match container names.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>nebula/utils.py</code> <pre><code>@classmethod\ndef remove_containers_by_prefix(cls, prefix):\n    \"\"\"\n    Removes all Docker containers whose names start with the given prefix.\n    Containers are forcibly removed even if they are running.\n\n    Args:\n        prefix (str): Prefix string to match container names.\n\n    Returns:\n        None\n    \"\"\"\n    try:\n        # Connect to Docker client\n        client = docker.from_env()\n\n        containers = client.containers.list(all=True)  # `all=True` to include stopped containers\n\n        # Iterate through containers and remove those with the matching prefix\n        for container in containers:\n            if container.name.startswith(prefix):\n                logging.info(f\"Removing container: {container.name}\")\n                container.remove(force=True)  # force=True to stop and remove if running\n                logging.info(f\"Container {container.name} removed successfully.\")\n\n    except docker.errors.APIError:\n        logging.exception(\"Error interacting with Docker\")\n    except Exception:\n        logging.exception(\"Unexpected error\")\n</code></pre>"},{"location":"api/utils/#nebula.utils.DockerUtils.remove_docker_network","title":"<code>remove_docker_network(network_name)</code>  <code>classmethod</code>","text":"<p>Removes a Docker network by name.</p> <p>Parameters:</p> Name Type Description Default <code>network_name</code> <code>str</code> <p>Name of the Docker network to remove.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>nebula/utils.py</code> <pre><code>@classmethod\ndef remove_docker_network(cls, network_name):\n    \"\"\"\n    Removes a Docker network by name.\n\n    Args:\n        network_name (str): Name of the Docker network to remove.\n\n    Returns:\n        None\n    \"\"\"\n    try:\n        # Connect to Docker\n        client = docker.from_env()\n\n        # Get the network by name\n        network = client.networks.get(network_name)\n\n        # Remove the network\n        network.remove()\n\n        logging.info(f\"Network {network_name} removed successfully.\")\n    except docker.errors.NotFound:\n        logging.exception(f\"Network {network_name} not found.\")\n    except docker.errors.APIError:\n        logging.exception(\"Error interacting with Docker\")\n    except Exception:\n        logging.exception(\"Unexpected error\")\n</code></pre>"},{"location":"api/utils/#nebula.utils.DockerUtils.remove_docker_networks_by_prefix","title":"<code>remove_docker_networks_by_prefix(prefix)</code>  <code>classmethod</code>","text":"<p>Removes all Docker networks whose names start with the given prefix.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>Prefix string to match network names.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>nebula/utils.py</code> <pre><code>@classmethod\ndef remove_docker_networks_by_prefix(cls, prefix):\n    \"\"\"\n    Removes all Docker networks whose names start with the given prefix.\n\n    Args:\n        prefix (str): Prefix string to match network names.\n\n    Returns:\n        None\n    \"\"\"\n    try:\n        # Connect to Docker\n        client = docker.from_env()\n\n        # List all networks\n        networks = client.networks.list()\n\n        # Filter and remove networks with names starting with the prefix\n        for network in networks:\n            if network.name.startswith(prefix):\n                network.remove()\n                logging.info(f\"Network {network.name} removed successfully.\")\n\n    except docker.errors.NotFound:\n        logging.info(f\"One or more networks with prefix {prefix} not found.\")\n    except docker.errors.APIError:\n        logging.info(\"Error interacting with Docker\")\n    except Exception:\n        logging.info(\"Unexpected error\")\n</code></pre>"},{"location":"api/utils/#nebula.utils.FileUtils","title":"<code>FileUtils</code>","text":"<p>Utility class for file operations.</p> Source code in <code>nebula/utils.py</code> <pre><code>class FileUtils:\n    \"\"\"\n    Utility class for file operations.\n    \"\"\"\n\n    @classmethod\n    def check_path(cls, base_path, relative_path):\n        \"\"\"\n        Joins and normalizes a base path with a relative path, then validates\n        that the resulting full path is inside the base path directory.\n\n        Args:\n            base_path (str): The base directory path.\n            relative_path (str): The relative path to join with the base path.\n\n        Returns:\n            str: The normalized full path.\n\n        Raises:\n            Exception: If the resulting path is outside the base directory.\n        \"\"\"\n        full_path = os.path.normpath(os.path.join(base_path, relative_path))\n        base_path = os.path.normpath(base_path)\n\n        if not full_path.startswith(base_path):\n            raise Exception(\"Not allowed\")\n        return full_path\n</code></pre>"},{"location":"api/utils/#nebula.utils.FileUtils.check_path","title":"<code>check_path(base_path, relative_path)</code>  <code>classmethod</code>","text":"<p>Joins and normalizes a base path with a relative path, then validates that the resulting full path is inside the base path directory.</p> <p>Parameters:</p> Name Type Description Default <code>base_path</code> <code>str</code> <p>The base directory path.</p> required <code>relative_path</code> <code>str</code> <p>The relative path to join with the base path.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The normalized full path.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the resulting path is outside the base directory.</p> Source code in <code>nebula/utils.py</code> <pre><code>@classmethod\ndef check_path(cls, base_path, relative_path):\n    \"\"\"\n    Joins and normalizes a base path with a relative path, then validates\n    that the resulting full path is inside the base path directory.\n\n    Args:\n        base_path (str): The base directory path.\n        relative_path (str): The relative path to join with the base path.\n\n    Returns:\n        str: The normalized full path.\n\n    Raises:\n        Exception: If the resulting path is outside the base directory.\n    \"\"\"\n    full_path = os.path.normpath(os.path.join(base_path, relative_path))\n    base_path = os.path.normpath(base_path)\n\n    if not full_path.startswith(base_path):\n        raise Exception(\"Not allowed\")\n    return full_path\n</code></pre>"},{"location":"api/utils/#nebula.utils.SocketUtils","title":"<code>SocketUtils</code>","text":"<p>Utility class for socket operations.</p> Source code in <code>nebula/utils.py</code> <pre><code>class SocketUtils:\n    \"\"\"\n    Utility class for socket operations.\n    \"\"\"\n\n    @classmethod\n    def is_port_open(cls, port):\n        \"\"\"\n        Checks if a TCP port is available (not currently bound) on localhost.\n\n        Args:\n            port (int): The port number to check.\n\n        Returns:\n            bool: True if the port is free (available), False if it is in use.\n        \"\"\"\n        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        try:\n            s.bind((\"127.0.0.1\", port))\n            s.close()\n            return True\n        except OSError:\n            return False\n\n    @classmethod\n    def find_free_port(cls, start_port=49152, end_port=65535):\n        \"\"\"\n        Finds and returns the first available TCP port within the specified range.\n\n        Args:\n            start_port (int, optional): Starting port number to check. Defaults to 49152.\n            end_port (int, optional): Ending port number to check. Defaults to 65535.\n\n        Returns:\n            int or None: The first free port found, or None if no free port is found.\n        \"\"\"\n        for port in range(start_port, end_port + 1):\n            if cls.is_port_open(port):\n                return port\n        return None\n</code></pre>"},{"location":"api/utils/#nebula.utils.SocketUtils.find_free_port","title":"<code>find_free_port(start_port=49152, end_port=65535)</code>  <code>classmethod</code>","text":"<p>Finds and returns the first available TCP port within the specified range.</p> <p>Parameters:</p> Name Type Description Default <code>start_port</code> <code>int</code> <p>Starting port number to check. Defaults to 49152.</p> <code>49152</code> <code>end_port</code> <code>int</code> <p>Ending port number to check. Defaults to 65535.</p> <code>65535</code> <p>Returns:</p> Type Description <p>int or None: The first free port found, or None if no free port is found.</p> Source code in <code>nebula/utils.py</code> <pre><code>@classmethod\ndef find_free_port(cls, start_port=49152, end_port=65535):\n    \"\"\"\n    Finds and returns the first available TCP port within the specified range.\n\n    Args:\n        start_port (int, optional): Starting port number to check. Defaults to 49152.\n        end_port (int, optional): Ending port number to check. Defaults to 65535.\n\n    Returns:\n        int or None: The first free port found, or None if no free port is found.\n    \"\"\"\n    for port in range(start_port, end_port + 1):\n        if cls.is_port_open(port):\n            return port\n    return None\n</code></pre>"},{"location":"api/utils/#nebula.utils.SocketUtils.is_port_open","title":"<code>is_port_open(port)</code>  <code>classmethod</code>","text":"<p>Checks if a TCP port is available (not currently bound) on localhost.</p> <p>Parameters:</p> Name Type Description Default <code>port</code> <code>int</code> <p>The port number to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the port is free (available), False if it is in use.</p> Source code in <code>nebula/utils.py</code> <pre><code>@classmethod\ndef is_port_open(cls, port):\n    \"\"\"\n    Checks if a TCP port is available (not currently bound) on localhost.\n\n    Args:\n        port (int): The port number to check.\n\n    Returns:\n        bool: True if the port is free (available), False if it is in use.\n    \"\"\"\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    try:\n        s.bind((\"127.0.0.1\", port))\n        s.close()\n        return True\n    except OSError:\n        return False\n</code></pre>"},{"location":"api/addons/","title":"Documentation for Addons Module","text":"<p>This package consists of several modules that handle different aspects of the network simulation:</p> <ol> <li><code>env.py</code>:</li> <li>Manages the environment configuration and settings.</li> <li> <p>It initializes the system environment, loads configuration parameters, and ensures correct operation of other components based on the simulation's settings.</p> </li> <li> <p><code>functions.py</code>:</p> </li> <li>Contains utility functions that are used across different parts of the simulation.</li> <li> <p>It provides helper methods for common operations like data processing, mathematical calculations, and other reusable functionalities.</p> </li> <li> <p><code>mobility.py</code>:</p> </li> <li>Models and simulates the mobility of nodes within the network.</li> <li> <p>It handles dynamic aspects of the simulation, such as node movement and position updates, based on mobility models and the simulation's configuration.</p> </li> <li> <p><code>reporter.py</code>:</p> </li> <li>Responsible for collecting and reporting data during the simulation.</li> <li> <p>It tracks various system metrics, including node status and network performance, and periodically sends updates to a controller or dashboard for analysis and monitoring.</p> </li> <li> <p><code>topologymanager.py</code>:</p> </li> <li>Manages the topology of the network.</li> <li>It handles the creation and maintenance of the network's structure (e.g., nodes and their connections), including generating different types of topologies like ring, random, or fully connected based on simulation parameters.</li> </ol> <p>Each of these modules plays a critical role in simulating a network environment, enabling real-time tracking, topology management, mobility simulation, and efficient reporting of results.</p>"},{"location":"api/addons/env/","title":"Documentation for Env Module","text":""},{"location":"api/addons/env/#nebula.addons.env.check_environment","title":"<code>check_environment()</code>","text":"<p>Logs the current environment configuration for the NEBULA platform.</p> <p>This function gathers and logs information about the operating system, hardware, Python version, PyTorch version (if installed), CPU configuration, and GPU configuration (if applicable). It provides insights into the system's capabilities and current usage statistics.</p> <p>Returns:</p> Type Description <p>None</p> Notes <ul> <li>The function logs the NEBULA platform version using the <code>__version__</code> variable.</li> <li>It checks the system's CPU load, available memory, and detailed GPU statistics using the <code>pynvml</code>   library if running on Windows or Linux.</li> <li>If any of the libraries required for gathering information (like <code>torch</code>, <code>psutil</code>, or <code>pynvml</code>)   are not installed, appropriate log messages will be generated indicating the absence of that information.</li> <li>If any unexpected error occurs during execution, it will be logged as an exception.</li> </ul> Source code in <code>nebula/addons/env.py</code> <pre><code>def check_environment():\n    \"\"\"\n    Logs the current environment configuration for the NEBULA platform.\n\n    This function gathers and logs information about the operating system, hardware, Python version,\n    PyTorch version (if installed), CPU configuration, and GPU configuration (if applicable). It provides\n    insights into the system's capabilities and current usage statistics.\n\n    Returns:\n        None\n\n    Notes:\n        - The function logs the NEBULA platform version using the `__version__` variable.\n        - It checks the system's CPU load, available memory, and detailed GPU statistics using the `pynvml`\n          library if running on Windows or Linux.\n        - If any of the libraries required for gathering information (like `torch`, `psutil`, or `pynvml`)\n          are not installed, appropriate log messages will be generated indicating the absence of that information.\n        - If any unexpected error occurs during execution, it will be logged as an exception.\n    \"\"\"\n    logging.info(f\"NEBULA Platform version: {__version__}\")\n    # check_version()\n\n    logging.info(\"======== Running Environment ========\")\n    logging.info(\"OS: \" + platform.platform())\n    logging.info(\"Hardware: \" + platform.machine())\n    logging.info(\"Python version: \" + sys.version)\n\n    try:\n        import torch\n\n        logging.info(\"PyTorch version: \" + torch.__version__)\n    except ImportError:\n        logging.info(\"PyTorch is not installed properly\")\n    except Exception:  # noqa: S110\n        pass\n\n    logging.info(\"======== CPU Configuration ========\")\n    try:\n        import psutil\n\n        load1, load5, load15 = psutil.getloadavg()\n        cpu_usage = (load15 / os.cpu_count()) * 100\n\n        logging.info(f\"The CPU usage is : {cpu_usage:.0f}%\")\n        logging.info(\n            f\"Available CPU Memory: {psutil.virtual_memory().available / 1024 / 1024 / 1024:.1f} G / {psutil.virtual_memory().total / 1024 / 1024 / 1024}G\"\n        )\n    except ImportError:\n        logging.info(\"No CPU information available\")\n    except Exception:  # noqa: S110\n        pass\n\n    if sys.platform == \"win32\" or sys.platform == \"linux\":\n        logging.info(\"======== GPU Configuration ========\")\n        try:\n            import pynvml\n\n            pynvml.nvmlInit()\n            devices = pynvml.nvmlDeviceGetCount()\n            for i in range(devices):\n                handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n                gpu_percent = pynvml.nvmlDeviceGetUtilizationRates(handle).gpu\n                gpu_temp = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)\n                gpu_mem = pynvml.nvmlDeviceGetMemoryInfo(handle)\n                gpu_mem_percent = gpu_mem.used / gpu_mem.total * 100\n                gpu_power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0\n                gpu_clocks = pynvml.nvmlDeviceGetClockInfo(handle, pynvml.NVML_CLOCK_SM)\n                gpu_memory_clocks = pynvml.nvmlDeviceGetClockInfo(handle, pynvml.NVML_CLOCK_MEM)\n                gpu_utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)\n                gpu_fan_speed = pynvml.nvmlDeviceGetFanSpeed(handle)\n                logging.info(f\"GPU{i} percent: {gpu_percent}\")\n                logging.info(f\"GPU{i} temp: {gpu_temp}\")\n                logging.info(f\"GPU{i} mem percent: {gpu_mem_percent}\")\n                logging.info(f\"GPU{i} power: {gpu_power}\")\n                logging.info(f\"GPU{i} clocks: {gpu_clocks}\")\n                logging.info(f\"GPU{i} memory clocks: {gpu_memory_clocks}\")\n                logging.info(f\"GPU{i} utilization: {gpu_utilization.gpu}\")\n                logging.info(f\"GPU{i} fan speed: {gpu_fan_speed}\")\n        except ImportError:\n            logging.info(\"pynvml module not found, GPU information unavailable\")\n        except Exception:  # noqa: S110\n            pass\n    else:\n        logging.info(\"GPU information unavailable\")\n</code></pre>"},{"location":"api/addons/env/#nebula.addons.env.check_version","title":"<code>check_version()</code>","text":"<p>Checks the current version of NEBULA and compares it with the latest version available in the repository.</p> <p>This function retrieves the latest NEBULA version from the specified GitHub repository and compares it with the version defined in the local NEBULA package. If the versions do not match, it logs a message prompting the user to update to the latest version.</p> <p>Returns:</p> Type Description <p>None</p> <p>Raises:</p> Type Description <code>SystemExit</code> <p>If the version check fails or an exception occurs during the request.</p> Notes <ul> <li>The version information is expected to be defined in the <code>__init__.py</code> file of the NEBULA package   using the <code>__version__</code> variable.</li> <li>If the latest version is not the same as the local version, the program will exit after logging   the necessary information.</li> <li>An exception during the request will be logged, and the program will also exit.</li> </ul> Source code in <code>nebula/addons/env.py</code> <pre><code>def check_version():\n    \"\"\"\n    Checks the current version of NEBULA and compares it with the latest version available in the repository.\n\n    This function retrieves the latest NEBULA version from the specified GitHub repository and compares\n    it with the version defined in the local NEBULA package. If the versions do not match, it logs a message\n    prompting the user to update to the latest version.\n\n    Returns:\n        None\n\n    Raises:\n        SystemExit: If the version check fails or an exception occurs during the request.\n\n    Notes:\n        - The version information is expected to be defined in the `__init__.py` file of the NEBULA package\n          using the `__version__` variable.\n        - If the latest version is not the same as the local version, the program will exit after logging\n          the necessary information.\n        - An exception during the request will be logged, and the program will also exit.\n    \"\"\"\n    logging.info(\"Checking NEBULA version...\")\n    try:\n        r = requests.get(\"https://raw.githubusercontent.com/CyberDataLab/nebula/main/nebula/__init__.py\", timeout=5)\n        if r.status_code == 200:\n            version = re.search(r'^__version__\\s*=\\s*[\\'\"]([^\\'\"]*)[\\'\"]', r.text, re.MULTILINE).group(1)\n            if version != __version__:\n                logging.info(\n                    f\"Your NEBULA version is {__version__} and the latest version is {version}. Please update your NEBULA version.\"\n                )\n                logging.info(\n                    \"You can update your NEBULA version downloading the latest version from https://github.com/CyberDataLab/nebula\"\n                )\n                sys.exit(0)\n            else:\n                logging.info(f\"Your NEBULA version is {__version__} and it is the latest version.\")\n    except Exception:\n        logging.exception(\"Error while checking NEBULA version\")\n        sys.exit(0)\n</code></pre>"},{"location":"api/addons/functions/","title":"Documentation for Functions Module","text":""},{"location":"api/addons/functions/#nebula.addons.functions.print_msg_box","title":"<code>print_msg_box(msg, indent=1, width=None, title=None, logger_name=None)</code>","text":"<p>Prints a formatted message box to the logger with an optional title.</p> <p>This function creates a visually appealing message box format for logging messages. It allows for indentation, custom width, and inclusion of a title. If the message is multiline, each line will be included in the box.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>The message to be displayed inside the box. Must be a string.</p> required <code>indent</code> <code>int</code> <p>The number of spaces to indent the message box. Default is 1.</p> <code>1</code> <code>width</code> <code>int</code> <p>The width of the message box. If not provided, it will be calculated                    based on the longest line of the message and the title (if provided).</p> <code>None</code> <code>title</code> <code>str</code> <p>An optional title for the message box. Must be a string if provided.</p> <code>None</code> <code>logger_name</code> <code>str</code> <p>The name of the logger to use. If not provided, the root logger                           will be used.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>msg</code> or <code>title</code> is not a string.</p> <p>Returns:</p> Type Description <p>None</p> Notes <ul> <li>The message box is bordered with decorative characters to enhance visibility in the logs.</li> <li>If the <code>width</code> is not provided, it will automatically adjust to fit the content.</li> </ul> Source code in <code>nebula/addons/functions.py</code> <pre><code>def print_msg_box(msg, indent=1, width=None, title=None, logger_name=None):\n    \"\"\"\n    Prints a formatted message box to the logger with an optional title.\n\n    This function creates a visually appealing message box format for logging messages.\n    It allows for indentation, custom width, and inclusion of a title. If the message is\n    multiline, each line will be included in the box.\n\n    Args:\n        msg (str): The message to be displayed inside the box. Must be a string.\n        indent (int, optional): The number of spaces to indent the message box. Default is 1.\n        width (int, optional): The width of the message box. If not provided, it will be calculated\n                               based on the longest line of the message and the title (if provided).\n        title (str, optional): An optional title for the message box. Must be a string if provided.\n        logger_name (str, optional): The name of the logger to use. If not provided, the root logger\n                                      will be used.\n\n    Raises:\n        TypeError: If `msg` or `title` is not a string.\n\n    Returns:\n        None\n\n    Notes:\n        - The message box is bordered with decorative characters to enhance visibility in the logs.\n        - If the `width` is not provided, it will automatically adjust to fit the content.\n    \"\"\"\n    logger = logging.getLogger(logger_name) if logger_name else logging.getLogger()\n\n    if not isinstance(msg, str):\n        raise TypeError(\"msg parameter must be a string\")  # noqa: TRY003\n\n    lines = msg.split(\"\\n\")\n    space = \" \" * indent\n    if not width:\n        width = max(map(len, lines))\n        if title:\n            width = max(width, len(title))\n    box = f\"\\n\u2554{'\u2550' * (width + indent * 2)}\u2557\\n\"  # upper_border\n    if title:\n        if not isinstance(title, str):\n            raise TypeError(\"title parameter must be a string\")  # noqa: TRY003\n        box += f\"\u2551{space}{title:&lt;{width}}{space}\u2551\\n\"  # title\n        box += f\"\u2551{space}{'-' * len(title):&lt;{width}}{space}\u2551\\n\"  # underscore\n    box += \"\".join([f\"\u2551{space}{line:&lt;{width}}{space}\u2551\\n\" for line in lines])\n    box += f\"\u255a{'\u2550' * (width + indent * 2)}\u255d\"  # lower_border\n    logger.info(box)\n</code></pre>"},{"location":"api/addons/mobility/","title":"Documentation for Mobility Module","text":""},{"location":"api/addons/mobility/#nebula.addons.mobility.Mobility","title":"<code>Mobility</code>","text":"Source code in <code>nebula/addons/mobility.py</code> <pre><code>class Mobility:\n    def __init__(self, config, verbose=False):\n        \"\"\"\n        Initializes the mobility module with specified configuration and communication manager.\n\n        This method sets up the mobility parameters required for the module, including grace time,\n        geographical change interval, mobility type, and other network conditions based on distance.\n        It also logs the initialized settings for the mobility system.\n\n        Args:\n            config (Config): Configuration object containing mobility parameters and settings.\n            cm (CommunicationsManager): An instance of the CommunicationsManager class used for handling\n                                         communication-related tasks within the mobility module.\n\n        Attributes:\n            grace_time (float): Time allocated for mobility processes to stabilize.\n            period (float): Interval at which geographic changes are made.\n            mobility (bool): Flag indicating whether mobility is enabled.\n            mobility_type (str): Type of mobility strategy to be used (e.g., random, nearest).\n            radius_federation (float): Radius for federation in meters.\n            scheme_mobility (str): Scheme to be used for managing mobility.\n            round_frequency (int): Number of rounds after which mobility changes are applied.\n            max_distance_with_direct_connections (float): Maximum distance for direct connections in meters.\n            max_movement_random_strategy (float): Maximum movement distance for the random strategy in meters.\n            max_movement_nearest_strategy (float): Maximum movement distance for the nearest strategy in meters.\n            max_initiate_approximation (float): Maximum distance for initiating approximation calculations.\n            network_conditions (dict): A dictionary containing network conditions (bandwidth and delay)\n                                       based on distance.\n            current_network_conditions (dict): A dictionary mapping addresses to their current network conditions.\n\n        Logs:\n            Mobility information upon initialization to provide insights into the current setup.\n\n        Raises:\n            KeyError: If the expected mobility configuration keys are not found in the provided config.\n        \"\"\"\n        logging.info(\"Starting mobility module...\")\n        self.config = config\n        self._verbose = verbose\n        self._running = asyncio.Event()\n        self._nodes_distances = {}\n        self._nodes_distances_lock = Locker(\"nodes_distances_lock\", async_lock=True)\n        self._mobility_task = None  # Track the background task\n\n        # Mobility configuration\n        self.mobility = self.config.participant[\"mobility_args\"][\"mobility\"]\n        self.mobility_type = self.config.participant[\"mobility_args\"][\"mobility_type\"]\n        self.grace_time = self.config.participant[\"mobility_args\"][\"grace_time_mobility\"]\n        self.period = self.config.participant[\"mobility_args\"][\"change_geo_interval\"]\n        # INFO: These values may change according to the needs of the federation\n        self.max_distance_with_direct_connections = 150  # meters\n        self.max_movement_random_strategy = 50  # meters\n        self.max_movement_nearest_strategy = 50  # meters\n        self.max_initiate_approximation = self.max_distance_with_direct_connections * 1.2\n        self.radius_federation = float(config.participant[\"mobility_args\"][\"radius_federation\"])\n        self.scheme_mobility = config.participant[\"mobility_args\"][\"scheme_mobility\"]\n        self.round_frequency = int(config.participant[\"mobility_args\"][\"round_frequency\"])\n        # Logging box with mobility information\n        mobility_msg = f\"Mobility: {self.mobility}\\nMobility type: {self.mobility_type}\\nRadius federation: {self.radius_federation}\\nScheme mobility: {self.scheme_mobility}\\nEach {self.round_frequency} rounds\"\n        print_msg_box(msg=mobility_msg, indent=2, title=\"Mobility information\")\n\n    @cached_property\n    def cm(self):\n        return CommunicationsManager.get_instance()\n\n    # @property\n    # def round(self):\n    #     \"\"\"\n    #     Gets the current round number from the Communications Manager.\n\n    #     This property retrieves the current round number that is being managed by the\n    #     CommunicationsManager instance associated with this module. It provides an\n    #     interface to access the ongoing round of the communication process without\n    #     directly exposing the underlying method in the CommunicationsManager.\n\n    #     Returns:\n    #         int: The current round number managed by the CommunicationsManager.\n    #     \"\"\"\n    #     return self.cm.get_round()\n\n    async def start(self):\n        \"\"\"\n        Initiates the mobility process by starting the associated task.\n\n        This method creates and schedules an asynchronous task to run the\n        `run_mobility` coroutine, which handles the mobility operations\n        for the module. It allows the mobility operations to run concurrently\n        without blocking the execution of other tasks.\n\n        Returns:\n            asyncio.Task: An asyncio Task object representing the scheduled\n                           `run_mobility` operation.\n        \"\"\"\n        await EventManager.get_instance().subscribe_addonevent(GPSEvent, self.update_nodes_distances)\n        await EventManager.get_instance().subscribe_addonevent(GPSEvent, self.update_nodes_distances)\n        self._running.set()\n        self._mobility_task = asyncio.create_task(self.run_mobility(), name=\"Mobility_run_mobility\")\n        return self._mobility_task\n\n    async def stop(self):\n        \"\"\"\n        Stops the mobility module.\n        \"\"\"\n        logging.info(\"Stopping Mobility module...\")\n        self._running.clear()\n\n        # Cancel the background task\n        if self._mobility_task and not self._mobility_task.done():\n            logging.info(\"\ud83d\uded1  Cancelling Mobility background task...\")\n            self._mobility_task.cancel()\n            try:\n                await self._mobility_task\n            except asyncio.CancelledError:\n                pass\n            self._mobility_task = None\n            logging.info(\"\ud83d\uded1  Mobility background task cancelled\")\n\n    async def is_running(self):\n        return self._running.is_set()\n\n    async def update_nodes_distances(self, gpsevent: GPSEvent):\n        distances = await gpsevent.get_event_data()\n        async with self._nodes_distances_lock:\n            self._nodes_distances = dict(distances)\n\n    async def run_mobility(self):\n        \"\"\"\n        Executes the mobility operations in a continuous loop.\n\n        This coroutine manages the mobility behavior of the module. It first\n        checks whether mobility is enabled. If mobility is not enabled, the\n        function returns immediately.\n\n        If mobility is enabled, the function will wait for the specified\n        grace time before entering an infinite loop where it performs the\n        following operations:\n\n        1. Changes the geographical location by calling the `change_geo_location` method.\n        2. Adjusts connections based on the current distance by calling\n           the `change_connections_based_on_distance` method.\n        3. Sleeps for a specified period (`self.period`) before repeating the operations.\n\n        This allows for periodic updates to the module's geographical location\n        and network connections as per the defined mobility strategy.\n\n        Raises:\n            Exception: May raise exceptions if `change_geo_location` or\n                        `change_connections_based_on_distance` encounters errors.\n        \"\"\"\n        if not self.mobility:\n            return\n        # await asyncio.sleep(self.grace_time)\n        while await self.is_running():\n            await self.change_geo_location()\n            await asyncio.sleep(self.period)\n\n    async def change_geo_location_random_strategy(self, latitude, longitude):\n        \"\"\"\n        Changes the geographical location of the entity using a random strategy.\n\n        This coroutine modifies the current geographical location by randomly\n        selecting a new position within a specified radius around the given\n        latitude and longitude. The new location is determined using polar\n        coordinates, where a random distance (radius) and angle are calculated.\n\n        Args:\n            latitude (float): The current latitude of the entity.\n            longitude (float): The current longitude of the entity.\n\n        Raises:\n            Exception: May raise exceptions if the `set_geo_location` method encounters errors.\n\n        Notes:\n            - The maximum movement distance is determined by `self.max_movement_random_strategy`.\n            - The calculated radius is converted from meters to degrees based on an approximate\n              conversion factor (1 degree is approximately 111 kilometers).\n        \"\"\"\n        if self._verbose:\n            logging.info(\"\ud83d\udccd  Changing geo location randomly\")\n        # radius_in_degrees = self.radius_federation / 111000\n        max_radius_in_degrees = self.max_movement_random_strategy / 111000\n        radius = random.uniform(0, max_radius_in_degrees)  # noqa: S311\n        angle = random.uniform(0, 2 * math.pi)  # noqa: S311\n        latitude += radius * math.cos(angle)\n        longitude += radius * math.sin(angle)\n        await self.set_geo_location(latitude, longitude)\n\n    async def change_geo_location_nearest_neighbor_strategy(\n        self, distance, latitude, longitude, neighbor_latitude, neighbor_longitude\n    ):\n        \"\"\"\n        Changes the geographical location of the entity towards the nearest neighbor.\n\n        This coroutine updates the current geographical location by calculating the direction\n        and distance to the nearest neighbor's coordinates. The movement towards the neighbor\n        is scaled based on the distance and the maximum movement allowed.\n\n        Args:\n            distance (float): The distance to the nearest neighbor.\n            latitude (float): The current latitude of the entity.\n            longitude (float): The current longitude of the entity.\n            neighbor_latitude (float): The latitude of the nearest neighbor.\n            neighbor_longitude (float): The longitude of the nearest neighbor.\n\n        Raises:\n            Exception: May raise exceptions if the `set_geo_location` method encounters errors.\n\n        Notes:\n            - The movement is scaled based on the maximum allowed distance defined by\n              `self.max_movement_nearest_strategy`.\n            - The angle to the neighbor is calculated using the arctangent of the difference in\n              coordinates to determine the direction of movement.\n            - The conversion from meters to degrees is based on approximate geographical conversion factors.\n        \"\"\"\n        if self._verbose:\n            logging.info(\"\ud83d\udccd  Changing geo location towards the nearest neighbor\")\n        scale_factor = min(1, self.max_movement_nearest_strategy / distance)\n        # Calculating angle to the neighbor\n        angle = math.atan2(neighbor_longitude - longitude, neighbor_latitude - latitude)\n        # Convert maximum movement to angle\n        max_lat_change = self.max_movement_nearest_strategy / 111000  # Change degree to latitude\n        max_lon_change = self.max_movement_nearest_strategy / (\n            111000 * math.cos(math.radians(latitude))\n        )  # Change dregree for longitude\n        # Scale and direction\n        delta_lat = max_lat_change * math.cos(angle) * scale_factor\n        delta_lon = max_lon_change * math.sin(angle) * scale_factor\n        # Update values\n        new_latitude = latitude + delta_lat\n        new_longitude = longitude + delta_lon\n        await self.set_geo_location(new_latitude, new_longitude)\n\n    async def set_geo_location(self, latitude, longitude):\n        \"\"\"\n        Sets the geographical location of the entity to the specified latitude and longitude.\n\n        This coroutine updates the latitude and longitude values in the configuration. If the\n        provided coordinates are out of bounds (latitude must be between -90 and 90, and\n        longitude must be between -180 and 180), the previous location is retained.\n\n        Args:\n            latitude (float): The new latitude to set.\n            longitude (float): The new longitude to set.\n\n        Raises:\n            None: This function does not raise any exceptions but retains the previous coordinates\n                  if the new ones are invalid.\n\n        Notes:\n            - The new location is logged for tracking purposes.\n            - The coordinates are expected to be in decimal degrees format.\n        \"\"\"\n\n        if latitude &lt; -90 or latitude &gt; 90 or longitude &lt; -180 or longitude &gt; 180:\n            # If the new location is out of bounds, we keep the old location\n            latitude = self.config.participant[\"mobility_args\"][\"latitude\"]\n            longitude = self.config.participant[\"mobility_args\"][\"longitude\"]\n\n        self.config.participant[\"mobility_args\"][\"latitude\"] = latitude\n        self.config.participant[\"mobility_args\"][\"longitude\"] = longitude\n        if self._verbose:\n            logging.info(f\"\ud83d\udccd  New geo location: {latitude}, {longitude}\")\n        cle = ChangeLocationEvent(latitude, longitude)\n        asyncio.create_task(EventManager.get_instance().publish_addonevent(cle))\n\n    async def change_geo_location(self):\n        \"\"\"\n        Changes the geographical location of the entity based on the current mobility strategy.\n\n        This coroutine checks the mobility type and decides whether to move towards the nearest neighbor\n        or change the geo location randomly. It uses the communications manager to obtain the current\n        connections and their distances.\n\n        If the number of undirected connections is greater than directed connections, the method will\n        attempt to find the nearest neighbor and move towards it if the distance exceeds a certain threshold.\n        Otherwise, it will randomly change the geo location.\n\n        Args:\n            None: This function does not take any arguments.\n\n        Raises:\n            Exception: If the neighbor's location or distance cannot be found.\n\n        Notes:\n            - The method expects the mobility type to be either \"topology\" or \"both\".\n            - It logs actions taken during the execution for tracking and debugging purposes.\n        \"\"\"\n        if self.mobility and (self.mobility_type == \"topology\" or self.mobility_type == \"both\"):\n            random.seed(time.time() + self.config.participant[\"device_args\"][\"idx\"])\n            latitude = float(self.config.participant[\"mobility_args\"][\"latitude\"])\n            longitude = float(self.config.participant[\"mobility_args\"][\"longitude\"])\n            if True:\n                # Get neighbor closer to me\n                async with self._nodes_distances_lock:\n                    sorted_list = sorted(self._nodes_distances.items(), key=lambda item: item[1][0])\n                    # Transformamos la lista para obtener solo direcci\u00f3n y coordenadas\n                    result = [(addr, dist, coords) for addr, (dist, coords) in sorted_list]\n\n                selected_neighbor = result[0] if result else None\n                if selected_neighbor:\n                    # logging.info(f\"\ud83d\udccd  Selected neighbor: {selected_neighbor}\")\n                    addr, dist, (lat, long) = selected_neighbor\n                    if dist &gt; self.max_initiate_approximation:\n                        # If the distance is too big, we move towards the neighbor\n                        if self._verbose:\n                            logging.info(f\"Moving towards nearest neighbor: {addr}\")\n                        await self.change_geo_location_nearest_neighbor_strategy(\n                            dist,\n                            latitude,\n                            longitude,\n                            lat,\n                            long,\n                        )\n                    else:\n                        await self.change_geo_location_random_strategy(latitude, longitude)\n                else:\n                    await self.change_geo_location_random_strategy(latitude, longitude)\n            else:\n                await self.change_geo_location_random_strategy(latitude, longitude)\n        else:\n            logging.error(f\"\ud83d\udccd  Mobility type {self.mobility_type} not implemented\")\n            return\n</code></pre>"},{"location":"api/addons/mobility/#nebula.addons.mobility.Mobility.__init__","title":"<code>__init__(config, verbose=False)</code>","text":"<p>Initializes the mobility module with specified configuration and communication manager.</p> <p>This method sets up the mobility parameters required for the module, including grace time, geographical change interval, mobility type, and other network conditions based on distance. It also logs the initialized settings for the mobility system.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>Configuration object containing mobility parameters and settings.</p> required <code>cm</code> <code>CommunicationsManager</code> <p>An instance of the CommunicationsManager class used for handling                          communication-related tasks within the mobility module.</p> required <p>Attributes:</p> Name Type Description <code>grace_time</code> <code>float</code> <p>Time allocated for mobility processes to stabilize.</p> <code>period</code> <code>float</code> <p>Interval at which geographic changes are made.</p> <code>mobility</code> <code>bool</code> <p>Flag indicating whether mobility is enabled.</p> <code>mobility_type</code> <code>str</code> <p>Type of mobility strategy to be used (e.g., random, nearest).</p> <code>radius_federation</code> <code>float</code> <p>Radius for federation in meters.</p> <code>scheme_mobility</code> <code>str</code> <p>Scheme to be used for managing mobility.</p> <code>round_frequency</code> <code>int</code> <p>Number of rounds after which mobility changes are applied.</p> <code>max_distance_with_direct_connections</code> <code>float</code> <p>Maximum distance for direct connections in meters.</p> <code>max_movement_random_strategy</code> <code>float</code> <p>Maximum movement distance for the random strategy in meters.</p> <code>max_movement_nearest_strategy</code> <code>float</code> <p>Maximum movement distance for the nearest strategy in meters.</p> <code>max_initiate_approximation</code> <code>float</code> <p>Maximum distance for initiating approximation calculations.</p> <code>network_conditions</code> <code>dict</code> <p>A dictionary containing network conditions (bandwidth and delay)                        based on distance.</p> <code>current_network_conditions</code> <code>dict</code> <p>A dictionary mapping addresses to their current network conditions.</p> Logs <p>Mobility information upon initialization to provide insights into the current setup.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the expected mobility configuration keys are not found in the provided config.</p> Source code in <code>nebula/addons/mobility.py</code> <pre><code>def __init__(self, config, verbose=False):\n    \"\"\"\n    Initializes the mobility module with specified configuration and communication manager.\n\n    This method sets up the mobility parameters required for the module, including grace time,\n    geographical change interval, mobility type, and other network conditions based on distance.\n    It also logs the initialized settings for the mobility system.\n\n    Args:\n        config (Config): Configuration object containing mobility parameters and settings.\n        cm (CommunicationsManager): An instance of the CommunicationsManager class used for handling\n                                     communication-related tasks within the mobility module.\n\n    Attributes:\n        grace_time (float): Time allocated for mobility processes to stabilize.\n        period (float): Interval at which geographic changes are made.\n        mobility (bool): Flag indicating whether mobility is enabled.\n        mobility_type (str): Type of mobility strategy to be used (e.g., random, nearest).\n        radius_federation (float): Radius for federation in meters.\n        scheme_mobility (str): Scheme to be used for managing mobility.\n        round_frequency (int): Number of rounds after which mobility changes are applied.\n        max_distance_with_direct_connections (float): Maximum distance for direct connections in meters.\n        max_movement_random_strategy (float): Maximum movement distance for the random strategy in meters.\n        max_movement_nearest_strategy (float): Maximum movement distance for the nearest strategy in meters.\n        max_initiate_approximation (float): Maximum distance for initiating approximation calculations.\n        network_conditions (dict): A dictionary containing network conditions (bandwidth and delay)\n                                   based on distance.\n        current_network_conditions (dict): A dictionary mapping addresses to their current network conditions.\n\n    Logs:\n        Mobility information upon initialization to provide insights into the current setup.\n\n    Raises:\n        KeyError: If the expected mobility configuration keys are not found in the provided config.\n    \"\"\"\n    logging.info(\"Starting mobility module...\")\n    self.config = config\n    self._verbose = verbose\n    self._running = asyncio.Event()\n    self._nodes_distances = {}\n    self._nodes_distances_lock = Locker(\"nodes_distances_lock\", async_lock=True)\n    self._mobility_task = None  # Track the background task\n\n    # Mobility configuration\n    self.mobility = self.config.participant[\"mobility_args\"][\"mobility\"]\n    self.mobility_type = self.config.participant[\"mobility_args\"][\"mobility_type\"]\n    self.grace_time = self.config.participant[\"mobility_args\"][\"grace_time_mobility\"]\n    self.period = self.config.participant[\"mobility_args\"][\"change_geo_interval\"]\n    # INFO: These values may change according to the needs of the federation\n    self.max_distance_with_direct_connections = 150  # meters\n    self.max_movement_random_strategy = 50  # meters\n    self.max_movement_nearest_strategy = 50  # meters\n    self.max_initiate_approximation = self.max_distance_with_direct_connections * 1.2\n    self.radius_federation = float(config.participant[\"mobility_args\"][\"radius_federation\"])\n    self.scheme_mobility = config.participant[\"mobility_args\"][\"scheme_mobility\"]\n    self.round_frequency = int(config.participant[\"mobility_args\"][\"round_frequency\"])\n    # Logging box with mobility information\n    mobility_msg = f\"Mobility: {self.mobility}\\nMobility type: {self.mobility_type}\\nRadius federation: {self.radius_federation}\\nScheme mobility: {self.scheme_mobility}\\nEach {self.round_frequency} rounds\"\n    print_msg_box(msg=mobility_msg, indent=2, title=\"Mobility information\")\n</code></pre>"},{"location":"api/addons/mobility/#nebula.addons.mobility.Mobility.change_geo_location","title":"<code>change_geo_location()</code>  <code>async</code>","text":"<p>Changes the geographical location of the entity based on the current mobility strategy.</p> <p>This coroutine checks the mobility type and decides whether to move towards the nearest neighbor or change the geo location randomly. It uses the communications manager to obtain the current connections and their distances.</p> <p>If the number of undirected connections is greater than directed connections, the method will attempt to find the nearest neighbor and move towards it if the distance exceeds a certain threshold. Otherwise, it will randomly change the geo location.</p> <p>Parameters:</p> Name Type Description Default <code>None</code> <p>This function does not take any arguments.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If the neighbor's location or distance cannot be found.</p> Notes <ul> <li>The method expects the mobility type to be either \"topology\" or \"both\".</li> <li>It logs actions taken during the execution for tracking and debugging purposes.</li> </ul> Source code in <code>nebula/addons/mobility.py</code> <pre><code>async def change_geo_location(self):\n    \"\"\"\n    Changes the geographical location of the entity based on the current mobility strategy.\n\n    This coroutine checks the mobility type and decides whether to move towards the nearest neighbor\n    or change the geo location randomly. It uses the communications manager to obtain the current\n    connections and their distances.\n\n    If the number of undirected connections is greater than directed connections, the method will\n    attempt to find the nearest neighbor and move towards it if the distance exceeds a certain threshold.\n    Otherwise, it will randomly change the geo location.\n\n    Args:\n        None: This function does not take any arguments.\n\n    Raises:\n        Exception: If the neighbor's location or distance cannot be found.\n\n    Notes:\n        - The method expects the mobility type to be either \"topology\" or \"both\".\n        - It logs actions taken during the execution for tracking and debugging purposes.\n    \"\"\"\n    if self.mobility and (self.mobility_type == \"topology\" or self.mobility_type == \"both\"):\n        random.seed(time.time() + self.config.participant[\"device_args\"][\"idx\"])\n        latitude = float(self.config.participant[\"mobility_args\"][\"latitude\"])\n        longitude = float(self.config.participant[\"mobility_args\"][\"longitude\"])\n        if True:\n            # Get neighbor closer to me\n            async with self._nodes_distances_lock:\n                sorted_list = sorted(self._nodes_distances.items(), key=lambda item: item[1][0])\n                # Transformamos la lista para obtener solo direcci\u00f3n y coordenadas\n                result = [(addr, dist, coords) for addr, (dist, coords) in sorted_list]\n\n            selected_neighbor = result[0] if result else None\n            if selected_neighbor:\n                # logging.info(f\"\ud83d\udccd  Selected neighbor: {selected_neighbor}\")\n                addr, dist, (lat, long) = selected_neighbor\n                if dist &gt; self.max_initiate_approximation:\n                    # If the distance is too big, we move towards the neighbor\n                    if self._verbose:\n                        logging.info(f\"Moving towards nearest neighbor: {addr}\")\n                    await self.change_geo_location_nearest_neighbor_strategy(\n                        dist,\n                        latitude,\n                        longitude,\n                        lat,\n                        long,\n                    )\n                else:\n                    await self.change_geo_location_random_strategy(latitude, longitude)\n            else:\n                await self.change_geo_location_random_strategy(latitude, longitude)\n        else:\n            await self.change_geo_location_random_strategy(latitude, longitude)\n    else:\n        logging.error(f\"\ud83d\udccd  Mobility type {self.mobility_type} not implemented\")\n        return\n</code></pre>"},{"location":"api/addons/mobility/#nebula.addons.mobility.Mobility.change_geo_location_nearest_neighbor_strategy","title":"<code>change_geo_location_nearest_neighbor_strategy(distance, latitude, longitude, neighbor_latitude, neighbor_longitude)</code>  <code>async</code>","text":"<p>Changes the geographical location of the entity towards the nearest neighbor.</p> <p>This coroutine updates the current geographical location by calculating the direction and distance to the nearest neighbor's coordinates. The movement towards the neighbor is scaled based on the distance and the maximum movement allowed.</p> <p>Parameters:</p> Name Type Description Default <code>distance</code> <code>float</code> <p>The distance to the nearest neighbor.</p> required <code>latitude</code> <code>float</code> <p>The current latitude of the entity.</p> required <code>longitude</code> <code>float</code> <p>The current longitude of the entity.</p> required <code>neighbor_latitude</code> <code>float</code> <p>The latitude of the nearest neighbor.</p> required <code>neighbor_longitude</code> <code>float</code> <p>The longitude of the nearest neighbor.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>May raise exceptions if the <code>set_geo_location</code> method encounters errors.</p> Notes <ul> <li>The movement is scaled based on the maximum allowed distance defined by   <code>self.max_movement_nearest_strategy</code>.</li> <li>The angle to the neighbor is calculated using the arctangent of the difference in   coordinates to determine the direction of movement.</li> <li>The conversion from meters to degrees is based on approximate geographical conversion factors.</li> </ul> Source code in <code>nebula/addons/mobility.py</code> <pre><code>async def change_geo_location_nearest_neighbor_strategy(\n    self, distance, latitude, longitude, neighbor_latitude, neighbor_longitude\n):\n    \"\"\"\n    Changes the geographical location of the entity towards the nearest neighbor.\n\n    This coroutine updates the current geographical location by calculating the direction\n    and distance to the nearest neighbor's coordinates. The movement towards the neighbor\n    is scaled based on the distance and the maximum movement allowed.\n\n    Args:\n        distance (float): The distance to the nearest neighbor.\n        latitude (float): The current latitude of the entity.\n        longitude (float): The current longitude of the entity.\n        neighbor_latitude (float): The latitude of the nearest neighbor.\n        neighbor_longitude (float): The longitude of the nearest neighbor.\n\n    Raises:\n        Exception: May raise exceptions if the `set_geo_location` method encounters errors.\n\n    Notes:\n        - The movement is scaled based on the maximum allowed distance defined by\n          `self.max_movement_nearest_strategy`.\n        - The angle to the neighbor is calculated using the arctangent of the difference in\n          coordinates to determine the direction of movement.\n        - The conversion from meters to degrees is based on approximate geographical conversion factors.\n    \"\"\"\n    if self._verbose:\n        logging.info(\"\ud83d\udccd  Changing geo location towards the nearest neighbor\")\n    scale_factor = min(1, self.max_movement_nearest_strategy / distance)\n    # Calculating angle to the neighbor\n    angle = math.atan2(neighbor_longitude - longitude, neighbor_latitude - latitude)\n    # Convert maximum movement to angle\n    max_lat_change = self.max_movement_nearest_strategy / 111000  # Change degree to latitude\n    max_lon_change = self.max_movement_nearest_strategy / (\n        111000 * math.cos(math.radians(latitude))\n    )  # Change dregree for longitude\n    # Scale and direction\n    delta_lat = max_lat_change * math.cos(angle) * scale_factor\n    delta_lon = max_lon_change * math.sin(angle) * scale_factor\n    # Update values\n    new_latitude = latitude + delta_lat\n    new_longitude = longitude + delta_lon\n    await self.set_geo_location(new_latitude, new_longitude)\n</code></pre>"},{"location":"api/addons/mobility/#nebula.addons.mobility.Mobility.change_geo_location_random_strategy","title":"<code>change_geo_location_random_strategy(latitude, longitude)</code>  <code>async</code>","text":"<p>Changes the geographical location of the entity using a random strategy.</p> <p>This coroutine modifies the current geographical location by randomly selecting a new position within a specified radius around the given latitude and longitude. The new location is determined using polar coordinates, where a random distance (radius) and angle are calculated.</p> <p>Parameters:</p> Name Type Description Default <code>latitude</code> <code>float</code> <p>The current latitude of the entity.</p> required <code>longitude</code> <code>float</code> <p>The current longitude of the entity.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>May raise exceptions if the <code>set_geo_location</code> method encounters errors.</p> Notes <ul> <li>The maximum movement distance is determined by <code>self.max_movement_random_strategy</code>.</li> <li>The calculated radius is converted from meters to degrees based on an approximate   conversion factor (1 degree is approximately 111 kilometers).</li> </ul> Source code in <code>nebula/addons/mobility.py</code> <pre><code>async def change_geo_location_random_strategy(self, latitude, longitude):\n    \"\"\"\n    Changes the geographical location of the entity using a random strategy.\n\n    This coroutine modifies the current geographical location by randomly\n    selecting a new position within a specified radius around the given\n    latitude and longitude. The new location is determined using polar\n    coordinates, where a random distance (radius) and angle are calculated.\n\n    Args:\n        latitude (float): The current latitude of the entity.\n        longitude (float): The current longitude of the entity.\n\n    Raises:\n        Exception: May raise exceptions if the `set_geo_location` method encounters errors.\n\n    Notes:\n        - The maximum movement distance is determined by `self.max_movement_random_strategy`.\n        - The calculated radius is converted from meters to degrees based on an approximate\n          conversion factor (1 degree is approximately 111 kilometers).\n    \"\"\"\n    if self._verbose:\n        logging.info(\"\ud83d\udccd  Changing geo location randomly\")\n    # radius_in_degrees = self.radius_federation / 111000\n    max_radius_in_degrees = self.max_movement_random_strategy / 111000\n    radius = random.uniform(0, max_radius_in_degrees)  # noqa: S311\n    angle = random.uniform(0, 2 * math.pi)  # noqa: S311\n    latitude += radius * math.cos(angle)\n    longitude += radius * math.sin(angle)\n    await self.set_geo_location(latitude, longitude)\n</code></pre>"},{"location":"api/addons/mobility/#nebula.addons.mobility.Mobility.run_mobility","title":"<code>run_mobility()</code>  <code>async</code>","text":"<p>Executes the mobility operations in a continuous loop.</p> <p>This coroutine manages the mobility behavior of the module. It first checks whether mobility is enabled. If mobility is not enabled, the function returns immediately.</p> <p>If mobility is enabled, the function will wait for the specified grace time before entering an infinite loop where it performs the following operations:</p> <ol> <li>Changes the geographical location by calling the <code>change_geo_location</code> method.</li> <li>Adjusts connections based on the current distance by calling    the <code>change_connections_based_on_distance</code> method.</li> <li>Sleeps for a specified period (<code>self.period</code>) before repeating the operations.</li> </ol> <p>This allows for periodic updates to the module's geographical location and network connections as per the defined mobility strategy.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>May raise exceptions if <code>change_geo_location</code> or         <code>change_connections_based_on_distance</code> encounters errors.</p> Source code in <code>nebula/addons/mobility.py</code> <pre><code>async def run_mobility(self):\n    \"\"\"\n    Executes the mobility operations in a continuous loop.\n\n    This coroutine manages the mobility behavior of the module. It first\n    checks whether mobility is enabled. If mobility is not enabled, the\n    function returns immediately.\n\n    If mobility is enabled, the function will wait for the specified\n    grace time before entering an infinite loop where it performs the\n    following operations:\n\n    1. Changes the geographical location by calling the `change_geo_location` method.\n    2. Adjusts connections based on the current distance by calling\n       the `change_connections_based_on_distance` method.\n    3. Sleeps for a specified period (`self.period`) before repeating the operations.\n\n    This allows for periodic updates to the module's geographical location\n    and network connections as per the defined mobility strategy.\n\n    Raises:\n        Exception: May raise exceptions if `change_geo_location` or\n                    `change_connections_based_on_distance` encounters errors.\n    \"\"\"\n    if not self.mobility:\n        return\n    # await asyncio.sleep(self.grace_time)\n    while await self.is_running():\n        await self.change_geo_location()\n        await asyncio.sleep(self.period)\n</code></pre>"},{"location":"api/addons/mobility/#nebula.addons.mobility.Mobility.set_geo_location","title":"<code>set_geo_location(latitude, longitude)</code>  <code>async</code>","text":"<p>Sets the geographical location of the entity to the specified latitude and longitude.</p> <p>This coroutine updates the latitude and longitude values in the configuration. If the provided coordinates are out of bounds (latitude must be between -90 and 90, and longitude must be between -180 and 180), the previous location is retained.</p> <p>Parameters:</p> Name Type Description Default <code>latitude</code> <code>float</code> <p>The new latitude to set.</p> required <code>longitude</code> <code>float</code> <p>The new longitude to set.</p> required <p>Raises:</p> Type Description <code>None</code> <p>This function does not raise any exceptions but retains the previous coordinates   if the new ones are invalid.</p> Notes <ul> <li>The new location is logged for tracking purposes.</li> <li>The coordinates are expected to be in decimal degrees format.</li> </ul> Source code in <code>nebula/addons/mobility.py</code> <pre><code>async def set_geo_location(self, latitude, longitude):\n    \"\"\"\n    Sets the geographical location of the entity to the specified latitude and longitude.\n\n    This coroutine updates the latitude and longitude values in the configuration. If the\n    provided coordinates are out of bounds (latitude must be between -90 and 90, and\n    longitude must be between -180 and 180), the previous location is retained.\n\n    Args:\n        latitude (float): The new latitude to set.\n        longitude (float): The new longitude to set.\n\n    Raises:\n        None: This function does not raise any exceptions but retains the previous coordinates\n              if the new ones are invalid.\n\n    Notes:\n        - The new location is logged for tracking purposes.\n        - The coordinates are expected to be in decimal degrees format.\n    \"\"\"\n\n    if latitude &lt; -90 or latitude &gt; 90 or longitude &lt; -180 or longitude &gt; 180:\n        # If the new location is out of bounds, we keep the old location\n        latitude = self.config.participant[\"mobility_args\"][\"latitude\"]\n        longitude = self.config.participant[\"mobility_args\"][\"longitude\"]\n\n    self.config.participant[\"mobility_args\"][\"latitude\"] = latitude\n    self.config.participant[\"mobility_args\"][\"longitude\"] = longitude\n    if self._verbose:\n        logging.info(f\"\ud83d\udccd  New geo location: {latitude}, {longitude}\")\n    cle = ChangeLocationEvent(latitude, longitude)\n    asyncio.create_task(EventManager.get_instance().publish_addonevent(cle))\n</code></pre>"},{"location":"api/addons/mobility/#nebula.addons.mobility.Mobility.start","title":"<code>start()</code>  <code>async</code>","text":"<p>Initiates the mobility process by starting the associated task.</p> <p>This method creates and schedules an asynchronous task to run the <code>run_mobility</code> coroutine, which handles the mobility operations for the module. It allows the mobility operations to run concurrently without blocking the execution of other tasks.</p> <p>Returns:</p> Type Description <p>asyncio.Task: An asyncio Task object representing the scheduled            <code>run_mobility</code> operation.</p> Source code in <code>nebula/addons/mobility.py</code> <pre><code>async def start(self):\n    \"\"\"\n    Initiates the mobility process by starting the associated task.\n\n    This method creates and schedules an asynchronous task to run the\n    `run_mobility` coroutine, which handles the mobility operations\n    for the module. It allows the mobility operations to run concurrently\n    without blocking the execution of other tasks.\n\n    Returns:\n        asyncio.Task: An asyncio Task object representing the scheduled\n                       `run_mobility` operation.\n    \"\"\"\n    await EventManager.get_instance().subscribe_addonevent(GPSEvent, self.update_nodes_distances)\n    await EventManager.get_instance().subscribe_addonevent(GPSEvent, self.update_nodes_distances)\n    self._running.set()\n    self._mobility_task = asyncio.create_task(self.run_mobility(), name=\"Mobility_run_mobility\")\n    return self._mobility_task\n</code></pre>"},{"location":"api/addons/mobility/#nebula.addons.mobility.Mobility.stop","title":"<code>stop()</code>  <code>async</code>","text":"<p>Stops the mobility module.</p> Source code in <code>nebula/addons/mobility.py</code> <pre><code>async def stop(self):\n    \"\"\"\n    Stops the mobility module.\n    \"\"\"\n    logging.info(\"Stopping Mobility module...\")\n    self._running.clear()\n\n    # Cancel the background task\n    if self._mobility_task and not self._mobility_task.done():\n        logging.info(\"\ud83d\uded1  Cancelling Mobility background task...\")\n        self._mobility_task.cancel()\n        try:\n            await self._mobility_task\n        except asyncio.CancelledError:\n            pass\n        self._mobility_task = None\n        logging.info(\"\ud83d\uded1  Mobility background task cancelled\")\n</code></pre>"},{"location":"api/addons/reporter/","title":"Documentation for Reporter Module","text":""},{"location":"api/addons/reporter/#nebula.addons.reporter.Reporter","title":"<code>Reporter</code>","text":"Source code in <code>nebula/addons/reporter.py</code> <pre><code>class Reporter:\n    def __init__(self, config, trainer):\n        \"\"\"\n        Initializes the reporter module for sending periodic updates to a dashboard controller.\n\n        This initializer sets up the configuration parameters required to report metrics and statistics\n        about the network, participant, and trainer. It connects to a specified URL endpoint where\n        these metrics will be logged, and it initializes values used for tracking network traffic.\n\n        Args:\n            config (dict): The configuration dictionary containing all setup parameters.\n            trainer (Trainer): The trainer object responsible for managing training sessions.\n            cm (CommunicationsManager): The communications manager handling network connections\n                                        and interactions.\n\n        Attributes:\n            frequency (int): The frequency at which the reporter sends updates.\n            grace_time (int): Grace period before starting the reporting.\n            data_queue (Queue): An asyncio queue for managing data to be reported.\n            url (str): The endpoint URL for reporting updates.\n            counter (int): Counter for tracking the number of reports sent.\n            first_net_metrics (bool): Flag indicating if this is the first collection of network metrics.\n            prev_bytes_sent (int), prev_bytes_recv (int), prev_packets_sent (int), prev_packets_recv (int):\n                Metrics for tracking network data sent and received.\n            acc_bytes_sent (int), acc_bytes_recv (int), acc_packets_sent (int), acc_packets_recv (int):\n                Accumulators for network traffic.\n\n        Raises:\n            None\n\n        Notes:\n            - Logs the start of the reporter module.\n            - Initializes both current and accumulated metrics for traffic monitoring.\n        \"\"\"\n        logging.info(\"Starting reporter module\")\n        self._cm = None\n        self.config = config\n        self.trainer = trainer\n        self.frequency = self.config.participant[\"reporter_args\"][\"report_frequency\"]\n        self.grace_time = self.config.participant[\"reporter_args\"][\"grace_time_reporter\"]\n        self.data_queue = asyncio.Queue()\n        self.url = f\"http://{self.config.participant['scenario_args']['controller']}/nodes/{self.config.participant['scenario_args']['name']}/update\"\n        self.counter = 0\n\n        self.first_net_metrics = True\n        self.prev_bytes_sent = 0\n        self.prev_bytes_recv = 0\n        self.prev_packets_sent = 0\n        self.prev_packets_recv = 0\n\n        self.acc_bytes_sent = 0\n        self.acc_bytes_recv = 0\n        self.acc_packets_sent = 0\n        self.acc_packets_recv = 0\n        self._running = asyncio.Event()\n        self._reporter_task = None  # Track the background task\n\n    @property\n    def cm(self):\n        if not self._cm:\n            from nebula.core.network.communications import CommunicationsManager\n\n            self._cm = CommunicationsManager.get_instance()\n            return self._cm\n        else:\n            return self._cm\n\n    async def enqueue_data(self, name, value):\n        \"\"\"\n        Asynchronously enqueues data for reporting.\n\n        This function adds a named data value pair to the data queue, which will later be processed\n        and sent to the designated reporting endpoint. The queue enables handling of reporting tasks\n        independently of other processes.\n\n        Args:\n            name (str): The name or identifier for the data item.\n            value (Any): The value of the data item to be reported.\n\n        Returns:\n            None\n\n        Notes:\n            - This function is asynchronous to allow non-blocking data enqueueing.\n            - Uses asyncio's queue to manage data, ensuring concurrency.\n        \"\"\"\n        await self.data_queue.put((name, value))\n\n    async def start(self):\n        \"\"\"\n        Starts the reporter module after a grace period.\n\n        This asynchronous function initiates the reporting process following a designated grace period.\n        It creates a background task to run the reporting loop, allowing data to be reported at defined intervals.\n\n        Returns:\n            asyncio.Task: The task for the reporter loop, which handles the data reporting asynchronously.\n\n        Notes:\n            - The grace period allows for a delay before the first reporting cycle.\n            - The reporter loop runs in the background, ensuring continuous data updates.\n        \"\"\"\n        self._running.set()\n        await asyncio.sleep(self.grace_time)\n        self._reporter_task = asyncio.create_task(self.run_reporter(), name=\"Reporter_run_reporter\")\n        return self._reporter_task\n\n    async def run_reporter(self):\n        \"\"\"\n        Runs the continuous reporting loop.\n\n        This asynchronous function performs periodic reporting tasks such as reporting resource usage,\n        data queue contents, and, optionally, status updates to the controller. The loop runs indefinitely,\n        updating the counter with each cycle to track the frequency of specific tasks.\n\n        Key Actions:\n            - Regularly reports the resource status.\n            - Reloads the configuration file every 50 cycles to reflect any updates.\n\n        Notes:\n            - The reporting frequency is determined by the 'report_frequency' setting in the config file.\n        \"\"\"\n        while self._running.is_set():\n            if self.config.participant[\"reporter_args\"][\"report_status_data_queue\"]:\n                if self.config.participant[\"scenario_args\"][\"controller\"] != \"nebula-test\":\n                    await self.__report_status_to_controller()\n                await self.__report_data_queue()\n            await self.__report_resources()\n            self.counter += 1\n            if self.counter % 50 == 0:\n                logging.info(\"Reloading config file...\")\n                self.cm.engine.config.reload_config_file()\n            await asyncio.sleep(self.frequency)\n\n    async def report_scenario_finished(self):\n        \"\"\"\n        Reports the scenario completion status to the controller.\n\n        This asynchronous function notifies the scenario controller that the participant has finished\n        its tasks. It sends a POST request to the designated controller URL, including the participant's\n        ID in the JSON payload.\n\n        URL Construction:\n            - The URL is dynamically built using the controller address and scenario name\n              from the configuration settings.\n\n        Parameters:\n            - idx (int): The unique identifier for this participant, sent in the request data.\n\n        Returns:\n            - bool: True if the report was successful (status 200), False otherwise.\n\n        Error Handling:\n            - Logs an error if the response status is not 200, indicating that the controller\n              might be temporarily overloaded.\n            - Logs exceptions if the connection attempt to the controller fails.\n        \"\"\"\n        url = f\"http://{self.config.participant['scenario_args']['controller']}/nodes/{self.config.participant['scenario_args']['name']}/done\"\n        data = json.dumps({\"idx\": self.config.participant[\"device_args\"][\"idx\"]})\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"User-Agent\": f\"NEBULA Participant {self.config.participant['device_args']['idx']}\",\n        }\n        try:\n            await self.__report_status_to_controller()\n        except Exception as e:\n            logging.exception(f\"Error reporting status before scenario finished: {e}\")\n        try:\n            async with aiohttp.ClientSession() as session, session.post(url, data=data, headers=headers) as response:\n                if response.status != 200:\n                    logging.error(\n                        f\"Error received from controller: {response.status} (probably there is overhead in the controller, trying again in the next round)\"\n                    )\n                    text = await response.text()\n                    logging.debug(text)\n                else:\n                    logging.info(\n                        f\"Participant {self.config.participant['device_args']['idx']} reported scenario finished\"\n                    )\n                    return True\n        except aiohttp.ClientError:\n            logging.exception(f\"Error connecting to the controller at {url}\")\n        return False\n\n    async def stop(self):\n        logging.info(\"\ud83d\udd0d  Stopping reporter module...\")\n        self._running.clear()\n\n        # Cancel the background task\n        if self._reporter_task and not self._reporter_task.done():\n            logging.info(\"\ud83d\uded1  Cancelling Reporter background task...\")\n            self._reporter_task.cancel()\n            try:\n                await self._reporter_task\n            except asyncio.CancelledError:\n                pass\n            self._reporter_task = None\n            logging.info(\"\ud83d\uded1  Reporter background task cancelled\")\n\n    async def __report_data_queue(self):\n        \"\"\"\n        Processes and reports queued data entries.\n\n        This asynchronous function iterates over the data queue, retrieving each name-value pair\n        and sending it to the trainer's logging mechanism. Once logged, each item is marked as done.\n\n        Functionality:\n            - Retrieves and logs all entries in the data queue until it is empty.\n            - Assumes that `log_data` can handle asynchronous execution for optimal performance.\n\n        Parameters:\n            - name (str): The identifier for the data entry (e.g., metric name).\n            - value (Any): The value of the data entry to be logged.\n\n        Returns:\n            - None\n\n        Notes:\n            - Each processed item is marked as done in the queue.\n        \"\"\"\n\n        while not self.data_queue.empty():\n            name, value = await self.data_queue.get()\n            await self.trainer.logger.log_data({name: value})  # Assuming log_data can be made async\n            self.data_queue.task_done()\n\n    async def __report_status_to_controller(self):\n        \"\"\"\n        Sends the participant's status to the controller.\n\n        This asynchronous function transmits the current participant configuration to the controller's\n        URL endpoint. It handles both client and general exceptions to ensure robust communication\n        with the controller, retrying in case of errors.\n\n        Functionality:\n            - Initiates a session to post participant data to the controller.\n            - Logs the response status, indicating issues when status is non-200.\n            - Retries after a short delay in case of connection errors or unhandled exceptions.\n\n        Parameters:\n            - None (uses internal `self.config.participant` data to build the payload).\n\n        Returns:\n            - None\n\n        Notes:\n            - Uses the participant index to specify the User-Agent in headers.\n            - Delays for 5 seconds upon general exceptions to avoid rapid retry loops.\n        \"\"\"\n        try:\n            async with (\n                aiohttp.ClientSession() as session,\n                session.post(\n                    self.url,\n                    data=json.dumps(self.config.participant),\n                    headers={\n                        \"Content-Type\": \"application/json\",\n                        \"User-Agent\": f\"NEBULA Participant {self.config.participant['device_args']['idx']}\",\n                    },\n                ) as response,\n            ):\n                if response.status != 200:\n                    logging.error(\n                        f\"Error received from controller: {response.status} (probably there is overhead in the controller, trying again in the next round)\"\n                    )\n                    text = await response.text()\n                    logging.debug(text)\n        except aiohttp.ClientError:\n            logging.exception(f\"Error connecting to the controller at {self.url}\")\n        except Exception:\n            logging.exception(\"Error sending status to controller, will try again in a few seconds\")\n            await asyncio.sleep(5)\n\n    async def __report_resources(self):\n        \"\"\"\n        Reports system resource usage metrics.\n\n        This asynchronous function gathers and logs CPU usage data for the participant's device,\n        and attempts to retrieve the CPU temperature (Linux systems only). Additionally, it measures\n        CPU usage specifically for the current process.\n\n        Functionality:\n            - Gathers total CPU usage (percentage) and attempts to retrieve CPU temperature.\n            - Uses `psutil` for non-blocking access to system data on Linux.\n            - Records CPU usage of the current process for finer monitoring.\n\n        Parameters:\n            - None\n\n        Notes:\n            - On non-Linux platforms, CPU temperature will default to 0.\n            - Uses `asyncio.to_thread` to call CPU and sensor readings without blocking the event loop.\n        \"\"\"\n        cpu_percent = psutil.cpu_percent()\n        cpu_temp = 0\n        try:\n            if sys.platform == \"linux\":\n                sensors = await asyncio.to_thread(psutil.sensors_temperatures)\n                cpu_temp = sensors.get(\"coretemp\")[0].current if sensors.get(\"coretemp\") else 0\n        except Exception:  # noqa: S110\n            pass\n\n        pid = os.getpid()\n        cpu_percent_process = await asyncio.to_thread(psutil.Process(pid).cpu_percent, interval=1)\n\n        process = psutil.Process(pid)\n        memory_process = await asyncio.to_thread(lambda: process.memory_info().rss / (1024**2))\n        memory_percent_process = process.memory_percent()\n        memory_info = await asyncio.to_thread(psutil.virtual_memory)\n        memory_percent = memory_info.percent\n        memory_used = memory_info.used / (1024**2)\n\n        disk_percent = psutil.disk_usage(\"/\").percent\n\n        net_io_counters = await asyncio.to_thread(psutil.net_io_counters)\n        bytes_sent = net_io_counters.bytes_sent\n        bytes_recv = net_io_counters.bytes_recv\n        packets_sent = net_io_counters.packets_sent\n        packets_recv = net_io_counters.packets_recv\n\n        if self.first_net_metrics:\n            bytes_sent_diff = 0\n            bytes_recv_diff = 0\n            packets_sent_diff = 0\n            packets_recv_diff = 0\n            self.first_net_metrics = False\n        else:\n            bytes_sent_diff = bytes_sent - self.prev_bytes_sent\n            bytes_recv_diff = bytes_recv - self.prev_bytes_recv\n            packets_sent_diff = packets_sent - self.prev_packets_sent\n            packets_recv_diff = packets_recv - self.prev_packets_recv\n\n        self.prev_bytes_sent = bytes_sent\n        self.prev_bytes_recv = bytes_recv\n        self.prev_packets_sent = packets_sent\n        self.prev_packets_recv = packets_recv\n\n        self.acc_bytes_sent += bytes_sent_diff\n        self.acc_bytes_recv += bytes_recv_diff\n        self.acc_packets_sent += packets_sent_diff\n        self.acc_packets_recv += packets_recv_diff\n\n        current_connections = await self.cm.get_addrs_current_connections(only_direct=True)\n\n        resources = {\n            \"W-CPU/CPU global (%)\": cpu_percent,\n            \"W-CPU/CPU process (%)\": cpu_percent_process,\n            \"W-CPU/CPU temperature (\u00b0)\": cpu_temp,\n            \"Z-RAM/RAM global (%)\": memory_percent,\n            \"Z-RAM/RAM global (MB)\": memory_used,\n            \"Z-RAM/RAM process (%)\": memory_percent_process,\n            \"Z-RAM/RAM process (MB)\": memory_process,\n            \"Y-Disk/Disk (%)\": disk_percent,\n            \"X-Network/Network (MB sent)\": round(self.acc_bytes_sent / (1024**2), 3),\n            \"X-Network/Network (MB received)\": round(self.acc_bytes_recv / (1024**2), 3),\n            \"X-Network/Network (packets sent)\": self.acc_packets_sent,\n            \"X-Network/Network (packets received)\": self.acc_packets_recv,\n            \"X-Network/Connections\": len(current_connections),\n        }\n        self.trainer.logger.log_data(resources)\n\n        if importlib.util.find_spec(\"pynvml\") is not None:\n            try:\n                import pynvml\n\n                await asyncio.to_thread(pynvml.nvmlInit)\n                devices = await asyncio.to_thread(pynvml.nvmlDeviceGetCount)\n                for i in range(devices):\n                    handle = await asyncio.to_thread(pynvml.nvmlDeviceGetHandleByIndex, i)\n                    gpu_percent = (await asyncio.to_thread(pynvml.nvmlDeviceGetUtilizationRates, handle)).gpu\n                    gpu_temp = await asyncio.to_thread(\n                        pynvml.nvmlDeviceGetTemperature,\n                        handle,\n                        pynvml.NVML_TEMPERATURE_GPU,\n                    )\n                    gpu_mem = await asyncio.to_thread(pynvml.nvmlDeviceGetMemoryInfo, handle)\n                    gpu_mem_percent = round(gpu_mem.used / gpu_mem.total * 100, 3)\n                    gpu_power = await asyncio.to_thread(pynvml.nvmlDeviceGetPowerUsage, handle) / 1000.0\n                    gpu_clocks = await asyncio.to_thread(pynvml.nvmlDeviceGetClockInfo, handle, pynvml.NVML_CLOCK_SM)\n                    gpu_memory_clocks = await asyncio.to_thread(\n                        pynvml.nvmlDeviceGetClockInfo, handle, pynvml.NVML_CLOCK_MEM\n                    )\n                    gpu_fan_speed = await asyncio.to_thread(pynvml.nvmlDeviceGetFanSpeed, handle)\n                    gpu_info = {\n                        f\"W-GPU/GPU{i} (%)\": gpu_percent,\n                        f\"W-GPU/GPU{i} temperature (\u00b0)\": gpu_temp,\n                        f\"W-GPU/GPU{i} memory (%)\": gpu_mem_percent,\n                        f\"W-GPU/GPU{i} power\": gpu_power,\n                        f\"W-GPU/GPU{i} clocks\": gpu_clocks,\n                        f\"W-GPU/GPU{i} memory clocks\": gpu_memory_clocks,\n                        f\"W-GPU/GPU{i} fan speed\": gpu_fan_speed,\n                    }\n                    self.trainer.logger.log_data(gpu_info)\n            except Exception:  # noqa: S110\n                pass\n</code></pre>"},{"location":"api/addons/reporter/#nebula.addons.reporter.Reporter.__init__","title":"<code>__init__(config, trainer)</code>","text":"<p>Initializes the reporter module for sending periodic updates to a dashboard controller.</p> <p>This initializer sets up the configuration parameters required to report metrics and statistics about the network, participant, and trainer. It connects to a specified URL endpoint where these metrics will be logged, and it initializes values used for tracking network traffic.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>The configuration dictionary containing all setup parameters.</p> required <code>trainer</code> <code>Trainer</code> <p>The trainer object responsible for managing training sessions.</p> required <code>cm</code> <code>CommunicationsManager</code> <p>The communications manager handling network connections                         and interactions.</p> required <p>Attributes:</p> Name Type Description <code>frequency</code> <code>int</code> <p>The frequency at which the reporter sends updates.</p> <code>grace_time</code> <code>int</code> <p>Grace period before starting the reporting.</p> <code>data_queue</code> <code>Queue</code> <p>An asyncio queue for managing data to be reported.</p> <code>url</code> <code>str</code> <p>The endpoint URL for reporting updates.</p> <code>counter</code> <code>int</code> <p>Counter for tracking the number of reports sent.</p> <code>first_net_metrics</code> <code>bool</code> <p>Flag indicating if this is the first collection of network metrics.</p> <code>prev_bytes_sent</code> <code>int), prev_bytes_recv (int), prev_packets_sent (int), prev_packets_recv (int</code> <p>Metrics for tracking network data sent and received.</p> <code>acc_bytes_sent</code> <code>int), acc_bytes_recv (int), acc_packets_sent (int), acc_packets_recv (int</code> <p>Accumulators for network traffic.</p> Notes <ul> <li>Logs the start of the reporter module.</li> <li>Initializes both current and accumulated metrics for traffic monitoring.</li> </ul> Source code in <code>nebula/addons/reporter.py</code> <pre><code>def __init__(self, config, trainer):\n    \"\"\"\n    Initializes the reporter module for sending periodic updates to a dashboard controller.\n\n    This initializer sets up the configuration parameters required to report metrics and statistics\n    about the network, participant, and trainer. It connects to a specified URL endpoint where\n    these metrics will be logged, and it initializes values used for tracking network traffic.\n\n    Args:\n        config (dict): The configuration dictionary containing all setup parameters.\n        trainer (Trainer): The trainer object responsible for managing training sessions.\n        cm (CommunicationsManager): The communications manager handling network connections\n                                    and interactions.\n\n    Attributes:\n        frequency (int): The frequency at which the reporter sends updates.\n        grace_time (int): Grace period before starting the reporting.\n        data_queue (Queue): An asyncio queue for managing data to be reported.\n        url (str): The endpoint URL for reporting updates.\n        counter (int): Counter for tracking the number of reports sent.\n        first_net_metrics (bool): Flag indicating if this is the first collection of network metrics.\n        prev_bytes_sent (int), prev_bytes_recv (int), prev_packets_sent (int), prev_packets_recv (int):\n            Metrics for tracking network data sent and received.\n        acc_bytes_sent (int), acc_bytes_recv (int), acc_packets_sent (int), acc_packets_recv (int):\n            Accumulators for network traffic.\n\n    Raises:\n        None\n\n    Notes:\n        - Logs the start of the reporter module.\n        - Initializes both current and accumulated metrics for traffic monitoring.\n    \"\"\"\n    logging.info(\"Starting reporter module\")\n    self._cm = None\n    self.config = config\n    self.trainer = trainer\n    self.frequency = self.config.participant[\"reporter_args\"][\"report_frequency\"]\n    self.grace_time = self.config.participant[\"reporter_args\"][\"grace_time_reporter\"]\n    self.data_queue = asyncio.Queue()\n    self.url = f\"http://{self.config.participant['scenario_args']['controller']}/nodes/{self.config.participant['scenario_args']['name']}/update\"\n    self.counter = 0\n\n    self.first_net_metrics = True\n    self.prev_bytes_sent = 0\n    self.prev_bytes_recv = 0\n    self.prev_packets_sent = 0\n    self.prev_packets_recv = 0\n\n    self.acc_bytes_sent = 0\n    self.acc_bytes_recv = 0\n    self.acc_packets_sent = 0\n    self.acc_packets_recv = 0\n    self._running = asyncio.Event()\n    self._reporter_task = None  # Track the background task\n</code></pre>"},{"location":"api/addons/reporter/#nebula.addons.reporter.Reporter.__report_data_queue","title":"<code>__report_data_queue()</code>  <code>async</code>","text":"<p>Processes and reports queued data entries.</p> <p>This asynchronous function iterates over the data queue, retrieving each name-value pair and sending it to the trainer's logging mechanism. Once logged, each item is marked as done.</p> Functionality <ul> <li>Retrieves and logs all entries in the data queue until it is empty.</li> <li>Assumes that <code>log_data</code> can handle asynchronous execution for optimal performance.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>-</code> <code>name (str</code> <p>The identifier for the data entry (e.g., metric name).</p> required <code>-</code> <code>value (Any</code> <p>The value of the data entry to be logged.</p> required <p>Returns:</p> Type Description <ul> <li>None</li> </ul> Notes <ul> <li>Each processed item is marked as done in the queue.</li> </ul> Source code in <code>nebula/addons/reporter.py</code> <pre><code>async def __report_data_queue(self):\n    \"\"\"\n    Processes and reports queued data entries.\n\n    This asynchronous function iterates over the data queue, retrieving each name-value pair\n    and sending it to the trainer's logging mechanism. Once logged, each item is marked as done.\n\n    Functionality:\n        - Retrieves and logs all entries in the data queue until it is empty.\n        - Assumes that `log_data` can handle asynchronous execution for optimal performance.\n\n    Parameters:\n        - name (str): The identifier for the data entry (e.g., metric name).\n        - value (Any): The value of the data entry to be logged.\n\n    Returns:\n        - None\n\n    Notes:\n        - Each processed item is marked as done in the queue.\n    \"\"\"\n\n    while not self.data_queue.empty():\n        name, value = await self.data_queue.get()\n        await self.trainer.logger.log_data({name: value})  # Assuming log_data can be made async\n        self.data_queue.task_done()\n</code></pre>"},{"location":"api/addons/reporter/#nebula.addons.reporter.Reporter.__report_resources","title":"<code>__report_resources()</code>  <code>async</code>","text":"<p>Reports system resource usage metrics.</p> <p>This asynchronous function gathers and logs CPU usage data for the participant's device, and attempts to retrieve the CPU temperature (Linux systems only). Additionally, it measures CPU usage specifically for the current process.</p> Functionality <ul> <li>Gathers total CPU usage (percentage) and attempts to retrieve CPU temperature.</li> <li>Uses <code>psutil</code> for non-blocking access to system data on Linux.</li> <li>Records CPU usage of the current process for finer monitoring.</li> </ul> Notes <ul> <li>On non-Linux platforms, CPU temperature will default to 0.</li> <li>Uses <code>asyncio.to_thread</code> to call CPU and sensor readings without blocking the event loop.</li> </ul> Source code in <code>nebula/addons/reporter.py</code> <pre><code>async def __report_resources(self):\n    \"\"\"\n    Reports system resource usage metrics.\n\n    This asynchronous function gathers and logs CPU usage data for the participant's device,\n    and attempts to retrieve the CPU temperature (Linux systems only). Additionally, it measures\n    CPU usage specifically for the current process.\n\n    Functionality:\n        - Gathers total CPU usage (percentage) and attempts to retrieve CPU temperature.\n        - Uses `psutil` for non-blocking access to system data on Linux.\n        - Records CPU usage of the current process for finer monitoring.\n\n    Parameters:\n        - None\n\n    Notes:\n        - On non-Linux platforms, CPU temperature will default to 0.\n        - Uses `asyncio.to_thread` to call CPU and sensor readings without blocking the event loop.\n    \"\"\"\n    cpu_percent = psutil.cpu_percent()\n    cpu_temp = 0\n    try:\n        if sys.platform == \"linux\":\n            sensors = await asyncio.to_thread(psutil.sensors_temperatures)\n            cpu_temp = sensors.get(\"coretemp\")[0].current if sensors.get(\"coretemp\") else 0\n    except Exception:  # noqa: S110\n        pass\n\n    pid = os.getpid()\n    cpu_percent_process = await asyncio.to_thread(psutil.Process(pid).cpu_percent, interval=1)\n\n    process = psutil.Process(pid)\n    memory_process = await asyncio.to_thread(lambda: process.memory_info().rss / (1024**2))\n    memory_percent_process = process.memory_percent()\n    memory_info = await asyncio.to_thread(psutil.virtual_memory)\n    memory_percent = memory_info.percent\n    memory_used = memory_info.used / (1024**2)\n\n    disk_percent = psutil.disk_usage(\"/\").percent\n\n    net_io_counters = await asyncio.to_thread(psutil.net_io_counters)\n    bytes_sent = net_io_counters.bytes_sent\n    bytes_recv = net_io_counters.bytes_recv\n    packets_sent = net_io_counters.packets_sent\n    packets_recv = net_io_counters.packets_recv\n\n    if self.first_net_metrics:\n        bytes_sent_diff = 0\n        bytes_recv_diff = 0\n        packets_sent_diff = 0\n        packets_recv_diff = 0\n        self.first_net_metrics = False\n    else:\n        bytes_sent_diff = bytes_sent - self.prev_bytes_sent\n        bytes_recv_diff = bytes_recv - self.prev_bytes_recv\n        packets_sent_diff = packets_sent - self.prev_packets_sent\n        packets_recv_diff = packets_recv - self.prev_packets_recv\n\n    self.prev_bytes_sent = bytes_sent\n    self.prev_bytes_recv = bytes_recv\n    self.prev_packets_sent = packets_sent\n    self.prev_packets_recv = packets_recv\n\n    self.acc_bytes_sent += bytes_sent_diff\n    self.acc_bytes_recv += bytes_recv_diff\n    self.acc_packets_sent += packets_sent_diff\n    self.acc_packets_recv += packets_recv_diff\n\n    current_connections = await self.cm.get_addrs_current_connections(only_direct=True)\n\n    resources = {\n        \"W-CPU/CPU global (%)\": cpu_percent,\n        \"W-CPU/CPU process (%)\": cpu_percent_process,\n        \"W-CPU/CPU temperature (\u00b0)\": cpu_temp,\n        \"Z-RAM/RAM global (%)\": memory_percent,\n        \"Z-RAM/RAM global (MB)\": memory_used,\n        \"Z-RAM/RAM process (%)\": memory_percent_process,\n        \"Z-RAM/RAM process (MB)\": memory_process,\n        \"Y-Disk/Disk (%)\": disk_percent,\n        \"X-Network/Network (MB sent)\": round(self.acc_bytes_sent / (1024**2), 3),\n        \"X-Network/Network (MB received)\": round(self.acc_bytes_recv / (1024**2), 3),\n        \"X-Network/Network (packets sent)\": self.acc_packets_sent,\n        \"X-Network/Network (packets received)\": self.acc_packets_recv,\n        \"X-Network/Connections\": len(current_connections),\n    }\n    self.trainer.logger.log_data(resources)\n\n    if importlib.util.find_spec(\"pynvml\") is not None:\n        try:\n            import pynvml\n\n            await asyncio.to_thread(pynvml.nvmlInit)\n            devices = await asyncio.to_thread(pynvml.nvmlDeviceGetCount)\n            for i in range(devices):\n                handle = await asyncio.to_thread(pynvml.nvmlDeviceGetHandleByIndex, i)\n                gpu_percent = (await asyncio.to_thread(pynvml.nvmlDeviceGetUtilizationRates, handle)).gpu\n                gpu_temp = await asyncio.to_thread(\n                    pynvml.nvmlDeviceGetTemperature,\n                    handle,\n                    pynvml.NVML_TEMPERATURE_GPU,\n                )\n                gpu_mem = await asyncio.to_thread(pynvml.nvmlDeviceGetMemoryInfo, handle)\n                gpu_mem_percent = round(gpu_mem.used / gpu_mem.total * 100, 3)\n                gpu_power = await asyncio.to_thread(pynvml.nvmlDeviceGetPowerUsage, handle) / 1000.0\n                gpu_clocks = await asyncio.to_thread(pynvml.nvmlDeviceGetClockInfo, handle, pynvml.NVML_CLOCK_SM)\n                gpu_memory_clocks = await asyncio.to_thread(\n                    pynvml.nvmlDeviceGetClockInfo, handle, pynvml.NVML_CLOCK_MEM\n                )\n                gpu_fan_speed = await asyncio.to_thread(pynvml.nvmlDeviceGetFanSpeed, handle)\n                gpu_info = {\n                    f\"W-GPU/GPU{i} (%)\": gpu_percent,\n                    f\"W-GPU/GPU{i} temperature (\u00b0)\": gpu_temp,\n                    f\"W-GPU/GPU{i} memory (%)\": gpu_mem_percent,\n                    f\"W-GPU/GPU{i} power\": gpu_power,\n                    f\"W-GPU/GPU{i} clocks\": gpu_clocks,\n                    f\"W-GPU/GPU{i} memory clocks\": gpu_memory_clocks,\n                    f\"W-GPU/GPU{i} fan speed\": gpu_fan_speed,\n                }\n                self.trainer.logger.log_data(gpu_info)\n        except Exception:  # noqa: S110\n            pass\n</code></pre>"},{"location":"api/addons/reporter/#nebula.addons.reporter.Reporter.__report_status_to_controller","title":"<code>__report_status_to_controller()</code>  <code>async</code>","text":"<p>Sends the participant's status to the controller.</p> <p>This asynchronous function transmits the current participant configuration to the controller's URL endpoint. It handles both client and general exceptions to ensure robust communication with the controller, retrying in case of errors.</p> Functionality <ul> <li>Initiates a session to post participant data to the controller.</li> <li>Logs the response status, indicating issues when status is non-200.</li> <li>Retries after a short delay in case of connection errors or unhandled exceptions.</li> </ul> <p>Returns:</p> Type Description <ul> <li>None</li> </ul> Notes <ul> <li>Uses the participant index to specify the User-Agent in headers.</li> <li>Delays for 5 seconds upon general exceptions to avoid rapid retry loops.</li> </ul> Source code in <code>nebula/addons/reporter.py</code> <pre><code>async def __report_status_to_controller(self):\n    \"\"\"\n    Sends the participant's status to the controller.\n\n    This asynchronous function transmits the current participant configuration to the controller's\n    URL endpoint. It handles both client and general exceptions to ensure robust communication\n    with the controller, retrying in case of errors.\n\n    Functionality:\n        - Initiates a session to post participant data to the controller.\n        - Logs the response status, indicating issues when status is non-200.\n        - Retries after a short delay in case of connection errors or unhandled exceptions.\n\n    Parameters:\n        - None (uses internal `self.config.participant` data to build the payload).\n\n    Returns:\n        - None\n\n    Notes:\n        - Uses the participant index to specify the User-Agent in headers.\n        - Delays for 5 seconds upon general exceptions to avoid rapid retry loops.\n    \"\"\"\n    try:\n        async with (\n            aiohttp.ClientSession() as session,\n            session.post(\n                self.url,\n                data=json.dumps(self.config.participant),\n                headers={\n                    \"Content-Type\": \"application/json\",\n                    \"User-Agent\": f\"NEBULA Participant {self.config.participant['device_args']['idx']}\",\n                },\n            ) as response,\n        ):\n            if response.status != 200:\n                logging.error(\n                    f\"Error received from controller: {response.status} (probably there is overhead in the controller, trying again in the next round)\"\n                )\n                text = await response.text()\n                logging.debug(text)\n    except aiohttp.ClientError:\n        logging.exception(f\"Error connecting to the controller at {self.url}\")\n    except Exception:\n        logging.exception(\"Error sending status to controller, will try again in a few seconds\")\n        await asyncio.sleep(5)\n</code></pre>"},{"location":"api/addons/reporter/#nebula.addons.reporter.Reporter.enqueue_data","title":"<code>enqueue_data(name, value)</code>  <code>async</code>","text":"<p>Asynchronously enqueues data for reporting.</p> <p>This function adds a named data value pair to the data queue, which will later be processed and sent to the designated reporting endpoint. The queue enables handling of reporting tasks independently of other processes.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name or identifier for the data item.</p> required <code>value</code> <code>Any</code> <p>The value of the data item to be reported.</p> required <p>Returns:</p> Type Description <p>None</p> Notes <ul> <li>This function is asynchronous to allow non-blocking data enqueueing.</li> <li>Uses asyncio's queue to manage data, ensuring concurrency.</li> </ul> Source code in <code>nebula/addons/reporter.py</code> <pre><code>async def enqueue_data(self, name, value):\n    \"\"\"\n    Asynchronously enqueues data for reporting.\n\n    This function adds a named data value pair to the data queue, which will later be processed\n    and sent to the designated reporting endpoint. The queue enables handling of reporting tasks\n    independently of other processes.\n\n    Args:\n        name (str): The name or identifier for the data item.\n        value (Any): The value of the data item to be reported.\n\n    Returns:\n        None\n\n    Notes:\n        - This function is asynchronous to allow non-blocking data enqueueing.\n        - Uses asyncio's queue to manage data, ensuring concurrency.\n    \"\"\"\n    await self.data_queue.put((name, value))\n</code></pre>"},{"location":"api/addons/reporter/#nebula.addons.reporter.Reporter.report_scenario_finished","title":"<code>report_scenario_finished()</code>  <code>async</code>","text":"<p>Reports the scenario completion status to the controller.</p> <p>This asynchronous function notifies the scenario controller that the participant has finished its tasks. It sends a POST request to the designated controller URL, including the participant's ID in the JSON payload.</p> URL Construction <ul> <li>The URL is dynamically built using the controller address and scenario name   from the configuration settings.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>-</code> <code>idx (int</code> <p>The unique identifier for this participant, sent in the request data.</p> required <p>Returns:</p> Type Description <ul> <li>bool: True if the report was successful (status 200), False otherwise.</li> </ul> Error Handling <ul> <li>Logs an error if the response status is not 200, indicating that the controller   might be temporarily overloaded.</li> <li>Logs exceptions if the connection attempt to the controller fails.</li> </ul> Source code in <code>nebula/addons/reporter.py</code> <pre><code>async def report_scenario_finished(self):\n    \"\"\"\n    Reports the scenario completion status to the controller.\n\n    This asynchronous function notifies the scenario controller that the participant has finished\n    its tasks. It sends a POST request to the designated controller URL, including the participant's\n    ID in the JSON payload.\n\n    URL Construction:\n        - The URL is dynamically built using the controller address and scenario name\n          from the configuration settings.\n\n    Parameters:\n        - idx (int): The unique identifier for this participant, sent in the request data.\n\n    Returns:\n        - bool: True if the report was successful (status 200), False otherwise.\n\n    Error Handling:\n        - Logs an error if the response status is not 200, indicating that the controller\n          might be temporarily overloaded.\n        - Logs exceptions if the connection attempt to the controller fails.\n    \"\"\"\n    url = f\"http://{self.config.participant['scenario_args']['controller']}/nodes/{self.config.participant['scenario_args']['name']}/done\"\n    data = json.dumps({\"idx\": self.config.participant[\"device_args\"][\"idx\"]})\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"User-Agent\": f\"NEBULA Participant {self.config.participant['device_args']['idx']}\",\n    }\n    try:\n        await self.__report_status_to_controller()\n    except Exception as e:\n        logging.exception(f\"Error reporting status before scenario finished: {e}\")\n    try:\n        async with aiohttp.ClientSession() as session, session.post(url, data=data, headers=headers) as response:\n            if response.status != 200:\n                logging.error(\n                    f\"Error received from controller: {response.status} (probably there is overhead in the controller, trying again in the next round)\"\n                )\n                text = await response.text()\n                logging.debug(text)\n            else:\n                logging.info(\n                    f\"Participant {self.config.participant['device_args']['idx']} reported scenario finished\"\n                )\n                return True\n    except aiohttp.ClientError:\n        logging.exception(f\"Error connecting to the controller at {url}\")\n    return False\n</code></pre>"},{"location":"api/addons/reporter/#nebula.addons.reporter.Reporter.run_reporter","title":"<code>run_reporter()</code>  <code>async</code>","text":"<p>Runs the continuous reporting loop.</p> <p>This asynchronous function performs periodic reporting tasks such as reporting resource usage, data queue contents, and, optionally, status updates to the controller. The loop runs indefinitely, updating the counter with each cycle to track the frequency of specific tasks.</p> Key Actions <ul> <li>Regularly reports the resource status.</li> <li>Reloads the configuration file every 50 cycles to reflect any updates.</li> </ul> Notes <ul> <li>The reporting frequency is determined by the 'report_frequency' setting in the config file.</li> </ul> Source code in <code>nebula/addons/reporter.py</code> <pre><code>async def run_reporter(self):\n    \"\"\"\n    Runs the continuous reporting loop.\n\n    This asynchronous function performs periodic reporting tasks such as reporting resource usage,\n    data queue contents, and, optionally, status updates to the controller. The loop runs indefinitely,\n    updating the counter with each cycle to track the frequency of specific tasks.\n\n    Key Actions:\n        - Regularly reports the resource status.\n        - Reloads the configuration file every 50 cycles to reflect any updates.\n\n    Notes:\n        - The reporting frequency is determined by the 'report_frequency' setting in the config file.\n    \"\"\"\n    while self._running.is_set():\n        if self.config.participant[\"reporter_args\"][\"report_status_data_queue\"]:\n            if self.config.participant[\"scenario_args\"][\"controller\"] != \"nebula-test\":\n                await self.__report_status_to_controller()\n            await self.__report_data_queue()\n        await self.__report_resources()\n        self.counter += 1\n        if self.counter % 50 == 0:\n            logging.info(\"Reloading config file...\")\n            self.cm.engine.config.reload_config_file()\n        await asyncio.sleep(self.frequency)\n</code></pre>"},{"location":"api/addons/reporter/#nebula.addons.reporter.Reporter.start","title":"<code>start()</code>  <code>async</code>","text":"<p>Starts the reporter module after a grace period.</p> <p>This asynchronous function initiates the reporting process following a designated grace period. It creates a background task to run the reporting loop, allowing data to be reported at defined intervals.</p> <p>Returns:</p> Type Description <p>asyncio.Task: The task for the reporter loop, which handles the data reporting asynchronously.</p> Notes <ul> <li>The grace period allows for a delay before the first reporting cycle.</li> <li>The reporter loop runs in the background, ensuring continuous data updates.</li> </ul> Source code in <code>nebula/addons/reporter.py</code> <pre><code>async def start(self):\n    \"\"\"\n    Starts the reporter module after a grace period.\n\n    This asynchronous function initiates the reporting process following a designated grace period.\n    It creates a background task to run the reporting loop, allowing data to be reported at defined intervals.\n\n    Returns:\n        asyncio.Task: The task for the reporter loop, which handles the data reporting asynchronously.\n\n    Notes:\n        - The grace period allows for a delay before the first reporting cycle.\n        - The reporter loop runs in the background, ensuring continuous data updates.\n    \"\"\"\n    self._running.set()\n    await asyncio.sleep(self.grace_time)\n    self._reporter_task = asyncio.create_task(self.run_reporter(), name=\"Reporter_run_reporter\")\n    return self._reporter_task\n</code></pre>"},{"location":"api/addons/topologymanager/","title":"Documentation for Topologymanager Module","text":""},{"location":"api/addons/topologymanager/#nebula.addons.topologymanager.TopologyManager","title":"<code>TopologyManager</code>","text":"Source code in <code>nebula/addons/topologymanager.py</code> <pre><code>class TopologyManager:\n    def __init__(\n        self,\n        scenario_name=None,\n        n_nodes=5,\n        b_symmetric=True,\n        undirected_neighbor_num=5,\n        topology=None,\n    ):\n        \"\"\"\n        Initializes a network topology for the scenario.\n\n        This constructor sets up a network topology with a given number of nodes, neighbors, and other parameters.\n        It includes options to specify whether the topology should be symmetric and the number of undirected neighbors for each node.\n        It also checks for constraints on the number of neighbors and the structure of the network.\n\n        Parameters:\n            - scenario_name (str, optional): Name of the scenario.\n            - n_nodes (int): Number of nodes in the network (default 5).\n            - b_symmetric (bool): Whether the topology is symmetric (default True).\n            - undirected_neighbor_num (int): Number of undirected neighbors for each node (default 5).\n            - topology (list, optional): Predefined topology, a list of nodes and connections (default None).\n\n        Raises:\n            - ValueError: If `undirected_neighbor_num` is less than 2.\n\n        Attributes:\n            - scenario_name (str): Name of the scenario.\n            - n_nodes (int): Number of nodes in the network.\n            - b_symmetric (bool): Whether the topology is symmetric.\n            - undirected_neighbor_num (int): Number of undirected neighbors.\n            - topology (list): Topology of the network.\n            - nodes (np.ndarray): Array of nodes initialized with zeroes.\n            - b_fully_connected (bool): Flag indicating if the topology is fully connected.\n        \"\"\"\n        self.scenario_name = scenario_name\n        if topology is None:\n            topology = []\n        self.n_nodes = n_nodes\n        self.b_symmetric = b_symmetric\n        self.undirected_neighbor_num = undirected_neighbor_num\n        self.topology = topology\n        # Initialize nodes with array of tuples (0,0,0) with size n_nodes\n        self.nodes = np.zeros((n_nodes, 3), dtype=np.int32)\n\n        self.b_fully_connected = False\n        if self.undirected_neighbor_num &lt; 2:\n            raise ValueError(\"undirected_neighbor_num must be greater than 2\")  # noqa: TRY003\n        # If the number of neighbors is larger than the number of nodes, then the topology is fully connected\n        if self.undirected_neighbor_num &gt;= self.n_nodes - 1 and self.b_symmetric:\n            self.b_fully_connected = True\n\n    def __getstate__(self):\n        \"\"\"\n        Serializes the object state for saving.\n\n        This method defines which attributes of the class should be serialized when the object is pickled (saved to a file).\n        It returns a dictionary containing the attributes that need to be preserved.\n\n        Returns:\n            dict: A dictionary containing the relevant attributes of the object for serialization.\n                - scenario_name (str): Name of the scenario.\n                - n_nodes (int): Number of nodes in the network.\n                - topology (list): Topology of the network.\n                - nodes (np.ndarray): Array of nodes in the network.\n        \"\"\"\n        # Return the attributes of the class that should be serialized\n        return {\n            \"scenario_name\": self.scenario_name,\n            \"n_nodes\": self.n_nodes,\n            \"topology\": self.topology,\n            \"nodes\": self.nodes,\n        }\n\n    def __setstate__(self, state):\n        \"\"\"\n        Restores the object state from the serialized data.\n\n        This method is called during deserialization (unpickling) to restore the object's state\n        by setting the attributes using the provided state dictionary.\n\n        Args:\n            state (dict): A dictionary containing the serialized data, including:\n                - scenario_name (str): Name of the scenario.\n                - n_nodes (int): Number of nodes in the network.\n                - topology (list): Topology of the network.\n                - nodes (np.ndarray): Array of nodes in the network.\n        \"\"\"\n        # Set the attributes of the class from the serialized state\n        self.scenario_name = state[\"scenario_name\"]\n        self.n_nodes = state[\"n_nodes\"]\n        self.topology = state[\"topology\"]\n        self.nodes = state[\"nodes\"]\n\n    def get_node_color(self, role):\n        \"\"\"\n        Returns the color associated with a given role.\n\n        The method maps roles to specific colors for visualization or representation purposes.\n\n        Args:\n            role (Role): The role for which the color is to be determined.\n\n        Returns:\n            str: The color associated with the given role. Defaults to \"red\" if the role is not recognized.\n        \"\"\"\n        role_colors = {\n            Role.AGGREGATOR: \"orange\",\n            Role.SERVER: \"green\",\n            Role.TRAINER: \"#6182bd\",\n            Role.PROXY: \"purple\",\n        }\n        return role_colors.get(role, \"red\")\n\n    def add_legend(self, roles):\n        \"\"\"\n        Adds a legend to the plot for different roles, associating each role with a color.\n\n        The method iterates through the provided roles and assigns the corresponding color to each one.\n        The colors are predefined in the legend_map, which associates each role with a specific color.\n\n        Args:\n            roles (iterable): A collection of roles for which the legend should be displayed.\n\n        Returns:\n            None: The function modifies the plot directly by adding the legend.\n        \"\"\"\n        legend_map = {\n            Role.AGGREGATOR: \"orange\",\n            Role.SERVER: \"green\",\n            Role.TRAINER: \"#6182bd\",\n            Role.PROXY: \"purple\",\n            Role.IDLE: \"red\",\n        }\n        for role, color in legend_map.items():\n            if role in roles:\n                plt.scatter([], [], c=color, label=role)\n        plt.legend()\n\n    def draw_graph(self, plot=False, path=None):\n        \"\"\"\n        Draws the network graph based on the topology and saves it as an image.\n\n        This method generates a visualization of the network's topology using NetworkX and Matplotlib.\n        It assigns colors to the nodes based on their role, draws the network's nodes and edges,\n        adds labels to the nodes, and includes a legend for clarity.\n        The resulting plot is saved as an image file.\n\n        Args:\n            plot (bool, optional): Whether to display the plot. Default is False.\n            path (str, optional): The file path where the image will be saved. If None, the image is saved\n                                  to a default location based on the scenario name.\n\n        Returns:\n            None: The method saves the plot as an image at the specified path.\n        \"\"\"\n        g = nx.from_numpy_array(self.topology)\n        pos = nx.spring_layout(g, k=0.15, iterations=20, seed=42)\n\n        fig = plt.figure(num=\"Network topology\", dpi=100, figsize=(6, 6), frameon=False)\n        ax = fig.add_axes([0, 0, 1, 1])\n        ax.set_xlim([-1.3, 1.3])\n        ax.set_ylim([-1.3, 1.3])\n        labels = {}\n        color_map = []\n        for k in range(self.n_nodes):\n            role = str(self.nodes[k][2])\n            color_map.append(self.get_node_color(role))\n            labels[k] = f\"P{k}\\n\" + str(self.nodes[k][0]) + \":\" + str(self.nodes[k][1])\n\n        nx.draw_networkx_nodes(g, pos, node_color=color_map, linewidths=2)\n        nx.draw_networkx_labels(g, pos, labels, font_size=10, font_weight=\"bold\")\n        nx.draw_networkx_edges(g, pos, width=2)\n\n        self.add_legend([str(node[2]) for node in self.nodes])\n        plt.savefig(f\"{path}\", dpi=100, bbox_inches=\"tight\", pad_inches=0)\n        plt.close()\n\n    def generate_topology(self):\n        \"\"\"\n        Generates the network topology based on the configured settings.\n\n        This method generates the network topology for the given scenario. It checks whether the topology\n        should be fully connected, symmetric, or asymmetric and then generates the network accordingly.\n\n        - If the topology is fully connected, all nodes will be directly connected to each other.\n        - If the topology is symmetric, neighbors will be chosen symmetrically between nodes.\n        - If the topology is asymmetric, neighbors will be picked randomly without symmetry.\n\n        Returns:\n            None: The method modifies the internal topology of the network.\n        \"\"\"\n        if self.b_fully_connected:\n            self.__fully_connected()\n            return\n\n        if self.topology is not None and len(self.topology) &gt; 0:\n            # Topology was already provided\n            return\n\n        if self.b_symmetric:\n            self.__randomly_pick_neighbors_symmetric()\n        else:\n            self.__randomly_pick_neighbors_asymmetric()\n\n    def generate_server_topology(self):\n        \"\"\"\n        Generates a server topology where the first node (usually the server) is connected to all other nodes.\n\n        This method initializes a topology matrix where the first node (typically the server) is connected to\n        every other node in the network. The first row and the first column of the matrix are set to 1, representing\n        connections to and from the server. The diagonal is set to 0 to indicate that no node is connected to itself.\n\n        Returns:\n            None: The method modifies the internal `self.topology` matrix.\n        \"\"\"\n        self.topology = np.zeros((self.n_nodes, self.n_nodes), dtype=np.float32)\n        self.topology[0, :] = 1\n        self.topology[:, 0] = 1\n        np.fill_diagonal(self.topology, 0)\n\n    def generate_ring_topology(self, increase_convergence=False):\n        \"\"\"\n        Generates a ring topology for the network.\n\n        In a ring topology, each node is connected to two other nodes in a circular fashion, forming a closed loop.\n        This method uses a private method to generate the topology, with an optional parameter to control whether\n        the convergence speed of the network should be increased.\n\n        Args:\n            increase_convergence (bool): Optional flag to increase the convergence speed in the topology.\n                                          Defaults to False.\n\n        Returns:\n            None: The method modifies the internal `self.topology` matrix to reflect the generated ring topology.\n        \"\"\"\n        self.__ring_topology(increase_convergence=increase_convergence)\n\n    def generate_random_topology(self, probability):\n        \"\"\"\n        Generates a random topology using Erdos-Renyi model with given probability.\n\n        Args:\n            probability (float): Probability of edge creation between any two nodes (0-1)\n\n        Returns:\n            None: Updates self.topology with the generated random topology\n        \"\"\"\n        random_graph = nx.erdos_renyi_graph(self.n_nodes, probability)\n        self.topology = nx.to_numpy_array(random_graph, dtype=np.float32)\n        np.fill_diagonal(self.topology, 0)  # No self-loops\n\n    @staticmethod\n    def get_coordinates(random_geo=True):\n        \"\"\"\n        Generates random geographical coordinates within predefined bounds for either Spain or Switzerland.\n\n        The method returns a random geographical coordinate (latitude, longitude). The bounds for random coordinates are\n        defined for two regions: Spain and Switzerland. The region is chosen randomly, and then the latitude and longitude\n        are selected within the corresponding bounds.\n\n        Parameters:\n            random_geo (bool): If set to True, the method generates random coordinates within the predefined bounds\n                                for Spain or Switzerland. If set to False, this method could be modified to return fixed\n                                coordinates.\n\n        Returns:\n            tuple: A tuple containing the latitude and longitude of the generated point.\n        \"\"\"\n        if random_geo:\n            #  Espa\u00f1a min_lat, max_lat, min_lon, max_lon                  Suiza min_lat, max_lat, min_lon, max_lon\n            bounds = (36.0, 43.0, -9.0, 3.3) if random.randint(0, 1) == 0 else (45.8, 47.8, 5.9, 10.5)  # noqa: S311\n\n            min_latitude, max_latitude, min_longitude, max_longitude = bounds\n            latitude = random.uniform(min_latitude, max_latitude)  # noqa: S311\n            longitude = random.uniform(min_longitude, max_longitude)  # noqa: S311\n\n            return latitude, longitude\n\n    def add_nodes(self, nodes):\n        \"\"\"\n        Sets the nodes of the topology.\n\n        This method updates the `nodes` attribute with the given list or array of nodes.\n\n        Parameters:\n            nodes (array-like): The new set of nodes to be assigned to the topology. It should be in a format compatible\n                                 with the existing `nodes` structure, typically an array or list.\n\n        Returns:\n            None\n        \"\"\"\n        self.nodes = nodes\n\n    def update_nodes(self, config_participants):\n        \"\"\"\n        Updates the nodes of the topology based on the provided configuration.\n\n        This method assigns a new set of nodes to the `nodes` attribute, typically based on the configuration of the participants.\n\n        Parameters:\n            config_participants (array-like): A new set of nodes, usually derived from the participants' configuration, to be assigned to the topology.\n\n        Returns:\n            None\n        \"\"\"\n        self.nodes = config_participants\n\n    def get_neighbors_string(self, node_idx):\n        \"\"\"\n        Retrieves the neighbors of a given node as a string representation.\n\n        This method checks the `topology` attribute to find the neighbors of the node at the specified index (`node_idx`). It then returns a string that lists the coordinates of each neighbor.\n\n        Parameters:\n            node_idx (int): The index of the node for which neighbors are to be retrieved.\n\n        Returns:\n            str: A space-separated string of neighbors' coordinates in the format \"latitude:longitude\".\n        \"\"\"\n        logging.info(f\"Getting neighbors for node {node_idx}\")\n        logging.info(f\"Topology shape: {self.topology.shape}\")\n\n        neighbors_data = []\n        for i in range(self.n_nodes):\n            if self.topology[node_idx][i] == 1:\n                neighbors_data.append(self.nodes[i])\n                logging.info(f\"Found neighbor at index {i}: {self.nodes[i]}\")\n\n        neighbors_data_strings = [f\"{i[0]}:{i[1]}\" for i in neighbors_data]\n        neighbors_data_string = \" \".join(neighbors_data_strings)\n        logging.info(f\"Neighbors of node participant_{node_idx}: {neighbors_data_string}\")\n        return neighbors_data_string\n\n    def __ring_topology(self, increase_convergence=False):\n        \"\"\"\n        Generates a ring topology for the nodes.\n\n        This method creates a ring topology for the network using the Watts-Strogatz model. Each node is connected to two neighbors, forming a ring. Optionally, additional random connections are added to increase convergence, making the network more connected.\n\n        Parameters:\n            increase_convergence (bool): If set to True, random connections will be added between nodes to increase the network's connectivity.\n\n        Returns:\n            None: The `topology` attribute of the class is updated with the generated ring topology.\n        \"\"\"\n        topology_ring = np.array(\n            nx.to_numpy_matrix(nx.watts_strogatz_graph(self.n_nodes, 2, 0)),\n            dtype=np.float32,\n        )\n\n        if increase_convergence:\n            # Create random links between nodes in topology_ring\n            for i in range(self.n_nodes):\n                for j in range(self.n_nodes):\n                    if topology_ring[i][j] == 0 and random.random() &lt; 0.1:  # noqa: S311\n                        topology_ring[i][j] = 1\n                        topology_ring[j][i] = 1\n\n        np.fill_diagonal(topology_ring, 0)\n        self.topology = topology_ring\n\n    def __randomly_pick_neighbors_symmetric(self):\n        \"\"\"\n        Generates a symmetric random topology by combining a ring topology with additional random links.\n\n        This method first creates a ring topology using the Watts-Strogatz model, where each node is connected to two neighbors. Then, it randomly adds links to each node (up to the specified number of neighbors) to form a symmetric topology. The result is a topology where each node has a fixed number of undirected neighbors, and the connections are symmetric between nodes.\n\n        Parameters:\n            None\n\n        Returns:\n            None: The `topology` attribute of the class is updated with the generated symmetric topology.\n        \"\"\"\n        # First generate a ring topology\n        topology_ring = np.array(\n            nx.to_numpy_matrix(nx.watts_strogatz_graph(self.n_nodes, 2, 0)),\n            dtype=np.float32,\n        )\n\n        np.fill_diagonal(topology_ring, 0)\n\n        # After, randomly add some links for each node (symmetric)\n        # If undirected_neighbor_num is X, then each node has X links to other nodes\n        k = int(self.undirected_neighbor_num)\n        topology_random_link = np.array(\n            nx.to_numpy_matrix(nx.watts_strogatz_graph(self.n_nodes, k, 0)),\n            dtype=np.float32,\n        )\n\n        # generate symmetric topology\n        topology_symmetric = topology_ring.copy()\n        for i in range(self.n_nodes):\n            for j in range(self.n_nodes):\n                if topology_symmetric[i][j] == 0 and topology_random_link[i][j] == 1:\n                    topology_symmetric[i][j] = topology_random_link[i][j]\n\n        np.fill_diagonal(topology_symmetric, 0)\n\n        self.topology = topology_symmetric\n\n    def __randomly_pick_neighbors_asymmetric(self):\n        \"\"\"\n        Generates an asymmetric random topology by combining a ring topology with additional random links and random deletions.\n\n        This method first creates a ring topology using the Watts-Strogatz model, where each node is connected to two neighbors. Then, it randomly adds links to each node to create a topology with a specified number of undirected neighbors. After that, it randomly deletes some of the links to introduce asymmetry. The result is a topology where nodes have a varying number of directed and undirected links, and the structure is asymmetric.\n\n        Parameters:\n            None\n\n        Returns:\n            None: The `topology` attribute of the class is updated with the generated asymmetric topology.\n        \"\"\"\n        # randomly add some links for each node (symmetric)\n        k = self.undirected_neighbor_num\n        topology_random_link = np.array(\n            nx.to_numpy_matrix(nx.watts_strogatz_graph(self.n_nodes, k, 0)),\n            dtype=np.float32,\n        )\n\n        np.fill_diagonal(topology_random_link, 0)\n\n        # first generate a ring topology\n        topology_ring = np.array(\n            nx.to_numpy_matrix(nx.watts_strogatz_graph(self.n_nodes, 2, 0)),\n            dtype=np.float32,\n        )\n\n        np.fill_diagonal(topology_ring, 0)\n\n        for i in range(self.n_nodes):\n            for j in range(self.n_nodes):\n                if topology_ring[i][j] == 0 and topology_random_link[i][j] == 1:\n                    topology_ring[i][j] = topology_random_link[i][j]\n\n        np.fill_diagonal(topology_ring, 0)\n\n        # randomly delete some links\n        out_link_set = set()\n        for i in range(self.n_nodes):\n            len_row_zero = 0\n            for j in range(self.n_nodes):\n                if topology_ring[i][j] == 0:\n                    len_row_zero += 1\n            random_selection = np.random.randint(2, size=len_row_zero)\n            index_of_zero = 0\n            for j in range(self.n_nodes):\n                out_link = j * self.n_nodes + i\n                if topology_ring[i][j] == 0:\n                    if random_selection[index_of_zero] == 1 and out_link not in out_link_set:\n                        topology_ring[i][j] = 1\n                        out_link_set.add(i * self.n_nodes + j)\n                    index_of_zero += 1\n\n        np.fill_diagonal(topology_ring, 0)\n\n        self.topology = topology_ring\n\n    def __fully_connected(self):\n        \"\"\"\n        Generates a fully connected topology where each node is connected to every other node.\n\n        This method creates a fully connected network by generating a Watts-Strogatz graph with the number of nodes set to `n_nodes` and the number of neighbors set to `n_nodes - 1`. The resulting graph is then converted into a numpy matrix and all missing links (i.e., non-ones in the adjacency matrix) are set to 1 to ensure complete connectivity. The diagonal elements are filled with zeros to avoid self-loops.\n\n        Parameters:\n            None\n\n        Returns:\n            None: The `topology` attribute of the class is updated with the generated fully connected topology.\n        \"\"\"\n        topology_fully_connected = np.array(\n            nx.to_numpy_matrix(nx.watts_strogatz_graph(self.n_nodes, self.n_nodes - 1, 0)),\n            dtype=np.float32,\n        )\n\n        np.fill_diagonal(topology_fully_connected, 0)\n\n        for i in range(self.n_nodes):\n            for j in range(self.n_nodes):\n                if topology_fully_connected[i][j] != 1:\n                    topology_fully_connected[i][j] = 1\n\n        np.fill_diagonal(topology_fully_connected, 0)\n\n        self.topology = topology_fully_connected\n</code></pre>"},{"location":"api/addons/topologymanager/#nebula.addons.topologymanager.TopologyManager.__fully_connected","title":"<code>__fully_connected()</code>","text":"<p>Generates a fully connected topology where each node is connected to every other node.</p> <p>This method creates a fully connected network by generating a Watts-Strogatz graph with the number of nodes set to <code>n_nodes</code> and the number of neighbors set to <code>n_nodes - 1</code>. The resulting graph is then converted into a numpy matrix and all missing links (i.e., non-ones in the adjacency matrix) are set to 1 to ensure complete connectivity. The diagonal elements are filled with zeros to avoid self-loops.</p> <p>Returns:</p> Name Type Description <code>None</code> <p>The <code>topology</code> attribute of the class is updated with the generated fully connected topology.</p> Source code in <code>nebula/addons/topologymanager.py</code> <pre><code>def __fully_connected(self):\n    \"\"\"\n    Generates a fully connected topology where each node is connected to every other node.\n\n    This method creates a fully connected network by generating a Watts-Strogatz graph with the number of nodes set to `n_nodes` and the number of neighbors set to `n_nodes - 1`. The resulting graph is then converted into a numpy matrix and all missing links (i.e., non-ones in the adjacency matrix) are set to 1 to ensure complete connectivity. The diagonal elements are filled with zeros to avoid self-loops.\n\n    Parameters:\n        None\n\n    Returns:\n        None: The `topology` attribute of the class is updated with the generated fully connected topology.\n    \"\"\"\n    topology_fully_connected = np.array(\n        nx.to_numpy_matrix(nx.watts_strogatz_graph(self.n_nodes, self.n_nodes - 1, 0)),\n        dtype=np.float32,\n    )\n\n    np.fill_diagonal(topology_fully_connected, 0)\n\n    for i in range(self.n_nodes):\n        for j in range(self.n_nodes):\n            if topology_fully_connected[i][j] != 1:\n                topology_fully_connected[i][j] = 1\n\n    np.fill_diagonal(topology_fully_connected, 0)\n\n    self.topology = topology_fully_connected\n</code></pre>"},{"location":"api/addons/topologymanager/#nebula.addons.topologymanager.TopologyManager.__getstate__","title":"<code>__getstate__()</code>","text":"<p>Serializes the object state for saving.</p> <p>This method defines which attributes of the class should be serialized when the object is pickled (saved to a file). It returns a dictionary containing the attributes that need to be preserved.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the relevant attributes of the object for serialization. - scenario_name (str): Name of the scenario. - n_nodes (int): Number of nodes in the network. - topology (list): Topology of the network. - nodes (np.ndarray): Array of nodes in the network.</p> Source code in <code>nebula/addons/topologymanager.py</code> <pre><code>def __getstate__(self):\n    \"\"\"\n    Serializes the object state for saving.\n\n    This method defines which attributes of the class should be serialized when the object is pickled (saved to a file).\n    It returns a dictionary containing the attributes that need to be preserved.\n\n    Returns:\n        dict: A dictionary containing the relevant attributes of the object for serialization.\n            - scenario_name (str): Name of the scenario.\n            - n_nodes (int): Number of nodes in the network.\n            - topology (list): Topology of the network.\n            - nodes (np.ndarray): Array of nodes in the network.\n    \"\"\"\n    # Return the attributes of the class that should be serialized\n    return {\n        \"scenario_name\": self.scenario_name,\n        \"n_nodes\": self.n_nodes,\n        \"topology\": self.topology,\n        \"nodes\": self.nodes,\n    }\n</code></pre>"},{"location":"api/addons/topologymanager/#nebula.addons.topologymanager.TopologyManager.__init__","title":"<code>__init__(scenario_name=None, n_nodes=5, b_symmetric=True, undirected_neighbor_num=5, topology=None)</code>","text":"<p>Initializes a network topology for the scenario.</p> <p>This constructor sets up a network topology with a given number of nodes, neighbors, and other parameters. It includes options to specify whether the topology should be symmetric and the number of undirected neighbors for each node. It also checks for constraints on the number of neighbors and the structure of the network.</p> <p>Parameters:</p> Name Type Description Default <code>-</code> <code>scenario_name (str</code> <p>Name of the scenario.</p> required <code>-</code> <code>n_nodes (int</code> <p>Number of nodes in the network (default 5).</p> required <code>-</code> <code>b_symmetric (bool</code> <p>Whether the topology is symmetric (default True).</p> required <code>-</code> <code>undirected_neighbor_num (int</code> <p>Number of undirected neighbors for each node (default 5).</p> required <code>-</code> <code>topology (list</code> <p>Predefined topology, a list of nodes and connections (default None).</p> required <p>Raises:</p> Type Description <code>-ValueError</code> <p>If <code>undirected_neighbor_num</code> is less than 2.</p> <p>Attributes:</p> Name Type Description <code>-</code> <code>scenario_name (str</code> <p>Name of the scenario.</p> <code>-</code> <code>n_nodes (int</code> <p>Number of nodes in the network.</p> <code>-</code> <code>b_symmetric (bool</code> <p>Whether the topology is symmetric.</p> <code>-</code> <code>undirected_neighbor_num (int</code> <p>Number of undirected neighbors.</p> <code>-</code> <code>topology (list</code> <p>Topology of the network.</p> <code>-</code> <code>nodes (np.ndarray</code> <p>Array of nodes initialized with zeroes.</p> <code>-</code> <code>b_fully_connected (bool</code> <p>Flag indicating if the topology is fully connected.</p> Source code in <code>nebula/addons/topologymanager.py</code> <pre><code>def __init__(\n    self,\n    scenario_name=None,\n    n_nodes=5,\n    b_symmetric=True,\n    undirected_neighbor_num=5,\n    topology=None,\n):\n    \"\"\"\n    Initializes a network topology for the scenario.\n\n    This constructor sets up a network topology with a given number of nodes, neighbors, and other parameters.\n    It includes options to specify whether the topology should be symmetric and the number of undirected neighbors for each node.\n    It also checks for constraints on the number of neighbors and the structure of the network.\n\n    Parameters:\n        - scenario_name (str, optional): Name of the scenario.\n        - n_nodes (int): Number of nodes in the network (default 5).\n        - b_symmetric (bool): Whether the topology is symmetric (default True).\n        - undirected_neighbor_num (int): Number of undirected neighbors for each node (default 5).\n        - topology (list, optional): Predefined topology, a list of nodes and connections (default None).\n\n    Raises:\n        - ValueError: If `undirected_neighbor_num` is less than 2.\n\n    Attributes:\n        - scenario_name (str): Name of the scenario.\n        - n_nodes (int): Number of nodes in the network.\n        - b_symmetric (bool): Whether the topology is symmetric.\n        - undirected_neighbor_num (int): Number of undirected neighbors.\n        - topology (list): Topology of the network.\n        - nodes (np.ndarray): Array of nodes initialized with zeroes.\n        - b_fully_connected (bool): Flag indicating if the topology is fully connected.\n    \"\"\"\n    self.scenario_name = scenario_name\n    if topology is None:\n        topology = []\n    self.n_nodes = n_nodes\n    self.b_symmetric = b_symmetric\n    self.undirected_neighbor_num = undirected_neighbor_num\n    self.topology = topology\n    # Initialize nodes with array of tuples (0,0,0) with size n_nodes\n    self.nodes = np.zeros((n_nodes, 3), dtype=np.int32)\n\n    self.b_fully_connected = False\n    if self.undirected_neighbor_num &lt; 2:\n        raise ValueError(\"undirected_neighbor_num must be greater than 2\")  # noqa: TRY003\n    # If the number of neighbors is larger than the number of nodes, then the topology is fully connected\n    if self.undirected_neighbor_num &gt;= self.n_nodes - 1 and self.b_symmetric:\n        self.b_fully_connected = True\n</code></pre>"},{"location":"api/addons/topologymanager/#nebula.addons.topologymanager.TopologyManager.__randomly_pick_neighbors_asymmetric","title":"<code>__randomly_pick_neighbors_asymmetric()</code>","text":"<p>Generates an asymmetric random topology by combining a ring topology with additional random links and random deletions.</p> <p>This method first creates a ring topology using the Watts-Strogatz model, where each node is connected to two neighbors. Then, it randomly adds links to each node to create a topology with a specified number of undirected neighbors. After that, it randomly deletes some of the links to introduce asymmetry. The result is a topology where nodes have a varying number of directed and undirected links, and the structure is asymmetric.</p> <p>Returns:</p> Name Type Description <code>None</code> <p>The <code>topology</code> attribute of the class is updated with the generated asymmetric topology.</p> Source code in <code>nebula/addons/topologymanager.py</code> <pre><code>def __randomly_pick_neighbors_asymmetric(self):\n    \"\"\"\n    Generates an asymmetric random topology by combining a ring topology with additional random links and random deletions.\n\n    This method first creates a ring topology using the Watts-Strogatz model, where each node is connected to two neighbors. Then, it randomly adds links to each node to create a topology with a specified number of undirected neighbors. After that, it randomly deletes some of the links to introduce asymmetry. The result is a topology where nodes have a varying number of directed and undirected links, and the structure is asymmetric.\n\n    Parameters:\n        None\n\n    Returns:\n        None: The `topology` attribute of the class is updated with the generated asymmetric topology.\n    \"\"\"\n    # randomly add some links for each node (symmetric)\n    k = self.undirected_neighbor_num\n    topology_random_link = np.array(\n        nx.to_numpy_matrix(nx.watts_strogatz_graph(self.n_nodes, k, 0)),\n        dtype=np.float32,\n    )\n\n    np.fill_diagonal(topology_random_link, 0)\n\n    # first generate a ring topology\n    topology_ring = np.array(\n        nx.to_numpy_matrix(nx.watts_strogatz_graph(self.n_nodes, 2, 0)),\n        dtype=np.float32,\n    )\n\n    np.fill_diagonal(topology_ring, 0)\n\n    for i in range(self.n_nodes):\n        for j in range(self.n_nodes):\n            if topology_ring[i][j] == 0 and topology_random_link[i][j] == 1:\n                topology_ring[i][j] = topology_random_link[i][j]\n\n    np.fill_diagonal(topology_ring, 0)\n\n    # randomly delete some links\n    out_link_set = set()\n    for i in range(self.n_nodes):\n        len_row_zero = 0\n        for j in range(self.n_nodes):\n            if topology_ring[i][j] == 0:\n                len_row_zero += 1\n        random_selection = np.random.randint(2, size=len_row_zero)\n        index_of_zero = 0\n        for j in range(self.n_nodes):\n            out_link = j * self.n_nodes + i\n            if topology_ring[i][j] == 0:\n                if random_selection[index_of_zero] == 1 and out_link not in out_link_set:\n                    topology_ring[i][j] = 1\n                    out_link_set.add(i * self.n_nodes + j)\n                index_of_zero += 1\n\n    np.fill_diagonal(topology_ring, 0)\n\n    self.topology = topology_ring\n</code></pre>"},{"location":"api/addons/topologymanager/#nebula.addons.topologymanager.TopologyManager.__randomly_pick_neighbors_symmetric","title":"<code>__randomly_pick_neighbors_symmetric()</code>","text":"<p>Generates a symmetric random topology by combining a ring topology with additional random links.</p> <p>This method first creates a ring topology using the Watts-Strogatz model, where each node is connected to two neighbors. Then, it randomly adds links to each node (up to the specified number of neighbors) to form a symmetric topology. The result is a topology where each node has a fixed number of undirected neighbors, and the connections are symmetric between nodes.</p> <p>Returns:</p> Name Type Description <code>None</code> <p>The <code>topology</code> attribute of the class is updated with the generated symmetric topology.</p> Source code in <code>nebula/addons/topologymanager.py</code> <pre><code>def __randomly_pick_neighbors_symmetric(self):\n    \"\"\"\n    Generates a symmetric random topology by combining a ring topology with additional random links.\n\n    This method first creates a ring topology using the Watts-Strogatz model, where each node is connected to two neighbors. Then, it randomly adds links to each node (up to the specified number of neighbors) to form a symmetric topology. The result is a topology where each node has a fixed number of undirected neighbors, and the connections are symmetric between nodes.\n\n    Parameters:\n        None\n\n    Returns:\n        None: The `topology` attribute of the class is updated with the generated symmetric topology.\n    \"\"\"\n    # First generate a ring topology\n    topology_ring = np.array(\n        nx.to_numpy_matrix(nx.watts_strogatz_graph(self.n_nodes, 2, 0)),\n        dtype=np.float32,\n    )\n\n    np.fill_diagonal(topology_ring, 0)\n\n    # After, randomly add some links for each node (symmetric)\n    # If undirected_neighbor_num is X, then each node has X links to other nodes\n    k = int(self.undirected_neighbor_num)\n    topology_random_link = np.array(\n        nx.to_numpy_matrix(nx.watts_strogatz_graph(self.n_nodes, k, 0)),\n        dtype=np.float32,\n    )\n\n    # generate symmetric topology\n    topology_symmetric = topology_ring.copy()\n    for i in range(self.n_nodes):\n        for j in range(self.n_nodes):\n            if topology_symmetric[i][j] == 0 and topology_random_link[i][j] == 1:\n                topology_symmetric[i][j] = topology_random_link[i][j]\n\n    np.fill_diagonal(topology_symmetric, 0)\n\n    self.topology = topology_symmetric\n</code></pre>"},{"location":"api/addons/topologymanager/#nebula.addons.topologymanager.TopologyManager.__ring_topology","title":"<code>__ring_topology(increase_convergence=False)</code>","text":"<p>Generates a ring topology for the nodes.</p> <p>This method creates a ring topology for the network using the Watts-Strogatz model. Each node is connected to two neighbors, forming a ring. Optionally, additional random connections are added to increase convergence, making the network more connected.</p> <p>Parameters:</p> Name Type Description Default <code>increase_convergence</code> <code>bool</code> <p>If set to True, random connections will be added between nodes to increase the network's connectivity.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>None</code> <p>The <code>topology</code> attribute of the class is updated with the generated ring topology.</p> Source code in <code>nebula/addons/topologymanager.py</code> <pre><code>def __ring_topology(self, increase_convergence=False):\n    \"\"\"\n    Generates a ring topology for the nodes.\n\n    This method creates a ring topology for the network using the Watts-Strogatz model. Each node is connected to two neighbors, forming a ring. Optionally, additional random connections are added to increase convergence, making the network more connected.\n\n    Parameters:\n        increase_convergence (bool): If set to True, random connections will be added between nodes to increase the network's connectivity.\n\n    Returns:\n        None: The `topology` attribute of the class is updated with the generated ring topology.\n    \"\"\"\n    topology_ring = np.array(\n        nx.to_numpy_matrix(nx.watts_strogatz_graph(self.n_nodes, 2, 0)),\n        dtype=np.float32,\n    )\n\n    if increase_convergence:\n        # Create random links between nodes in topology_ring\n        for i in range(self.n_nodes):\n            for j in range(self.n_nodes):\n                if topology_ring[i][j] == 0 and random.random() &lt; 0.1:  # noqa: S311\n                    topology_ring[i][j] = 1\n                    topology_ring[j][i] = 1\n\n    np.fill_diagonal(topology_ring, 0)\n    self.topology = topology_ring\n</code></pre>"},{"location":"api/addons/topologymanager/#nebula.addons.topologymanager.TopologyManager.__setstate__","title":"<code>__setstate__(state)</code>","text":"<p>Restores the object state from the serialized data.</p> <p>This method is called during deserialization (unpickling) to restore the object's state by setting the attributes using the provided state dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>A dictionary containing the serialized data, including: - scenario_name (str): Name of the scenario. - n_nodes (int): Number of nodes in the network. - topology (list): Topology of the network. - nodes (np.ndarray): Array of nodes in the network.</p> required Source code in <code>nebula/addons/topologymanager.py</code> <pre><code>def __setstate__(self, state):\n    \"\"\"\n    Restores the object state from the serialized data.\n\n    This method is called during deserialization (unpickling) to restore the object's state\n    by setting the attributes using the provided state dictionary.\n\n    Args:\n        state (dict): A dictionary containing the serialized data, including:\n            - scenario_name (str): Name of the scenario.\n            - n_nodes (int): Number of nodes in the network.\n            - topology (list): Topology of the network.\n            - nodes (np.ndarray): Array of nodes in the network.\n    \"\"\"\n    # Set the attributes of the class from the serialized state\n    self.scenario_name = state[\"scenario_name\"]\n    self.n_nodes = state[\"n_nodes\"]\n    self.topology = state[\"topology\"]\n    self.nodes = state[\"nodes\"]\n</code></pre>"},{"location":"api/addons/topologymanager/#nebula.addons.topologymanager.TopologyManager.add_legend","title":"<code>add_legend(roles)</code>","text":"<p>Adds a legend to the plot for different roles, associating each role with a color.</p> <p>The method iterates through the provided roles and assigns the corresponding color to each one. The colors are predefined in the legend_map, which associates each role with a specific color.</p> <p>Parameters:</p> Name Type Description Default <code>roles</code> <code>iterable</code> <p>A collection of roles for which the legend should be displayed.</p> required <p>Returns:</p> Name Type Description <code>None</code> <p>The function modifies the plot directly by adding the legend.</p> Source code in <code>nebula/addons/topologymanager.py</code> <pre><code>def add_legend(self, roles):\n    \"\"\"\n    Adds a legend to the plot for different roles, associating each role with a color.\n\n    The method iterates through the provided roles and assigns the corresponding color to each one.\n    The colors are predefined in the legend_map, which associates each role with a specific color.\n\n    Args:\n        roles (iterable): A collection of roles for which the legend should be displayed.\n\n    Returns:\n        None: The function modifies the plot directly by adding the legend.\n    \"\"\"\n    legend_map = {\n        Role.AGGREGATOR: \"orange\",\n        Role.SERVER: \"green\",\n        Role.TRAINER: \"#6182bd\",\n        Role.PROXY: \"purple\",\n        Role.IDLE: \"red\",\n    }\n    for role, color in legend_map.items():\n        if role in roles:\n            plt.scatter([], [], c=color, label=role)\n    plt.legend()\n</code></pre>"},{"location":"api/addons/topologymanager/#nebula.addons.topologymanager.TopologyManager.add_nodes","title":"<code>add_nodes(nodes)</code>","text":"<p>Sets the nodes of the topology.</p> <p>This method updates the <code>nodes</code> attribute with the given list or array of nodes.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>array - like</code> <p>The new set of nodes to be assigned to the topology. It should be in a format compatible                  with the existing <code>nodes</code> structure, typically an array or list.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>nebula/addons/topologymanager.py</code> <pre><code>def add_nodes(self, nodes):\n    \"\"\"\n    Sets the nodes of the topology.\n\n    This method updates the `nodes` attribute with the given list or array of nodes.\n\n    Parameters:\n        nodes (array-like): The new set of nodes to be assigned to the topology. It should be in a format compatible\n                             with the existing `nodes` structure, typically an array or list.\n\n    Returns:\n        None\n    \"\"\"\n    self.nodes = nodes\n</code></pre>"},{"location":"api/addons/topologymanager/#nebula.addons.topologymanager.TopologyManager.draw_graph","title":"<code>draw_graph(plot=False, path=None)</code>","text":"<p>Draws the network graph based on the topology and saves it as an image.</p> <p>This method generates a visualization of the network's topology using NetworkX and Matplotlib. It assigns colors to the nodes based on their role, draws the network's nodes and edges, adds labels to the nodes, and includes a legend for clarity. The resulting plot is saved as an image file.</p> <p>Parameters:</p> Name Type Description Default <code>plot</code> <code>bool</code> <p>Whether to display the plot. Default is False.</p> <code>False</code> <code>path</code> <code>str</code> <p>The file path where the image will be saved. If None, the image is saved                   to a default location based on the scenario name.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>None</code> <p>The method saves the plot as an image at the specified path.</p> Source code in <code>nebula/addons/topologymanager.py</code> <pre><code>def draw_graph(self, plot=False, path=None):\n    \"\"\"\n    Draws the network graph based on the topology and saves it as an image.\n\n    This method generates a visualization of the network's topology using NetworkX and Matplotlib.\n    It assigns colors to the nodes based on their role, draws the network's nodes and edges,\n    adds labels to the nodes, and includes a legend for clarity.\n    The resulting plot is saved as an image file.\n\n    Args:\n        plot (bool, optional): Whether to display the plot. Default is False.\n        path (str, optional): The file path where the image will be saved. If None, the image is saved\n                              to a default location based on the scenario name.\n\n    Returns:\n        None: The method saves the plot as an image at the specified path.\n    \"\"\"\n    g = nx.from_numpy_array(self.topology)\n    pos = nx.spring_layout(g, k=0.15, iterations=20, seed=42)\n\n    fig = plt.figure(num=\"Network topology\", dpi=100, figsize=(6, 6), frameon=False)\n    ax = fig.add_axes([0, 0, 1, 1])\n    ax.set_xlim([-1.3, 1.3])\n    ax.set_ylim([-1.3, 1.3])\n    labels = {}\n    color_map = []\n    for k in range(self.n_nodes):\n        role = str(self.nodes[k][2])\n        color_map.append(self.get_node_color(role))\n        labels[k] = f\"P{k}\\n\" + str(self.nodes[k][0]) + \":\" + str(self.nodes[k][1])\n\n    nx.draw_networkx_nodes(g, pos, node_color=color_map, linewidths=2)\n    nx.draw_networkx_labels(g, pos, labels, font_size=10, font_weight=\"bold\")\n    nx.draw_networkx_edges(g, pos, width=2)\n\n    self.add_legend([str(node[2]) for node in self.nodes])\n    plt.savefig(f\"{path}\", dpi=100, bbox_inches=\"tight\", pad_inches=0)\n    plt.close()\n</code></pre>"},{"location":"api/addons/topologymanager/#nebula.addons.topologymanager.TopologyManager.generate_random_topology","title":"<code>generate_random_topology(probability)</code>","text":"<p>Generates a random topology using Erdos-Renyi model with given probability.</p> <p>Parameters:</p> Name Type Description Default <code>probability</code> <code>float</code> <p>Probability of edge creation between any two nodes (0-1)</p> required <p>Returns:</p> Name Type Description <code>None</code> <p>Updates self.topology with the generated random topology</p> Source code in <code>nebula/addons/topologymanager.py</code> <pre><code>def generate_random_topology(self, probability):\n    \"\"\"\n    Generates a random topology using Erdos-Renyi model with given probability.\n\n    Args:\n        probability (float): Probability of edge creation between any two nodes (0-1)\n\n    Returns:\n        None: Updates self.topology with the generated random topology\n    \"\"\"\n    random_graph = nx.erdos_renyi_graph(self.n_nodes, probability)\n    self.topology = nx.to_numpy_array(random_graph, dtype=np.float32)\n    np.fill_diagonal(self.topology, 0)  # No self-loops\n</code></pre>"},{"location":"api/addons/topologymanager/#nebula.addons.topologymanager.TopologyManager.generate_ring_topology","title":"<code>generate_ring_topology(increase_convergence=False)</code>","text":"<p>Generates a ring topology for the network.</p> <p>In a ring topology, each node is connected to two other nodes in a circular fashion, forming a closed loop. This method uses a private method to generate the topology, with an optional parameter to control whether the convergence speed of the network should be increased.</p> <p>Parameters:</p> Name Type Description Default <code>increase_convergence</code> <code>bool</code> <p>Optional flag to increase the convergence speed in the topology.                           Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>None</code> <p>The method modifies the internal <code>self.topology</code> matrix to reflect the generated ring topology.</p> Source code in <code>nebula/addons/topologymanager.py</code> <pre><code>def generate_ring_topology(self, increase_convergence=False):\n    \"\"\"\n    Generates a ring topology for the network.\n\n    In a ring topology, each node is connected to two other nodes in a circular fashion, forming a closed loop.\n    This method uses a private method to generate the topology, with an optional parameter to control whether\n    the convergence speed of the network should be increased.\n\n    Args:\n        increase_convergence (bool): Optional flag to increase the convergence speed in the topology.\n                                      Defaults to False.\n\n    Returns:\n        None: The method modifies the internal `self.topology` matrix to reflect the generated ring topology.\n    \"\"\"\n    self.__ring_topology(increase_convergence=increase_convergence)\n</code></pre>"},{"location":"api/addons/topologymanager/#nebula.addons.topologymanager.TopologyManager.generate_server_topology","title":"<code>generate_server_topology()</code>","text":"<p>Generates a server topology where the first node (usually the server) is connected to all other nodes.</p> <p>This method initializes a topology matrix where the first node (typically the server) is connected to every other node in the network. The first row and the first column of the matrix are set to 1, representing connections to and from the server. The diagonal is set to 0 to indicate that no node is connected to itself.</p> <p>Returns:</p> Name Type Description <code>None</code> <p>The method modifies the internal <code>self.topology</code> matrix.</p> Source code in <code>nebula/addons/topologymanager.py</code> <pre><code>def generate_server_topology(self):\n    \"\"\"\n    Generates a server topology where the first node (usually the server) is connected to all other nodes.\n\n    This method initializes a topology matrix where the first node (typically the server) is connected to\n    every other node in the network. The first row and the first column of the matrix are set to 1, representing\n    connections to and from the server. The diagonal is set to 0 to indicate that no node is connected to itself.\n\n    Returns:\n        None: The method modifies the internal `self.topology` matrix.\n    \"\"\"\n    self.topology = np.zeros((self.n_nodes, self.n_nodes), dtype=np.float32)\n    self.topology[0, :] = 1\n    self.topology[:, 0] = 1\n    np.fill_diagonal(self.topology, 0)\n</code></pre>"},{"location":"api/addons/topologymanager/#nebula.addons.topologymanager.TopologyManager.generate_topology","title":"<code>generate_topology()</code>","text":"<p>Generates the network topology based on the configured settings.</p> <p>This method generates the network topology for the given scenario. It checks whether the topology should be fully connected, symmetric, or asymmetric and then generates the network accordingly.</p> <ul> <li>If the topology is fully connected, all nodes will be directly connected to each other.</li> <li>If the topology is symmetric, neighbors will be chosen symmetrically between nodes.</li> <li>If the topology is asymmetric, neighbors will be picked randomly without symmetry.</li> </ul> <p>Returns:</p> Name Type Description <code>None</code> <p>The method modifies the internal topology of the network.</p> Source code in <code>nebula/addons/topologymanager.py</code> <pre><code>def generate_topology(self):\n    \"\"\"\n    Generates the network topology based on the configured settings.\n\n    This method generates the network topology for the given scenario. It checks whether the topology\n    should be fully connected, symmetric, or asymmetric and then generates the network accordingly.\n\n    - If the topology is fully connected, all nodes will be directly connected to each other.\n    - If the topology is symmetric, neighbors will be chosen symmetrically between nodes.\n    - If the topology is asymmetric, neighbors will be picked randomly without symmetry.\n\n    Returns:\n        None: The method modifies the internal topology of the network.\n    \"\"\"\n    if self.b_fully_connected:\n        self.__fully_connected()\n        return\n\n    if self.topology is not None and len(self.topology) &gt; 0:\n        # Topology was already provided\n        return\n\n    if self.b_symmetric:\n        self.__randomly_pick_neighbors_symmetric()\n    else:\n        self.__randomly_pick_neighbors_asymmetric()\n</code></pre>"},{"location":"api/addons/topologymanager/#nebula.addons.topologymanager.TopologyManager.get_coordinates","title":"<code>get_coordinates(random_geo=True)</code>  <code>staticmethod</code>","text":"<p>Generates random geographical coordinates within predefined bounds for either Spain or Switzerland.</p> <p>The method returns a random geographical coordinate (latitude, longitude). The bounds for random coordinates are defined for two regions: Spain and Switzerland. The region is chosen randomly, and then the latitude and longitude are selected within the corresponding bounds.</p> <p>Parameters:</p> Name Type Description Default <code>random_geo</code> <code>bool</code> <p>If set to True, the method generates random coordinates within the predefined bounds                 for Spain or Switzerland. If set to False, this method could be modified to return fixed                 coordinates.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing the latitude and longitude of the generated point.</p> Source code in <code>nebula/addons/topologymanager.py</code> <pre><code>@staticmethod\ndef get_coordinates(random_geo=True):\n    \"\"\"\n    Generates random geographical coordinates within predefined bounds for either Spain or Switzerland.\n\n    The method returns a random geographical coordinate (latitude, longitude). The bounds for random coordinates are\n    defined for two regions: Spain and Switzerland. The region is chosen randomly, and then the latitude and longitude\n    are selected within the corresponding bounds.\n\n    Parameters:\n        random_geo (bool): If set to True, the method generates random coordinates within the predefined bounds\n                            for Spain or Switzerland. If set to False, this method could be modified to return fixed\n                            coordinates.\n\n    Returns:\n        tuple: A tuple containing the latitude and longitude of the generated point.\n    \"\"\"\n    if random_geo:\n        #  Espa\u00f1a min_lat, max_lat, min_lon, max_lon                  Suiza min_lat, max_lat, min_lon, max_lon\n        bounds = (36.0, 43.0, -9.0, 3.3) if random.randint(0, 1) == 0 else (45.8, 47.8, 5.9, 10.5)  # noqa: S311\n\n        min_latitude, max_latitude, min_longitude, max_longitude = bounds\n        latitude = random.uniform(min_latitude, max_latitude)  # noqa: S311\n        longitude = random.uniform(min_longitude, max_longitude)  # noqa: S311\n\n        return latitude, longitude\n</code></pre>"},{"location":"api/addons/topologymanager/#nebula.addons.topologymanager.TopologyManager.get_neighbors_string","title":"<code>get_neighbors_string(node_idx)</code>","text":"<p>Retrieves the neighbors of a given node as a string representation.</p> <p>This method checks the <code>topology</code> attribute to find the neighbors of the node at the specified index (<code>node_idx</code>). It then returns a string that lists the coordinates of each neighbor.</p> <p>Parameters:</p> Name Type Description Default <code>node_idx</code> <code>int</code> <p>The index of the node for which neighbors are to be retrieved.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>A space-separated string of neighbors' coordinates in the format \"latitude:longitude\".</p> Source code in <code>nebula/addons/topologymanager.py</code> <pre><code>def get_neighbors_string(self, node_idx):\n    \"\"\"\n    Retrieves the neighbors of a given node as a string representation.\n\n    This method checks the `topology` attribute to find the neighbors of the node at the specified index (`node_idx`). It then returns a string that lists the coordinates of each neighbor.\n\n    Parameters:\n        node_idx (int): The index of the node for which neighbors are to be retrieved.\n\n    Returns:\n        str: A space-separated string of neighbors' coordinates in the format \"latitude:longitude\".\n    \"\"\"\n    logging.info(f\"Getting neighbors for node {node_idx}\")\n    logging.info(f\"Topology shape: {self.topology.shape}\")\n\n    neighbors_data = []\n    for i in range(self.n_nodes):\n        if self.topology[node_idx][i] == 1:\n            neighbors_data.append(self.nodes[i])\n            logging.info(f\"Found neighbor at index {i}: {self.nodes[i]}\")\n\n    neighbors_data_strings = [f\"{i[0]}:{i[1]}\" for i in neighbors_data]\n    neighbors_data_string = \" \".join(neighbors_data_strings)\n    logging.info(f\"Neighbors of node participant_{node_idx}: {neighbors_data_string}\")\n    return neighbors_data_string\n</code></pre>"},{"location":"api/addons/topologymanager/#nebula.addons.topologymanager.TopologyManager.get_node_color","title":"<code>get_node_color(role)</code>","text":"<p>Returns the color associated with a given role.</p> <p>The method maps roles to specific colors for visualization or representation purposes.</p> <p>Parameters:</p> Name Type Description Default <code>role</code> <code>Role</code> <p>The role for which the color is to be determined.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The color associated with the given role. Defaults to \"red\" if the role is not recognized.</p> Source code in <code>nebula/addons/topologymanager.py</code> <pre><code>def get_node_color(self, role):\n    \"\"\"\n    Returns the color associated with a given role.\n\n    The method maps roles to specific colors for visualization or representation purposes.\n\n    Args:\n        role (Role): The role for which the color is to be determined.\n\n    Returns:\n        str: The color associated with the given role. Defaults to \"red\" if the role is not recognized.\n    \"\"\"\n    role_colors = {\n        Role.AGGREGATOR: \"orange\",\n        Role.SERVER: \"green\",\n        Role.TRAINER: \"#6182bd\",\n        Role.PROXY: \"purple\",\n    }\n    return role_colors.get(role, \"red\")\n</code></pre>"},{"location":"api/addons/topologymanager/#nebula.addons.topologymanager.TopologyManager.update_nodes","title":"<code>update_nodes(config_participants)</code>","text":"<p>Updates the nodes of the topology based on the provided configuration.</p> <p>This method assigns a new set of nodes to the <code>nodes</code> attribute, typically based on the configuration of the participants.</p> <p>Parameters:</p> Name Type Description Default <code>config_participants</code> <code>array - like</code> <p>A new set of nodes, usually derived from the participants' configuration, to be assigned to the topology.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>nebula/addons/topologymanager.py</code> <pre><code>def update_nodes(self, config_participants):\n    \"\"\"\n    Updates the nodes of the topology based on the provided configuration.\n\n    This method assigns a new set of nodes to the `nodes` attribute, typically based on the configuration of the participants.\n\n    Parameters:\n        config_participants (array-like): A new set of nodes, usually derived from the participants' configuration, to be assigned to the topology.\n\n    Returns:\n        None\n    \"\"\"\n    self.nodes = config_participants\n</code></pre>"},{"location":"api/addons/attacks/","title":"Documentation for Attacks Module","text":""},{"location":"api/addons/attacks/attacks/","title":"Documentation for Attacks Module","text":""},{"location":"api/addons/attacks/attacks/#nebula.addons.attacks.attacks.Attack","title":"<code>Attack</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for implementing various attack behaviors by dynamically injecting malicious behavior into existing functions or methods.</p> <p>This class provides an interface for replacing benign functions with malicious behaviors and for defining specific attack implementations. Subclasses must implement the <code>attack</code> and <code>_inject_malicious_behaviour</code> methods.</p> Source code in <code>nebula/addons/attacks/attacks.py</code> <pre><code>class Attack(ABC):\n    \"\"\"\n    Base class for implementing various attack behaviors by dynamically injecting\n    malicious behavior into existing functions or methods.\n\n    This class provides an interface for replacing benign functions with malicious\n    behaviors and for defining specific attack implementations. Subclasses must\n    implement the `attack` and `_inject_malicious_behaviour` methods.\n    \"\"\"\n\n    async def _replace_benign_function(function_route: str, malicious_behaviour):\n        \"\"\"\n        Dynamically replace a method in a class with a malicious behavior.\n\n        Args:\n            function_route (str): The route to the class and method to be replaced, in the format 'module.class.method'.\n            malicious_behaviour (callable): The malicious function that will replace the target method.\n\n        Raises:\n            AttributeError: If the specified class does not have the target method.\n            ImportError: If the module specified in `function_route` cannot be imported.\n            Exception: If any other error occurs during the process.\n\n        Returns:\n            None\n        \"\"\"\n        try:\n            *module_route, class_and_func = function_route.rsplit(\".\", maxsplit=1)\n            module = \".\".join(module_route)\n            class_name, function_name = class_and_func.split(\".\")\n\n            # Import the module\n            module_obj = importlib.import_module(module)\n\n            # Retrieve the class\n            changing_class = getattr(module_obj, class_name)\n\n            # Verify the class has the target method\n            if not hasattr(changing_class, function_name):\n                raise AttributeError(f\"Class '{class_name}' has no method named: '{function_name}'.\")\n\n            # Replace the original method with the malicious behavior\n            setattr(changing_class, function_name, malicious_behaviour)\n            print(f\"Function '{function_name}' has been replaced with '{malicious_behaviour.__name__}'.\")\n        except Exception as e:\n            logging.exception(f\"Error replacing function: {e}\")\n\n    @abstractmethod\n    async def attack(self):\n        \"\"\"\n        Abstract method to define the attack logic.\n\n        Subclasses must implement this method to specify the actions to perform\n        during an attack.\n\n        Raises:\n            NotImplementedError: If the method is not implemented in a subclass.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    async def _inject_malicious_behaviour(self, target_function: callable, *args, **kwargs) -&gt; None:\n        \"\"\"\n        Abstract method to inject a malicious behavior into an existing function.\n\n        This method must be implemented in subclasses to define how the malicious\n        behavior should interact with the target function.\n\n        Args:\n            target_function (callable): The function to inject the malicious behavior into.\n            *args: Positional arguments for the malicious behavior.\n            **kwargs: Keyword arguments for the malicious behavior.\n\n        Raises:\n            NotImplementedError: If the method is not implemented in a subclass.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api/addons/attacks/attacks/#nebula.addons.attacks.attacks.Attack.attack","title":"<code>attack()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Abstract method to define the attack logic.</p> <p>Subclasses must implement this method to specify the actions to perform during an attack.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the method is not implemented in a subclass.</p> Source code in <code>nebula/addons/attacks/attacks.py</code> <pre><code>@abstractmethod\nasync def attack(self):\n    \"\"\"\n    Abstract method to define the attack logic.\n\n    Subclasses must implement this method to specify the actions to perform\n    during an attack.\n\n    Raises:\n        NotImplementedError: If the method is not implemented in a subclass.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/addons/attacks/attacks/#nebula.addons.attacks.attacks.create_attack","title":"<code>create_attack(engine)</code>","text":"<p>Creates an attack object based on the attack name specified in the engine configuration.</p> <p>This function uses a predefined map of available attacks (<code>ATTACK_MAP</code>) to instantiate the corresponding attack class based on the attack name in the configuration. The attack parameters are also extracted from the configuration and passed when creating the attack object.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>object</code> <p>The training engine object containing the configuration for the attack.</p> required <p>Returns:</p> Name Type Description <code>Attack</code> <code>Attack</code> <p>An instance of the specified attack class.</p> <p>Raises:</p> Type Description <code>AttackException</code> <p>If the specified attack name is not found in the <code>ATTACK_MAP</code>.</p> Source code in <code>nebula/addons/attacks/attacks.py</code> <pre><code>def create_attack(engine) -&gt; Attack:\n    \"\"\"\n    Creates an attack object based on the attack name specified in the engine configuration.\n\n    This function uses a predefined map of available attacks (`ATTACK_MAP`) to instantiate\n    the corresponding attack class based on the attack name in the configuration. The attack\n    parameters are also extracted from the configuration and passed when creating the attack object.\n\n    Args:\n        engine (object): The training engine object containing the configuration for the attack.\n\n    Returns:\n        Attack: An instance of the specified attack class.\n\n    Raises:\n        AttackException: If the specified attack name is not found in the `ATTACK_MAP`.\n    \"\"\"\n    from nebula.addons.attacks.communications.delayerattack import DelayerAttack\n    from nebula.addons.attacks.communications.floodingattack import FloodingAttack\n    from nebula.addons.attacks.dataset.datapoison import SamplePoisoningAttack\n    from nebula.addons.attacks.dataset.labelflipping import LabelFlippingAttack\n    from nebula.addons.attacks.model.gllneuroninversion import GLLNeuronInversionAttack\n    from nebula.addons.attacks.model.modelpoison import ModelPoisonAttack\n    from nebula.addons.attacks.model.swappingweights import SwappingWeightsAttack\n\n    ATTACK_MAP = {\n        \"GLL Neuron Inversion\": GLLNeuronInversionAttack,\n        \"Swapping Weights\": SwappingWeightsAttack,\n        \"Delayer\": DelayerAttack,\n        \"Flooding\": FloodingAttack,\n        \"Label Flipping\": LabelFlippingAttack,\n        \"Sample Poisoning\": SamplePoisoningAttack,\n        \"Model Poisoning\": ModelPoisonAttack,\n    }\n\n    # Get attack name and parameters from the engine configuration\n    attack_params = engine.config.participant[\"adversarial_args\"].get(\"attack_params\", {})\n    attack_name = attack_params.get(\"attacks\", None)\n    if attack_name is None:\n        raise AttackException(\"No attack specified\")\n\n    # Look up the attack class based on the attack name\n    attack = ATTACK_MAP.get(attack_name)\n\n    # If the attack is found, return an instance of the attack class\n    if attack:\n        return attack(engine, dict(attack_params))\n    else:\n        # If the attack name is not found, raise an exception\n        raise AttackException(f\"Attack {attack_name} not found\")\n</code></pre>"},{"location":"api/addons/attacks/communications/","title":"Documentation for Communications Module","text":""},{"location":"api/addons/attacks/communications/communicationattack/","title":"Documentation for Communicationattack Module","text":""},{"location":"api/addons/attacks/communications/communicationattack/#nebula.addons.attacks.communications.communicationattack.CommunicationAttack","title":"<code>CommunicationAttack</code>","text":"<p>               Bases: <code>Attack</code></p> Source code in <code>nebula/addons/attacks/communications/communicationattack.py</code> <pre><code>class CommunicationAttack(Attack):\n    def __init__(\n        self,\n        engine,\n        target_class,\n        target_method,\n        round_start_attack,\n        round_stop_attack,\n        attack_interval,\n        decorator_args=None,\n        selectivity_percentage: int = 100,\n        selection_interval: int = None,\n    ):\n        super().__init__()\n        self.engine = engine\n        self.target_class = target_class\n        self.target_method = target_method\n        self.decorator_args = decorator_args\n        self.round_start_attack = round_start_attack\n        self.round_stop_attack = round_stop_attack\n        self.attack_interval = attack_interval\n        self.original_method = getattr(target_class, target_method, None)\n        self.selectivity_percentage = selectivity_percentage\n        self.selection_interval = selection_interval\n        self.last_selection_round = 0\n        self.targets = set()\n\n        if not self.original_method:\n            raise AttributeError(f\"Method {target_method} not found in class {target_class}\")\n\n    @abstractmethod\n    def decorator(self, *args):\n        \"\"\"Decorator that adds malicious behavior to the execution of the original method.\"\"\"\n        pass\n\n    async def select_targets(self):\n        \"\"\"\n        Selects a subset of neighboring nodes as attack targets based on the configured selectivity percentage.\n\n        This method determines which neighboring nodes should be targeted in the current round of attack.\n        If the selectivity percentage is less than 100%, it samples a subset of the currently connected direct neighbors.\n        The selection behavior can be influenced by a `selection_interval`:\n            - If `selection_interval` is set, target selection occurs only at rounds that are multiples of this interval.\n            - If no interval is defined but no targets have been selected yet, targets are selected once.\n        If the selectivity is 100%, all direct neighbors are selected as targets.\n\n        Target addresses are retrieved from the CommunicationsManager (only direct connections).\n        The number of selected targets is at least 1.\n\n        Logs are emitted at each selection event to indicate which targets were chosen.\n\n        Increments the internal `last_selection_round` counter after execution.\n\n        Notes:\n            - The `self.targets` attribute is updated in-place.\n            - The `self.last_selection_round` attribute tracks when the selection was last performed.\n\n    \"\"\"\n        if self.selectivity_percentage != 100:\n            if self.selection_interval:\n                if self.last_selection_round % self.selection_interval == 0:\n                    logging.info(\"Recalculating targets...\")\n                    all_nodes = await CommunicationsManager.get_instance().get_addrs_current_connections(only_direct=True)\n                    num_targets = max(1, int(len(all_nodes) * (self.selectivity_percentage / 100)))\n                    self.targets = set(random.sample(list(all_nodes), num_targets))\n            elif not self.targets:\n                logging.info(\"Calculating targets...\")\n                all_nodes = await CommunicationsManager.get_instance().get_addrs_current_connections(only_direct=True)\n                num_targets = max(1, int(len(all_nodes) * (self.selectivity_percentage / 100)))\n                self.targets = set(random.sample(list(all_nodes), num_targets))\n        else:\n            logging.info(\"All neighbors selected as targets\")\n            self.targets = await CommunicationsManager.get_instance().get_addrs_current_connections(only_direct=True)\n\n        logging.info(f\"Selected {self.selectivity_percentage}% targets from neighbors: {self.targets}\")\n        self.last_selection_round += 1\n\n    async def _inject_malicious_behaviour(self):\n        \"\"\"Inject malicious behavior into the target method.\"\"\"\n        decorated_method = self.decorator(self.decorator_args)(self.original_method)\n\n        setattr(\n            self.target_class,\n            self.target_method,\n            types.MethodType(decorated_method, self.target_class),\n        )\n\n    async def _restore_original_behaviour(self):\n        \"\"\"Restore the original behavior of the target method.\"\"\"\n        setattr(self.target_class, self.target_method, self.original_method)\n\n    async def attack(self):\n        \"\"\"Perform the attack logic based on the current round.\"\"\"\n        if self.engine.round not in range(self.round_start_attack, self.round_stop_attack + 1):\n            pass\n        elif self.engine.round == self.round_stop_attack:\n            logging.info(f\"[{self.__class__.__name__}] Stoping attack\")\n            await self._restore_original_behaviour()\n        elif (self.engine.round == self.round_start_attack) or (\n            (self.engine.round - self.round_start_attack) % self.attack_interval == 0\n        ):\n            await self.select_targets()\n            logging.info(f\"[{self.__class__.__name__}] Performing attack\")\n            await self._inject_malicious_behaviour()\n        else:\n            await self._restore_original_behaviour()\n</code></pre>"},{"location":"api/addons/attacks/communications/communicationattack/#nebula.addons.attacks.communications.communicationattack.CommunicationAttack.attack","title":"<code>attack()</code>  <code>async</code>","text":"<p>Perform the attack logic based on the current round.</p> Source code in <code>nebula/addons/attacks/communications/communicationattack.py</code> <pre><code>async def attack(self):\n    \"\"\"Perform the attack logic based on the current round.\"\"\"\n    if self.engine.round not in range(self.round_start_attack, self.round_stop_attack + 1):\n        pass\n    elif self.engine.round == self.round_stop_attack:\n        logging.info(f\"[{self.__class__.__name__}] Stoping attack\")\n        await self._restore_original_behaviour()\n    elif (self.engine.round == self.round_start_attack) or (\n        (self.engine.round - self.round_start_attack) % self.attack_interval == 0\n    ):\n        await self.select_targets()\n        logging.info(f\"[{self.__class__.__name__}] Performing attack\")\n        await self._inject_malicious_behaviour()\n    else:\n        await self._restore_original_behaviour()\n</code></pre>"},{"location":"api/addons/attacks/communications/communicationattack/#nebula.addons.attacks.communications.communicationattack.CommunicationAttack.decorator","title":"<code>decorator(*args)</code>  <code>abstractmethod</code>","text":"<p>Decorator that adds malicious behavior to the execution of the original method.</p> Source code in <code>nebula/addons/attacks/communications/communicationattack.py</code> <pre><code>@abstractmethod\ndef decorator(self, *args):\n    \"\"\"Decorator that adds malicious behavior to the execution of the original method.\"\"\"\n    pass\n</code></pre>"},{"location":"api/addons/attacks/communications/communicationattack/#nebula.addons.attacks.communications.communicationattack.CommunicationAttack.select_targets","title":"<code>select_targets()</code>  <code>async</code>","text":"<p>Selects a subset of neighboring nodes as attack targets based on the configured selectivity percentage.</p> <p>This method determines which neighboring nodes should be targeted in the current round of attack. If the selectivity percentage is less than 100%, it samples a subset of the currently connected direct neighbors. The selection behavior can be influenced by a <code>selection_interval</code>:     - If <code>selection_interval</code> is set, target selection occurs only at rounds that are multiples of this interval.     - If no interval is defined but no targets have been selected yet, targets are selected once. If the selectivity is 100%, all direct neighbors are selected as targets.</p> <p>Target addresses are retrieved from the CommunicationsManager (only direct connections). The number of selected targets is at least 1.</p> <p>Logs are emitted at each selection event to indicate which targets were chosen.</p> <p>Increments the internal <code>last_selection_round</code> counter after execution.</p> Notes <ul> <li>The <code>self.targets</code> attribute is updated in-place.</li> <li>The <code>self.last_selection_round</code> attribute tracks when the selection was last performed.</li> </ul> Source code in <code>nebula/addons/attacks/communications/communicationattack.py</code> <pre><code>async def select_targets(self):\n    \"\"\"\n    Selects a subset of neighboring nodes as attack targets based on the configured selectivity percentage.\n\n    This method determines which neighboring nodes should be targeted in the current round of attack.\n    If the selectivity percentage is less than 100%, it samples a subset of the currently connected direct neighbors.\n    The selection behavior can be influenced by a `selection_interval`:\n        - If `selection_interval` is set, target selection occurs only at rounds that are multiples of this interval.\n        - If no interval is defined but no targets have been selected yet, targets are selected once.\n    If the selectivity is 100%, all direct neighbors are selected as targets.\n\n    Target addresses are retrieved from the CommunicationsManager (only direct connections).\n    The number of selected targets is at least 1.\n\n    Logs are emitted at each selection event to indicate which targets were chosen.\n\n    Increments the internal `last_selection_round` counter after execution.\n\n    Notes:\n        - The `self.targets` attribute is updated in-place.\n        - The `self.last_selection_round` attribute tracks when the selection was last performed.\n\n\"\"\"\n    if self.selectivity_percentage != 100:\n        if self.selection_interval:\n            if self.last_selection_round % self.selection_interval == 0:\n                logging.info(\"Recalculating targets...\")\n                all_nodes = await CommunicationsManager.get_instance().get_addrs_current_connections(only_direct=True)\n                num_targets = max(1, int(len(all_nodes) * (self.selectivity_percentage / 100)))\n                self.targets = set(random.sample(list(all_nodes), num_targets))\n        elif not self.targets:\n            logging.info(\"Calculating targets...\")\n            all_nodes = await CommunicationsManager.get_instance().get_addrs_current_connections(only_direct=True)\n            num_targets = max(1, int(len(all_nodes) * (self.selectivity_percentage / 100)))\n            self.targets = set(random.sample(list(all_nodes), num_targets))\n    else:\n        logging.info(\"All neighbors selected as targets\")\n        self.targets = await CommunicationsManager.get_instance().get_addrs_current_connections(only_direct=True)\n\n    logging.info(f\"Selected {self.selectivity_percentage}% targets from neighbors: {self.targets}\")\n    self.last_selection_round += 1\n</code></pre>"},{"location":"api/addons/attacks/communications/delayerattack/","title":"Documentation for Delayerattack Module","text":""},{"location":"api/addons/attacks/communications/delayerattack/#nebula.addons.attacks.communications.delayerattack.DelayerAttack","title":"<code>DelayerAttack</code>","text":"<p>               Bases: <code>CommunicationAttack</code></p> <p>Implements an attack that delays the execution of a target method by a specified amount of time.</p> Source code in <code>nebula/addons/attacks/communications/delayerattack.py</code> <pre><code>class DelayerAttack(CommunicationAttack):\n    \"\"\"\n    Implements an attack that delays the execution of a target method by a specified amount of time.\n    \"\"\"\n\n    def __init__(self, engine, attack_params: dict):\n        \"\"\"\n        Initializes the DelayerAttack with the engine and attack parameters.\n\n        Args:\n            engine: The engine managing the attack context.\n            attack_params (dict): Parameters for the attack, including the delay duration.\n        \"\"\"\n        try:\n            self.delay = int(attack_params[\"delay\"])\n            round_start = int(attack_params[\"round_start_attack\"])\n            round_stop = int(attack_params[\"round_stop_attack\"])\n            attack_interval = int(attack_params[\"attack_interval\"])\n            self.target_percentage = int(attack_params[\"target_percentage\"])\n            self.selection_interval = int(attack_params[\"selection_interval\"])\n        except KeyError as e:\n            raise ValueError(f\"Missing required attack parameter: {e}\")\n        except ValueError:\n            raise ValueError(\"Invalid value in attack_params. Ensure all values are integers.\")\n\n        super().__init__(\n            engine,\n            CommunicationsManager.get_instance(),\n            \"send_message\",\n            round_start,\n            round_stop,\n            attack_interval,\n            self.delay,\n            self.target_percentage,\n            self.selection_interval,\n        )\n\n    def decorator(self, delay: int):\n            \"\"\"\n            Decorator that adds a delay to the execution of the original method.\n\n            Args:\n                delay (int): The time in seconds to delay the method execution.\n\n            Returns:\n                function: A decorator function that wraps the target method with the delay logic.\n            \"\"\"\n\n            def decorator(func):\n                @wraps(func)\n                async def wrapper(*args, **kwargs):\n                    if len(args) == 4 and args[3] == \"model\":\n                        dest_addr = args[1]\n                        if dest_addr in self.targets:\n                            logging.info(f\"[DelayerAttack] Delaying model propagation to {dest_addr} by {delay} seconds\")\n                            await asyncio.sleep(delay)\n                    _, *new_args = args  # Exclude self argument\n                    return await func(*new_args)\n\n                return wrapper\n\n            return decorator\n</code></pre>"},{"location":"api/addons/attacks/communications/delayerattack/#nebula.addons.attacks.communications.delayerattack.DelayerAttack.__init__","title":"<code>__init__(engine, attack_params)</code>","text":"<p>Initializes the DelayerAttack with the engine and attack parameters.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <p>The engine managing the attack context.</p> required <code>attack_params</code> <code>dict</code> <p>Parameters for the attack, including the delay duration.</p> required Source code in <code>nebula/addons/attacks/communications/delayerattack.py</code> <pre><code>def __init__(self, engine, attack_params: dict):\n    \"\"\"\n    Initializes the DelayerAttack with the engine and attack parameters.\n\n    Args:\n        engine: The engine managing the attack context.\n        attack_params (dict): Parameters for the attack, including the delay duration.\n    \"\"\"\n    try:\n        self.delay = int(attack_params[\"delay\"])\n        round_start = int(attack_params[\"round_start_attack\"])\n        round_stop = int(attack_params[\"round_stop_attack\"])\n        attack_interval = int(attack_params[\"attack_interval\"])\n        self.target_percentage = int(attack_params[\"target_percentage\"])\n        self.selection_interval = int(attack_params[\"selection_interval\"])\n    except KeyError as e:\n        raise ValueError(f\"Missing required attack parameter: {e}\")\n    except ValueError:\n        raise ValueError(\"Invalid value in attack_params. Ensure all values are integers.\")\n\n    super().__init__(\n        engine,\n        CommunicationsManager.get_instance(),\n        \"send_message\",\n        round_start,\n        round_stop,\n        attack_interval,\n        self.delay,\n        self.target_percentage,\n        self.selection_interval,\n    )\n</code></pre>"},{"location":"api/addons/attacks/communications/delayerattack/#nebula.addons.attacks.communications.delayerattack.DelayerAttack.decorator","title":"<code>decorator(delay)</code>","text":"<p>Decorator that adds a delay to the execution of the original method.</p> <p>Parameters:</p> Name Type Description Default <code>delay</code> <code>int</code> <p>The time in seconds to delay the method execution.</p> required <p>Returns:</p> Name Type Description <code>function</code> <p>A decorator function that wraps the target method with the delay logic.</p> Source code in <code>nebula/addons/attacks/communications/delayerattack.py</code> <pre><code>def decorator(self, delay: int):\n        \"\"\"\n        Decorator that adds a delay to the execution of the original method.\n\n        Args:\n            delay (int): The time in seconds to delay the method execution.\n\n        Returns:\n            function: A decorator function that wraps the target method with the delay logic.\n        \"\"\"\n\n        def decorator(func):\n            @wraps(func)\n            async def wrapper(*args, **kwargs):\n                if len(args) == 4 and args[3] == \"model\":\n                    dest_addr = args[1]\n                    if dest_addr in self.targets:\n                        logging.info(f\"[DelayerAttack] Delaying model propagation to {dest_addr} by {delay} seconds\")\n                        await asyncio.sleep(delay)\n                _, *new_args = args  # Exclude self argument\n                return await func(*new_args)\n\n            return wrapper\n\n        return decorator\n</code></pre>"},{"location":"api/addons/attacks/communications/floodingattack/","title":"Documentation for Floodingattack Module","text":""},{"location":"api/addons/attacks/communications/floodingattack/#nebula.addons.attacks.communications.floodingattack.FloodingAttack","title":"<code>FloodingAttack</code>","text":"<p>               Bases: <code>CommunicationAttack</code></p> <p>Implements an attack that delays the execution of a target method by a specified amount of time.</p> Source code in <code>nebula/addons/attacks/communications/floodingattack.py</code> <pre><code>class FloodingAttack(CommunicationAttack):\n    \"\"\"\n    Implements an attack that delays the execution of a target method by a specified amount of time.\n    \"\"\"\n\n    def __init__(self, engine, attack_params: dict):\n        \"\"\"\n        Initializes the DelayerAttack with the engine and attack parameters.\n\n        Args:\n            engine: The engine managing the attack context.\n            attack_params (dict): Parameters for the attack, including the delay duration.\n        \"\"\"\n        try:\n            round_start = int(attack_params[\"round_start_attack\"])\n            round_stop = int(attack_params[\"round_stop_attack\"])\n            attack_interval = int(attack_params[\"attack_interval\"])\n            self.flooding_factor = int(attack_params[\"flooding_factor\"])\n            self.target_percentage = int(attack_params[\"target_percentage\"])\n            self.selection_interval = int(attack_params[\"selection_interval\"])\n        except KeyError as e:\n            raise ValueError(f\"Missing required attack parameter: {e}\")\n        except ValueError:\n            raise ValueError(\"Invalid value in attack_params. Ensure all values are integers.\")\n\n        self.verbose = False\n\n        super().__init__(\n            engine,\n            CommunicationsManager.get_instance(),\n            \"send_message\",\n            round_start,\n            round_stop,\n            attack_interval,\n            self.flooding_factor,\n            self.target_percentage,\n            self.selection_interval,\n        )\n\n    def decorator(self, flooding_factor: int):\n        \"\"\"\n        Decorator that adds a delay to the execution of the original method.\n\n        Args:\n            flooding_factor (int): The number of times to repeat the function execution.\n\n        Returns:\n            function: A decorator function that wraps the target method with the delay logic.\n        \"\"\"\n\n        def decorator(func):\n            @wraps(func)\n            async def wrapper(*args, **kwargs):\n                if len(args) == 4 and args[3] == \"model\":\n                    dest_addr = args[1]\n                    if dest_addr in self.targets:\n                        logging.info(f\"[FloodingAttack] Flooding message to {dest_addr} by {flooding_factor} times\")\n                        for i in range(flooding_factor):\n                            if self.verbose:\n                                logging.info(\n                                    f\"[FloodingAttack] Sending duplicate {i + 1}/{flooding_factor} to {dest_addr}\"\n                                )\n                            _, *new_args = args  # Exclude self argument\n                            await func(*new_args, **kwargs)\n                _, *new_args = args \n                return await func(*new_args)\n\n            return wrapper\n\n        return decorator\n</code></pre>"},{"location":"api/addons/attacks/communications/floodingattack/#nebula.addons.attacks.communications.floodingattack.FloodingAttack.__init__","title":"<code>__init__(engine, attack_params)</code>","text":"<p>Initializes the DelayerAttack with the engine and attack parameters.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <p>The engine managing the attack context.</p> required <code>attack_params</code> <code>dict</code> <p>Parameters for the attack, including the delay duration.</p> required Source code in <code>nebula/addons/attacks/communications/floodingattack.py</code> <pre><code>def __init__(self, engine, attack_params: dict):\n    \"\"\"\n    Initializes the DelayerAttack with the engine and attack parameters.\n\n    Args:\n        engine: The engine managing the attack context.\n        attack_params (dict): Parameters for the attack, including the delay duration.\n    \"\"\"\n    try:\n        round_start = int(attack_params[\"round_start_attack\"])\n        round_stop = int(attack_params[\"round_stop_attack\"])\n        attack_interval = int(attack_params[\"attack_interval\"])\n        self.flooding_factor = int(attack_params[\"flooding_factor\"])\n        self.target_percentage = int(attack_params[\"target_percentage\"])\n        self.selection_interval = int(attack_params[\"selection_interval\"])\n    except KeyError as e:\n        raise ValueError(f\"Missing required attack parameter: {e}\")\n    except ValueError:\n        raise ValueError(\"Invalid value in attack_params. Ensure all values are integers.\")\n\n    self.verbose = False\n\n    super().__init__(\n        engine,\n        CommunicationsManager.get_instance(),\n        \"send_message\",\n        round_start,\n        round_stop,\n        attack_interval,\n        self.flooding_factor,\n        self.target_percentage,\n        self.selection_interval,\n    )\n</code></pre>"},{"location":"api/addons/attacks/communications/floodingattack/#nebula.addons.attacks.communications.floodingattack.FloodingAttack.decorator","title":"<code>decorator(flooding_factor)</code>","text":"<p>Decorator that adds a delay to the execution of the original method.</p> <p>Parameters:</p> Name Type Description Default <code>flooding_factor</code> <code>int</code> <p>The number of times to repeat the function execution.</p> required <p>Returns:</p> Name Type Description <code>function</code> <p>A decorator function that wraps the target method with the delay logic.</p> Source code in <code>nebula/addons/attacks/communications/floodingattack.py</code> <pre><code>def decorator(self, flooding_factor: int):\n    \"\"\"\n    Decorator that adds a delay to the execution of the original method.\n\n    Args:\n        flooding_factor (int): The number of times to repeat the function execution.\n\n    Returns:\n        function: A decorator function that wraps the target method with the delay logic.\n    \"\"\"\n\n    def decorator(func):\n        @wraps(func)\n        async def wrapper(*args, **kwargs):\n            if len(args) == 4 and args[3] == \"model\":\n                dest_addr = args[1]\n                if dest_addr in self.targets:\n                    logging.info(f\"[FloodingAttack] Flooding message to {dest_addr} by {flooding_factor} times\")\n                    for i in range(flooding_factor):\n                        if self.verbose:\n                            logging.info(\n                                f\"[FloodingAttack] Sending duplicate {i + 1}/{flooding_factor} to {dest_addr}\"\n                            )\n                        _, *new_args = args  # Exclude self argument\n                        await func(*new_args, **kwargs)\n            _, *new_args = args \n            return await func(*new_args)\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"api/addons/attacks/dataset/","title":"Documentation for Dataset Module","text":""},{"location":"api/addons/attacks/dataset/datapoison/","title":"Documentation for Datapoison Module","text":"<p>This module provides classes for data poisoning attacks in datasets, allowing for the simulation of data poisoning by adding noise or modifying specific data points.</p> <p>Classes: - SamplePoisoningAttack: Main attack class that implements the DatasetAttack interface - DataPoisoningStrategy: Abstract base class for poisoning strategies - TargetedSamplePoisoningStrategy: Implementation for targeted poisoning (X pattern) - NonTargetedSamplePoisoningStrategy: Implementation for non-targeted poisoning (noise-based)</p>"},{"location":"api/addons/attacks/dataset/datapoison/#nebula.addons.attacks.dataset.datapoison.DataPoisoningStrategy","title":"<code>DataPoisoningStrategy</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for poisoning strategies.</p> Source code in <code>nebula/addons/attacks/dataset/datapoison.py</code> <pre><code>class DataPoisoningStrategy(ABC):\n    \"\"\"Abstract base class for poisoning strategies.\"\"\"\n\n    @abstractmethod\n    def poison_data(\n        self,\n        dataset,\n        indices: list[int],\n        poisoned_percent: float,\n        poisoned_noise_percent: float,\n    ) -&gt; \"Dataset\":\n        \"\"\"\n        Abstract method to poison data in the dataset.\n\n        Args:\n            dataset: The dataset to modify\n            indices: List of indices to consider for poisoning\n            poisoned_percent: Percentage of data to poison (0-100)\n            poisoned_noise_percent: Percentage of noise to apply (0-100)\n\n        Returns:\n            Modified dataset with poisoned data\n        \"\"\"\n        pass\n\n    def _convert_to_tensor(self, data: torch.Tensor | Image.Image | tuple) -&gt; torch.Tensor:\n        \"\"\"\n        Convert input data to tensor format.\n\n        Args:\n            data: Input data that can be a tensor, PIL Image, or tuple\n\n        Returns:\n            Tensor representation of the input data\n        \"\"\"\n        if isinstance(data, tuple):\n            data = data[0]\n\n        if isinstance(data, Image.Image):\n            return torch.tensor(np.array(data))\n        elif isinstance(data, torch.Tensor):\n            return data\n        else:\n            return torch.tensor(data)\n\n    def _handle_single_point(self, tensor: torch.Tensor) -&gt; tuple[torch.Tensor, bool]:\n        \"\"\"\n        Handle single point tensors by reshaping them.\n\n        Args:\n            tensor: Input tensor\n\n        Returns:\n            Tuple of (reshaped tensor, is_single_point flag)\n        \"\"\"\n        is_single_point = False\n        if len(tensor.shape) == 0:\n            tensor = tensor.view(-1)\n            is_single_point = True\n        return tensor, is_single_point\n</code></pre>"},{"location":"api/addons/attacks/dataset/datapoison/#nebula.addons.attacks.dataset.datapoison.DataPoisoningStrategy.poison_data","title":"<code>poison_data(dataset, indices, poisoned_percent, poisoned_noise_percent)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to poison data in the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <p>The dataset to modify</p> required <code>indices</code> <code>list[int]</code> <p>List of indices to consider for poisoning</p> required <code>poisoned_percent</code> <code>float</code> <p>Percentage of data to poison (0-100)</p> required <code>poisoned_noise_percent</code> <code>float</code> <p>Percentage of noise to apply (0-100)</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>Modified dataset with poisoned data</p> Source code in <code>nebula/addons/attacks/dataset/datapoison.py</code> <pre><code>@abstractmethod\ndef poison_data(\n    self,\n    dataset,\n    indices: list[int],\n    poisoned_percent: float,\n    poisoned_noise_percent: float,\n) -&gt; \"Dataset\":\n    \"\"\"\n    Abstract method to poison data in the dataset.\n\n    Args:\n        dataset: The dataset to modify\n        indices: List of indices to consider for poisoning\n        poisoned_percent: Percentage of data to poison (0-100)\n        poisoned_noise_percent: Percentage of noise to apply (0-100)\n\n    Returns:\n        Modified dataset with poisoned data\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/addons/attacks/dataset/datapoison/#nebula.addons.attacks.dataset.datapoison.NonTargetedSamplePoisoningStrategy","title":"<code>NonTargetedSamplePoisoningStrategy</code>","text":"<p>               Bases: <code>DataPoisoningStrategy</code></p> <p>Implementation of non-targeted poisoning strategy using noise.</p> Source code in <code>nebula/addons/attacks/dataset/datapoison.py</code> <pre><code>class NonTargetedSamplePoisoningStrategy(DataPoisoningStrategy):\n    \"\"\"Implementation of non-targeted poisoning strategy using noise.\"\"\"\n\n    def __init__(self, noise_type: str):\n        \"\"\"\n        Initialize non-targeted poisoning strategy.\n\n        Args:\n            noise_type: Type of noise to apply (salt, gaussian, s&amp;p, nlp_rawdata)\n        \"\"\"\n        self.noise_type = noise_type.lower()\n\n    def apply_noise(self, t: torch.Tensor | Image.Image, poisoned_noise_percent: float) -&gt; torch.Tensor:\n        \"\"\"\n        Applies noise to a tensor based on the specified noise type and poisoning percentage.\n\n        Args:\n            t: The input tensor or PIL Image to which noise will be applied\n            poisoned_noise_percent: The percentage of noise to be applied (0-100)\n\n        Returns:\n            The tensor with noise applied\n        \"\"\"\n        t = self._convert_to_tensor(t)\n        t, is_single_point = self._handle_single_point(t)\n\n        arr = t.detach().cpu().numpy()\n        poisoned_ratio = poisoned_noise_percent / 100.0\n\n        logging.info(\n            f\"[{self.__class__.__name__}] Applying noise to data with noise type: {self.noise_type} and amount: {poisoned_ratio} (float)\"\n        )\n\n        if self.noise_type == \"salt\":\n            poisoned = torch.tensor(random_noise(arr, mode=self.noise_type, amount=poisoned_ratio))\n        elif self.noise_type == \"gaussian\":\n            poisoned = torch.tensor(random_noise(arr, mode=self.noise_type, mean=0, var=poisoned_ratio, clip=True))\n        elif self.noise_type == \"s&amp;p\":\n            poisoned = torch.tensor(random_noise(arr, mode=self.noise_type, amount=poisoned_ratio))\n        elif self.noise_type == \"nlp_rawdata\":\n            poisoned = self.poison_to_nlp_rawdata(arr, poisoned_ratio)\n        else:\n            logging.info(f\"ERROR: noise_type '{self.noise_type}' not supported in data poison attack.\")\n            return t\n\n        if is_single_point:\n            poisoned = poisoned[0]\n\n        return poisoned\n\n    def poison_to_nlp_rawdata(self, text_data: list, poisoned_ratio: float) -&gt; list:\n        \"\"\"\n        Poisons NLP data by setting word vectors to zero with a given probability.\n\n        Args:\n            text_data: List of word vectors\n            poisoned_ratio: Fraction of non-zero vectors to set to zero\n\n        Returns:\n            Modified text data with some word vectors set to zero\n        \"\"\"\n        non_zero_vector_indice = [i for i in range(0, len(text_data)) if text_data[i][0] != 0]\n        non_zero_vector_len = len(non_zero_vector_indice)\n\n        num_poisoned_token = int(poisoned_ratio * non_zero_vector_len)\n        if num_poisoned_token == 0 or num_poisoned_token &gt; non_zero_vector_len:\n            return text_data\n\n        poisoned_token_indice = random.sample(non_zero_vector_indice, num_poisoned_token)\n        zero_vector = torch.Tensor(np.zeros(len(text_data[0][0])))\n        for i in poisoned_token_indice:\n            text_data[i] = zero_vector\n        return text_data\n\n    def poison_data(\n        self,\n        dataset,\n        indices: list[int],\n        poisoned_percent: float,\n        poisoned_noise_percent: float,\n    ) -&gt; \"Dataset\":\n        \"\"\"\n        Applies noise-based poisoning to the dataset.\n\n        Args:\n            dataset: The dataset to modify\n            indices: List of indices to consider for poisoning\n            poisoned_percent: Percentage of data to poison (0-100)\n            poisoned_noise_percent: Percentage of noise to apply (0-100)\n\n        Returns:\n            Modified dataset with poisoned data\n        \"\"\"\n        logging.info(f\"[{self.__class__.__name__}] Poisoning data with noise type: {self.noise_type}\")\n        new_dataset = copy.deepcopy(dataset)\n        if not isinstance(new_dataset.targets, np.ndarray):\n            new_dataset.targets = np.array(new_dataset.targets)\n        else:\n            new_dataset.targets = new_dataset.targets.copy()\n\n        num_indices = len(indices)\n        num_poisoned = int(poisoned_percent * num_indices / 100.0)\n\n        if num_indices == 0 or num_poisoned &gt; num_indices:\n            return new_dataset\n\n        poisoned_indices = random.sample(indices, num_poisoned)\n        logging.info(f\"Number of poisoned samples: {num_poisoned}\")\n\n        for i in poisoned_indices:\n            t = new_dataset.data[i]\n            poisoned = self.apply_noise(t, poisoned_noise_percent)\n\n            if isinstance(t, tuple):\n                poisoned = (poisoned, t[1])\n\n            new_dataset.data[i] = poisoned\n\n        return new_dataset\n</code></pre>"},{"location":"api/addons/attacks/dataset/datapoison/#nebula.addons.attacks.dataset.datapoison.NonTargetedSamplePoisoningStrategy.__init__","title":"<code>__init__(noise_type)</code>","text":"<p>Initialize non-targeted poisoning strategy.</p> <p>Parameters:</p> Name Type Description Default <code>noise_type</code> <code>str</code> <p>Type of noise to apply (salt, gaussian, s&amp;p, nlp_rawdata)</p> required Source code in <code>nebula/addons/attacks/dataset/datapoison.py</code> <pre><code>def __init__(self, noise_type: str):\n    \"\"\"\n    Initialize non-targeted poisoning strategy.\n\n    Args:\n        noise_type: Type of noise to apply (salt, gaussian, s&amp;p, nlp_rawdata)\n    \"\"\"\n    self.noise_type = noise_type.lower()\n</code></pre>"},{"location":"api/addons/attacks/dataset/datapoison/#nebula.addons.attacks.dataset.datapoison.NonTargetedSamplePoisoningStrategy.apply_noise","title":"<code>apply_noise(t, poisoned_noise_percent)</code>","text":"<p>Applies noise to a tensor based on the specified noise type and poisoning percentage.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor | Image</code> <p>The input tensor or PIL Image to which noise will be applied</p> required <code>poisoned_noise_percent</code> <code>float</code> <p>The percentage of noise to be applied (0-100)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The tensor with noise applied</p> Source code in <code>nebula/addons/attacks/dataset/datapoison.py</code> <pre><code>def apply_noise(self, t: torch.Tensor | Image.Image, poisoned_noise_percent: float) -&gt; torch.Tensor:\n    \"\"\"\n    Applies noise to a tensor based on the specified noise type and poisoning percentage.\n\n    Args:\n        t: The input tensor or PIL Image to which noise will be applied\n        poisoned_noise_percent: The percentage of noise to be applied (0-100)\n\n    Returns:\n        The tensor with noise applied\n    \"\"\"\n    t = self._convert_to_tensor(t)\n    t, is_single_point = self._handle_single_point(t)\n\n    arr = t.detach().cpu().numpy()\n    poisoned_ratio = poisoned_noise_percent / 100.0\n\n    logging.info(\n        f\"[{self.__class__.__name__}] Applying noise to data with noise type: {self.noise_type} and amount: {poisoned_ratio} (float)\"\n    )\n\n    if self.noise_type == \"salt\":\n        poisoned = torch.tensor(random_noise(arr, mode=self.noise_type, amount=poisoned_ratio))\n    elif self.noise_type == \"gaussian\":\n        poisoned = torch.tensor(random_noise(arr, mode=self.noise_type, mean=0, var=poisoned_ratio, clip=True))\n    elif self.noise_type == \"s&amp;p\":\n        poisoned = torch.tensor(random_noise(arr, mode=self.noise_type, amount=poisoned_ratio))\n    elif self.noise_type == \"nlp_rawdata\":\n        poisoned = self.poison_to_nlp_rawdata(arr, poisoned_ratio)\n    else:\n        logging.info(f\"ERROR: noise_type '{self.noise_type}' not supported in data poison attack.\")\n        return t\n\n    if is_single_point:\n        poisoned = poisoned[0]\n\n    return poisoned\n</code></pre>"},{"location":"api/addons/attacks/dataset/datapoison/#nebula.addons.attacks.dataset.datapoison.NonTargetedSamplePoisoningStrategy.poison_data","title":"<code>poison_data(dataset, indices, poisoned_percent, poisoned_noise_percent)</code>","text":"<p>Applies noise-based poisoning to the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <p>The dataset to modify</p> required <code>indices</code> <code>list[int]</code> <p>List of indices to consider for poisoning</p> required <code>poisoned_percent</code> <code>float</code> <p>Percentage of data to poison (0-100)</p> required <code>poisoned_noise_percent</code> <code>float</code> <p>Percentage of noise to apply (0-100)</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>Modified dataset with poisoned data</p> Source code in <code>nebula/addons/attacks/dataset/datapoison.py</code> <pre><code>def poison_data(\n    self,\n    dataset,\n    indices: list[int],\n    poisoned_percent: float,\n    poisoned_noise_percent: float,\n) -&gt; \"Dataset\":\n    \"\"\"\n    Applies noise-based poisoning to the dataset.\n\n    Args:\n        dataset: The dataset to modify\n        indices: List of indices to consider for poisoning\n        poisoned_percent: Percentage of data to poison (0-100)\n        poisoned_noise_percent: Percentage of noise to apply (0-100)\n\n    Returns:\n        Modified dataset with poisoned data\n    \"\"\"\n    logging.info(f\"[{self.__class__.__name__}] Poisoning data with noise type: {self.noise_type}\")\n    new_dataset = copy.deepcopy(dataset)\n    if not isinstance(new_dataset.targets, np.ndarray):\n        new_dataset.targets = np.array(new_dataset.targets)\n    else:\n        new_dataset.targets = new_dataset.targets.copy()\n\n    num_indices = len(indices)\n    num_poisoned = int(poisoned_percent * num_indices / 100.0)\n\n    if num_indices == 0 or num_poisoned &gt; num_indices:\n        return new_dataset\n\n    poisoned_indices = random.sample(indices, num_poisoned)\n    logging.info(f\"Number of poisoned samples: {num_poisoned}\")\n\n    for i in poisoned_indices:\n        t = new_dataset.data[i]\n        poisoned = self.apply_noise(t, poisoned_noise_percent)\n\n        if isinstance(t, tuple):\n            poisoned = (poisoned, t[1])\n\n        new_dataset.data[i] = poisoned\n\n    return new_dataset\n</code></pre>"},{"location":"api/addons/attacks/dataset/datapoison/#nebula.addons.attacks.dataset.datapoison.NonTargetedSamplePoisoningStrategy.poison_to_nlp_rawdata","title":"<code>poison_to_nlp_rawdata(text_data, poisoned_ratio)</code>","text":"<p>Poisons NLP data by setting word vectors to zero with a given probability.</p> <p>Parameters:</p> Name Type Description Default <code>text_data</code> <code>list</code> <p>List of word vectors</p> required <code>poisoned_ratio</code> <code>float</code> <p>Fraction of non-zero vectors to set to zero</p> required <p>Returns:</p> Type Description <code>list</code> <p>Modified text data with some word vectors set to zero</p> Source code in <code>nebula/addons/attacks/dataset/datapoison.py</code> <pre><code>def poison_to_nlp_rawdata(self, text_data: list, poisoned_ratio: float) -&gt; list:\n    \"\"\"\n    Poisons NLP data by setting word vectors to zero with a given probability.\n\n    Args:\n        text_data: List of word vectors\n        poisoned_ratio: Fraction of non-zero vectors to set to zero\n\n    Returns:\n        Modified text data with some word vectors set to zero\n    \"\"\"\n    non_zero_vector_indice = [i for i in range(0, len(text_data)) if text_data[i][0] != 0]\n    non_zero_vector_len = len(non_zero_vector_indice)\n\n    num_poisoned_token = int(poisoned_ratio * non_zero_vector_len)\n    if num_poisoned_token == 0 or num_poisoned_token &gt; non_zero_vector_len:\n        return text_data\n\n    poisoned_token_indice = random.sample(non_zero_vector_indice, num_poisoned_token)\n    zero_vector = torch.Tensor(np.zeros(len(text_data[0][0])))\n    for i in poisoned_token_indice:\n        text_data[i] = zero_vector\n    return text_data\n</code></pre>"},{"location":"api/addons/attacks/dataset/datapoison/#nebula.addons.attacks.dataset.datapoison.SamplePoisoningAttack","title":"<code>SamplePoisoningAttack</code>","text":"<p>               Bases: <code>DatasetAttack</code></p> <p>Implements a data poisoning attack on a training dataset.</p> Source code in <code>nebula/addons/attacks/dataset/datapoison.py</code> <pre><code>class SamplePoisoningAttack(DatasetAttack):\n    \"\"\"\n    Implements a data poisoning attack on a training dataset.\n    \"\"\"\n\n    def __init__(self, engine, attack_params: Dict):\n        \"\"\"\n        Initialize the sample poisoning attack.\n\n        Args:\n            engine: The engine managing the attack context\n            attack_params: Dictionary containing attack parameters\n        \"\"\"\n        try:\n            round_start = int(attack_params[\"round_start_attack\"])\n            round_stop = int(attack_params[\"round_stop_attack\"])\n            attack_interval = int(attack_params[\"attack_interval\"])\n        except KeyError as e:\n            raise ValueError(f\"Missing required attack parameter: {e}\")\n        except ValueError:\n            raise ValueError(\"Invalid value in attack_params. Ensure all values are integers.\")\n\n        super().__init__(engine, round_start, round_stop, attack_interval)\n        self.datamodule = engine._trainer.datamodule\n        self.poisoned_percent = float(attack_params[\"poisoned_sample_percent\"])\n        self.poisoned_noise_percent = float(attack_params[\"poisoned_noise_percent\"])\n\n        # Create the appropriate strategy based on whether the attack is targeted\n        if attack_params.get(\"targeted\", False):\n            target_label = int(attack_params.get(\"target_label\") or attack_params.get(\"targetLabel\", 4))\n            self.strategy = TargetedSamplePoisoningStrategy(target_label)\n        else:\n            noise_type = (attack_params.get(\"noise_type\") or attack_params.get(\"noiseType\", \"Gaussian\")).lower()\n            self.strategy = NonTargetedSamplePoisoningStrategy(noise_type)\n\n    def get_malicious_dataset(self):\n        \"\"\"\n        Creates a malicious dataset by poisoning selected data points.\n\n        Returns:\n            Dataset: The modified dataset with poisoned data\n        \"\"\"\n        return self.strategy.poison_data(\n            self.datamodule.train_set,\n            self.datamodule.train_set_indices,\n            self.poisoned_percent,\n            self.poisoned_noise_percent,\n        )\n</code></pre>"},{"location":"api/addons/attacks/dataset/datapoison/#nebula.addons.attacks.dataset.datapoison.SamplePoisoningAttack.__init__","title":"<code>__init__(engine, attack_params)</code>","text":"<p>Initialize the sample poisoning attack.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <p>The engine managing the attack context</p> required <code>attack_params</code> <code>Dict</code> <p>Dictionary containing attack parameters</p> required Source code in <code>nebula/addons/attacks/dataset/datapoison.py</code> <pre><code>def __init__(self, engine, attack_params: Dict):\n    \"\"\"\n    Initialize the sample poisoning attack.\n\n    Args:\n        engine: The engine managing the attack context\n        attack_params: Dictionary containing attack parameters\n    \"\"\"\n    try:\n        round_start = int(attack_params[\"round_start_attack\"])\n        round_stop = int(attack_params[\"round_stop_attack\"])\n        attack_interval = int(attack_params[\"attack_interval\"])\n    except KeyError as e:\n        raise ValueError(f\"Missing required attack parameter: {e}\")\n    except ValueError:\n        raise ValueError(\"Invalid value in attack_params. Ensure all values are integers.\")\n\n    super().__init__(engine, round_start, round_stop, attack_interval)\n    self.datamodule = engine._trainer.datamodule\n    self.poisoned_percent = float(attack_params[\"poisoned_sample_percent\"])\n    self.poisoned_noise_percent = float(attack_params[\"poisoned_noise_percent\"])\n\n    # Create the appropriate strategy based on whether the attack is targeted\n    if attack_params.get(\"targeted\", False):\n        target_label = int(attack_params.get(\"target_label\") or attack_params.get(\"targetLabel\", 4))\n        self.strategy = TargetedSamplePoisoningStrategy(target_label)\n    else:\n        noise_type = (attack_params.get(\"noise_type\") or attack_params.get(\"noiseType\", \"Gaussian\")).lower()\n        self.strategy = NonTargetedSamplePoisoningStrategy(noise_type)\n</code></pre>"},{"location":"api/addons/attacks/dataset/datapoison/#nebula.addons.attacks.dataset.datapoison.SamplePoisoningAttack.get_malicious_dataset","title":"<code>get_malicious_dataset()</code>","text":"<p>Creates a malicious dataset by poisoning selected data points.</p> <p>Returns:</p> Name Type Description <code>Dataset</code> <p>The modified dataset with poisoned data</p> Source code in <code>nebula/addons/attacks/dataset/datapoison.py</code> <pre><code>def get_malicious_dataset(self):\n    \"\"\"\n    Creates a malicious dataset by poisoning selected data points.\n\n    Returns:\n        Dataset: The modified dataset with poisoned data\n    \"\"\"\n    return self.strategy.poison_data(\n        self.datamodule.train_set,\n        self.datamodule.train_set_indices,\n        self.poisoned_percent,\n        self.poisoned_noise_percent,\n    )\n</code></pre>"},{"location":"api/addons/attacks/dataset/datapoison/#nebula.addons.attacks.dataset.datapoison.TargetedSamplePoisoningStrategy","title":"<code>TargetedSamplePoisoningStrategy</code>","text":"<p>               Bases: <code>DataPoisoningStrategy</code></p> <p>Implementation of targeted poisoning strategy using X pattern.</p> Source code in <code>nebula/addons/attacks/dataset/datapoison.py</code> <pre><code>class TargetedSamplePoisoningStrategy(DataPoisoningStrategy):\n    \"\"\"Implementation of targeted poisoning strategy using X pattern.\"\"\"\n\n    def __init__(self, target_label: int):\n        \"\"\"\n        Initialize targeted poisoning strategy.\n\n        Args:\n            target_label: The label to target for poisoning\n        \"\"\"\n        self.target_label = target_label\n\n    def add_x_to_image(self, img: torch.Tensor | Image.Image) -&gt; torch.Tensor:\n        \"\"\"\n        Adds a 10x10 pixel 'X' mark to the top-left corner of an image.\n\n        Args:\n            img: Input image tensor or PIL Image\n\n        Returns:\n            Modified image with X pattern\n        \"\"\"\n        logging.info(f\"[{self.__class__.__name__}] Adding X pattern to image\")\n        img = self._convert_to_tensor(img)\n        img, is_single_point = self._handle_single_point(img)\n\n        # Handle batch dimension if present\n        if len(img.shape) &gt; 3:\n            batch_size = img.shape[0]\n            img = img.view(-1, *img.shape[-3:])\n        else:\n            batch_size = 1\n\n        # Ensure image is large enough\n        if img.shape[-2] &lt; 10 or img.shape[-1] &lt; 10:\n            logging.warning(f\"Image too small for X pattern: {img.shape}\")\n            return img\n\n        # Determine if image is normalized (0-1) or not (0-255)\n        is_normalized = img.max() &lt;= 1.0\n        pattern_value = 1.0 if is_normalized else 255.0\n\n        # Create X pattern\n        for i in range(0, 10):\n            for j in range(0, 10):\n                if i + j &lt;= 9 or i == j:\n                    if len(img.shape) == 3:  # RGB image\n                        img[..., i, j] = pattern_value\n                    else:  # Grayscale image\n                        img[..., i, j] = pattern_value\n\n        # Restore batch dimension if it was present\n        if batch_size &gt; 1:\n            img = img.view(batch_size, *img.shape[1:])\n\n        if is_single_point:\n            img = img[0]\n\n        return img\n\n    def poison_data(\n        self,\n        dataset,\n        indices: list[int],\n        poisoned_percent: float,\n        poisoned_noise_percent: float,\n    ) -&gt; \"Dataset\":\n        \"\"\"\n        Applies X-pattern poisoning to targeted samples.\n\n        Args:\n            dataset: The dataset to modify\n            indices: List of indices to consider for poisoning\n            poisoned_percent: Not used in targeted poisoning\n            poisoned_noise_percent: Not used in targeted poisoning\n\n        Returns:\n            Modified dataset with poisoned data\n        \"\"\"\n        logging.info(f\"[{self.__class__.__name__}] Poisoning data with X pattern for target label: {self.target_label}\")\n        new_dataset = copy.deepcopy(dataset)\n        if not isinstance(new_dataset.targets, np.ndarray):\n            new_dataset.targets = np.array(new_dataset.targets)\n        else:\n            new_dataset.targets = new_dataset.targets.copy()\n\n        for i in indices:\n            if int(new_dataset.targets[i]) == int(self.target_label):\n                t = new_dataset.data[i]\n                logging.info(f\"[{self.__class__.__name__}] Adding X pattern to image\")\n                poisoned = self.add_x_to_image(t)\n\n                if isinstance(t, tuple):\n                    poisoned = (poisoned, t[1])\n\n                new_dataset.data[i] = poisoned\n\n        return new_dataset\n</code></pre>"},{"location":"api/addons/attacks/dataset/datapoison/#nebula.addons.attacks.dataset.datapoison.TargetedSamplePoisoningStrategy.__init__","title":"<code>__init__(target_label)</code>","text":"<p>Initialize targeted poisoning strategy.</p> <p>Parameters:</p> Name Type Description Default <code>target_label</code> <code>int</code> <p>The label to target for poisoning</p> required Source code in <code>nebula/addons/attacks/dataset/datapoison.py</code> <pre><code>def __init__(self, target_label: int):\n    \"\"\"\n    Initialize targeted poisoning strategy.\n\n    Args:\n        target_label: The label to target for poisoning\n    \"\"\"\n    self.target_label = target_label\n</code></pre>"},{"location":"api/addons/attacks/dataset/datapoison/#nebula.addons.attacks.dataset.datapoison.TargetedSamplePoisoningStrategy.add_x_to_image","title":"<code>add_x_to_image(img)</code>","text":"<p>Adds a 10x10 pixel 'X' mark to the top-left corner of an image.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>Tensor | Image</code> <p>Input image tensor or PIL Image</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Modified image with X pattern</p> Source code in <code>nebula/addons/attacks/dataset/datapoison.py</code> <pre><code>def add_x_to_image(self, img: torch.Tensor | Image.Image) -&gt; torch.Tensor:\n    \"\"\"\n    Adds a 10x10 pixel 'X' mark to the top-left corner of an image.\n\n    Args:\n        img: Input image tensor or PIL Image\n\n    Returns:\n        Modified image with X pattern\n    \"\"\"\n    logging.info(f\"[{self.__class__.__name__}] Adding X pattern to image\")\n    img = self._convert_to_tensor(img)\n    img, is_single_point = self._handle_single_point(img)\n\n    # Handle batch dimension if present\n    if len(img.shape) &gt; 3:\n        batch_size = img.shape[0]\n        img = img.view(-1, *img.shape[-3:])\n    else:\n        batch_size = 1\n\n    # Ensure image is large enough\n    if img.shape[-2] &lt; 10 or img.shape[-1] &lt; 10:\n        logging.warning(f\"Image too small for X pattern: {img.shape}\")\n        return img\n\n    # Determine if image is normalized (0-1) or not (0-255)\n    is_normalized = img.max() &lt;= 1.0\n    pattern_value = 1.0 if is_normalized else 255.0\n\n    # Create X pattern\n    for i in range(0, 10):\n        for j in range(0, 10):\n            if i + j &lt;= 9 or i == j:\n                if len(img.shape) == 3:  # RGB image\n                    img[..., i, j] = pattern_value\n                else:  # Grayscale image\n                    img[..., i, j] = pattern_value\n\n    # Restore batch dimension if it was present\n    if batch_size &gt; 1:\n        img = img.view(batch_size, *img.shape[1:])\n\n    if is_single_point:\n        img = img[0]\n\n    return img\n</code></pre>"},{"location":"api/addons/attacks/dataset/datapoison/#nebula.addons.attacks.dataset.datapoison.TargetedSamplePoisoningStrategy.poison_data","title":"<code>poison_data(dataset, indices, poisoned_percent, poisoned_noise_percent)</code>","text":"<p>Applies X-pattern poisoning to targeted samples.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <p>The dataset to modify</p> required <code>indices</code> <code>list[int]</code> <p>List of indices to consider for poisoning</p> required <code>poisoned_percent</code> <code>float</code> <p>Not used in targeted poisoning</p> required <code>poisoned_noise_percent</code> <code>float</code> <p>Not used in targeted poisoning</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>Modified dataset with poisoned data</p> Source code in <code>nebula/addons/attacks/dataset/datapoison.py</code> <pre><code>def poison_data(\n    self,\n    dataset,\n    indices: list[int],\n    poisoned_percent: float,\n    poisoned_noise_percent: float,\n) -&gt; \"Dataset\":\n    \"\"\"\n    Applies X-pattern poisoning to targeted samples.\n\n    Args:\n        dataset: The dataset to modify\n        indices: List of indices to consider for poisoning\n        poisoned_percent: Not used in targeted poisoning\n        poisoned_noise_percent: Not used in targeted poisoning\n\n    Returns:\n        Modified dataset with poisoned data\n    \"\"\"\n    logging.info(f\"[{self.__class__.__name__}] Poisoning data with X pattern for target label: {self.target_label}\")\n    new_dataset = copy.deepcopy(dataset)\n    if not isinstance(new_dataset.targets, np.ndarray):\n        new_dataset.targets = np.array(new_dataset.targets)\n    else:\n        new_dataset.targets = new_dataset.targets.copy()\n\n    for i in indices:\n        if int(new_dataset.targets[i]) == int(self.target_label):\n            t = new_dataset.data[i]\n            logging.info(f\"[{self.__class__.__name__}] Adding X pattern to image\")\n            poisoned = self.add_x_to_image(t)\n\n            if isinstance(t, tuple):\n                poisoned = (poisoned, t[1])\n\n            new_dataset.data[i] = poisoned\n\n    return new_dataset\n</code></pre>"},{"location":"api/addons/attacks/dataset/datasetattack/","title":"Documentation for Datasetattack Module","text":""},{"location":"api/addons/attacks/dataset/datasetattack/#nebula.addons.attacks.dataset.datasetattack.DatasetAttack","title":"<code>DatasetAttack</code>","text":"<p>               Bases: <code>Attack</code></p> <p>Implements an attack that replaces the training dataset with a malicious version during specific rounds of the engine's execution.</p> <p>This attack modifies the dataset used by the engine's trainer to introduce malicious data, potentially impacting the model's training process.</p> Source code in <code>nebula/addons/attacks/dataset/datasetattack.py</code> <pre><code>class DatasetAttack(Attack):\n    \"\"\"\n    Implements an attack that replaces the training dataset with a malicious version\n    during specific rounds of the engine's execution.\n\n    This attack modifies the dataset used by the engine's trainer to introduce malicious\n    data, potentially impacting the model's training process.\n    \"\"\"\n\n    def __init__(self, engine, round_start_attack, round_stop_attack, attack_interval):\n        \"\"\"\n        Initializes the DatasetAttack with the given engine.\n\n        Args:\n            engine: The engine managing the attack context.\n        \"\"\"\n        self.engine = engine\n        self.round_start_attack = round_start_attack\n        self.round_stop_attack = round_stop_attack\n        self.attack_interval = attack_interval\n\n    async def attack(self):\n        \"\"\"\n        Performs the attack by replacing the training dataset with a malicious version.\n\n        During the specified rounds of the attack, the engine's trainer is provided\n        with a malicious dataset. The attack is stopped when the engine reaches the\n        designated stop round.\n        \"\"\"\n        if self.engine.round not in range(self.round_start_attack, self.round_stop_attack + 1):\n            pass\n        elif self.engine.round == self.round_stop_attack:\n            logging.info(f\"[{self.__class__.__name__}] Stopping attack\")\n        elif self.engine.round &gt;= self.round_start_attack and (\n            (self.engine.round - self.round_start_attack) % self.attack_interval == 0\n        ):\n            logging.info(f\"[{self.__class__.__name__}] Performing attack\")\n            self.engine.trainer.datamodule.train_set = self.get_malicious_dataset()\n\n    async def _inject_malicious_behaviour(self, target_function, *args, **kwargs):\n        \"\"\"\n        Abstract method for injecting malicious behavior into a target function.\n\n        This method is not implemented in this class and must be overridden by subclasses\n        if additional malicious behavior is required.\n\n        Args:\n            target_function (callable): The function to inject the malicious behavior into.\n            *args: Positional arguments for the malicious behavior.\n            **kwargs: Keyword arguments for the malicious behavior.\n\n        Raises:\n            NotImplementedError: This method is not implemented in this class.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_malicious_dataset(self):\n        \"\"\"\n        Abstract method to retrieve the malicious dataset.\n\n        Subclasses must implement this method to define how the malicious dataset\n        is created or retrieved.\n\n        Raises:\n            NotImplementedError: If the method is not implemented in a subclass.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api/addons/attacks/dataset/datasetattack/#nebula.addons.attacks.dataset.datasetattack.DatasetAttack.__init__","title":"<code>__init__(engine, round_start_attack, round_stop_attack, attack_interval)</code>","text":"<p>Initializes the DatasetAttack with the given engine.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <p>The engine managing the attack context.</p> required Source code in <code>nebula/addons/attacks/dataset/datasetattack.py</code> <pre><code>def __init__(self, engine, round_start_attack, round_stop_attack, attack_interval):\n    \"\"\"\n    Initializes the DatasetAttack with the given engine.\n\n    Args:\n        engine: The engine managing the attack context.\n    \"\"\"\n    self.engine = engine\n    self.round_start_attack = round_start_attack\n    self.round_stop_attack = round_stop_attack\n    self.attack_interval = attack_interval\n</code></pre>"},{"location":"api/addons/attacks/dataset/datasetattack/#nebula.addons.attacks.dataset.datasetattack.DatasetAttack.attack","title":"<code>attack()</code>  <code>async</code>","text":"<p>Performs the attack by replacing the training dataset with a malicious version.</p> <p>During the specified rounds of the attack, the engine's trainer is provided with a malicious dataset. The attack is stopped when the engine reaches the designated stop round.</p> Source code in <code>nebula/addons/attacks/dataset/datasetattack.py</code> <pre><code>async def attack(self):\n    \"\"\"\n    Performs the attack by replacing the training dataset with a malicious version.\n\n    During the specified rounds of the attack, the engine's trainer is provided\n    with a malicious dataset. The attack is stopped when the engine reaches the\n    designated stop round.\n    \"\"\"\n    if self.engine.round not in range(self.round_start_attack, self.round_stop_attack + 1):\n        pass\n    elif self.engine.round == self.round_stop_attack:\n        logging.info(f\"[{self.__class__.__name__}] Stopping attack\")\n    elif self.engine.round &gt;= self.round_start_attack and (\n        (self.engine.round - self.round_start_attack) % self.attack_interval == 0\n    ):\n        logging.info(f\"[{self.__class__.__name__}] Performing attack\")\n        self.engine.trainer.datamodule.train_set = self.get_malicious_dataset()\n</code></pre>"},{"location":"api/addons/attacks/dataset/datasetattack/#nebula.addons.attacks.dataset.datasetattack.DatasetAttack.get_malicious_dataset","title":"<code>get_malicious_dataset()</code>  <code>abstractmethod</code>","text":"<p>Abstract method to retrieve the malicious dataset.</p> <p>Subclasses must implement this method to define how the malicious dataset is created or retrieved.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the method is not implemented in a subclass.</p> Source code in <code>nebula/addons/attacks/dataset/datasetattack.py</code> <pre><code>@abstractmethod\ndef get_malicious_dataset(self):\n    \"\"\"\n    Abstract method to retrieve the malicious dataset.\n\n    Subclasses must implement this method to define how the malicious dataset\n    is created or retrieved.\n\n    Raises:\n        NotImplementedError: If the method is not implemented in a subclass.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/addons/attacks/dataset/labelflipping/","title":"Documentation for Labelflipping Module","text":"<p>This module provides classes for label flipping attacks in datasets, allowing for the simulation of label noise as a form of data poisoning. It implements both targeted and non-targeted label flipping attacks.</p> <p>Classes: - LabelFlippingAttack: Main attack class that implements the DatasetAttack interface - LabelFlippingStrategy: Abstract base class for label flipping strategies - TargetedLabelFlippingStrategy: Implementation for targeted label flipping - NonTargetedLabelFlippingStrategy: Implementation for non-targeted label flipping</p>"},{"location":"api/addons/attacks/dataset/labelflipping/#nebula.addons.attacks.dataset.labelflipping.LabelFlippingAttack","title":"<code>LabelFlippingAttack</code>","text":"<p>               Bases: <code>DatasetAttack</code></p> <p>Implements a label flipping attack that can be either targeted or non-targeted.</p> Source code in <code>nebula/addons/attacks/dataset/labelflipping.py</code> <pre><code>class LabelFlippingAttack(DatasetAttack):\n    \"\"\"\n    Implements a label flipping attack that can be either targeted or non-targeted.\n    \"\"\"\n\n    def __init__(self, engine, attack_params: Dict):\n        \"\"\"\n        Initialize the label flipping attack.\n\n        Args:\n            engine: The engine managing the attack context\n            attack_params: Dictionary containing attack parameters\n        \"\"\"\n        try:\n            round_start = int(attack_params[\"round_start_attack\"])\n            round_stop = int(attack_params[\"round_stop_attack\"])\n            attack_interval = int(attack_params[\"attack_interval\"])\n        except KeyError as e:\n            raise ValueError(f\"Missing required attack parameter: {e}\")\n        except ValueError:\n            raise ValueError(\"Invalid value in attack_params. Ensure all values are integers.\")\n\n        super().__init__(engine, round_start, round_stop, attack_interval)\n        self.datamodule = engine._trainer.datamodule\n        self.poisoned_percent = float(attack_params[\"poisoned_sample_percent\"])\n\n        # Create the appropriate strategy based on whether the attack is targeted\n        if attack_params.get(\"targeted\", False):\n            target_label = int(attack_params.get(\"target_label\") or attack_params.get(\"targetLabel\", 4))\n            target_changed_label = int(\n                attack_params.get(\"target_changed_label\") or attack_params.get(\"targetChangedLabel\", 7)\n            )\n            self.strategy = TargetedLabelFlippingStrategy(target_label, target_changed_label)\n        else:\n            self.strategy = NonTargetedLabelFlippingStrategy()\n\n    def get_malicious_dataset(self):\n        \"\"\"\n        Creates a malicious dataset by flipping the labels of selected data points.\n\n        Returns:\n            Dataset: The modified dataset with flipped labels\n        \"\"\"\n        return self.strategy.flip_labels(\n            self.datamodule.train_set,\n            self.datamodule.train_set_indices,\n            self.poisoned_percent,\n        )\n</code></pre>"},{"location":"api/addons/attacks/dataset/labelflipping/#nebula.addons.attacks.dataset.labelflipping.LabelFlippingAttack.__init__","title":"<code>__init__(engine, attack_params)</code>","text":"<p>Initialize the label flipping attack.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <p>The engine managing the attack context</p> required <code>attack_params</code> <code>Dict</code> <p>Dictionary containing attack parameters</p> required Source code in <code>nebula/addons/attacks/dataset/labelflipping.py</code> <pre><code>def __init__(self, engine, attack_params: Dict):\n    \"\"\"\n    Initialize the label flipping attack.\n\n    Args:\n        engine: The engine managing the attack context\n        attack_params: Dictionary containing attack parameters\n    \"\"\"\n    try:\n        round_start = int(attack_params[\"round_start_attack\"])\n        round_stop = int(attack_params[\"round_stop_attack\"])\n        attack_interval = int(attack_params[\"attack_interval\"])\n    except KeyError as e:\n        raise ValueError(f\"Missing required attack parameter: {e}\")\n    except ValueError:\n        raise ValueError(\"Invalid value in attack_params. Ensure all values are integers.\")\n\n    super().__init__(engine, round_start, round_stop, attack_interval)\n    self.datamodule = engine._trainer.datamodule\n    self.poisoned_percent = float(attack_params[\"poisoned_sample_percent\"])\n\n    # Create the appropriate strategy based on whether the attack is targeted\n    if attack_params.get(\"targeted\", False):\n        target_label = int(attack_params.get(\"target_label\") or attack_params.get(\"targetLabel\", 4))\n        target_changed_label = int(\n            attack_params.get(\"target_changed_label\") or attack_params.get(\"targetChangedLabel\", 7)\n        )\n        self.strategy = TargetedLabelFlippingStrategy(target_label, target_changed_label)\n    else:\n        self.strategy = NonTargetedLabelFlippingStrategy()\n</code></pre>"},{"location":"api/addons/attacks/dataset/labelflipping/#nebula.addons.attacks.dataset.labelflipping.LabelFlippingAttack.get_malicious_dataset","title":"<code>get_malicious_dataset()</code>","text":"<p>Creates a malicious dataset by flipping the labels of selected data points.</p> <p>Returns:</p> Name Type Description <code>Dataset</code> <p>The modified dataset with flipped labels</p> Source code in <code>nebula/addons/attacks/dataset/labelflipping.py</code> <pre><code>def get_malicious_dataset(self):\n    \"\"\"\n    Creates a malicious dataset by flipping the labels of selected data points.\n\n    Returns:\n        Dataset: The modified dataset with flipped labels\n    \"\"\"\n    return self.strategy.flip_labels(\n        self.datamodule.train_set,\n        self.datamodule.train_set_indices,\n        self.poisoned_percent,\n    )\n</code></pre>"},{"location":"api/addons/attacks/dataset/labelflipping/#nebula.addons.attacks.dataset.labelflipping.LabelFlippingStrategy","title":"<code>LabelFlippingStrategy</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for label flipping strategies.</p> Source code in <code>nebula/addons/attacks/dataset/labelflipping.py</code> <pre><code>class LabelFlippingStrategy(ABC):\n    \"\"\"Abstract base class for label flipping strategies.\"\"\"\n\n    @abstractmethod\n    def flip_labels(\n        self,\n        dataset,\n        indices: list[int],\n        poisoned_percent: float,\n    ) -&gt; \"Dataset\":\n        \"\"\"\n        Abstract method to flip labels in the dataset.\n\n        Args:\n            dataset: The dataset to modify\n            indices: List of indices to consider for flipping\n            poisoned_percent: Percentage of labels to change (0-100)\n\n        Returns:\n            Modified dataset with flipped labels\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/addons/attacks/dataset/labelflipping/#nebula.addons.attacks.dataset.labelflipping.LabelFlippingStrategy.flip_labels","title":"<code>flip_labels(dataset, indices, poisoned_percent)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to flip labels in the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <p>The dataset to modify</p> required <code>indices</code> <code>list[int]</code> <p>List of indices to consider for flipping</p> required <code>poisoned_percent</code> <code>float</code> <p>Percentage of labels to change (0-100)</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>Modified dataset with flipped labels</p> Source code in <code>nebula/addons/attacks/dataset/labelflipping.py</code> <pre><code>@abstractmethod\ndef flip_labels(\n    self,\n    dataset,\n    indices: list[int],\n    poisoned_percent: float,\n) -&gt; \"Dataset\":\n    \"\"\"\n    Abstract method to flip labels in the dataset.\n\n    Args:\n        dataset: The dataset to modify\n        indices: List of indices to consider for flipping\n        poisoned_percent: Percentage of labels to change (0-100)\n\n    Returns:\n        Modified dataset with flipped labels\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/addons/attacks/dataset/labelflipping/#nebula.addons.attacks.dataset.labelflipping.NonTargetedLabelFlippingStrategy","title":"<code>NonTargetedLabelFlippingStrategy</code>","text":"<p>               Bases: <code>LabelFlippingStrategy</code></p> <p>Implementation of non-targeted label flipping strategy.</p> Source code in <code>nebula/addons/attacks/dataset/labelflipping.py</code> <pre><code>class NonTargetedLabelFlippingStrategy(LabelFlippingStrategy):\n    \"\"\"Implementation of non-targeted label flipping strategy.\"\"\"\n\n    def flip_labels(\n        self,\n        dataset,\n        indices: list[int],\n        poisoned_percent: float,\n    ) -&gt; \"Dataset\":\n        \"\"\"\n        Flips labels randomly to different classes.\n\n        Args:\n            dataset: The dataset to modify\n            indices: List of indices to consider for flipping\n            poisoned_percent: Percentage of labels to change (0-100)\n\n        Returns:\n            Modified dataset with flipped labels\n        \"\"\"\n        new_dataset = copy.deepcopy(dataset)\n        if not isinstance(new_dataset.targets, np.ndarray):\n            new_dataset.targets = np.array(new_dataset.targets)\n        else:\n            new_dataset.targets = new_dataset.targets.copy()\n\n        num_indices = len(indices)\n        num_flipped = int(poisoned_percent * num_indices / 100.0)\n\n        if num_indices == 0 or num_flipped &gt; num_indices:\n            return new_dataset\n\n        flipped_indices = random.sample(indices, num_flipped)\n        class_list = list(set(new_dataset.targets.tolist()))\n\n        for i in flipped_indices:\n            current_label = new_dataset.targets[i]\n            new_label = random.choice(class_list)\n            while new_label == current_label:\n                new_label = random.choice(class_list)\n            new_dataset.targets[i] = new_label\n\n        return new_dataset\n</code></pre>"},{"location":"api/addons/attacks/dataset/labelflipping/#nebula.addons.attacks.dataset.labelflipping.NonTargetedLabelFlippingStrategy.flip_labels","title":"<code>flip_labels(dataset, indices, poisoned_percent)</code>","text":"<p>Flips labels randomly to different classes.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <p>The dataset to modify</p> required <code>indices</code> <code>list[int]</code> <p>List of indices to consider for flipping</p> required <code>poisoned_percent</code> <code>float</code> <p>Percentage of labels to change (0-100)</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>Modified dataset with flipped labels</p> Source code in <code>nebula/addons/attacks/dataset/labelflipping.py</code> <pre><code>def flip_labels(\n    self,\n    dataset,\n    indices: list[int],\n    poisoned_percent: float,\n) -&gt; \"Dataset\":\n    \"\"\"\n    Flips labels randomly to different classes.\n\n    Args:\n        dataset: The dataset to modify\n        indices: List of indices to consider for flipping\n        poisoned_percent: Percentage of labels to change (0-100)\n\n    Returns:\n        Modified dataset with flipped labels\n    \"\"\"\n    new_dataset = copy.deepcopy(dataset)\n    if not isinstance(new_dataset.targets, np.ndarray):\n        new_dataset.targets = np.array(new_dataset.targets)\n    else:\n        new_dataset.targets = new_dataset.targets.copy()\n\n    num_indices = len(indices)\n    num_flipped = int(poisoned_percent * num_indices / 100.0)\n\n    if num_indices == 0 or num_flipped &gt; num_indices:\n        return new_dataset\n\n    flipped_indices = random.sample(indices, num_flipped)\n    class_list = list(set(new_dataset.targets.tolist()))\n\n    for i in flipped_indices:\n        current_label = new_dataset.targets[i]\n        new_label = random.choice(class_list)\n        while new_label == current_label:\n            new_label = random.choice(class_list)\n        new_dataset.targets[i] = new_label\n\n    return new_dataset\n</code></pre>"},{"location":"api/addons/attacks/dataset/labelflipping/#nebula.addons.attacks.dataset.labelflipping.TargetedLabelFlippingStrategy","title":"<code>TargetedLabelFlippingStrategy</code>","text":"<p>               Bases: <code>LabelFlippingStrategy</code></p> <p>Implementation of targeted label flipping strategy.</p> Source code in <code>nebula/addons/attacks/dataset/labelflipping.py</code> <pre><code>class TargetedLabelFlippingStrategy(LabelFlippingStrategy):\n    \"\"\"Implementation of targeted label flipping strategy.\"\"\"\n\n    def __init__(self, target_label: int, target_changed_label: int):\n        \"\"\"\n        Initialize targeted label flipping strategy.\n\n        Args:\n            target_label: The label to change\n            target_changed_label: The label to change to\n        \"\"\"\n        self.target_label = target_label\n        self.target_changed_label = target_changed_label\n\n    def flip_labels(\n        self,\n        dataset,\n        indices: list[int],\n        poisoned_percent: float,\n    ) -&gt; \"Dataset\":\n        \"\"\"\n        Flips labels from target_label to target_changed_label.\n\n        Args:\n            dataset: The dataset to modify\n            indices: List of indices to consider for flipping\n            poisoned_percent: Percentage of labels to change (0-100)\n\n        Returns:\n            Modified dataset with flipped labels\n        \"\"\"\n        new_dataset = copy.deepcopy(dataset)\n        if not isinstance(new_dataset.targets, np.ndarray):\n            new_dataset.targets = np.array(new_dataset.targets)\n        else:\n            new_dataset.targets = new_dataset.targets.copy()\n\n        for i in indices:\n            if int(new_dataset.targets[i]) == self.target_label:\n                new_dataset.targets[i] = self.target_changed_label\n\n        if self.target_label in new_dataset.targets:\n            logging.info(f\"[{self.__class__.__name__}] Target label {self.target_label} still present after flipping.\")\n        else:\n            logging.info(\n                f\"[{self.__class__.__name__}] Target label {self.target_label} successfully flipped to {self.target_changed_label}.\"\n            )\n\n        return new_dataset\n</code></pre>"},{"location":"api/addons/attacks/dataset/labelflipping/#nebula.addons.attacks.dataset.labelflipping.TargetedLabelFlippingStrategy.__init__","title":"<code>__init__(target_label, target_changed_label)</code>","text":"<p>Initialize targeted label flipping strategy.</p> <p>Parameters:</p> Name Type Description Default <code>target_label</code> <code>int</code> <p>The label to change</p> required <code>target_changed_label</code> <code>int</code> <p>The label to change to</p> required Source code in <code>nebula/addons/attacks/dataset/labelflipping.py</code> <pre><code>def __init__(self, target_label: int, target_changed_label: int):\n    \"\"\"\n    Initialize targeted label flipping strategy.\n\n    Args:\n        target_label: The label to change\n        target_changed_label: The label to change to\n    \"\"\"\n    self.target_label = target_label\n    self.target_changed_label = target_changed_label\n</code></pre>"},{"location":"api/addons/attacks/dataset/labelflipping/#nebula.addons.attacks.dataset.labelflipping.TargetedLabelFlippingStrategy.flip_labels","title":"<code>flip_labels(dataset, indices, poisoned_percent)</code>","text":"<p>Flips labels from target_label to target_changed_label.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <p>The dataset to modify</p> required <code>indices</code> <code>list[int]</code> <p>List of indices to consider for flipping</p> required <code>poisoned_percent</code> <code>float</code> <p>Percentage of labels to change (0-100)</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>Modified dataset with flipped labels</p> Source code in <code>nebula/addons/attacks/dataset/labelflipping.py</code> <pre><code>def flip_labels(\n    self,\n    dataset,\n    indices: list[int],\n    poisoned_percent: float,\n) -&gt; \"Dataset\":\n    \"\"\"\n    Flips labels from target_label to target_changed_label.\n\n    Args:\n        dataset: The dataset to modify\n        indices: List of indices to consider for flipping\n        poisoned_percent: Percentage of labels to change (0-100)\n\n    Returns:\n        Modified dataset with flipped labels\n    \"\"\"\n    new_dataset = copy.deepcopy(dataset)\n    if not isinstance(new_dataset.targets, np.ndarray):\n        new_dataset.targets = np.array(new_dataset.targets)\n    else:\n        new_dataset.targets = new_dataset.targets.copy()\n\n    for i in indices:\n        if int(new_dataset.targets[i]) == self.target_label:\n            new_dataset.targets[i] = self.target_changed_label\n\n    if self.target_label in new_dataset.targets:\n        logging.info(f\"[{self.__class__.__name__}] Target label {self.target_label} still present after flipping.\")\n    else:\n        logging.info(\n            f\"[{self.__class__.__name__}] Target label {self.target_label} successfully flipped to {self.target_changed_label}.\"\n        )\n\n    return new_dataset\n</code></pre>"},{"location":"api/addons/attacks/model/","title":"Documentation for Model Module","text":""},{"location":"api/addons/attacks/model/gllneuroninversion/","title":"Documentation for Gllneuroninversion Module","text":""},{"location":"api/addons/attacks/model/gllneuroninversion/#nebula.addons.attacks.model.gllneuroninversion.GLLNeuronInversionAttack","title":"<code>GLLNeuronInversionAttack</code>","text":"<p>               Bases: <code>ModelAttack</code></p> <p>Implements a neuron inversion attack on the received model weights.</p> <p>This attack aims to invert the values of neurons in specific layers by replacing their values with random noise, potentially disrupting the model's functionality during aggregation.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>object</code> <p>The training engine object that manages the aggregator.</p> required <code>_</code> <code>any</code> <p>A placeholder argument (not used in this class).</p> required Source code in <code>nebula/addons/attacks/model/gllneuroninversion.py</code> <pre><code>class GLLNeuronInversionAttack(ModelAttack):\n    \"\"\"\n    Implements a neuron inversion attack on the received model weights.\n\n    This attack aims to invert the values of neurons in specific layers\n    by replacing their values with random noise, potentially disrupting the model's\n    functionality during aggregation.\n\n    Args:\n        engine (object): The training engine object that manages the aggregator.\n        _ (any): A placeholder argument (not used in this class).\n    \"\"\"\n\n    def __init__(self, engine, attack_params):\n        \"\"\"\n        Initializes the GLLNeuronInversionAttack with the specified engine.\n\n        Args:\n            engine (object): The training engine object.\n            _ (any): A placeholder argument (not used in this class).\n        \"\"\"\n        try:\n            round_start = int(attack_params[\"round_start_attack\"])\n            round_stop = int(attack_params[\"round_stop_attack\"])\n            attack_interval = int(attack_params[\"attack_interval\"])\n        except KeyError as e:\n            raise ValueError(f\"Missing required attack parameter: {e}\")\n        except ValueError:\n            raise ValueError(\"Invalid value in attack_params. Ensure all values are integers.\")\n\n        super().__init__(engine, round_start, round_stop, attack_interval)\n\n    def model_attack(self, received_weights):\n        \"\"\"\n        Applies a neuron inversion attack by injecting high-magnitude random noise into a target layer.\n\n        This attack targets a specific layer (typically the penultimate fully connected layer)\n        and overwrites all its weights with large random values. The intent is to cause extreme\n        activations or exploding gradients, which can degrade model performance or destabilize training.\n\n        Args:\n            received_weights (dict): Dictionary of model weights with parameter names as keys.\n\n        Returns:\n            dict: Modified model weights after injecting noise into the selected layer.\n        \"\"\"\n        logging.info(\"[NeuronInversionAttack] Injecting random noise into neuron layer\")\n\n        # Get list of layer names\n        layer_keys = list(received_weights.keys())\n        target_key = layer_keys[-2]  # Target penultimate weight matrix\n        target_weights = received_weights[target_key]\n\n        # Use configurable scale or default to a high perturbation\n        # noise_scale = getattr(self, 'noise_scale', 1e4)\n        noise_scale = 10000\n        logging.info(f\"Target layer: {target_key}, Noise scale: {noise_scale}\")\n\n        # Inject random noise of the same shape and type\n        received_weights[target_key] = torch.empty_like(target_weights).uniform_(0, noise_scale)\n\n        return received_weights\n</code></pre>"},{"location":"api/addons/attacks/model/gllneuroninversion/#nebula.addons.attacks.model.gllneuroninversion.GLLNeuronInversionAttack.__init__","title":"<code>__init__(engine, attack_params)</code>","text":"<p>Initializes the GLLNeuronInversionAttack with the specified engine.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>object</code> <p>The training engine object.</p> required <code>_</code> <code>any</code> <p>A placeholder argument (not used in this class).</p> required Source code in <code>nebula/addons/attacks/model/gllneuroninversion.py</code> <pre><code>def __init__(self, engine, attack_params):\n    \"\"\"\n    Initializes the GLLNeuronInversionAttack with the specified engine.\n\n    Args:\n        engine (object): The training engine object.\n        _ (any): A placeholder argument (not used in this class).\n    \"\"\"\n    try:\n        round_start = int(attack_params[\"round_start_attack\"])\n        round_stop = int(attack_params[\"round_stop_attack\"])\n        attack_interval = int(attack_params[\"attack_interval\"])\n    except KeyError as e:\n        raise ValueError(f\"Missing required attack parameter: {e}\")\n    except ValueError:\n        raise ValueError(\"Invalid value in attack_params. Ensure all values are integers.\")\n\n    super().__init__(engine, round_start, round_stop, attack_interval)\n</code></pre>"},{"location":"api/addons/attacks/model/gllneuroninversion/#nebula.addons.attacks.model.gllneuroninversion.GLLNeuronInversionAttack.model_attack","title":"<code>model_attack(received_weights)</code>","text":"<p>Applies a neuron inversion attack by injecting high-magnitude random noise into a target layer.</p> <p>This attack targets a specific layer (typically the penultimate fully connected layer) and overwrites all its weights with large random values. The intent is to cause extreme activations or exploding gradients, which can degrade model performance or destabilize training.</p> <p>Parameters:</p> Name Type Description Default <code>received_weights</code> <code>dict</code> <p>Dictionary of model weights with parameter names as keys.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Modified model weights after injecting noise into the selected layer.</p> Source code in <code>nebula/addons/attacks/model/gllneuroninversion.py</code> <pre><code>def model_attack(self, received_weights):\n    \"\"\"\n    Applies a neuron inversion attack by injecting high-magnitude random noise into a target layer.\n\n    This attack targets a specific layer (typically the penultimate fully connected layer)\n    and overwrites all its weights with large random values. The intent is to cause extreme\n    activations or exploding gradients, which can degrade model performance or destabilize training.\n\n    Args:\n        received_weights (dict): Dictionary of model weights with parameter names as keys.\n\n    Returns:\n        dict: Modified model weights after injecting noise into the selected layer.\n    \"\"\"\n    logging.info(\"[NeuronInversionAttack] Injecting random noise into neuron layer\")\n\n    # Get list of layer names\n    layer_keys = list(received_weights.keys())\n    target_key = layer_keys[-2]  # Target penultimate weight matrix\n    target_weights = received_weights[target_key]\n\n    # Use configurable scale or default to a high perturbation\n    # noise_scale = getattr(self, 'noise_scale', 1e4)\n    noise_scale = 10000\n    logging.info(f\"Target layer: {target_key}, Noise scale: {noise_scale}\")\n\n    # Inject random noise of the same shape and type\n    received_weights[target_key] = torch.empty_like(target_weights).uniform_(0, noise_scale)\n\n    return received_weights\n</code></pre>"},{"location":"api/addons/attacks/model/modelattack/","title":"Documentation for Modelattack Module","text":""},{"location":"api/addons/attacks/model/modelattack/#nebula.addons.attacks.model.modelattack.ModelAttack","title":"<code>ModelAttack</code>","text":"<p>               Bases: <code>Attack</code></p> <p>Base class for implementing model attacks, which modify the behavior of model aggregation methods.</p> <p>This class defines a decorator for introducing malicious behavior into the aggregation process and requires subclasses to implement the model-specific attack logic.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>object</code> <p>The engine object that manages the aggregator for              model aggregation.</p> required Source code in <code>nebula/addons/attacks/model/modelattack.py</code> <pre><code>class ModelAttack(Attack):\n    \"\"\"\n    Base class for implementing model attacks, which modify the behavior of\n    model aggregation methods.\n\n    This class defines a decorator for introducing malicious behavior into the\n    aggregation process and requires subclasses to implement the model-specific\n    attack logic.\n\n    Args:\n        engine (object): The engine object that manages the aggregator for\n                         model aggregation.\n    \"\"\"\n\n    def __init__(self, engine, round_start_attack, round_stop_attack, attack_interval):\n        \"\"\"\n        Initializes the ModelAttack with the specified engine.\n\n        Args:\n            engine (object): The engine object that includes the aggregator.\n        \"\"\"\n        super().__init__()\n        self.engine = engine\n        self.aggregator = engine._aggregator\n        self.original_aggregation = engine.aggregator.run_aggregation\n        self.round_start_attack = round_start_attack\n        self.round_stop_attack = round_stop_attack\n        self.attack_interval = attack_interval\n        self._behaviour_injected = False\n\n    def aggregator_decorator(self):\n        \"\"\"\n        Decorator that adds a delay to the execution of the original method.\n\n        Args:\n            delay (int or float): The time in seconds to delay the method execution.\n\n        Returns:\n            function: A decorator function that wraps the target method with\n                      the delay logic and potentially modifies the aggregation\n                      behavior to inject malicious changes.\n        \"\"\"\n\n        # The actual decorator function that will be applied to the target method\n        def decorator(func):\n            @wraps(func)  # Preserves the metadata of the original function\n            def wrapper(*args):\n                _, *new_args = args  # Exclude self argument\n                accum = func(*new_args)\n                logging.info(f\"malicious_aggregate | original aggregation result={accum}\")\n\n                if new_args is not None:\n                    accum = self.model_attack(accum)\n                    logging.info(f\"malicious_aggregate | attack aggregation result={accum}\")\n                return accum\n\n            return wrapper\n\n        return decorator\n\n    @abstractmethod\n    def model_attack(self, received_weights):\n        \"\"\"\n        Abstract method that applies the specific model attack logic.\n\n        This method should be implemented in subclasses to define the attack\n        logic on the received model weights.\n\n        Args:\n            received_weights (any): The aggregated model weights to be modified.\n\n        Returns:\n            any: The modified model weights after applying the attack.\n        \"\"\"\n        raise NotImplementedError\n\n    async def _inject_malicious_behaviour(self):\n        \"\"\"\n        Modifies the `propagate` method of the aggregator to include the delay\n        introduced by the decorator.\n\n        This method wraps the original aggregation method with the malicious\n        decorator to inject the attack behavior into the aggregation process.\n        \"\"\"\n        if not self._behaviour_injected:\n            self._behaviour_injected = True\n            decorated_aggregation = self.aggregator_decorator()(self.aggregator.run_aggregation)\n            self.aggregator.run_aggregation = types.MethodType(decorated_aggregation, self.aggregator)\n\n    async def _restore_original_behaviour(self):\n        \"\"\"\n        Restores the original behaviour of the `run_aggregation` method.\n        \"\"\"\n        self.aggregator.run_aggregation = self.original_aggregation\n\n    async def attack(self):\n        \"\"\"\n        Initiates the malicious attack by injecting the malicious behavior\n        into the aggregation process.\n\n        This method logs the attack and calls the method to modify the aggregator.\n        \"\"\"\n        if self.engine.round not in range(self.round_start_attack, self.round_stop_attack + 1):\n            pass\n        elif self.engine.round == self.round_stop_attack:\n            logging.info(f\"[{self.__class__.__name__}] Stopping attack\")\n            await self._restore_original_behaviour()\n        elif (self.engine.round == self.round_start_attack) or (\n            (self.engine.round - self.round_start_attack) % self.attack_interval == 0\n        ):\n            logging.info(f\"[{self.__class__.__name__}] Performing attack\")\n            await self._inject_malicious_behaviour()\n        else:\n            await self._restore_original_behaviour()\n</code></pre>"},{"location":"api/addons/attacks/model/modelattack/#nebula.addons.attacks.model.modelattack.ModelAttack.__init__","title":"<code>__init__(engine, round_start_attack, round_stop_attack, attack_interval)</code>","text":"<p>Initializes the ModelAttack with the specified engine.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>object</code> <p>The engine object that includes the aggregator.</p> required Source code in <code>nebula/addons/attacks/model/modelattack.py</code> <pre><code>def __init__(self, engine, round_start_attack, round_stop_attack, attack_interval):\n    \"\"\"\n    Initializes the ModelAttack with the specified engine.\n\n    Args:\n        engine (object): The engine object that includes the aggregator.\n    \"\"\"\n    super().__init__()\n    self.engine = engine\n    self.aggregator = engine._aggregator\n    self.original_aggregation = engine.aggregator.run_aggregation\n    self.round_start_attack = round_start_attack\n    self.round_stop_attack = round_stop_attack\n    self.attack_interval = attack_interval\n    self._behaviour_injected = False\n</code></pre>"},{"location":"api/addons/attacks/model/modelattack/#nebula.addons.attacks.model.modelattack.ModelAttack.aggregator_decorator","title":"<code>aggregator_decorator()</code>","text":"<p>Decorator that adds a delay to the execution of the original method.</p> <p>Parameters:</p> Name Type Description Default <code>delay</code> <code>int or float</code> <p>The time in seconds to delay the method execution.</p> required <p>Returns:</p> Name Type Description <code>function</code> <p>A decorator function that wraps the target method with       the delay logic and potentially modifies the aggregation       behavior to inject malicious changes.</p> Source code in <code>nebula/addons/attacks/model/modelattack.py</code> <pre><code>def aggregator_decorator(self):\n    \"\"\"\n    Decorator that adds a delay to the execution of the original method.\n\n    Args:\n        delay (int or float): The time in seconds to delay the method execution.\n\n    Returns:\n        function: A decorator function that wraps the target method with\n                  the delay logic and potentially modifies the aggregation\n                  behavior to inject malicious changes.\n    \"\"\"\n\n    # The actual decorator function that will be applied to the target method\n    def decorator(func):\n        @wraps(func)  # Preserves the metadata of the original function\n        def wrapper(*args):\n            _, *new_args = args  # Exclude self argument\n            accum = func(*new_args)\n            logging.info(f\"malicious_aggregate | original aggregation result={accum}\")\n\n            if new_args is not None:\n                accum = self.model_attack(accum)\n                logging.info(f\"malicious_aggregate | attack aggregation result={accum}\")\n            return accum\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"api/addons/attacks/model/modelattack/#nebula.addons.attacks.model.modelattack.ModelAttack.attack","title":"<code>attack()</code>  <code>async</code>","text":"<p>Initiates the malicious attack by injecting the malicious behavior into the aggregation process.</p> <p>This method logs the attack and calls the method to modify the aggregator.</p> Source code in <code>nebula/addons/attacks/model/modelattack.py</code> <pre><code>async def attack(self):\n    \"\"\"\n    Initiates the malicious attack by injecting the malicious behavior\n    into the aggregation process.\n\n    This method logs the attack and calls the method to modify the aggregator.\n    \"\"\"\n    if self.engine.round not in range(self.round_start_attack, self.round_stop_attack + 1):\n        pass\n    elif self.engine.round == self.round_stop_attack:\n        logging.info(f\"[{self.__class__.__name__}] Stopping attack\")\n        await self._restore_original_behaviour()\n    elif (self.engine.round == self.round_start_attack) or (\n        (self.engine.round - self.round_start_attack) % self.attack_interval == 0\n    ):\n        logging.info(f\"[{self.__class__.__name__}] Performing attack\")\n        await self._inject_malicious_behaviour()\n    else:\n        await self._restore_original_behaviour()\n</code></pre>"},{"location":"api/addons/attacks/model/modelattack/#nebula.addons.attacks.model.modelattack.ModelAttack.model_attack","title":"<code>model_attack(received_weights)</code>  <code>abstractmethod</code>","text":"<p>Abstract method that applies the specific model attack logic.</p> <p>This method should be implemented in subclasses to define the attack logic on the received model weights.</p> <p>Parameters:</p> Name Type Description Default <code>received_weights</code> <code>any</code> <p>The aggregated model weights to be modified.</p> required <p>Returns:</p> Name Type Description <code>any</code> <p>The modified model weights after applying the attack.</p> Source code in <code>nebula/addons/attacks/model/modelattack.py</code> <pre><code>@abstractmethod\ndef model_attack(self, received_weights):\n    \"\"\"\n    Abstract method that applies the specific model attack logic.\n\n    This method should be implemented in subclasses to define the attack\n    logic on the received model weights.\n\n    Args:\n        received_weights (any): The aggregated model weights to be modified.\n\n    Returns:\n        any: The modified model weights after applying the attack.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/addons/attacks/model/modelpoison/","title":"Documentation for Modelpoison Module","text":"<p>This module provides classes for model poisoning attacks, allowing for the simulation of model poisoning by adding different types of noise to model parameters.</p> <p>Classes: - ModelPoisonAttack: Main attack class that implements the ModelAttack interface - ModelPoisoningStrategy: Abstract base class for model poisoning strategies - GaussianNoiseStrategy: Implementation for Gaussian noise poisoning - SaltNoiseStrategy: Implementation for salt noise poisoning - SaltAndPepperNoiseStrategy: Implementation for salt-and-pepper noise poisoning</p>"},{"location":"api/addons/attacks/model/modelpoison/#nebula.addons.attacks.model.modelpoison.GaussianNoiseStrategy","title":"<code>GaussianNoiseStrategy</code>","text":"<p>               Bases: <code>ModelPoisoningStrategy</code></p> <p>Implementation of Gaussian noise poisoning strategy.</p> Source code in <code>nebula/addons/attacks/model/modelpoison.py</code> <pre><code>class GaussianNoiseStrategy(ModelPoisoningStrategy):\n    \"\"\"Implementation of Gaussian noise poisoning strategy.\"\"\"\n\n    def apply_noise(\n        self,\n        model: OrderedDict,\n        poisoned_noise_percent: float,\n    ) -&gt; OrderedDict:\n        \"\"\"\n        Applies Gaussian-distributed additive noise to model parameters.\n\n        Args:\n            model: The model's parameters organized as an OrderedDict\n            poisoned_noise_percent: Percentage of noise to apply (0-100)\n\n        Returns:\n            Modified model parameters with Gaussian noise\n        \"\"\"\n        poisoned_model = OrderedDict()\n        poisoned_ratio = poisoned_noise_percent / 100.0\n\n        for layer in model:\n            bt = model[layer]\n            t = bt.detach().clone()\n            single_point = False\n            if len(t.shape) == 0:\n                t = t.view(-1)\n                single_point = True\n                logging.info(f\"Layer {layer} is a single point, reshaping to {t.shape}\")\n\n            logging.info(f\"Applying gaussian noise to layer {layer}\")\n            poisoned = torch.tensor(random_noise(t, mode=\"gaussian\", mean=0, var=poisoned_ratio, clip=True))\n\n            if single_point:\n                poisoned = poisoned[0]\n                logging.info(f\"Layer {layer} is a single point, reshaping to {poisoned.shape}\")\n            poisoned_model[layer] = poisoned\n\n        return poisoned_model\n</code></pre>"},{"location":"api/addons/attacks/model/modelpoison/#nebula.addons.attacks.model.modelpoison.GaussianNoiseStrategy.apply_noise","title":"<code>apply_noise(model, poisoned_noise_percent)</code>","text":"<p>Applies Gaussian-distributed additive noise to model parameters.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>OrderedDict</code> <p>The model's parameters organized as an OrderedDict</p> required <code>poisoned_noise_percent</code> <code>float</code> <p>Percentage of noise to apply (0-100)</p> required <p>Returns:</p> Type Description <code>OrderedDict</code> <p>Modified model parameters with Gaussian noise</p> Source code in <code>nebula/addons/attacks/model/modelpoison.py</code> <pre><code>def apply_noise(\n    self,\n    model: OrderedDict,\n    poisoned_noise_percent: float,\n) -&gt; OrderedDict:\n    \"\"\"\n    Applies Gaussian-distributed additive noise to model parameters.\n\n    Args:\n        model: The model's parameters organized as an OrderedDict\n        poisoned_noise_percent: Percentage of noise to apply (0-100)\n\n    Returns:\n        Modified model parameters with Gaussian noise\n    \"\"\"\n    poisoned_model = OrderedDict()\n    poisoned_ratio = poisoned_noise_percent / 100.0\n\n    for layer in model:\n        bt = model[layer]\n        t = bt.detach().clone()\n        single_point = False\n        if len(t.shape) == 0:\n            t = t.view(-1)\n            single_point = True\n            logging.info(f\"Layer {layer} is a single point, reshaping to {t.shape}\")\n\n        logging.info(f\"Applying gaussian noise to layer {layer}\")\n        poisoned = torch.tensor(random_noise(t, mode=\"gaussian\", mean=0, var=poisoned_ratio, clip=True))\n\n        if single_point:\n            poisoned = poisoned[0]\n            logging.info(f\"Layer {layer} is a single point, reshaping to {poisoned.shape}\")\n        poisoned_model[layer] = poisoned\n\n    return poisoned_model\n</code></pre>"},{"location":"api/addons/attacks/model/modelpoison/#nebula.addons.attacks.model.modelpoison.ModelPoisonAttack","title":"<code>ModelPoisonAttack</code>","text":"<p>               Bases: <code>ModelAttack</code></p> <p>Implements a model poisoning attack by modifying the received model weights during the aggregation process.</p> Source code in <code>nebula/addons/attacks/model/modelpoison.py</code> <pre><code>class ModelPoisonAttack(ModelAttack):\n    \"\"\"\n    Implements a model poisoning attack by modifying the received model weights\n    during the aggregation process.\n    \"\"\"\n\n    def __init__(self, engine, attack_params: Dict):\n        \"\"\"\n        Initialize the model poisoning attack.\n\n        Args:\n            engine: The engine managing the attack context\n            attack_params: Dictionary containing attack parameters\n        \"\"\"\n        try:\n            round_start = int(attack_params[\"round_start_attack\"])\n            round_stop = int(attack_params[\"round_stop_attack\"])\n            attack_interval = int(attack_params[\"attack_interval\"])\n        except KeyError as e:\n            raise ValueError(f\"Missing required attack parameter: {e}\")\n        except ValueError:\n            raise ValueError(\"Invalid value in attack_params. Ensure all values are integers.\")\n\n        super().__init__(engine, round_start, round_stop, attack_interval)\n        self.poisoned_noise_percent = float(attack_params[\"poisoned_noise_percent\"])\n        noise_type = attack_params[\"noise_type\"].lower()\n\n        # Create the appropriate strategy based on noise type\n        if noise_type == \"gaussian\":\n            self.strategy = GaussianNoiseStrategy()\n        elif noise_type == \"salt\":\n            self.strategy = SaltNoiseStrategy()\n        elif noise_type == \"s&amp;p\":\n            self.strategy = SaltAndPepperNoiseStrategy()\n        else:\n            raise ValueError(f\"Unsupported noise type: {noise_type}\")\n\n    def model_attack(self, received_weights: OrderedDict) -&gt; OrderedDict:\n        \"\"\"\n        Applies the model poisoning attack by modifying the received model weights.\n\n        Args:\n            received_weights: The aggregated model weights to be poisoned\n\n        Returns:\n            The modified model weights after applying the poisoning attack\n        \"\"\"\n        return self.strategy.apply_noise(received_weights, self.poisoned_noise_percent)\n</code></pre>"},{"location":"api/addons/attacks/model/modelpoison/#nebula.addons.attacks.model.modelpoison.ModelPoisonAttack.__init__","title":"<code>__init__(engine, attack_params)</code>","text":"<p>Initialize the model poisoning attack.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <p>The engine managing the attack context</p> required <code>attack_params</code> <code>Dict</code> <p>Dictionary containing attack parameters</p> required Source code in <code>nebula/addons/attacks/model/modelpoison.py</code> <pre><code>def __init__(self, engine, attack_params: Dict):\n    \"\"\"\n    Initialize the model poisoning attack.\n\n    Args:\n        engine: The engine managing the attack context\n        attack_params: Dictionary containing attack parameters\n    \"\"\"\n    try:\n        round_start = int(attack_params[\"round_start_attack\"])\n        round_stop = int(attack_params[\"round_stop_attack\"])\n        attack_interval = int(attack_params[\"attack_interval\"])\n    except KeyError as e:\n        raise ValueError(f\"Missing required attack parameter: {e}\")\n    except ValueError:\n        raise ValueError(\"Invalid value in attack_params. Ensure all values are integers.\")\n\n    super().__init__(engine, round_start, round_stop, attack_interval)\n    self.poisoned_noise_percent = float(attack_params[\"poisoned_noise_percent\"])\n    noise_type = attack_params[\"noise_type\"].lower()\n\n    # Create the appropriate strategy based on noise type\n    if noise_type == \"gaussian\":\n        self.strategy = GaussianNoiseStrategy()\n    elif noise_type == \"salt\":\n        self.strategy = SaltNoiseStrategy()\n    elif noise_type == \"s&amp;p\":\n        self.strategy = SaltAndPepperNoiseStrategy()\n    else:\n        raise ValueError(f\"Unsupported noise type: {noise_type}\")\n</code></pre>"},{"location":"api/addons/attacks/model/modelpoison/#nebula.addons.attacks.model.modelpoison.ModelPoisonAttack.model_attack","title":"<code>model_attack(received_weights)</code>","text":"<p>Applies the model poisoning attack by modifying the received model weights.</p> <p>Parameters:</p> Name Type Description Default <code>received_weights</code> <code>OrderedDict</code> <p>The aggregated model weights to be poisoned</p> required <p>Returns:</p> Type Description <code>OrderedDict</code> <p>The modified model weights after applying the poisoning attack</p> Source code in <code>nebula/addons/attacks/model/modelpoison.py</code> <pre><code>def model_attack(self, received_weights: OrderedDict) -&gt; OrderedDict:\n    \"\"\"\n    Applies the model poisoning attack by modifying the received model weights.\n\n    Args:\n        received_weights: The aggregated model weights to be poisoned\n\n    Returns:\n        The modified model weights after applying the poisoning attack\n    \"\"\"\n    return self.strategy.apply_noise(received_weights, self.poisoned_noise_percent)\n</code></pre>"},{"location":"api/addons/attacks/model/modelpoison/#nebula.addons.attacks.model.modelpoison.ModelPoisoningStrategy","title":"<code>ModelPoisoningStrategy</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for model poisoning strategies.</p> Source code in <code>nebula/addons/attacks/model/modelpoison.py</code> <pre><code>class ModelPoisoningStrategy(ABC):\n    \"\"\"Abstract base class for model poisoning strategies.\"\"\"\n\n    @abstractmethod\n    def apply_noise(\n        self,\n        model: OrderedDict,\n        poisoned_noise_percent: float,\n    ) -&gt; OrderedDict:\n        \"\"\"\n        Abstract method to apply noise to model parameters.\n\n        Args:\n            model: The model's parameters organized as an OrderedDict\n            poisoned_noise_percent: Percentage of noise to apply (0-100)\n\n        Returns:\n            Modified model parameters with noise applied\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/addons/attacks/model/modelpoison/#nebula.addons.attacks.model.modelpoison.ModelPoisoningStrategy.apply_noise","title":"<code>apply_noise(model, poisoned_noise_percent)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to apply noise to model parameters.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>OrderedDict</code> <p>The model's parameters organized as an OrderedDict</p> required <code>poisoned_noise_percent</code> <code>float</code> <p>Percentage of noise to apply (0-100)</p> required <p>Returns:</p> Type Description <code>OrderedDict</code> <p>Modified model parameters with noise applied</p> Source code in <code>nebula/addons/attacks/model/modelpoison.py</code> <pre><code>@abstractmethod\ndef apply_noise(\n    self,\n    model: OrderedDict,\n    poisoned_noise_percent: float,\n) -&gt; OrderedDict:\n    \"\"\"\n    Abstract method to apply noise to model parameters.\n\n    Args:\n        model: The model's parameters organized as an OrderedDict\n        poisoned_noise_percent: Percentage of noise to apply (0-100)\n\n    Returns:\n        Modified model parameters with noise applied\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/addons/attacks/model/modelpoison/#nebula.addons.attacks.model.modelpoison.SaltAndPepperNoiseStrategy","title":"<code>SaltAndPepperNoiseStrategy</code>","text":"<p>               Bases: <code>ModelPoisoningStrategy</code></p> <p>Implementation of salt-and-pepper noise poisoning strategy.</p> Source code in <code>nebula/addons/attacks/model/modelpoison.py</code> <pre><code>class SaltAndPepperNoiseStrategy(ModelPoisoningStrategy):\n    \"\"\"Implementation of salt-and-pepper noise poisoning strategy.\"\"\"\n\n    def apply_noise(\n        self,\n        model: OrderedDict,\n        poisoned_noise_percent: float,\n    ) -&gt; OrderedDict:\n        \"\"\"\n        Applies salt-and-pepper noise to model parameters.\n\n        Args:\n            model: The model's parameters organized as an OrderedDict\n            poisoned_noise_percent: Percentage of noise to apply (0-100)\n\n        Returns:\n            Modified model parameters with salt-and-pepper noise\n        \"\"\"\n        poisoned_model = OrderedDict()\n        poisoned_ratio = poisoned_noise_percent / 100.0\n\n        for layer in model:\n            bt = model[layer]\n            t = bt.detach().clone()\n            single_point = False\n            if len(t.shape) == 0:\n                t = t.view(-1)\n                single_point = True\n                logging.info(f\"Layer {layer} is a single point, reshaping to {t.shape}\")\n\n            logging.info(f\"Applying salt-and-pepper noise to layer {layer}\")\n            poisoned = torch.tensor(random_noise(t, mode=\"s&amp;p\", amount=poisoned_ratio))\n\n            if single_point:\n                poisoned = poisoned[0]\n                logging.info(f\"Layer {layer} is a single point, reshaping to {poisoned.shape}\")\n            poisoned_model[layer] = poisoned\n\n        return poisoned_model\n</code></pre>"},{"location":"api/addons/attacks/model/modelpoison/#nebula.addons.attacks.model.modelpoison.SaltAndPepperNoiseStrategy.apply_noise","title":"<code>apply_noise(model, poisoned_noise_percent)</code>","text":"<p>Applies salt-and-pepper noise to model parameters.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>OrderedDict</code> <p>The model's parameters organized as an OrderedDict</p> required <code>poisoned_noise_percent</code> <code>float</code> <p>Percentage of noise to apply (0-100)</p> required <p>Returns:</p> Type Description <code>OrderedDict</code> <p>Modified model parameters with salt-and-pepper noise</p> Source code in <code>nebula/addons/attacks/model/modelpoison.py</code> <pre><code>def apply_noise(\n    self,\n    model: OrderedDict,\n    poisoned_noise_percent: float,\n) -&gt; OrderedDict:\n    \"\"\"\n    Applies salt-and-pepper noise to model parameters.\n\n    Args:\n        model: The model's parameters organized as an OrderedDict\n        poisoned_noise_percent: Percentage of noise to apply (0-100)\n\n    Returns:\n        Modified model parameters with salt-and-pepper noise\n    \"\"\"\n    poisoned_model = OrderedDict()\n    poisoned_ratio = poisoned_noise_percent / 100.0\n\n    for layer in model:\n        bt = model[layer]\n        t = bt.detach().clone()\n        single_point = False\n        if len(t.shape) == 0:\n            t = t.view(-1)\n            single_point = True\n            logging.info(f\"Layer {layer} is a single point, reshaping to {t.shape}\")\n\n        logging.info(f\"Applying salt-and-pepper noise to layer {layer}\")\n        poisoned = torch.tensor(random_noise(t, mode=\"s&amp;p\", amount=poisoned_ratio))\n\n        if single_point:\n            poisoned = poisoned[0]\n            logging.info(f\"Layer {layer} is a single point, reshaping to {poisoned.shape}\")\n        poisoned_model[layer] = poisoned\n\n    return poisoned_model\n</code></pre>"},{"location":"api/addons/attacks/model/modelpoison/#nebula.addons.attacks.model.modelpoison.SaltNoiseStrategy","title":"<code>SaltNoiseStrategy</code>","text":"<p>               Bases: <code>ModelPoisoningStrategy</code></p> <p>Implementation of salt noise poisoning strategy.</p> Source code in <code>nebula/addons/attacks/model/modelpoison.py</code> <pre><code>class SaltNoiseStrategy(ModelPoisoningStrategy):\n    \"\"\"Implementation of salt noise poisoning strategy.\"\"\"\n\n    def apply_noise(\n        self,\n        model: OrderedDict,\n        poisoned_noise_percent: float,\n    ) -&gt; OrderedDict:\n        \"\"\"\n        Applies salt noise to model parameters.\n\n        Args:\n            model: The model's parameters organized as an OrderedDict\n            poisoned_noise_percent: Percentage of noise to apply (0-100)\n\n        Returns:\n            Modified model parameters with salt noise\n        \"\"\"\n        poisoned_model = OrderedDict()\n        poisoned_ratio = poisoned_noise_percent / 100.0\n\n        for layer in model:\n            bt = model[layer]\n            t = bt.detach().clone()\n            single_point = False\n            if len(t.shape) == 0:\n                t = t.view(-1)\n                single_point = True\n                logging.info(f\"Layer {layer} is a single point, reshaping to {t.shape}\")\n\n            logging.info(f\"Applying salt noise to layer {layer}\")\n            poisoned = torch.tensor(random_noise(t, mode=\"salt\", amount=poisoned_ratio))\n\n            if single_point:\n                poisoned = poisoned[0]\n                logging.info(f\"Layer {layer} is a single point, reshaping to {poisoned.shape}\")\n            poisoned_model[layer] = poisoned\n\n        return poisoned_model\n</code></pre>"},{"location":"api/addons/attacks/model/modelpoison/#nebula.addons.attacks.model.modelpoison.SaltNoiseStrategy.apply_noise","title":"<code>apply_noise(model, poisoned_noise_percent)</code>","text":"<p>Applies salt noise to model parameters.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>OrderedDict</code> <p>The model's parameters organized as an OrderedDict</p> required <code>poisoned_noise_percent</code> <code>float</code> <p>Percentage of noise to apply (0-100)</p> required <p>Returns:</p> Type Description <code>OrderedDict</code> <p>Modified model parameters with salt noise</p> Source code in <code>nebula/addons/attacks/model/modelpoison.py</code> <pre><code>def apply_noise(\n    self,\n    model: OrderedDict,\n    poisoned_noise_percent: float,\n) -&gt; OrderedDict:\n    \"\"\"\n    Applies salt noise to model parameters.\n\n    Args:\n        model: The model's parameters organized as an OrderedDict\n        poisoned_noise_percent: Percentage of noise to apply (0-100)\n\n    Returns:\n        Modified model parameters with salt noise\n    \"\"\"\n    poisoned_model = OrderedDict()\n    poisoned_ratio = poisoned_noise_percent / 100.0\n\n    for layer in model:\n        bt = model[layer]\n        t = bt.detach().clone()\n        single_point = False\n        if len(t.shape) == 0:\n            t = t.view(-1)\n            single_point = True\n            logging.info(f\"Layer {layer} is a single point, reshaping to {t.shape}\")\n\n        logging.info(f\"Applying salt noise to layer {layer}\")\n        poisoned = torch.tensor(random_noise(t, mode=\"salt\", amount=poisoned_ratio))\n\n        if single_point:\n            poisoned = poisoned[0]\n            logging.info(f\"Layer {layer} is a single point, reshaping to {poisoned.shape}\")\n        poisoned_model[layer] = poisoned\n\n    return poisoned_model\n</code></pre>"},{"location":"api/addons/attacks/model/swappingweights/","title":"Documentation for Swappingweights Module","text":""},{"location":"api/addons/attacks/model/swappingweights/#nebula.addons.attacks.model.swappingweights.SwappingWeightsAttack","title":"<code>SwappingWeightsAttack</code>","text":"<p>               Bases: <code>ModelAttack</code></p> <p>Implements a swapping weights attack on the received model weights.</p> <p>This attack performs stochastic swapping of weights in a specified layer of the model, potentially disrupting its performance. The attack is not deterministic, and its performance can vary. The code may not work as expected for some layers due to reshaping, and its computational cost scales quadratically with the layer size. It should not be applied to the last layer, as it would make the attack detectable due to high loss on the malicious node.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>object</code> <p>The training engine object that manages the aggregator.</p> required <code>attack_params</code> <code>dict</code> <p>Parameters for the attack, including: - layer_idx (int): The index of the layer where the weights will be swapped.</p> required Source code in <code>nebula/addons/attacks/model/swappingweights.py</code> <pre><code>class SwappingWeightsAttack(ModelAttack):\n    \"\"\"\n    Implements a swapping weights attack on the received model weights.\n\n    This attack performs stochastic swapping of weights in a specified layer of the model,\n    potentially disrupting its performance. The attack is not deterministic, and its performance\n    can vary. The code may not work as expected for some layers due to reshaping, and its\n    computational cost scales quadratically with the layer size. It should not be applied to\n    the last layer, as it would make the attack detectable due to high loss on the malicious node.\n\n    Args:\n        engine (object): The training engine object that manages the aggregator.\n        attack_params (dict): Parameters for the attack, including:\n            - layer_idx (int): The index of the layer where the weights will be swapped.\n    \"\"\"\n\n    def __init__(self, engine, attack_params):\n        \"\"\"\n        Initializes the SwappingWeightsAttack with the specified engine and parameters.\n\n        Args:\n            engine (object): The training engine object.\n            attack_params (dict): Dictionary of attack parameters, including the layer index.\n        \"\"\"\n        try:\n            round_start = int(attack_params[\"round_start_attack\"])\n            round_stop = int(attack_params[\"round_stop_attack\"])\n            attack_interval = int(attack_params[\"attack_interval\"])\n        except KeyError as e:\n            raise ValueError(f\"Missing required attack parameter: {e}\")\n        except ValueError:\n            raise ValueError(\"Invalid value in attack_params. Ensure all values are integers.\")\n\n        super().__init__(engine, round_start, round_stop, attack_interval)\n\n        self.layer_idx = int(attack_params[\"layer_idx\"])\n\n    def model_attack(self, received_weights):\n        \"\"\"\n        Performs a similarity-based weight swapping attack to subtly sabotage a federated model.\n\n        This attack targets a specific layer of the neural network (typically a fully connected layer)\n        and permutes its neurons (rows in the weight matrix) in a way that alters the model's behavior\n        without changing its structure or dimensions.\n\n        Steps:\n        1. Computes a pairwise cosine similarity matrix between the rows (neurons) of the selected layer.\n        2. Finds neuron pairs that are mutually dissimilar (i.e., each is the most dissimilar to the other).\n        These pairs are good candidates for swapping as the operation is more disruptive.\n        3. For the remaining neurons (not in such pairs), applies a random permutation ensuring no neuron\n        stays in its original position.\n        4. Applies the final permutation to:\n        - The target layer (permutes rows).\n        - The next layer (permutes corresponding rows).\n        - The following layer (if any), where the permutation is applied to columns (to preserve consistency).\n\n        This subtle attack degrades model performance while preserving architectural validity and\n        avoiding obvious signs of tampering.\n\n        Args:\n            received_weights (dict): Dictionary of aggregated model weights with parameter names as keys.\n\n        Returns:\n            dict: Modified model weights with rows of selected layers permuted.\n        \"\"\"\n        logging.info(\"[SwappingWeightsAttack] Performing swapping weights attack\")\n        # Extract weight matrix for the target layer\n        layer_keys = list(received_weights.keys())\n        w = received_weights[layer_keys[self.layer_idx]]\n\n        # Compute cosine similarity matrix\n        sim_matrix = torch.nn.functional.cosine_similarity(\n            w.unsqueeze(1), w.unsqueeze(0), dim=2\n        )\n\n        # Greedy mutual minimum pairing\n        perm = -torch.ones(sim_matrix.shape[0], dtype=torch.long)\n        mutual_rows = []\n\n        for i in range(sim_matrix.shape[0]):\n            j = torch.argmin(sim_matrix[i])\n            if torch.argmin(sim_matrix[j]) == i:\n                perm[i] = j\n                mutual_rows.append(i)\n\n        # Fully permute the remaining rows\n        remaining_rows = torch.tensor([i for i in range(sim_matrix.shape[0]) if i not in mutual_rows])\n        shuffled = remaining_rows[torch.randperm(len(remaining_rows))]\n        while torch.any(remaining_rows == shuffled):\n            shuffled = remaining_rows[torch.randperm(len(remaining_rows))]\n\n        perm[remaining_rows] = shuffled\n\n        # Apply permutation to current and next layers\n        received_weights[layer_keys[self.layer_idx]] = w[perm]\n        received_weights[layer_keys[self.layer_idx + 1]] = received_weights[layer_keys[self.layer_idx + 1]][perm]\n\n        # If there's a third layer and it matches output shape, permute columns accordingly\n        if self.layer_idx + 2 &lt; len(layer_keys):\n            received_weights[layer_keys[self.layer_idx + 2]] = received_weights[layer_keys[self.layer_idx + 2]][:, perm]\n\n        return received_weights\n</code></pre>"},{"location":"api/addons/attacks/model/swappingweights/#nebula.addons.attacks.model.swappingweights.SwappingWeightsAttack.__init__","title":"<code>__init__(engine, attack_params)</code>","text":"<p>Initializes the SwappingWeightsAttack with the specified engine and parameters.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>object</code> <p>The training engine object.</p> required <code>attack_params</code> <code>dict</code> <p>Dictionary of attack parameters, including the layer index.</p> required Source code in <code>nebula/addons/attacks/model/swappingweights.py</code> <pre><code>def __init__(self, engine, attack_params):\n    \"\"\"\n    Initializes the SwappingWeightsAttack with the specified engine and parameters.\n\n    Args:\n        engine (object): The training engine object.\n        attack_params (dict): Dictionary of attack parameters, including the layer index.\n    \"\"\"\n    try:\n        round_start = int(attack_params[\"round_start_attack\"])\n        round_stop = int(attack_params[\"round_stop_attack\"])\n        attack_interval = int(attack_params[\"attack_interval\"])\n    except KeyError as e:\n        raise ValueError(f\"Missing required attack parameter: {e}\")\n    except ValueError:\n        raise ValueError(\"Invalid value in attack_params. Ensure all values are integers.\")\n\n    super().__init__(engine, round_start, round_stop, attack_interval)\n\n    self.layer_idx = int(attack_params[\"layer_idx\"])\n</code></pre>"},{"location":"api/addons/attacks/model/swappingweights/#nebula.addons.attacks.model.swappingweights.SwappingWeightsAttack.model_attack","title":"<code>model_attack(received_weights)</code>","text":"<p>Performs a similarity-based weight swapping attack to subtly sabotage a federated model.</p> <p>This attack targets a specific layer of the neural network (typically a fully connected layer) and permutes its neurons (rows in the weight matrix) in a way that alters the model's behavior without changing its structure or dimensions.</p> <p>Steps: 1. Computes a pairwise cosine similarity matrix between the rows (neurons) of the selected layer. 2. Finds neuron pairs that are mutually dissimilar (i.e., each is the most dissimilar to the other). These pairs are good candidates for swapping as the operation is more disruptive. 3. For the remaining neurons (not in such pairs), applies a random permutation ensuring no neuron stays in its original position. 4. Applies the final permutation to: - The target layer (permutes rows). - The next layer (permutes corresponding rows). - The following layer (if any), where the permutation is applied to columns (to preserve consistency).</p> <p>This subtle attack degrades model performance while preserving architectural validity and avoiding obvious signs of tampering.</p> <p>Parameters:</p> Name Type Description Default <code>received_weights</code> <code>dict</code> <p>Dictionary of aggregated model weights with parameter names as keys.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Modified model weights with rows of selected layers permuted.</p> Source code in <code>nebula/addons/attacks/model/swappingweights.py</code> <pre><code>def model_attack(self, received_weights):\n    \"\"\"\n    Performs a similarity-based weight swapping attack to subtly sabotage a federated model.\n\n    This attack targets a specific layer of the neural network (typically a fully connected layer)\n    and permutes its neurons (rows in the weight matrix) in a way that alters the model's behavior\n    without changing its structure or dimensions.\n\n    Steps:\n    1. Computes a pairwise cosine similarity matrix between the rows (neurons) of the selected layer.\n    2. Finds neuron pairs that are mutually dissimilar (i.e., each is the most dissimilar to the other).\n    These pairs are good candidates for swapping as the operation is more disruptive.\n    3. For the remaining neurons (not in such pairs), applies a random permutation ensuring no neuron\n    stays in its original position.\n    4. Applies the final permutation to:\n    - The target layer (permutes rows).\n    - The next layer (permutes corresponding rows).\n    - The following layer (if any), where the permutation is applied to columns (to preserve consistency).\n\n    This subtle attack degrades model performance while preserving architectural validity and\n    avoiding obvious signs of tampering.\n\n    Args:\n        received_weights (dict): Dictionary of aggregated model weights with parameter names as keys.\n\n    Returns:\n        dict: Modified model weights with rows of selected layers permuted.\n    \"\"\"\n    logging.info(\"[SwappingWeightsAttack] Performing swapping weights attack\")\n    # Extract weight matrix for the target layer\n    layer_keys = list(received_weights.keys())\n    w = received_weights[layer_keys[self.layer_idx]]\n\n    # Compute cosine similarity matrix\n    sim_matrix = torch.nn.functional.cosine_similarity(\n        w.unsqueeze(1), w.unsqueeze(0), dim=2\n    )\n\n    # Greedy mutual minimum pairing\n    perm = -torch.ones(sim_matrix.shape[0], dtype=torch.long)\n    mutual_rows = []\n\n    for i in range(sim_matrix.shape[0]):\n        j = torch.argmin(sim_matrix[i])\n        if torch.argmin(sim_matrix[j]) == i:\n            perm[i] = j\n            mutual_rows.append(i)\n\n    # Fully permute the remaining rows\n    remaining_rows = torch.tensor([i for i in range(sim_matrix.shape[0]) if i not in mutual_rows])\n    shuffled = remaining_rows[torch.randperm(len(remaining_rows))]\n    while torch.any(remaining_rows == shuffled):\n        shuffled = remaining_rows[torch.randperm(len(remaining_rows))]\n\n    perm[remaining_rows] = shuffled\n\n    # Apply permutation to current and next layers\n    received_weights[layer_keys[self.layer_idx]] = w[perm]\n    received_weights[layer_keys[self.layer_idx + 1]] = received_weights[layer_keys[self.layer_idx + 1]][perm]\n\n    # If there's a third layer and it matches output shape, permute columns accordingly\n    if self.layer_idx + 2 &lt; len(layer_keys):\n        received_weights[layer_keys[self.layer_idx + 2]] = received_weights[layer_keys[self.layer_idx + 2]][:, perm]\n\n    return received_weights\n</code></pre>"},{"location":"api/addons/gps/","title":"Documentation for Gps Module","text":""},{"location":"api/addons/gps/gpsmodule/","title":"Documentation for Gpsmodule Module","text":""},{"location":"api/addons/gps/gpsmodule/#nebula.addons.gps.gpsmodule.GPSModule","title":"<code>GPSModule</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class representing a GPS module interface.</p> <p>This class defines the required asynchronous methods that any concrete GPS module implementation must provide. These methods allow for lifecycle control (start/stop), status checking, and distance calculation between coordinates.</p> <p>Any subclass must implement all the following asynchronous methods: - <code>start()</code>: Begins GPS tracking or data acquisition. - <code>stop()</code>: Halts the GPS module's operation. - <code>is_running()</code>: Checks whether the GPS module is currently active. - <code>calculate_distance()</code>: Computes the distance between two geographic coordinates (latitude and longitude).</p> <p>All implementations should ensure that methods are non-blocking and integrate smoothly with async event loops.</p> Source code in <code>nebula/addons/gps/gpsmodule.py</code> <pre><code>class GPSModule(ABC):\n    \"\"\"\n    Abstract base class representing a GPS module interface.\n\n    This class defines the required asynchronous methods that any concrete GPS module implementation must provide.\n    These methods allow for lifecycle control (start/stop), status checking, and distance calculation between coordinates.\n\n    Any subclass must implement all the following asynchronous methods:\n    - `start()`: Begins GPS tracking or data acquisition.\n    - `stop()`: Halts the GPS module's operation.\n    - `is_running()`: Checks whether the GPS module is currently active.\n    - `calculate_distance()`: Computes the distance between two geographic coordinates (latitude and longitude).\n\n    All implementations should ensure that methods are non-blocking and integrate smoothly with async event loops.\n    \"\"\"\n\n    @abstractmethod\n    async def start(self):\n        \"\"\"\n        Starts the GPS module operation.\n\n        This may involve initiating hardware tracking, establishing connections, or beginning periodic updates.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def stop(self):\n        \"\"\"\n        Stops the GPS module operation.\n\n        Ensures that any background tasks or hardware interactions are properly terminated.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def is_running(self):\n        \"\"\"\n        Checks whether the GPS module is currently active.\n\n        Returns:\n            bool: True if the module is running, False otherwise.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def calculate_distance(self, self_lat, self_long, other_lat, other_long):\n        \"\"\"\n        Calculates the distance between two geographic points.\n\n        Args:\n            self_lat (float): Latitude of the source point.\n            self_long (float): Longitude of the source point.\n            other_lat (float): Latitude of the target point.\n            other_long (float): Longitude of the target point.\n\n        Returns:\n            float: Distance in meters (or implementation-defined units) between the two coordinates.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/addons/gps/gpsmodule/#nebula.addons.gps.gpsmodule.GPSModule.calculate_distance","title":"<code>calculate_distance(self_lat, self_long, other_lat, other_long)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Calculates the distance between two geographic points.</p> <p>Parameters:</p> Name Type Description Default <code>self_lat</code> <code>float</code> <p>Latitude of the source point.</p> required <code>self_long</code> <code>float</code> <p>Longitude of the source point.</p> required <code>other_lat</code> <code>float</code> <p>Latitude of the target point.</p> required <code>other_long</code> <code>float</code> <p>Longitude of the target point.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>Distance in meters (or implementation-defined units) between the two coordinates.</p> Source code in <code>nebula/addons/gps/gpsmodule.py</code> <pre><code>@abstractmethod\nasync def calculate_distance(self, self_lat, self_long, other_lat, other_long):\n    \"\"\"\n    Calculates the distance between two geographic points.\n\n    Args:\n        self_lat (float): Latitude of the source point.\n        self_long (float): Longitude of the source point.\n        other_lat (float): Latitude of the target point.\n        other_long (float): Longitude of the target point.\n\n    Returns:\n        float: Distance in meters (or implementation-defined units) between the two coordinates.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/addons/gps/gpsmodule/#nebula.addons.gps.gpsmodule.GPSModule.is_running","title":"<code>is_running()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Checks whether the GPS module is currently active.</p> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the module is running, False otherwise.</p> Source code in <code>nebula/addons/gps/gpsmodule.py</code> <pre><code>@abstractmethod\nasync def is_running(self):\n    \"\"\"\n    Checks whether the GPS module is currently active.\n\n    Returns:\n        bool: True if the module is running, False otherwise.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/addons/gps/gpsmodule/#nebula.addons.gps.gpsmodule.GPSModule.start","title":"<code>start()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Starts the GPS module operation.</p> <p>This may involve initiating hardware tracking, establishing connections, or beginning periodic updates.</p> Source code in <code>nebula/addons/gps/gpsmodule.py</code> <pre><code>@abstractmethod\nasync def start(self):\n    \"\"\"\n    Starts the GPS module operation.\n\n    This may involve initiating hardware tracking, establishing connections, or beginning periodic updates.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/addons/gps/gpsmodule/#nebula.addons.gps.gpsmodule.GPSModule.stop","title":"<code>stop()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Stops the GPS module operation.</p> <p>Ensures that any background tasks or hardware interactions are properly terminated.</p> Source code in <code>nebula/addons/gps/gpsmodule.py</code> <pre><code>@abstractmethod\nasync def stop(self):\n    \"\"\"\n    Stops the GPS module operation.\n\n    Ensures that any background tasks or hardware interactions are properly terminated.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/addons/gps/nebulagps/","title":"Documentation for Nebulagps Module","text":""},{"location":"api/addons/gps/nebulagps/#nebula.addons.gps.nebulagps.NebulaGPS","title":"<code>NebulaGPS</code>","text":"<p>               Bases: <code>GPSModule</code></p> Source code in <code>nebula/addons/gps/nebulagps.py</code> <pre><code>class NebulaGPS(GPSModule):\n    BROADCAST_IP = \"255.255.255.255\"  # Broadcast IP\n    BROADCAST_PORT = 50001  # Port used for GPS\n    INTERFACE = \"eth2\"  # Interface to avoid network conditions\n\n    def __init__(self, config, addr, update_interval: float = 5.0, verbose=False):\n        self._config = config\n        self._addr = addr\n        self.update_interval = update_interval  # Frequency\n        self._node_locations = {}  # Dictionary for storing node locations\n        self._broadcast_socket = None\n        self._nodes_location_lock = Locker(\"nodes_location_lock\", async_lock=True)\n        self._verbose = verbose\n        self._running = asyncio.Event()\n        self._background_tasks = []  # Track background tasks\n\n    async def start(self):\n        \"\"\"Starts the GPS service, sending and receiving locations.\"\"\"\n        logging.info(\"Starting NebulaGPS service...\")\n        self._running.set()\n\n        # Create broadcast socket\n        self._broadcast_socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n        self._broadcast_socket.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, 1)\n\n        # Bind socket on eth2 to also receive data\n        self._broadcast_socket.bind((\"\", self.BROADCAST_PORT))\n\n        # Start sending and receiving tasks\n        self._background_tasks = [\n            asyncio.create_task(self._send_location_loop(), name=\"NebulaGPS_send_location\"),\n            asyncio.create_task(self._receive_location_loop(), name=\"NebulaGPS_receive_location\"),\n            asyncio.create_task(self._notify_geolocs(), name=\"NebulaGPS_notify_geolocs\"),\n        ]\n\n    async def stop(self):\n        \"\"\"Stops the GPS service.\"\"\"\n        logging.info(\"\ud83d\uded1  Stopping NebulaGPS service...\")\n        self._running.clear()\n        logging.info(\"\ud83d\uded1  NebulaGPS _running event cleared\")\n\n        # Cancel all background tasks\n        if self._background_tasks:\n            logging.info(f\"\ud83d\uded1  Cancelling {len(self._background_tasks)} background tasks...\")\n            for task in self._background_tasks:\n                if not task.done():\n                    task.cancel()\n                    try:\n                        await task\n                    except asyncio.CancelledError:\n                        pass\n            self._background_tasks.clear()\n            logging.info(\"\ud83d\uded1  All background tasks cancelled\")\n\n        if self._broadcast_socket:\n            self._broadcast_socket.close()\n            self._broadcast_socket = None\n            logging.info(\"\ud83d\uded1  NebulaGPS broadcast socket closed\")\n        logging.info(\"\u2705  NebulaGPS service stopped successfully\")\n\n    async def is_running(self):\n        return self._running.is_set()\n\n    async def get_geoloc(self):\n        latitude = self._config.participant[\"mobility_args\"][\"latitude\"]\n        longitude = self._config.participant[\"mobility_args\"][\"longitude\"]\n        return (latitude, longitude)\n\n    async def calculate_distance(self, self_lat, self_long, other_lat, other_long):\n        distance_m = distance.distance((self_lat, self_long), (other_lat, other_long)).m\n        return distance_m\n\n    async def _send_location_loop(self):\n        \"\"\"Send the geolocation periodically by broadcast.\"\"\"\n        while await self.is_running():\n            # Check if learning cycle has finished\n            try:\n                from nebula.core.network.communications import CommunicationsManager\n\n                cm = CommunicationsManager.get_instance()\n                if await cm.learning_finished():\n                    logging.info(\"GPS: Learning cycle finished, stopping location broadcast\")\n                    break\n            except Exception:\n                pass  # If we can't get the communications manager, continue\n\n            latitude, longitude = await self.get_geoloc()  # Obtener ubicaci\u00f3n actual\n            message = f\"GPS-UPDATE {self._addr} {latitude} {longitude}\"\n            self._broadcast_socket.sendto(message.encode(), (self.BROADCAST_IP, self.BROADCAST_PORT))\n            if self._verbose:\n                logging.info(f\"Sent GPS location: ({latitude}, {longitude})\")\n            await asyncio.sleep(self.update_interval)\n\n    async def _receive_location_loop(self):\n        \"\"\"Listens to and stores geolocations from other nodes.\"\"\"\n        while await self.is_running():\n            # Check if learning cycle has finished\n            try:\n                from nebula.core.network.communications import CommunicationsManager\n\n                cm = CommunicationsManager.get_instance()\n                if await cm.learning_finished():\n                    logging.info(\"GPS: Learning cycle finished, stopping location reception\")\n                    break\n            except Exception:\n                pass  # If we can't get the communications manager, continue\n\n            try:\n                data, addr = await asyncio.get_running_loop().run_in_executor(\n                    None, self._broadcast_socket.recvfrom, 1024\n                )\n                message = data.decode().strip()\n                if message.startswith(\"GPS-UPDATE\"):\n                    _, sender_addr, lat, lon = message.split()\n                    if sender_addr != self._addr:\n                        async with self._nodes_location_lock:\n                            self._node_locations[sender_addr] = (float(lat), float(lon))\n                    if self._verbose:\n                        logging.info(f\"Received GPS from {addr[0]}: {lat}, {lon}\")\n            except Exception as e:\n                logging.exception(f\"Error receiving GPS update: {e}\")\n\n    async def _notify_geolocs(self):\n        while await self.is_running():\n            # Check if learning cycle has finished\n            try:\n                from nebula.core.network.communications import CommunicationsManager\n\n                cm = CommunicationsManager.get_instance()\n                if await cm.learning_finished():\n                    logging.info(\"GPS: Learning cycle finished, stopping geolocation notifications\")\n                    break\n            except Exception:\n                pass  # If we can't get the communications manager, continue\n\n            await asyncio.sleep(self.update_interval)\n            await self._nodes_location_lock.acquire_async()\n            geolocs: dict = self._node_locations.copy()\n            await self._nodes_location_lock.release_async()\n            if geolocs:\n                distances = {}\n                self_lat, self_long = await self.get_geoloc()\n                for addr, (lat, long) in geolocs.items():\n                    dist = await self.calculate_distance(self_lat, self_long, lat, long)\n                    distances[addr] = (dist, (lat, long))\n\n                self._config.update_nodes_distance(distances)\n                gpsevent = GPSEvent(distances)\n                asyncio.create_task(EventManager.get_instance().publish_addonevent(gpsevent))\n</code></pre>"},{"location":"api/addons/gps/nebulagps/#nebula.addons.gps.nebulagps.NebulaGPS.start","title":"<code>start()</code>  <code>async</code>","text":"<p>Starts the GPS service, sending and receiving locations.</p> Source code in <code>nebula/addons/gps/nebulagps.py</code> <pre><code>async def start(self):\n    \"\"\"Starts the GPS service, sending and receiving locations.\"\"\"\n    logging.info(\"Starting NebulaGPS service...\")\n    self._running.set()\n\n    # Create broadcast socket\n    self._broadcast_socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    self._broadcast_socket.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, 1)\n\n    # Bind socket on eth2 to also receive data\n    self._broadcast_socket.bind((\"\", self.BROADCAST_PORT))\n\n    # Start sending and receiving tasks\n    self._background_tasks = [\n        asyncio.create_task(self._send_location_loop(), name=\"NebulaGPS_send_location\"),\n        asyncio.create_task(self._receive_location_loop(), name=\"NebulaGPS_receive_location\"),\n        asyncio.create_task(self._notify_geolocs(), name=\"NebulaGPS_notify_geolocs\"),\n    ]\n</code></pre>"},{"location":"api/addons/gps/nebulagps/#nebula.addons.gps.nebulagps.NebulaGPS.stop","title":"<code>stop()</code>  <code>async</code>","text":"<p>Stops the GPS service.</p> Source code in <code>nebula/addons/gps/nebulagps.py</code> <pre><code>async def stop(self):\n    \"\"\"Stops the GPS service.\"\"\"\n    logging.info(\"\ud83d\uded1  Stopping NebulaGPS service...\")\n    self._running.clear()\n    logging.info(\"\ud83d\uded1  NebulaGPS _running event cleared\")\n\n    # Cancel all background tasks\n    if self._background_tasks:\n        logging.info(f\"\ud83d\uded1  Cancelling {len(self._background_tasks)} background tasks...\")\n        for task in self._background_tasks:\n            if not task.done():\n                task.cancel()\n                try:\n                    await task\n                except asyncio.CancelledError:\n                    pass\n        self._background_tasks.clear()\n        logging.info(\"\ud83d\uded1  All background tasks cancelled\")\n\n    if self._broadcast_socket:\n        self._broadcast_socket.close()\n        self._broadcast_socket = None\n        logging.info(\"\ud83d\uded1  NebulaGPS broadcast socket closed\")\n    logging.info(\"\u2705  NebulaGPS service stopped successfully\")\n</code></pre>"},{"location":"api/addons/networksimulation/","title":"Documentation for Networksimulation Module","text":""},{"location":"api/addons/networksimulation/nebulanetworksimulator/","title":"Documentation for Nebulanetworksimulator Module","text":""},{"location":"api/addons/networksimulation/networksimulator/","title":"Documentation for Networksimulator Module","text":""},{"location":"api/addons/networksimulation/networksimulator/#nebula.addons.networksimulation.networksimulator.NetworkSimulator","title":"<code>NetworkSimulator</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class representing a network simulator interface.</p> <p>This interface defines the required methods for controlling and simulating network conditions between nodes. A concrete implementation is expected to manage artificial delays, bandwidth restrictions, packet loss,  or other configurable conditions typically used in network emulation or testing.</p> <p>Required asynchronous methods: - <code>start()</code>: Initializes the network simulation module. - <code>stop()</code>: Shuts down the simulation and cleans up any active conditions. - <code>set_thresholds(thresholds)</code>: Configures system-wide thresholds (e.g., max/min delay or distance mappings). - <code>set_network_conditions(dest_addr, distance)</code>: Applies network constraints to a target address based on distance.</p> <p>Synchronous method: - <code>clear_network_conditions(interface)</code>: Clears any simulated network configuration for a given interface.</p> <p>All asynchronous methods should be non-blocking to support integration in async systems.</p> Source code in <code>nebula/addons/networksimulation/networksimulator.py</code> <pre><code>class NetworkSimulator(ABC):\n    \"\"\"\n    Abstract base class representing a network simulator interface.\n\n    This interface defines the required methods for controlling and simulating network conditions between nodes.\n    A concrete implementation is expected to manage artificial delays, bandwidth restrictions, packet loss, \n    or other configurable conditions typically used in network emulation or testing.\n\n    Required asynchronous methods:\n    - `start()`: Initializes the network simulation module.\n    - `stop()`: Shuts down the simulation and cleans up any active conditions.\n    - `set_thresholds(thresholds)`: Configures system-wide thresholds (e.g., max/min delay or distance mappings).\n    - `set_network_conditions(dest_addr, distance)`: Applies network constraints to a target address based on distance.\n\n    Synchronous method:\n    - `clear_network_conditions(interface)`: Clears any simulated network configuration for a given interface.\n\n    All asynchronous methods should be non-blocking to support integration in async systems.\n    \"\"\"\n\n    @abstractmethod\n    async def start(self):\n        \"\"\"\n        Starts the network simulation module.\n\n        This might involve preparing network interfaces, initializing tools like `tc`, or configuring internal state.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def stop(self):\n        \"\"\"\n        Stops the network simulation module.\n\n        Cleans up any modifications made to network interfaces or system configuration.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def set_thresholds(self, thresholds: dict):\n        \"\"\"\n        Sets threshold values for simulating conditions.\n\n        Args:\n            thresholds (dict): A dictionary specifying condition thresholds,\n                               e.g., {'low': 100, 'medium': 200, 'high': 300}, or distance-delay mappings.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def set_network_conditions(self, dest_addr, distance):\n        \"\"\"\n        Applies network simulation settings to a given destination based on the computed distance.\n\n        Args:\n            dest_addr (str): The address of the destination node (e.g., IP or identifier).\n            distance (float): The physical or logical distance used to determine the simulation severity.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def clear_network_conditions(self, interface):\n        \"\"\"\n        Clears any simulated network conditions applied to the specified network interface.\n\n        Args:\n            interface (str): The name of the network interface to restore (e.g., 'eth0').\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/addons/networksimulation/networksimulator/#nebula.addons.networksimulation.networksimulator.NetworkSimulator.clear_network_conditions","title":"<code>clear_network_conditions(interface)</code>  <code>abstractmethod</code>","text":"<p>Clears any simulated network conditions applied to the specified network interface.</p> <p>Parameters:</p> Name Type Description Default <code>interface</code> <code>str</code> <p>The name of the network interface to restore (e.g., 'eth0').</p> required Source code in <code>nebula/addons/networksimulation/networksimulator.py</code> <pre><code>@abstractmethod\ndef clear_network_conditions(self, interface):\n    \"\"\"\n    Clears any simulated network conditions applied to the specified network interface.\n\n    Args:\n        interface (str): The name of the network interface to restore (e.g., 'eth0').\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/addons/networksimulation/networksimulator/#nebula.addons.networksimulation.networksimulator.NetworkSimulator.set_network_conditions","title":"<code>set_network_conditions(dest_addr, distance)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Applies network simulation settings to a given destination based on the computed distance.</p> <p>Parameters:</p> Name Type Description Default <code>dest_addr</code> <code>str</code> <p>The address of the destination node (e.g., IP or identifier).</p> required <code>distance</code> <code>float</code> <p>The physical or logical distance used to determine the simulation severity.</p> required Source code in <code>nebula/addons/networksimulation/networksimulator.py</code> <pre><code>@abstractmethod\nasync def set_network_conditions(self, dest_addr, distance):\n    \"\"\"\n    Applies network simulation settings to a given destination based on the computed distance.\n\n    Args:\n        dest_addr (str): The address of the destination node (e.g., IP or identifier).\n        distance (float): The physical or logical distance used to determine the simulation severity.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/addons/networksimulation/networksimulator/#nebula.addons.networksimulation.networksimulator.NetworkSimulator.set_thresholds","title":"<code>set_thresholds(thresholds)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Sets threshold values for simulating conditions.</p> <p>Parameters:</p> Name Type Description Default <code>thresholds</code> <code>dict</code> <p>A dictionary specifying condition thresholds,                e.g., {'low': 100, 'medium': 200, 'high': 300}, or distance-delay mappings.</p> required Source code in <code>nebula/addons/networksimulation/networksimulator.py</code> <pre><code>@abstractmethod\nasync def set_thresholds(self, thresholds: dict):\n    \"\"\"\n    Sets threshold values for simulating conditions.\n\n    Args:\n        thresholds (dict): A dictionary specifying condition thresholds,\n                           e.g., {'low': 100, 'medium': 200, 'high': 300}, or distance-delay mappings.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/addons/networksimulation/networksimulator/#nebula.addons.networksimulation.networksimulator.NetworkSimulator.start","title":"<code>start()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Starts the network simulation module.</p> <p>This might involve preparing network interfaces, initializing tools like <code>tc</code>, or configuring internal state.</p> Source code in <code>nebula/addons/networksimulation/networksimulator.py</code> <pre><code>@abstractmethod\nasync def start(self):\n    \"\"\"\n    Starts the network simulation module.\n\n    This might involve preparing network interfaces, initializing tools like `tc`, or configuring internal state.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/addons/networksimulation/networksimulator/#nebula.addons.networksimulation.networksimulator.NetworkSimulator.stop","title":"<code>stop()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Stops the network simulation module.</p> <p>Cleans up any modifications made to network interfaces or system configuration.</p> Source code in <code>nebula/addons/networksimulation/networksimulator.py</code> <pre><code>@abstractmethod\nasync def stop(self):\n    \"\"\"\n    Stops the network simulation module.\n\n    Cleans up any modifications made to network interfaces or system configuration.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/addons/reputation/","title":"Documentation for Reputation Module","text":""},{"location":"api/addons/reputation/reputation/","title":"Documentation for Reputation Module","text":""},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Metrics","title":"<code>Metrics</code>","text":"Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>class Metrics:\n    def __init__(\n        self,\n        num_round=None,\n        current_round=None,\n        fraction_changed=None,\n        threshold=None,\n        latency=None,\n    ):\n        \"\"\"\n        Initialize a Metrics instance to store various evaluation metrics for a participant.\n\n        Args:\n            num_round (optional): The current round number.\n            current_round (optional): The round when the metric is measured.\n            fraction_changed (optional): Fraction of parameters changed.\n            threshold (optional): Threshold used for evaluating changes.\n            latency (optional): Latency value for model arrival.\n        \"\"\"\n        self.fraction_of_params_changed = {\n            \"fraction_changed\": fraction_changed,\n            \"threshold\": threshold,\n            \"round\": num_round,\n        }\n\n        self.model_arrival_latency = {\"latency\": latency, \"round\": num_round, \"round_received\": current_round}\n\n        self.messages = []\n\n        self.similarity = []\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Metrics.__init__","title":"<code>__init__(num_round=None, current_round=None, fraction_changed=None, threshold=None, latency=None)</code>","text":"<p>Initialize a Metrics instance to store various evaluation metrics for a participant.</p> <p>Parameters:</p> Name Type Description Default <code>num_round</code> <code>optional</code> <p>The current round number.</p> <code>None</code> <code>current_round</code> <code>optional</code> <p>The round when the metric is measured.</p> <code>None</code> <code>fraction_changed</code> <code>optional</code> <p>Fraction of parameters changed.</p> <code>None</code> <code>threshold</code> <code>optional</code> <p>Threshold used for evaluating changes.</p> <code>None</code> <code>latency</code> <code>optional</code> <p>Latency value for model arrival.</p> <code>None</code> Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>def __init__(\n    self,\n    num_round=None,\n    current_round=None,\n    fraction_changed=None,\n    threshold=None,\n    latency=None,\n):\n    \"\"\"\n    Initialize a Metrics instance to store various evaluation metrics for a participant.\n\n    Args:\n        num_round (optional): The current round number.\n        current_round (optional): The round when the metric is measured.\n        fraction_changed (optional): Fraction of parameters changed.\n        threshold (optional): Threshold used for evaluating changes.\n        latency (optional): Latency value for model arrival.\n    \"\"\"\n    self.fraction_of_params_changed = {\n        \"fraction_changed\": fraction_changed,\n        \"threshold\": threshold,\n        \"round\": num_round,\n    }\n\n    self.model_arrival_latency = {\"latency\": latency, \"round\": num_round, \"round_received\": current_round}\n\n    self.messages = []\n\n    self.similarity = []\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation","title":"<code>Reputation</code>","text":"<p>Class to define and manage the reputation of a participant in the network.</p> <p>The class handles collection of metrics, calculation of static and dynamic reputation, updating history, and communication of reputation scores to neighbors.</p> Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>class Reputation:\n    \"\"\"\n    Class to define and manage the reputation of a participant in the network.\n\n    The class handles collection of metrics, calculation of static and dynamic reputation,\n    updating history, and communication of reputation scores to neighbors.\n    \"\"\"\n\n    REPUTATION_THRESHOLD = 0.6\n    SIMILARITY_THRESHOLD = 0.6\n    INITIAL_ROUND_FOR_REPUTATION = 1\n    INITIAL_ROUND_FOR_FRACTION = 1\n    HISTORY_ROUNDS_LOOKBACK = 4\n    WEIGHTED_HISTORY_ROUNDS = 3\n    FRACTION_ANOMALY_MULTIPLIER = 1.20\n    THRESHOLD_ANOMALY_MULTIPLIER = 1.15\n\n    # Augmentation factors\n    LATENCY_AUGMENT_FACTOR = 1.4\n    MESSAGE_AUGMENT_FACTOR_EARLY = 2.0\n    MESSAGE_AUGMENT_FACTOR_NORMAL = 1.1\n\n    # Penalty and decay factors\n    HISTORICAL_PENALTY_THRESHOLD = 0.9\n    NEGATIVE_LATENCY_PENALTY = 0.3\n    CURRENT_VALUE_WEIGHT_HIGH = 0.9\n    CURRENT_VALUE_WEIGHT_LOW = 0.2\n    PAST_VALUE_WEIGHT_HIGH = 0.8\n    PAST_VALUE_WEIGHT_LOW = 0.1\n    ZERO_VALUE_DECAY_FACTOR = 0.1\n    REPUTATION_CURRENT_WEIGHT = 0.9\n    REPUTATION_FEEDBACK_WEIGHT = 0.1\n    THRESHOLD_VARIANCE_MULTIPLIER = 0.1\n    DYNAMIC_MIN_WEIGHT_THRESHOLD = 0.1\n    REPUTATION_SCALING_THRESHOLD = 0.7\n    REPUTATION_SCALING_RANGE = 0.3\n\n    def __init__(self, engine: \"Engine\", config: \"Config\"):\n        \"\"\"\n        Initialize the Reputation system.\n\n        Args:\n            engine (Engine): The engine instance providing the runtime context.\n            config (Config): The configuration object with participant settings.\n        \"\"\"\n        self._engine = engine\n        self._config = config\n        self._addr = engine.addr\n        self._log_dir = engine.log_dir\n        self._idx = engine.idx\n\n        self._initialize_data_structures()\n        self._configure_constants()\n        self._load_configuration()\n        self._setup_connection_metrics()\n        self._configure_metric_weights()\n        self._log_initialization_info()\n\n    def _configure_constants(self):\n        \"\"\"Configure system constants from config or use defaults.\"\"\"\n        reputation_config = self._config.participant.get(\"defense_args\", {}).get(\"reputation\", {})\n        constants_config = reputation_config.get(\"constants\", {})\n\n        self.REPUTATION_THRESHOLD = constants_config.get(\"reputation_threshold\", self.REPUTATION_THRESHOLD)\n        self.SIMILARITY_THRESHOLD = constants_config.get(\"similarity_threshold\", self.SIMILARITY_THRESHOLD)\n        self.INITIAL_ROUND_FOR_REPUTATION = constants_config.get(\"initial_round_for_reputation\", self.INITIAL_ROUND_FOR_REPUTATION)\n        self.INITIAL_ROUND_FOR_FRACTION = constants_config.get(\"initial_round_for_fraction\", self.INITIAL_ROUND_FOR_FRACTION)\n        self.HISTORY_ROUNDS_LOOKBACK = constants_config.get(\"history_rounds_lookback\", self.HISTORY_ROUNDS_LOOKBACK)\n        self.WEIGHTED_HISTORY_ROUNDS = constants_config.get(\"weighted_history_rounds\", self.WEIGHTED_HISTORY_ROUNDS)\n        self.FRACTION_ANOMALY_MULTIPLIER = constants_config.get(\"fraction_anomaly_multiplier\", self.FRACTION_ANOMALY_MULTIPLIER)\n        self.THRESHOLD_ANOMALY_MULTIPLIER = constants_config.get(\"threshold_anomaly_multiplier\", self.THRESHOLD_ANOMALY_MULTIPLIER)\n        self.LATENCY_AUGMENT_FACTOR = constants_config.get(\"latency_augment_factor\", self.LATENCY_AUGMENT_FACTOR)\n        self.MESSAGE_AUGMENT_FACTOR_EARLY = constants_config.get(\"message_augment_factor_early\", self.MESSAGE_AUGMENT_FACTOR_EARLY)\n        self.MESSAGE_AUGMENT_FACTOR_NORMAL = constants_config.get(\"message_augment_factor_normal\", self.MESSAGE_AUGMENT_FACTOR_NORMAL)\n        self.HISTORICAL_PENALTY_THRESHOLD = constants_config.get(\"historical_penalty_threshold\", self.HISTORICAL_PENALTY_THRESHOLD)\n        self.NEGATIVE_LATENCY_PENALTY = constants_config.get(\"negative_latency_penalty\", self.NEGATIVE_LATENCY_PENALTY)\n        self.CURRENT_VALUE_WEIGHT_HIGH = constants_config.get(\"current_value_weight_high\", self.CURRENT_VALUE_WEIGHT_HIGH)\n        self.CURRENT_VALUE_WEIGHT_LOW = constants_config.get(\"current_value_weight_low\", self.CURRENT_VALUE_WEIGHT_LOW)\n        self.PAST_VALUE_WEIGHT_HIGH = constants_config.get(\"past_value_weight_high\", self.PAST_VALUE_WEIGHT_HIGH)\n        self.PAST_VALUE_WEIGHT_LOW = constants_config.get(\"past_value_weight_low\", self.PAST_VALUE_WEIGHT_LOW)\n        self.ZERO_VALUE_DECAY_FACTOR = constants_config.get(\"zero_value_decay_factor\", self.ZERO_VALUE_DECAY_FACTOR)\n        self.REPUTATION_CURRENT_WEIGHT = constants_config.get(\"reputation_current_weight\", self.REPUTATION_CURRENT_WEIGHT)\n        self.REPUTATION_FEEDBACK_WEIGHT = constants_config.get(\"reputation_feedback_weight\", self.REPUTATION_FEEDBACK_WEIGHT)\n        self.THRESHOLD_VARIANCE_MULTIPLIER = constants_config.get(\"threshold_variance_multiplier\", self.THRESHOLD_VARIANCE_MULTIPLIER)\n        self.DYNAMIC_MIN_WEIGHT_THRESHOLD = constants_config.get(\"dynamic_min_weight_threshold\", self.DYNAMIC_MIN_WEIGHT_THRESHOLD)\n        self.REPUTATION_SCALING_THRESHOLD = constants_config.get(\"reputation_scaling_threshold\", self.REPUTATION_SCALING_THRESHOLD)\n        self.REPUTATION_SCALING_RANGE = constants_config.get(\"reputation_scaling_range\", self.REPUTATION_SCALING_RANGE)\n\n    def _initialize_data_structures(self):\n        \"\"\"Initialize all data structures used by the reputation system.\"\"\"\n        self.reputation = {}\n        self.reputation_with_feedback = {}\n        self.reputation_with_all_feedback = {}\n        self.reputation_history = {}\n        self.rejected_nodes = set()\n        self.fraction_of_params_changed = {}\n        self.history_data = {}\n        self.metric_weights = {}\n        self.connection_metrics = {}\n        self.messages_number_message = []\n        self.number_message_history = {}\n        self._messages_received_from_sources = {}\n        self.round_timing_info = {}\n        self.neighbor_reputation_history = {}\n        self.fraction_changed_history = {}\n        self.messages_model_arrival_latency = {}\n        self.model_arrival_latency_history = {}\n        self.previous_threshold_number_message = {}\n        self.previous_std_dev_number_message = {}\n        self.previous_percentile_25_number_message = {}\n        self.previous_percentile_85_number_message = {}\n\n    def _load_configuration(self):\n        \"\"\"Load and validate reputation configuration.\"\"\"\n        reputation_config = self._config.participant[\"defense_args\"][\"reputation\"]\n        self._enabled = reputation_config[\"enabled\"]\n        self._metrics = reputation_config[\"metrics\"]\n        self._initial_reputation = float(reputation_config[\"initial_reputation\"])\n        self._weighting_factor = reputation_config[\"weighting_factor\"]\n\n        if not isinstance(self._metrics, dict):\n            logging.error(f\"Invalid metrics configuration: expected dict, got {type(self._metrics)}\")\n            self._metrics = {}\n\n    def _setup_connection_metrics(self):\n        \"\"\"Initialize metrics for each neighbor.\"\"\"\n        neighbors_str = self._config.participant[\"network_args\"][\"neighbors\"]\n        for neighbor in neighbors_str.split():\n            self.connection_metrics[neighbor] = Metrics()\n\n    def _configure_metric_weights(self):\n        \"\"\"Configure weights for different metrics based on weighting factor.\"\"\"\n        default_weight = 0.25\n        metric_names = [\"model_arrival_latency\", \"model_similarity\", \"num_messages\", \"fraction_parameters_changed\"]\n\n        if self._weighting_factor == \"static\":\n            self._weight_model_arrival_latency = float(\n                self._metrics.get(\"model_arrival_latency\", {}).get(\"weight\", default_weight)\n            )\n            self._weight_model_similarity = float(\n                self._metrics.get(\"model_similarity\", {}).get(\"weight\", default_weight)\n            )\n            self._weight_num_messages = float(\n                self._metrics.get(\"num_messages\", {}).get(\"weight\", default_weight)\n            )\n            self._weight_fraction_params_changed = float(\n                self._metrics.get(\"fraction_parameters_changed\", {}).get(\"weight\", default_weight)\n            )\n        else:\n            for metric_name in metric_names:\n                if metric_name not in self._metrics:\n                    self._metrics[metric_name] = {}\n                elif not isinstance(self._metrics[metric_name], dict):\n                    self._metrics[metric_name] = {\"enabled\": bool(self._metrics[metric_name])}\n                self._metrics[metric_name][\"weight\"] = default_weight\n\n            self._weight_model_arrival_latency = default_weight\n            self._weight_model_similarity = default_weight\n            self._weight_num_messages = default_weight\n            self._weight_fraction_params_changed = default_weight\n\n    def _log_initialization_info(self):\n        \"\"\"Log initialization information.\"\"\"\n        msg = f\"Reputation system: {self._enabled}\"\n        msg += f\"\\nReputation metrics: {self._metrics}\"\n        msg += f\"\\nInitial reputation: {self._initial_reputation}\"\n        print_msg_box(msg=msg, indent=2, title=\"Defense information\")\n\n    @property\n    def engine(self):\n        return self._engine\n\n    def _is_metric_enabled(self, metric_name: str, metrics_config: dict = None) -&gt; bool:\n        \"\"\"\n        Check if a specific metric is enabled based on the provided configuration.\n\n        Args:\n            metric_name (str): The name of the metric to check.\n            metrics_config (dict, optional): The configuration dictionary for metrics. \n                                           If None, uses the instance's _metrics.\n\n        Returns:\n            bool: True if the metric is enabled, False otherwise.\n        \"\"\"\n        config_to_use = metrics_config if metrics_config is not None else getattr(self, '_metrics', None)\n\n        if not isinstance(config_to_use, dict):\n            if metrics_config is not None:\n                logging.warning(f\"metrics_config is not a dictionary: {type(metrics_config)}\")\n            else:\n                logging.warning(\"_metrics is not properly initialized\")\n            return False\n\n        metric_config = config_to_use.get(metric_name)\n        if metric_config is None:\n            return False\n\n        if isinstance(metric_config, dict):\n            return metric_config.get('enabled', True)\n        return bool(metric_config)\n\n    def save_data(\n        self,\n        type_data: str,\n        nei: str,\n        addr: str,\n        num_round: int = None,\n        time: float = None,\n        current_round: int = None,\n        fraction_changed: float = None,\n        threshold: float = None,\n        latency: float = None,\n    ):\n        \"\"\"\n        Save data between nodes and aggregated models.\n\n        Args:\n            type_data: Type of data to save ('number_message', 'fraction_of_params_changed', 'model_arrival_latency')\n            nei: Neighbor identifier\n            addr: Address identifier\n            num_round: Round number\n            time: Timestamp\n            current_round: Current round number\n            fraction_changed: Fraction of parameters changed\n            threshold: Threshold value\n            latency: Latency value\n        \"\"\"\n        if addr == nei:\n            return\n\n        if nei not in self.connection_metrics:\n            logging.warning(f\"Neighbor {nei} not found in connection_metrics\")\n            return\n\n        try:\n            metrics_instance = self.connection_metrics[nei]\n\n            if type_data == \"number_message\":\n                message_data = {\"time\": time, \"current_round\": current_round}\n                if not isinstance(metrics_instance.messages, list):\n                    metrics_instance.messages = []\n                metrics_instance.messages.append(message_data)\n            elif type_data == \"fraction_of_params_changed\":\n                fraction_data = {\n                    \"fraction_changed\": fraction_changed,\n                    \"threshold\": threshold,\n                    \"current_round\": current_round,\n                }\n                metrics_instance.fraction_of_params_changed.update(fraction_data)\n            elif type_data == \"model_arrival_latency\":\n                latency_data = {\n                    \"latency\": latency,\n                    \"round\": num_round,\n                    \"round_received\": current_round,\n                }\n                metrics_instance.model_arrival_latency.update(latency_data)\n            else:\n                logging.warning(f\"Unknown data type: {type_data}\")\n\n        except Exception:\n            logging.exception(f\"Error saving data for type {type_data} and neighbor {nei}\")\n\n    async def setup(self):\n        \"\"\"Set up the reputation system by subscribing to relevant events.\"\"\"\n        if self._enabled:\n            await EventManager.get_instance().subscribe_node_event(RoundStartEvent, self.on_round_start)\n            await EventManager.get_instance().subscribe_node_event(AggregationEvent, self.calculate_reputation)\n            if self._is_metric_enabled(\"model_similarity\"):\n                await EventManager.get_instance().subscribe_node_event(UpdateReceivedEvent, self.recollect_similarity)\n            if self._is_metric_enabled(\"fraction_parameters_changed\"):\n                await EventManager.get_instance().subscribe_node_event(\n                    UpdateReceivedEvent, self.recollect_fraction_of_parameters_changed\n                )\n            if self._is_metric_enabled(\"model_arrival_latency\"):\n                await EventManager.get_instance().subscribe_node_event(\n                    UpdateReceivedEvent, self.recollect_model_arrival_latency\n                )\n            if self._is_metric_enabled(\"num_messages\"):\n                await EventManager.get_instance().subscribe((\"model\", \"update\"), self.recollect_number_message)\n                await EventManager.get_instance().subscribe((\"model\", \"initialization\"), self.recollect_number_message)\n                await EventManager.get_instance().subscribe((\"control\", \"alive\"), self.recollect_number_message)\n                await EventManager.get_instance().subscribe(\n                    (\"federation\", \"federation_models_included\"), self.recollect_number_message\n                )\n                await EventManager.get_instance().subscribe_node_event(DuplicatedMessageEvent, self.recollect_duplicated_number_message)\n\n    async def init_reputation(\n        self, federation_nodes=None, round_num=None, last_feedback_round=None, init_reputation=None\n    ):\n        \"\"\"\n        Initialize the reputation system.\n\n        Args:\n            federation_nodes: List of federation node identifiers\n            round_num: Current round number  \n            last_feedback_round: Last round that received feedback\n            init_reputation: Initial reputation value to assign\n        \"\"\"\n        if not self._enabled:\n            return\n\n        if not self._validate_init_parameters(federation_nodes, round_num, init_reputation):\n            return\n\n        neighbors = self._validate_federation_nodes(federation_nodes)\n        if not neighbors:\n            logging.error(\"init_reputation | No valid neighbors found\")\n            return\n\n        await self._initialize_neighbor_reputations(neighbors, round_num, last_feedback_round, init_reputation)\n\n    def _validate_init_parameters(self, federation_nodes, round_num, init_reputation) -&gt; bool:\n        \"\"\"Validate initialization parameters.\"\"\"\n        if not federation_nodes:\n            logging.error(\"init_reputation | No federation nodes provided\")\n            return False\n\n        if round_num is None:\n            logging.warning(\"init_reputation | Round number not provided\")\n\n        if init_reputation is None:\n            logging.warning(\"init_reputation | Initial reputation value not provided\")\n\n        return True\n\n    async def _initialize_neighbor_reputations(self, neighbors: list, round_num: int, last_feedback_round: int, init_reputation: float):\n        \"\"\"Initialize reputation entries for all neighbors.\"\"\"\n        for nei in neighbors:\n            self._create_or_update_reputation_entry(nei, round_num, last_feedback_round, init_reputation)\n            await self.save_reputation_history_in_memory(self._addr, nei, init_reputation)\n\n    def _create_or_update_reputation_entry(self, nei: str, round_num: int, last_feedback_round: int, init_reputation: float):\n        \"\"\"Create or update a single reputation entry.\"\"\"\n        reputation_data = {\n            \"reputation\": init_reputation,\n            \"round\": round_num,\n            \"last_feedback_round\": last_feedback_round,\n        }\n\n        if nei not in self.reputation:\n            self.reputation[nei] = reputation_data\n        elif self.reputation[nei].get(\"reputation\") is None:\n            self.reputation[nei].update(reputation_data)\n\n    def _validate_federation_nodes(self, federation_nodes) -&gt; list:\n        \"\"\"\n        Validate and filter federation nodes.\n\n        Args:\n            federation_nodes: List of federation node identifiers\n\n        Returns:\n            list: List of valid node identifiers\n        \"\"\"\n        if not federation_nodes:\n            return []\n\n        valid_nodes = [node for node in federation_nodes if node and str(node).strip()]\n\n        if not valid_nodes:\n            logging.warning(\"No valid federation nodes found after filtering\")\n\n        return valid_nodes\n\n    async def _calculate_static_reputation(\n        self,\n        addr: str,\n        nei: str,\n        metric_values: dict,\n    ):\n        \"\"\"\n        Calculate the static reputation of a participant using weighted metrics.\n\n        Args:\n            addr: The participant's address\n            nei: The neighbor's address  \n            metric_values: Dictionary with metric values\n        \"\"\"\n        static_weights = {\n            \"num_messages\": self._weight_num_messages,\n            \"model_similarity\": self._weight_model_similarity,\n            \"fraction_parameters_changed\": self._weight_fraction_params_changed,\n            \"model_arrival_latency\": self._weight_model_arrival_latency,\n        }\n\n        reputation_static = sum(\n            metric_values.get(metric_name, 0) * static_weights[metric_name] \n            for metric_name in static_weights\n        )\n\n        logging.info(f\"Static reputation for node {nei} at round {await self.engine.get_round()}: {reputation_static}\")\n\n        avg_reputation = await self.save_reputation_history_in_memory(self.engine.addr, nei, reputation_static)\n\n        metrics_data = {\n            \"addr\": addr,\n            \"nei\": nei,\n            \"round\": await self.engine.get_round(),\n            \"reputation_without_feedback\": avg_reputation,\n            **{f\"average_{name}\": weight for name, weight in static_weights.items()}\n        }\n\n        await self._update_reputation_record(nei, avg_reputation, metrics_data)\n\n    async def _calculate_dynamic_reputation(self, addr, neighbors):\n        \"\"\"\n        Calculate the dynamic reputation of a participant.\n\n        Args:\n            addr (str): The IP address of the participant.\n            neighbors (list): The list of neighbors.\n        \"\"\"\n        if not hasattr(self, '_metrics') or self._metrics is None:\n            logging.warning(\"_metrics is not properly initialized\")\n            return\n\n        average_weights = await self._calculate_average_weights()\n        await self._process_neighbors_reputation(addr, neighbors, average_weights)\n\n    async def _calculate_average_weights(self):\n        \"\"\"Calculate average weights for all enabled metrics.\"\"\"\n        average_weights = {}\n\n        for metric_name in self.history_data.keys():\n            if self._is_metric_enabled(metric_name):\n                average_weights[metric_name] = await self._get_metric_average_weight(metric_name)\n\n        return average_weights\n\n    async def _get_metric_average_weight(self, metric_name):\n        \"\"\"Get the average weight for a specific metric.\"\"\"\n        if metric_name not in self.history_data or not self.history_data[metric_name]:\n            logging.debug(f\"No history data available for metric: {metric_name}\")\n            return 0\n\n        valid_entries = [\n            entry for entry in self.history_data[metric_name]\n            if (entry.get(\"round\") is not None and \n                entry[\"round\"] &gt;= await self._engine.get_round() and \n                entry.get(\"weight\") not in [None, -1])\n        ]\n\n        if not valid_entries:\n            return 0\n\n        try:\n            weights = [entry[\"weight\"] for entry in valid_entries if entry.get(\"weight\") is not None]\n            return sum(weights) / len(weights) if weights else 0\n        except (TypeError, ZeroDivisionError) as e:\n            logging.warning(f\"Error calculating average weight for {metric_name}: {e}\")\n            return 0\n\n    async def _process_neighbors_reputation(self, addr, neighbors, average_weights):\n        \"\"\"Process reputation calculation for all neighbors.\"\"\"\n        for nei in neighbors:\n            metric_values = await self._get_neighbor_metric_values(nei)\n\n            if all(metric_name in metric_values for metric_name in average_weights):\n                await self._update_neighbor_reputation(addr, nei, metric_values, average_weights)\n\n    async def _get_neighbor_metric_values(self, nei):\n        \"\"\"Get metric values for a specific neighbor in the current round.\"\"\"\n        metric_values = {}\n\n        for metric_name in self.history_data:\n            if self._is_metric_enabled(metric_name):\n                for entry in self.history_data.get(metric_name, []):\n                    if (entry.get(\"round\") == await self._engine.get_round() and\n                        entry.get(\"metric_name\") == metric_name and\n                        entry.get(\"nei\") == nei):\n                        metric_values[metric_name] = entry.get(\"metric_value\", 0)\n                        break\n\n        return metric_values\n\n    async def _update_neighbor_reputation(self, addr, nei, metric_values, average_weights):\n        \"\"\"Update reputation for a specific neighbor.\"\"\"\n        reputation_with_weights = sum(\n            metric_values.get(metric_name, 0) * average_weights[metric_name] \n            for metric_name in average_weights\n        )\n\n        logging.info(\n            f\"Dynamic reputation with weights for {nei} at round {await self._engine.get_round()}: {reputation_with_weights}\"\n        )\n\n        avg_reputation = await self.save_reputation_history_in_memory(self._engine.addr, nei, reputation_with_weights)\n\n        metrics_data = {\n            \"addr\": addr,\n            \"nei\": nei,\n            \"round\": await self._engine.get_round(),\n            \"reputation_without_feedback\": avg_reputation,\n        }\n\n        for metric_name in metric_values:\n            metrics_data[f\"average_{metric_name}\"] = average_weights[metric_name]\n\n        await self._update_reputation_record(nei, avg_reputation, metrics_data)\n\n    async def _update_reputation_record(self, nei: str, reputation: float, data: dict):\n        \"\"\"\n        Update the reputation record of a participant.\n\n        Args:\n            nei: The neighbor identifier\n            reputation: The reputation value\n            data: Additional data to update (currently unused)\n        \"\"\"\n        current_round = await self._engine.get_round()\n\n        if nei not in self.reputation:\n            self.reputation[nei] = {\n                \"reputation\": reputation,\n                \"round\": current_round,\n                \"last_feedback_round\": -1,\n            }\n        else:\n            self.reputation[nei][\"reputation\"] = reputation\n            self.reputation[nei][\"round\"] = current_round\n\n        logging.info(f\"Reputation of node {nei}: {self.reputation[nei]['reputation']}\")\n\n        if self.reputation[nei][\"reputation\"] &lt; self.REPUTATION_THRESHOLD and current_round &gt; 0:\n            self.rejected_nodes.add(nei)\n            logging.info(f\"Rejected node {nei} at round {current_round}\")\n\n    def calculate_weighted_values(\n        self,\n        avg_messages_number_message_normalized,\n        similarity_reputation,\n        fraction_score_asign,\n        avg_model_arrival_latency,\n        history_data,\n        current_round,\n        addr,\n        nei,\n        reputation_metrics,\n    ):\n        \"\"\"\n        Calculate the weighted values for each metric.\n        \"\"\"\n        if current_round is None:\n            return\n\n        self._ensure_history_data_structure(history_data)\n        active_metrics = self._get_active_metrics(\n            avg_messages_number_message_normalized,\n            similarity_reputation,\n            fraction_score_asign,\n            avg_model_arrival_latency,\n            reputation_metrics\n        )\n        self._add_current_metrics_to_history(active_metrics, history_data, current_round, addr, nei)\n\n        if current_round &gt;= self.INITIAL_ROUND_FOR_REPUTATION and len(active_metrics) &gt; 0:\n            adjusted_weights = self._calculate_dynamic_weights(active_metrics, history_data)\n        else:\n            adjusted_weights = self._calculate_uniform_weights(active_metrics)\n\n        self._update_history_with_weights(active_metrics, history_data, adjusted_weights, current_round, nei)\n\n    def _ensure_history_data_structure(self, history_data: dict):\n        \"\"\"Ensure all required keys exist in history data structure.\"\"\"\n        required_keys = [\n            \"num_messages\",\n            \"model_similarity\", \n            \"fraction_parameters_changed\",\n            \"model_arrival_latency\",\n        ]\n\n        for key in required_keys:\n            if key not in history_data:\n                history_data[key] = []\n\n    def _get_active_metrics(\n        self,\n        avg_messages_number_message_normalized,\n        similarity_reputation,\n        fraction_score_asign,\n        avg_model_arrival_latency,\n        reputation_metrics\n    ) -&gt; dict:\n        \"\"\"Get the dictionary of active metrics based on configuration.\"\"\"\n        all_metrics = {\n            \"num_messages\": avg_messages_number_message_normalized,\n            \"model_similarity\": similarity_reputation,\n            \"fraction_parameters_changed\": fraction_score_asign,\n            \"model_arrival_latency\": avg_model_arrival_latency,\n        }\n\n        return {k: v for k, v in all_metrics.items() if self._is_metric_enabled(k, reputation_metrics)}\n\n    def _add_current_metrics_to_history(self, active_metrics: dict, history_data: dict, current_round: int, addr: str, nei: str):\n        \"\"\"Add current metric values to history data.\"\"\"\n        for metric_name, current_value in active_metrics.items():\n            history_data[metric_name].append({\n                \"round\": current_round,\n                \"addr\": addr,\n                \"nei\": nei,\n                \"metric_name\": metric_name,\n                \"metric_value\": current_value,\n                \"weight\": None,\n            })\n\n    def _calculate_dynamic_weights(self, active_metrics: dict, history_data: dict) -&gt; dict:\n        \"\"\"Calculate dynamic weights based on metric deviations.\"\"\"\n        deviations = self._calculate_metric_deviations(active_metrics, history_data)\n\n        if all(deviation == 0.0 for deviation in deviations.values()):\n            return self._generate_random_weights(active_metrics)\n        else:\n            normalized_weights = self._normalize_deviation_weights(deviations)\n            return self._adjust_weights_with_minimum(normalized_weights, deviations)\n\n    def _calculate_metric_deviations(self, active_metrics: dict, history_data: dict) -&gt; dict:\n        \"\"\"Calculate deviations of current metrics from historical means.\"\"\"\n        deviations = {}\n\n        for metric_name, current_value in active_metrics.items():\n            historical_values = history_data[metric_name]\n            metric_values = [\n                entry[\"metric_value\"]\n                for entry in historical_values\n                if \"metric_value\" in entry and entry[\"metric_value\"] != 0\n            ]\n\n            mean_value = np.mean(metric_values) if metric_values else 0\n            deviation = abs(current_value - mean_value)\n            deviations[metric_name] = deviation\n\n        return deviations\n\n    def _generate_random_weights(self, active_metrics: dict) -&gt; dict:\n        \"\"\"Generate random normalized weights when all deviations are zero.\"\"\"\n        num_metrics = len(active_metrics)\n        random_weights = [random.random() for _ in range(num_metrics)]\n        total_random_weight = sum(random_weights)\n\n        return {\n            metric_name: weight / total_random_weight\n            for metric_name, weight in zip(active_metrics, random_weights, strict=False)\n        }\n\n    def _normalize_deviation_weights(self, deviations: dict) -&gt; dict:\n        \"\"\"Normalize weights based on deviations.\"\"\"\n        max_deviation = max(deviations.values()) if deviations else 1\n        normalized_weights = {\n            metric_name: (deviation / max_deviation) \n            for metric_name, deviation in deviations.items()\n        }\n\n        total_weight = sum(normalized_weights.values())\n        if total_weight &gt; 0:\n            return {\n                metric_name: weight / total_weight \n                for metric_name, weight in normalized_weights.items()\n            }\n        else:\n            num_metrics = len(deviations)\n            return dict.fromkeys(deviations.keys(), 1 / num_metrics)\n\n    def _adjust_weights_with_minimum(self, normalized_weights: dict, deviations: dict) -&gt; dict:\n        \"\"\"Apply minimum weight constraints and renormalize.\"\"\"\n        mean_deviation = np.mean(list(deviations.values()))\n        dynamic_min_weight = max(self.DYNAMIC_MIN_WEIGHT_THRESHOLD, mean_deviation / (mean_deviation + 1))\n\n        adjusted_weights = {}\n        total_adjusted_weight = 0\n\n        for metric_name, weight in normalized_weights.items():\n            adjusted_weight = max(weight, dynamic_min_weight)\n            adjusted_weights[metric_name] = adjusted_weight\n            total_adjusted_weight += adjusted_weight\n\n        # Renormalize if total weight exceeds 1\n        if total_adjusted_weight &gt; 1:\n            for metric_name in adjusted_weights:\n                adjusted_weights[metric_name] /= total_adjusted_weight\n\n        return adjusted_weights\n\n    def _calculate_uniform_weights(self, active_metrics: dict) -&gt; dict:\n        \"\"\"Calculate uniform weights for all active metrics.\"\"\"\n        num_metrics = len(active_metrics)\n        if num_metrics == 0:\n            return {}\n        return dict.fromkeys(active_metrics, 1 / num_metrics)\n\n    def _update_history_with_weights(self, active_metrics: dict, history_data: dict, weights: dict, current_round: int, nei: str):\n        \"\"\"Update history entries with calculated weights.\"\"\"\n        for metric_name in active_metrics:\n            weight = weights.get(metric_name, -1)\n            for entry in history_data[metric_name]:\n                if (entry[\"metric_name\"] == metric_name and \n                    entry[\"round\"] == current_round and \n                    entry[\"nei\"] == nei):\n                    entry[\"weight\"] = weight\n\n    async def calculate_value_metrics(self, addr, nei, metrics_active=None):\n        \"\"\"\n        Calculate the reputation of each participant based on the data stored in self.connection_metrics.\n\n        Args:\n            addr (str): Source IP address.\n            nei (str): Destination IP address.\n            metrics_active (dict): The active metrics.\n        \"\"\"\n        try:\n            current_round = await self._engine.get_round()\n            metrics_instance = self.connection_metrics.get(nei)\n\n            if not metrics_instance:\n                logging.warning(f\"No metrics found for neighbor {nei}\")\n                return self._get_default_metric_values()\n\n            metric_results = {\n                \"messages\": self._process_num_messages_metric(metrics_instance, addr, nei, current_round, metrics_active),\n                \"fraction\": self._process_fraction_parameters_metric(metrics_instance, addr, nei, current_round, metrics_active),\n                \"latency\": self._process_model_arrival_latency_metric(metrics_instance, addr, nei, current_round, metrics_active),\n                \"similarity\": self._process_model_similarity_metric(nei, current_round, metrics_active)\n            }\n\n            self._log_metrics_graphics(metric_results, addr, nei, current_round)\n\n            return (\n                metric_results[\"messages\"][\"avg\"],\n                metric_results[\"similarity\"],\n                metric_results[\"fraction\"],\n                metric_results[\"latency\"]\n            )\n\n        except Exception as e:\n            logging.exception(f\"Error calculating reputation. Type: {type(e).__name__}\")\n            return 0, 0, 0, 0\n\n    def _get_default_metric_values(self) -&gt; tuple:\n        \"\"\"Return default metric values when no metrics instance is found.\"\"\"\n        return (0, 0, 0, 0)\n\n    def _process_num_messages_metric(self, metrics_instance, addr: str, nei: str, current_round: int, metrics_active) -&gt; dict:\n        \"\"\"Process the number of messages metric.\"\"\"\n        if not self._is_metric_enabled(\"num_messages\", metrics_active):\n            return {\"normalized\": 0, \"count\": 0, \"avg\": 0}\n\n        filtered_messages = [\n            msg for msg in metrics_instance.messages if msg.get(\"current_round\") == current_round\n        ]\n\n        for msg in filtered_messages:\n            self.messages_number_message.append({\n                \"number_message\": msg.get(\"time\"),\n                \"current_round\": msg.get(\"current_round\"),\n                \"key\": (addr, nei),\n            })\n\n        normalized, count = self.manage_metric_number_message(\n            self.messages_number_message, addr, nei, current_round, True\n        )\n\n        avg = self.save_number_message_history(addr, nei, normalized, current_round)\n\n        if avg is None and current_round &gt; self.HISTORY_ROUNDS_LOOKBACK:\n            avg = self.number_message_history[(addr, nei)][current_round - 1][\"avg_number_message\"]\n\n        return {\"normalized\": normalized, \"count\": count, \"avg\": avg or 0}\n\n    def _process_fraction_parameters_metric(self, metrics_instance, addr: str, nei: str, current_round: int, metrics_active) -&gt; float:\n        \"\"\"Process the fraction of parameters changed metric.\"\"\"\n        if not self._is_metric_enabled(\"fraction_parameters_changed\", metrics_active):\n            return 0\n\n        score_fraction = 0\n        if metrics_instance.fraction_of_params_changed.get(\"current_round\") == current_round:\n            fraction_changed = metrics_instance.fraction_of_params_changed.get(\"fraction_changed\")\n            threshold = metrics_instance.fraction_of_params_changed.get(\"threshold\")\n            score_fraction = self.analyze_anomalies(addr, nei, current_round, fraction_changed, threshold)\n\n        if current_round &gt;= self.INITIAL_ROUND_FOR_FRACTION:\n            return self._calculate_fraction_score_assignment(addr, nei, current_round, score_fraction)\n        else:\n            return 0\n\n    def _calculate_fraction_score_assignment(self, addr: str, nei: str, current_round: int, score_fraction: float) -&gt; float:\n        \"\"\"Calculate the final fraction score assignment.\"\"\"\n        key_current = (addr, nei, current_round)\n\n        if score_fraction &gt; 0:\n            return self._calculate_positive_fraction_score(addr, nei, current_round, score_fraction, key_current)\n        else:\n            return self._calculate_zero_fraction_score(addr, nei, current_round, key_current)\n\n    def _calculate_positive_fraction_score(self, addr: str, nei: str, current_round: int, score_fraction: float, key_current: tuple) -&gt; float:\n        \"\"\"Calculate fraction score when current score is positive.\"\"\"\n        past_scores = []\n        for i in range(1, 5):\n            key_prev = (addr, nei, current_round - i)\n            score_prev = self.fraction_changed_history.get(key_prev, {}).get(\"finally_fraction_score\")\n            if score_prev is not None and score_prev &gt; 0:\n                past_scores.append(score_prev)\n\n        if past_scores:\n            avg_past = sum(past_scores) / len(past_scores)\n            fraction_score_asign = score_fraction * 0.2 + avg_past * 0.8\n        else:\n            fraction_score_asign = score_fraction\n\n        self.fraction_changed_history[key_current][\"finally_fraction_score\"] = fraction_score_asign\n        return fraction_score_asign\n\n    def _calculate_zero_fraction_score(self, addr: str, nei: str, current_round: int, key_current: tuple) -&gt; float:\n        \"\"\"Calculate fraction score when current score is zero.\"\"\"\n        key_prev = (addr, nei, current_round - 1)\n        prev_score = self.fraction_changed_history.get(key_prev, {}).get(\"finally_fraction_score\")\n\n        if prev_score is not None:\n            fraction_score_asign = prev_score * self.ZERO_VALUE_DECAY_FACTOR\n        else:\n            fraction_neighbors_scores = {\n                key: value.get(\"finally_fraction_score\")\n                for key, value in self.fraction_changed_history.items()\n                if value.get(\"finally_fraction_score\") is not None\n            }\n            fraction_score_asign = np.mean(list(fraction_neighbors_scores.values())) if fraction_neighbors_scores else 0\n\n        if key_current not in self.fraction_changed_history:\n            self.fraction_changed_history[key_current] = {}\n\n        self.fraction_changed_history[key_current][\"finally_fraction_score\"] = fraction_score_asign\n        return fraction_score_asign\n\n    def _process_model_arrival_latency_metric(self, metrics_instance, addr: str, nei: str, current_round: int, metrics_active) -&gt; float:\n        \"\"\"Process the model arrival latency metric.\"\"\"\n        if not self._is_metric_enabled(\"model_arrival_latency\", metrics_active):\n            return 0\n\n        latency_normalized = 0\n        if metrics_instance.model_arrival_latency.get(\"round_received\") == current_round:\n            round_num = metrics_instance.model_arrival_latency.get(\"round\")\n            latency = metrics_instance.model_arrival_latency.get(\"latency\")\n            latency_normalized = self.manage_model_arrival_latency(addr, nei, latency, current_round, round_num)\n\n        if latency_normalized &gt;= 0:\n            avg_latency = self.save_model_arrival_latency_history(nei, latency_normalized, current_round)\n            if avg_latency is None and current_round &gt; 1:\n                avg_latency = self.model_arrival_latency_history[(addr, nei)][current_round - 1][\"score\"]\n            return avg_latency or 0\n\n        return 0\n\n    def _process_model_similarity_metric(self, nei: str, current_round: int, metrics_active) -&gt; float:\n        \"\"\"Process the model similarity metric.\"\"\"\n        if current_round &gt;= 1 and self._is_metric_enabled(\"model_similarity\", metrics_active):\n            return self.calculate_similarity_from_metrics(nei, current_round)\n        return 0\n\n    def _log_metrics_graphics(self, metric_results: dict, addr: str, nei: str, current_round: int):\n        \"\"\"Log graphics for all calculated metrics.\"\"\"\n        self.create_graphics_to_metrics(\n            metric_results[\"messages\"][\"count\"],\n            metric_results[\"messages\"][\"avg\"],\n            metric_results[\"similarity\"],\n            metric_results[\"fraction\"],\n            metric_results[\"latency\"],\n            addr,\n            nei,\n            current_round,\n            self.engine.total_rounds,\n        )\n\n    def create_graphics_to_metrics(\n        self,\n        number_message_count: float,\n        number_message_norm: float,\n        similarity: float,\n        fraction: float,\n        model_arrival_latency: float,\n        addr: str,\n        nei: str,\n        current_round: int,\n        total_rounds: int,\n    ):\n        \"\"\"\n        Create and log graphics for reputation metrics.\n\n        Args:\n            number_message_count: Count of messages for logging\n            number_message_norm: Normalized message metric\n            similarity: Similarity metric value\n            fraction: Fraction of parameters changed metric\n            model_arrival_latency: Model arrival latency metric\n            addr: Address identifier\n            nei: Neighbor identifier\n            current_round: Current round number\n            total_rounds: Total number of rounds\n        \"\"\"\n        if current_round is None or current_round &gt;= total_rounds:\n            return\n\n        self.engine.trainer._logger.log_data(\n            {f\"R-Model_arrival_latency_reputation/{addr}\": {nei: model_arrival_latency}}, \n            step=current_round\n        )\n        self.engine.trainer._logger.log_data(\n            {f\"R-Count_messages_number_message_reputation/{addr}\": {nei: number_message_count}}, \n            step=current_round\n        )\n        self.engine.trainer._logger.log_data(\n            {f\"R-number_message_reputation/{addr}\": {nei: number_message_norm}}, \n            step=current_round\n        )\n        self.engine.trainer._logger.log_data(\n            {f\"R-Similarity_reputation/{addr}\": {nei: similarity}}, \n            step=current_round\n        )\n        self.engine.trainer._logger.log_data(\n            {f\"R-Fraction_reputation/{addr}\": {nei: fraction}}, \n            step=current_round\n        )\n\n    def analyze_anomalies(\n        self,\n        addr,\n        nei,\n        current_round,\n        fraction_changed,\n        threshold,\n    ):\n        \"\"\"\n        Analyze anomalies in the fraction of parameters changed.\n\n        Returns:\n            float: The fraction score between 0 and 1.\n        \"\"\"\n        try:\n            key = (addr, nei, current_round)\n            self._initialize_fraction_history_entry(key, fraction_changed, threshold)\n\n            if current_round == 0:\n                return self._handle_initial_round_anomalies(key, fraction_changed, threshold)\n            else:\n                return self._handle_subsequent_round_anomalies(key, addr, nei, current_round, fraction_changed, threshold)\n\n        except Exception:\n            logging.exception(\"Error analyzing anomalies\")\n            return -1\n\n    def _initialize_fraction_history_entry(self, key: tuple, fraction_changed: float, threshold: float):\n        \"\"\"Initialize fraction history entry if it doesn't exist.\"\"\"\n        if key not in self.fraction_changed_history:\n            self.fraction_changed_history[key] = {\n                \"fraction_changed\": fraction_changed or 0,\n                \"threshold\": threshold or 0,\n                \"fraction_score\": None,\n                \"fraction_anomaly\": False,\n                \"threshold_anomaly\": False,\n                \"mean_fraction\": None,\n                \"std_dev_fraction\": None,\n                \"mean_threshold\": None,\n                \"std_dev_threshold\": None,\n            }\n\n    def _handle_initial_round_anomalies(self, key: tuple, fraction_changed: float, threshold: float) -&gt; float:\n        \"\"\"Handle anomaly analysis for the initial round (round 0).\"\"\"\n        self.fraction_changed_history[key].update({\n            \"mean_fraction\": fraction_changed,\n            \"std_dev_fraction\": 0.0,\n            \"mean_threshold\": threshold,\n            \"std_dev_threshold\": 0.0,\n            \"fraction_score\": 1.0,\n        })\n        return 1.0\n\n    def _handle_subsequent_round_anomalies(\n        self, key: tuple, addr: str, nei: str, current_round: int, fraction_changed: float, threshold: float\n    ) -&gt; float:\n        \"\"\"Handle anomaly analysis for subsequent rounds.\"\"\"\n        prev_stats = self._find_previous_valid_stats(addr, nei, current_round)\n\n        if prev_stats is None:\n            logging.warning(f\"No valid previous stats found for {addr}, {nei}, round {current_round}\")\n            return 1.0\n\n        anomalies = self._detect_anomalies(fraction_changed, threshold, prev_stats)\n        values = self._calculate_anomaly_values(fraction_changed, threshold, prev_stats, anomalies)\n        fraction_score = self._calculate_combined_score(values)\n        self._update_fraction_statistics(key, fraction_changed, threshold, prev_stats, anomalies, fraction_score)\n\n        return max(fraction_score, 0)\n\n    def _find_previous_valid_stats(self, addr: str, nei: str, current_round: int) -&gt; dict:\n        \"\"\"Find the most recent valid statistics from previous rounds.\"\"\"\n        for i in range(1, current_round + 1):\n            candidate_key = (addr, nei, current_round - i)\n            candidate_data = self.fraction_changed_history.get(candidate_key, {})\n\n            required_keys = [\"mean_fraction\", \"std_dev_fraction\", \"mean_threshold\", \"std_dev_threshold\"]\n            if all(candidate_data.get(k) is not None for k in required_keys):\n                return candidate_data\n\n        return None\n\n    def _detect_anomalies(self, current_fraction: float, current_threshold: float, prev_stats: dict) -&gt; dict:\n        \"\"\"Detect if current values are anomalous compared to previous statistics.\"\"\"\n        upper_mean_fraction = (prev_stats[\"mean_fraction\"] + prev_stats[\"std_dev_fraction\"]) * self.FRACTION_ANOMALY_MULTIPLIER\n        upper_mean_threshold = (prev_stats[\"mean_threshold\"] + prev_stats[\"std_dev_threshold\"]) * self.THRESHOLD_ANOMALY_MULTIPLIER\n\n        return {\n            \"fraction_anomaly\": current_fraction &gt; upper_mean_fraction,\n            \"threshold_anomaly\": current_threshold &gt; upper_mean_threshold,\n            \"upper_mean_fraction\": upper_mean_fraction,\n            \"upper_mean_threshold\": upper_mean_threshold,\n        }\n\n    def _calculate_anomaly_values(\n        self, current_fraction: float, current_threshold: float, prev_stats: dict, anomalies: dict\n    ) -&gt; dict:\n        \"\"\"Calculate penalty values for fraction and threshold anomalies.\"\"\"\n        fraction_value = 1.0\n        threshold_value = 1.0\n\n        if anomalies[\"fraction_anomaly\"]:\n            mean_fraction_prev = prev_stats[\"mean_fraction\"]\n            if mean_fraction_prev &gt; 0:\n                penalization_factor = abs(current_fraction - mean_fraction_prev) / mean_fraction_prev\n                fraction_value = 1 - (1 / (1 + np.exp(-penalization_factor)))\n\n        if anomalies[\"threshold_anomaly\"]:\n            mean_threshold_prev = prev_stats[\"mean_threshold\"]\n            if mean_threshold_prev &gt; 0:\n                penalization_factor = abs(current_threshold - mean_threshold_prev) / mean_threshold_prev\n                threshold_value = 1 - (1 / (1 + np.exp(-penalization_factor)))\n\n        return {\n            \"fraction_value\": fraction_value,\n            \"threshold_value\": threshold_value,\n        }\n\n    def _calculate_combined_score(self, values: dict) -&gt; float:\n        \"\"\"Calculate the combined fraction score from individual values.\"\"\"\n        fraction_weight = 0.5\n        threshold_weight = 0.5\n        return fraction_weight * values[\"fraction_value\"] + threshold_weight * values[\"threshold_value\"]\n\n    def _update_fraction_statistics(\n        self, key: tuple, current_fraction: float, current_threshold: float, \n        prev_stats: dict, anomalies: dict, fraction_score: float\n    ):\n        \"\"\"Update the fraction statistics for the current round.\"\"\"\n        self.fraction_changed_history[key][\"fraction_anomaly\"] = anomalies[\"fraction_anomaly\"]\n        self.fraction_changed_history[key][\"threshold_anomaly\"] = anomalies[\"threshold_anomaly\"]\n\n        self.fraction_changed_history[key][\"mean_fraction\"] = (current_fraction + prev_stats[\"mean_fraction\"]) / 2\n        self.fraction_changed_history[key][\"mean_threshold\"] = (current_threshold + prev_stats[\"mean_threshold\"]) / 2\n\n        fraction_variance = ((current_fraction - prev_stats[\"mean_fraction\"]) ** 2 + prev_stats[\"std_dev_fraction\"] ** 2) / 2\n        threshold_variance = ((self.THRESHOLD_VARIANCE_MULTIPLIER * (current_threshold - prev_stats[\"mean_threshold\"]) ** 2) + prev_stats[\"std_dev_threshold\"] ** 2) / 2\n\n        self.fraction_changed_history[key][\"std_dev_fraction\"] = np.sqrt(fraction_variance)\n        self.fraction_changed_history[key][\"std_dev_threshold\"] = np.sqrt(threshold_variance)\n        self.fraction_changed_history[key][\"fraction_score\"] = fraction_score\n\n    def manage_model_arrival_latency(self, addr, nei, latency, current_round, round_num):\n        \"\"\"\n        Manage the model_arrival_latency metric using latency.\n\n        Args:\n            addr (str): Source IP address.\n            nei (str): Destination IP address.\n            latency (float): Latency value for the current model_arrival_latency.\n            current_round (int): The current round of the program.\n            round_num (int): The round number of the model_arrival_latency.\n\n        Returns:\n            float: Normalized score between 0 and 1 for model_arrival_latency.\n        \"\"\"\n        try:\n            current_key = nei\n\n            self._initialize_latency_round_entry(current_round, current_key, latency)\n\n            if current_round &gt;= 1:\n                score = self._calculate_latency_score(current_round, current_key, latency)\n                self._update_latency_entry_with_score(current_round, current_key, score)\n            else:\n                score = 0\n\n            return score\n\n        except Exception as e:\n            logging.exception(f\"Error managing model_arrival_latency: {e}\")\n            return 0\n\n    def _initialize_latency_round_entry(self, current_round: int, current_key: str, latency: float):\n        \"\"\"Initialize latency entry for the current round.\"\"\"\n        if current_round not in self.model_arrival_latency_history:\n            self.model_arrival_latency_history[current_round] = {}\n\n        self.model_arrival_latency_history[current_round][current_key] = {\n            \"latency\": latency,\n            \"score\": 0.0,\n        }\n\n    def _calculate_latency_score(self, current_round: int, current_key: str, latency: float) -&gt; float:\n        \"\"\"Calculate the latency score based on historical data.\"\"\"\n        target_round = self._get_target_round_for_latency(current_round)\n        all_latencies = self._get_all_latencies_for_round(target_round)\n\n        if not all_latencies:\n            return 0.0\n\n        mean_latency = np.mean(all_latencies)\n        augment_mean = mean_latency * self.LATENCY_AUGMENT_FACTOR\n\n        if latency is None:\n            logging.info(f\"latency is None in round {current_round} for nei {current_key}\")\n            return -0.5\n\n        if latency &lt;= augment_mean:\n            return 1.0\n        else:\n            return 1 / (1 + np.exp(abs(latency - mean_latency) / mean_latency)) if mean_latency != 0 else 0.0\n\n    def _get_target_round_for_latency(self, current_round: int) -&gt; int:\n        \"\"\"Get the target round for latency calculation.\"\"\"\n        target_round = current_round - 1\n        return target_round if target_round in self.model_arrival_latency_history else current_round\n\n    def _get_all_latencies_for_round(self, target_round: int) -&gt; list:\n        \"\"\"Get all valid latencies for the target round.\"\"\"\n        return [\n            data[\"latency\"]\n            for data in self.model_arrival_latency_history.get(target_round, {}).values()\n            if data.get(\"latency\") not in (None, 0.0)\n        ]\n\n    def _update_latency_entry_with_score(self, current_round: int, current_key: str, score: float):\n        \"\"\"Update the latency entry with calculated score and mean.\"\"\"\n        target_round = self._get_target_round_for_latency(current_round)\n        all_latencies = self._get_all_latencies_for_round(target_round)\n        mean_latency = np.mean(all_latencies) if all_latencies else 0\n\n        self.model_arrival_latency_history[current_round][current_key].update({\n            \"mean_latency\": mean_latency,\n            \"score\": score,\n        })\n\n    def save_model_arrival_latency_history(self, nei, model_arrival_latency, round_num):\n        \"\"\"\n        Save the model_arrival_latency history of a participant (addr) regarding its neighbor (nei) in memory.\n        Use 3 rounds for the average.\n        Args:\n            nei (str): The neighboring node involved.\n            model_arrival_latency (float): The model_arrival_latency value to be saved.\n            round_num (int): The current round number.\n\n        Returns:\n            float: The smoothed average model_arrival_latency including the current round.\n        \"\"\"\n        try:\n            current_key = nei\n\n            self._initialize_latency_history_entry(round_num, current_key, model_arrival_latency)\n\n            if model_arrival_latency &gt; 0 and round_num &gt;= 1:\n                avg_model_arrival_latency = self._calculate_latency_weighted_average_positive(\n                    round_num, current_key, model_arrival_latency\n                )\n            elif model_arrival_latency == 0 and round_num &gt;= 1:\n                avg_model_arrival_latency = self._calculate_latency_weighted_average_zero(\n                    round_num, current_key\n                )\n            elif model_arrival_latency &lt; 0 and round_num &gt;= 1:\n                avg_model_arrival_latency = abs(model_arrival_latency) * self.NEGATIVE_LATENCY_PENALTY\n            else:\n                avg_model_arrival_latency = 0\n\n            self.model_arrival_latency_history[round_num][current_key][\"avg_model_arrival_latency\"] = (\n                avg_model_arrival_latency\n            )\n\n            return avg_model_arrival_latency\n\n        except Exception:\n            logging.exception(\"Error saving model_arrival_latency history\")\n\n    def _initialize_latency_history_entry(self, round_num: int, current_key: str, latency_value: float):\n        \"\"\"Initialize latency history entry for the given round and key.\"\"\"\n        if round_num not in self.model_arrival_latency_history:\n            self.model_arrival_latency_history[round_num] = {}\n\n        if current_key not in self.model_arrival_latency_history[round_num]:\n            self.model_arrival_latency_history[round_num][current_key] = {}\n\n        self.model_arrival_latency_history[round_num][current_key].update({\n            \"score\": latency_value,\n        })\n\n    def _calculate_latency_weighted_average_positive(self, round_num: int, current_key: str, current_value: float) -&gt; float:\n        \"\"\"Calculate weighted average for positive latency values.\"\"\"\n        past_values = []\n        for r in range(round_num - 3, round_num):\n            val = (\n                self.model_arrival_latency_history.get(r, {})\n                .get(current_key, {})\n                .get(\"avg_model_arrival_latency\", None)\n            )\n            if val is not None and val != 0:\n                past_values.append(val)\n\n        if past_values:\n            avg_past = sum(past_values) / len(past_values)\n            return current_value * self.CURRENT_VALUE_WEIGHT_LOW + avg_past * self.PAST_VALUE_WEIGHT_HIGH\n        else:\n            return current_value\n\n    def _calculate_latency_weighted_average_zero(self, round_num: int, current_key: str) -&gt; float:\n        \"\"\"Calculate weighted average when current latency value is zero.\"\"\"\n        previous_avg = (\n            self.model_arrival_latency_history.get(round_num - 1, {})\n            .get(current_key, {})\n            .get(\"avg_model_arrival_latency\", None)\n        )\n        return previous_avg * self.ZERO_VALUE_DECAY_FACTOR if previous_avg is not None else 0\n\n    def manage_metric_number_message(\n        self, messages_number_message: list, addr: str, nei: str, current_round: int, metric_active: bool = True\n    ) -&gt; tuple[float, int]:\n        \"\"\"\n        Manage the number of messages metric for a specific neighbor.\n\n        Args:\n            messages_number_message: List of message data\n            addr: Source address\n            nei: Neighbor address\n            current_round: Current round number\n            metric_active: Whether the metric is active\n\n        Returns:\n            Tuple of (normalized_messages, messages_count)\n        \"\"\"\n        try:\n            if current_round == 0 or not metric_active:\n                return 0.0, 0\n\n            messages_count = self._count_relevant_messages(messages_number_message, addr, nei, current_round)\n            neighbor_stats = self._calculate_neighbor_statistics(messages_number_message, current_round)\n\n            normalized_messages = self._calculate_normalized_messages(messages_count, neighbor_stats)\n\n            normalized_messages = self._apply_historical_penalty(\n                normalized_messages, addr, nei, current_round\n            )\n\n            self._store_message_history(addr, nei, current_round, normalized_messages)\n            normalized_messages = max(0.001, normalized_messages)\n\n            return normalized_messages, messages_count\n\n        except Exception:\n            logging.exception(\"Error managing metric number_message\")\n            return 0.0, 0\n\n    def _count_relevant_messages(self, messages: list, addr: str, nei: str, current_round: int) -&gt; int:\n        \"\"\"Count messages relevant to the current address-neighbor pair and round.\"\"\"\n        current_addr_nei = (addr, nei)\n        relevant_messages = [\n            msg for msg in messages\n            if msg[\"key\"] == current_addr_nei and msg[\"current_round\"] == current_round\n        ]\n        return len(relevant_messages)\n\n    def _calculate_neighbor_statistics(self, messages: list, current_round: int) -&gt; dict:\n        \"\"\"Calculate statistical metrics for all neighbors in the previous round.\"\"\"\n        previous_round = current_round - 1\n        all_messages_previous_round = [\n            m for m in messages if m.get(\"current_round\") == previous_round\n        ]\n\n        neighbor_counts = {}\n        for m in all_messages_previous_round:\n            key = m.get(\"key\")\n            neighbor_counts[key] = neighbor_counts.get(key, 0) + 1\n\n        counts_all_neighbors = list(neighbor_counts.values())\n\n        if not counts_all_neighbors:\n            return {\n                \"percentile_reference\": 0,\n                \"std_dev\": 0,\n                \"mean_messages\": 0,\n                \"augment_mean\": 0,\n            }\n\n        mean_messages = np.mean(counts_all_neighbors)\n\n        return {\n            \"percentile_reference\": np.percentile(counts_all_neighbors, 25),\n            \"std_dev\": np.std(counts_all_neighbors),\n            \"mean_messages\": mean_messages,\n            \"augment_mean\": mean_messages * self.MESSAGE_AUGMENT_FACTOR_EARLY if current_round &lt;= self.INITIAL_ROUND_FOR_REPUTATION else mean_messages * self.MESSAGE_AUGMENT_FACTOR_NORMAL,\n        }\n\n    def _calculate_normalized_messages(self, messages_count: int, neighbor_stats: dict) -&gt; float:\n        \"\"\"Calculate normalized message score with relative and extra penalties.\"\"\"\n        normalized_messages = 1.0\n        penalties_applied = []\n\n        relative_increase = self._calculate_relative_increase(messages_count, neighbor_stats[\"percentile_reference\"])\n        dynamic_margin = self._calculate_dynamic_margin(neighbor_stats)\n\n        if relative_increase &gt; dynamic_margin:\n            penalty_ratio = self._calculate_penalty_ratio(relative_increase, dynamic_margin)\n            normalized_messages *= np.exp(-(penalty_ratio**2))\n            penalties_applied.append(f\"relative_penalty({penalty_ratio:.3f})\")\n\n        if self._should_apply_extra_penalty(messages_count, neighbor_stats):\n            extra_penalty_factor = self._calculate_extra_penalty_factor(messages_count, neighbor_stats)\n            normalized_messages *= np.exp(-((extra_penalty_factor) ** 2))\n            penalties_applied.append(f\"extra_penalty({extra_penalty_factor:.3f})\")\n\n        if penalties_applied:\n            logging.debug(f\"Message penalties applied: {', '.join(penalties_applied)} -&gt; score: {normalized_messages:.4f}\")\n\n        return normalized_messages\n\n    def _calculate_relative_increase(self, messages_count: int, percentile_reference: float) -&gt; float:\n        \"\"\"Calculate the relative increase compared to percentile reference.\"\"\"\n        if percentile_reference &gt; 0:\n            raw_relative_increase = (messages_count - percentile_reference) / percentile_reference\n            return np.log1p(raw_relative_increase)\n        return 0.0\n\n    def _calculate_dynamic_margin(self, neighbor_stats: dict) -&gt; float:\n        \"\"\"Calculate dynamic margin for penalty application.\"\"\"\n        std_dev = neighbor_stats[\"std_dev\"]\n        percentile_reference = neighbor_stats[\"percentile_reference\"]\n        return (std_dev + 1) / (np.log1p(percentile_reference) + 1)\n\n    def _calculate_penalty_ratio(self, relative_increase: float, dynamic_margin: float) -&gt; float:\n        \"\"\"Calculate penalty ratio for relative increase penalty.\"\"\"\n        epsilon = 1e-6  # Small constant to avoid division by zero\n        return np.log1p(relative_increase - dynamic_margin) / (np.log1p(dynamic_margin + epsilon) + epsilon)\n\n    def _should_apply_extra_penalty(self, messages_count: int, neighbor_stats: dict) -&gt; bool:\n        \"\"\"Determine if extra penalty should be applied.\"\"\"\n        return (neighbor_stats[\"mean_messages\"] &gt; 0 and \n                messages_count &gt; neighbor_stats[\"augment_mean\"])\n\n    def _calculate_extra_penalty_factor(self, messages_count: int, neighbor_stats: dict) -&gt; float:\n        \"\"\"Calculate the extra penalty factor.\"\"\"\n        epsilon = 1e-6\n        mean_messages = neighbor_stats[\"mean_messages\"]\n        augment_mean = neighbor_stats[\"augment_mean\"]\n\n        extra_penalty = (messages_count - mean_messages) / (mean_messages + epsilon)\n        amplification = 1 + (augment_mean / (mean_messages + epsilon))\n        return extra_penalty * amplification\n\n    def _apply_historical_penalty(self, normalized_messages: float, addr: str, nei: str, current_round: int) -&gt; float:\n        \"\"\"Apply historical penalty based on previous round's score.\"\"\"\n        if current_round &lt;= 1:\n            return normalized_messages\n\n        prev_data = (\n            self.number_message_history.get((addr, nei), {})\n            .get(current_round - 1, {})\n        )\n\n        prev_score = prev_data.get(\"normalized_messages\")\n        was_previously_penalized = prev_data.get(\"was_penalized\", False)\n\n        if prev_score is not None and prev_score &lt; self.HISTORICAL_PENALTY_THRESHOLD:\n            original_score = normalized_messages\n\n            if was_previously_penalized:\n                penalty_factor = self.HISTORICAL_PENALTY_THRESHOLD * 0.8\n                logging.debug(f\"Repeated penalty applied to {nei}: stricter historical penalty\")\n            else:\n                penalty_factor = self.HISTORICAL_PENALTY_THRESHOLD\n\n            normalized_messages *= penalty_factor\n            logging.debug(f\"Historical penalty applied to {nei}: {original_score:.4f} -&gt; {normalized_messages:.4f} (prev_score: {prev_score:.4f}, was_penalized: {was_previously_penalized})\")\n\n        return normalized_messages\n\n    def _store_message_history(self, addr: str, nei: str, current_round: int, normalized_messages: float):\n        \"\"\"Store the normalized messages in history.\"\"\"\n        key = (addr, nei)\n        if key not in self.number_message_history:\n            self.number_message_history[key] = {}\n\n        was_penalized = normalized_messages &lt; 1.0\n\n        self.number_message_history[key][current_round] = {\n            \"normalized_messages\": normalized_messages,\n            \"was_penalized\": was_penalized,\n            \"penalty_severity\": 1.0 - normalized_messages if was_penalized else 0.0\n        }\n\n    def save_number_message_history(self, addr, nei, messages_number_message_normalized, current_round):\n        \"\"\"\n        Save the number_message history of a participant (addr) regarding its neighbor (nei) in memory.\n        Uses a weighted average of the past 3 rounds to smooth the result.\n\n        Returns:\n            float: The weighted average including the current round.\n        \"\"\"\n        try:\n            key = (addr, nei)\n\n            self._initialize_message_history_entry(key, current_round, messages_number_message_normalized)\n\n            if messages_number_message_normalized &gt; 0 and current_round &gt;= 1:\n                avg_number_message = self._calculate_weighted_average_positive(key, current_round, messages_number_message_normalized)\n            elif messages_number_message_normalized == 0 and current_round &gt;= 1:\n                avg_number_message = self._calculate_weighted_average_zero(key, current_round)\n            elif messages_number_message_normalized &lt; 0 and current_round &gt;= 1:\n                avg_number_message = abs(messages_number_message_normalized) * self.NEGATIVE_LATENCY_PENALTY\n            else:\n                avg_number_message = 0\n\n            self.number_message_history[key][current_round][\"avg_number_message\"] = avg_number_message\n            return avg_number_message\n\n        except Exception:\n            logging.exception(\"Error saving number_message history\")\n            return -1\n\n    def _initialize_message_history_entry(self, key: tuple, current_round: int, messages_normalized: float):\n        \"\"\"Initialize message history entry for the given key and round.\"\"\"\n        if key not in self.number_message_history:\n            self.number_message_history[key] = {}\n\n        if current_round not in self.number_message_history[key]:\n            self.number_message_history[key][current_round] = {}\n\n        self.number_message_history[key][current_round].update({\n            \"number_message\": messages_normalized,\n        })\n\n    def _calculate_weighted_average_positive(self, key: tuple, current_round: int, current_value: float) -&gt; float:\n        \"\"\"Calculate weighted average for positive message values.\"\"\"\n        past_values = []\n        for r in range(current_round - self.WEIGHTED_HISTORY_ROUNDS, current_round):\n            val = self.number_message_history.get(key, {}).get(r, {}).get(\"avg_number_message\", None)\n            if val is not None and val != 0:\n                past_values.append(val)\n\n        if past_values:\n            avg_past = sum(past_values) / len(past_values)\n            return current_value * self.CURRENT_VALUE_WEIGHT_HIGH + avg_past * self.PAST_VALUE_WEIGHT_LOW\n        else:\n            return current_value\n\n    def _calculate_weighted_average_zero(self, key: tuple, current_round: int) -&gt; float:\n        \"\"\"Calculate weighted average when current message value is zero.\"\"\"\n        previous_avg = (\n            self.number_message_history.get(key, {})\n            .get(current_round - 1, {})\n            .get(\"avg_number_message\", None)\n        )\n        return previous_avg * self.ZERO_VALUE_DECAY_FACTOR if previous_avg is not None else 0\n\n    async def save_reputation_history_in_memory(self, addr: str, nei: str, reputation: float) -&gt; float:\n        \"\"\"\n        Save reputation history and calculate weighted average.\n\n        Args:\n            addr: The node's identifier\n            nei: The neighboring node identifier  \n            reputation: The reputation value to save\n\n        Returns:\n            float: The weighted average reputation\n        \"\"\"\n        try:\n            key = (addr, nei)\n            current_round = await self._engine.get_round()\n\n            if key not in self.reputation_history:\n                self.reputation_history[key] = {}\n\n            self.reputation_history[key][current_round] = reputation\n\n            rounds = sorted(self.reputation_history[key].keys(), reverse=True)[:2]\n\n            if len(rounds) &gt;= 2:\n                current_rep = self.reputation_history[key][rounds[0]]\n                previous_rep = self.reputation_history[key][rounds[1]]\n\n                current_weight = self.REPUTATION_CURRENT_WEIGHT\n                previous_weight = self.REPUTATION_FEEDBACK_WEIGHT\n                avg_reputation = (current_rep * current_weight) + (previous_rep * previous_weight)\n\n                logging.info(f\"Current reputation: {current_rep}, Previous reputation: {previous_rep}\")\n                logging.info(f\"Reputation ponderated: {avg_reputation}\")\n            else:\n                avg_reputation = reputation\n\n            return avg_reputation\n\n        except Exception:\n            logging.exception(\"Error saving reputation history\")\n            return -1\n\n    def calculate_similarity_from_metrics(self, nei: str, current_round: int) -&gt; float:\n        \"\"\"\n        Calculate the similarity value from stored similarity metrics.\n\n        Args:\n            nei: The neighbor identifier\n            current_round: The current round number\n\n        Returns:\n            float: The computed similarity value (0.0 if no metrics found)\n        \"\"\"\n        try:\n            metrics_instance = self.connection_metrics.get(nei)\n            if not metrics_instance:\n                return 0.0\n\n            relevant_metrics = [\n                metric for metric in metrics_instance.similarity \n                if metric.get(\"nei\") == nei and metric.get(\"current_round\") == current_round\n            ]\n\n            if not relevant_metrics:\n                relevant_metrics = [\n                    metric for metric in metrics_instance.similarity \n                    if metric.get(\"nei\") == nei\n                ]\n\n            if not relevant_metrics:\n                return 0.0\n            neighbor_metric = relevant_metrics[-1]\n\n            similarity_weights = {\n                \"cosine\": 0.25,\n                \"euclidean\": 0.25, \n                \"manhattan\": 0.25,\n                \"pearson_correlation\": 0.25,\n            }\n\n            similarity_value = sum(\n                similarity_weights[metric_name] * float(neighbor_metric.get(metric_name, 0))\n                for metric_name in similarity_weights\n            )\n\n            return max(0.0, min(1.0, similarity_value))\n\n        except Exception:\n            return 0.0\n\n    async def calculate_reputation(self, ae: AggregationEvent):\n        \"\"\"\n        Calculate the reputation of the node based on the active metrics.\n\n        Args:\n            ae (AggregationEvent): The aggregation event.\n        \"\"\"\n        if not self._enabled:\n            return\n\n        (updates, _, _) = await ae.get_event_data()\n        await self._log_reputation_calculation_start()\n\n        neighbors = set(await self._engine._cm.get_addrs_current_connections(only_direct=True))\n\n        await self._process_neighbor_metrics(neighbors)\n        await self._calculate_reputation_by_factor(neighbors)\n        await self._handle_initial_reputation()\n        await self._process_feedback()\n        await self._finalize_reputation_calculation(updates, neighbors)\n\n    async def _log_reputation_calculation_start(self):\n        \"\"\"Log the start of reputation calculation with relevant information.\"\"\"\n        current_round = await self._engine.get_round()\n        logging.info(f\"Calculating reputation at round {current_round}\")\n        logging.info(f\"Active metrics: {self._metrics}\")\n        logging.info(f\"rejected nodes at round {current_round}: {self.rejected_nodes}\")\n        self.rejected_nodes.clear()\n        logging.info(f\"Rejected nodes clear: {self.rejected_nodes}\")\n\n    async def _process_neighbor_metrics(self, neighbors):\n        \"\"\"Process metrics for each neighbor.\"\"\"\n        for nei in neighbors:\n            metrics = await self.calculate_value_metrics(\n                self._addr, nei, metrics_active=self._metrics\n            )\n\n            if self._weighting_factor == \"dynamic\":\n                await self._process_dynamic_metrics(nei, metrics)\n            elif self._weighting_factor == \"static\" and await self._engine.get_round() &gt;= 1:\n                await self._process_static_metrics(nei, metrics)\n\n    async def _process_dynamic_metrics(self, nei, metrics):\n        \"\"\"Process metrics for dynamic weighting factor.\"\"\"\n        (metric_messages_number, metric_similarity, metric_fraction, metric_model_arrival_latency) = metrics\n\n        self.calculate_weighted_values(\n            metric_messages_number,\n            metric_similarity,\n            metric_fraction,\n            metric_model_arrival_latency,\n            self.history_data,\n            await self._engine.get_round(),\n            self._addr,\n            nei,\n            self._metrics,\n        )\n\n    async def _process_static_metrics(self, nei, metrics):\n        \"\"\"Process metrics for static weighting factor.\"\"\"\n        (metric_messages_number, metric_similarity, metric_fraction, metric_model_arrival_latency) = metrics\n\n        metric_values_dict = {\n            \"num_messages\": metric_messages_number,\n            \"model_similarity\": metric_similarity,\n            \"fraction_parameters_changed\": metric_fraction,\n            \"model_arrival_latency\": metric_model_arrival_latency,\n        }\n        await self._calculate_static_reputation(self._addr, nei, metric_values_dict)\n\n    async def _calculate_reputation_by_factor(self, neighbors):\n        \"\"\"Calculate reputation based on the weighting factor.\"\"\"\n        if self._weighting_factor == \"dynamic\" and await self._engine.get_round() &gt;= 1:\n            await self._calculate_dynamic_reputation(self._addr, neighbors)\n\n    async def _handle_initial_reputation(self):\n        \"\"\"Handle reputation initialization for the first round.\"\"\"\n        if await self._engine.get_round() &lt; 1 and self._enabled:\n            federation = self._engine.config.participant[\"network_args\"][\"neighbors\"].split()\n            await self.init_reputation(\n                federation_nodes=federation,\n                round_num=await self._engine.get_round(),\n                last_feedback_round=-1,\n                init_reputation=self._initial_reputation,\n            )\n\n    async def _process_feedback(self):\n        \"\"\"Process and include feedback in reputation.\"\"\"\n        status = await self.include_feedback_in_reputation()\n        current_round = await self._engine.get_round()\n\n        if status:\n            logging.info(f\"Feedback included in reputation at round {current_round}\")\n        else:\n            logging.info(f\"Feedback not included in reputation at round {current_round}\")\n\n    async def _finalize_reputation_calculation(self, updates, neighbors):\n        \"\"\"Finalize reputation calculation by creating graphics and sending data.\"\"\"\n        if self.reputation is not None:\n            self.create_graphic_reputation(self._addr, await self._engine.get_round())\n            await self.update_process_aggregation(updates)\n            await self.send_reputation_to_neighbors(neighbors)\n\n    async def send_reputation_to_neighbors(self, neighbors):\n        \"\"\"\n        Send the calculated reputation to the neighbors.\n        \"\"\"\n        for nei, data in self.reputation.items():\n            if data[\"reputation\"] is not None:\n                neighbors_to_send = [neighbor for neighbor in neighbors if neighbor != nei]\n\n                for neighbor in neighbors_to_send:\n                    message = self._engine.cm.create_message(\n                        \"reputation\",\n                        \"share\",\n                        node_id=nei,\n                        score=float(data[\"reputation\"]),\n                        round=await self._engine.get_round(),\n                    )\n                    await self._engine.cm.send_message(neighbor, message)\n                    logging.info(\n                        f\"Sending reputation to node {nei} from node {neighbor} with reputation {data['reputation']}\"\n                    )\n\n    def create_graphic_reputation(self, addr: str, round_num: int):\n        \"\"\"\n        Log reputation data for visualization.\n\n        Args:\n            addr: The node address\n            round_num: The round number for logging step\n        \"\"\"\n        try:\n            valid_reputations = {\n                node_id: float(data[\"reputation\"])\n                for node_id, data in self.reputation.items()\n                if data.get(\"reputation\") is not None\n            }\n\n            if valid_reputations:\n                reputation_data = {f\"Reputation/{addr}\": valid_reputations}\n                self._engine.trainer._logger.log_data(reputation_data, step=round_num)\n\n        except Exception:\n            logging.exception(\"Error creating reputation graphic\")\n\n    async def update_process_aggregation(self, updates):\n        \"\"\"\n        Update the process of aggregation by removing rejected nodes from the updates and\n        scaling the weights of the models based on their reputation.\n        \"\"\"\n        for rn in self.rejected_nodes:\n            if rn in updates:\n                updates.pop(rn)\n\n        if await self.engine.get_round() &gt;= 1:\n            for nei in list(updates.keys()):\n                if nei in self.reputation:\n                    rep = self.reputation[nei].get(\"reputation\", 0)\n                    if rep &gt;= self.REPUTATION_SCALING_THRESHOLD:\n                        weight = (rep - self.REPUTATION_SCALING_THRESHOLD) / self.REPUTATION_SCALING_RANGE\n                        model_dict = updates[nei][0]\n                        extra_data = updates[nei][1]\n\n                        scaled_model = {k: v * weight for k, v in model_dict.items()}\n                        updates[nei] = (scaled_model, extra_data)\n\n                        logging.info(f\"\u2705 Nei {nei} with reputation {rep:.4f}, scaled model with weight {weight:.4f}\")\n                    else:\n                        logging.info(f\"\u26d4 Nei {nei} with reputation {rep:.4f}, model rejected\")\n\n        logging.info(f\"Updates after rejected nodes: {list(updates.keys())}\")\n        logging.info(f\"Nodes rejected: {self.rejected_nodes}\")\n\n    async def include_feedback_in_reputation(self):\n        \"\"\"\n        Include feedback of neighbors in the reputation.\n        \"\"\"\n        weight_current_reputation = self.REPUTATION_CURRENT_WEIGHT\n        weight_feedback = self.REPUTATION_FEEDBACK_WEIGHT\n\n        if self.reputation_with_all_feedback is None:\n            logging.info(\"No feedback received.\")\n            return False\n\n        updated = False\n\n        for (current_node, node_ip, round_num), scores in self.reputation_with_all_feedback.items():\n            if not scores:\n                logging.info(f\"No feedback received for node {node_ip} in round {round_num}\")\n                continue\n\n            if node_ip not in self.reputation:\n                logging.info(f\"No reputation for node {node_ip}\")\n                continue\n\n            if (\n                \"last_feedback_round\" in self.reputation[node_ip]\n                and self.reputation[node_ip][\"last_feedback_round\"] &gt;= round_num\n            ):\n                continue\n\n            avg_feedback = sum(scores) / len(scores)\n            logging.info(f\"Receive feedback to node {node_ip} with average score {avg_feedback}\")\n\n            current_reputation = self.reputation[node_ip][\"reputation\"]\n            if current_reputation is None:\n                logging.info(f\"No reputation calculate for node {node_ip}.\")\n                continue\n\n            combined_reputation = (current_reputation * weight_current_reputation) + (avg_feedback * weight_feedback)\n            logging.info(f\"Combined reputation for node {node_ip} in round {round_num}: {combined_reputation}\")\n\n            self.reputation[node_ip] = {\n                \"reputation\": combined_reputation,\n                \"round\": await self._engine.get_round(),\n                \"last_feedback_round\": round_num,\n            }\n            updated = True\n            logging.info(f\"Updated self.reputation for {node_ip}: {self.reputation[node_ip]}\")\n\n        if updated:\n            return True\n        else:\n            return False\n\n    async def on_round_start(self, rse: RoundStartEvent):\n        \"\"\"\n        Handle the start of a new round and initialize the round timing information.\n        \"\"\"\n        (round_id, start_time, expected_nodes) = await rse.get_event_data()\n        if round_id not in self.round_timing_info:\n            self.round_timing_info[round_id] = {}\n        self.round_timing_info[round_id][\"start_time\"] = start_time\n        expected_nodes.difference_update(self.rejected_nodes)\n        expected_nodes = list(expected_nodes)\n        self._recalculate_pending_latencies(round_id)\n\n    async def recollect_model_arrival_latency(self, ure: UpdateReceivedEvent):\n        (decoded_model, weight, source, round_num, local) = await ure.get_event_data()\n        current_round = await self._engine.get_round()\n\n        self.round_timing_info.setdefault(round_num, {})\n\n        if round_num == current_round:\n            await self._process_current_round(round_num, source)\n        elif round_num &gt; current_round:\n            self.round_timing_info[round_num][\"pending_recalculation\"] = True\n            self.round_timing_info[round_num].setdefault(\"pending_sources\", set()).add(source)\n            logging.info(f\"Model from future round {round_num} stored, pending recalculation.\")\n        else:\n            await self._process_past_round(round_num, source)\n\n        self._recalculate_pending_latencies(current_round)\n\n    async def _process_current_round(self, round_num, source):\n        \"\"\"\n        Process models that arrive in the current round.\n        \"\"\"\n        if \"start_time\" in self.round_timing_info[round_num]:\n            current_time = time.time()\n            self.round_timing_info[round_num].setdefault(\"model_received_time\", {})\n            existing_time = self.round_timing_info[round_num][\"model_received_time\"].get(source)\n            if existing_time is None or current_time &lt; existing_time:\n                self.round_timing_info[round_num][\"model_received_time\"][source] = current_time\n\n            start_time = self.round_timing_info[round_num][\"start_time\"]\n            duration = current_time - start_time\n            self.round_timing_info[round_num][\"duration\"] = duration\n\n            logging.info(f\"Source {source}, round {round_num}, duration: {duration:.4f} seconds\")\n\n            self.save_data(\n                \"model_arrival_latency\",\n                source,\n                self._addr,\n                num_round=round_num,\n                current_round=await self._engine.get_round(),\n                latency=duration,\n            )\n        else:\n            logging.info(f\"Start time not yet available for round {round_num}.\")\n\n    async def _process_past_round(self, round_num, source):\n        \"\"\"\n        Process models that arrive in past rounds.\n        \"\"\"\n        logging.info(f\"Model from past round {round_num} received, storing for recalculation.\")\n        current_time = time.time()\n        self.round_timing_info.setdefault(round_num, {})\n        self.round_timing_info[round_num].setdefault(\"model_received_time\", {})\n        existing_time = self.round_timing_info[round_num][\"model_received_time\"].get(source)\n        if existing_time is None or current_time &lt; existing_time:\n            self.round_timing_info[round_num][\"model_received_time\"][source] = current_time\n\n        prev_start_time = self.round_timing_info.get(round_num, {}).get(\"start_time\")\n        if prev_start_time:\n            duration = current_time - prev_start_time\n            self.round_timing_info[round_num][\"duration\"] = duration\n\n            self.save_data(\n                \"model_arrival_latency\",\n                source,\n                self._addr,\n                num_round=round_num,\n                current_round=await self._engine.get_round(),\n                latency=duration,\n            )\n        else:\n            logging.info(f\"Start time for previous round {round_num - 1} not available yet.\")\n\n    def _recalculate_pending_latencies(self, current_round):\n        \"\"\"\n        Recalculate latencies for rounds that have pending recalculation.\n        \"\"\"\n        logging.info(\"Recalculating latencies for rounds with pending recalculation.\")\n        for r_num, r_data in self.round_timing_info.items():\n            new_time = time.time()\n            if r_data.get(\"pending_recalculation\"):\n                if \"start_time\" in r_data and \"model_received_time\" in r_data:\n                    r_data.setdefault(\"model_received_time\", {})\n\n                    for src in list(r_data[\"pending_sources\"]):\n                        existing_time = r_data[\"model_received_time\"].get(src)\n                        if existing_time is None or new_time &lt; existing_time:\n                            r_data[\"model_received_time\"][src] = new_time\n                        duration = new_time - r_data[\"start_time\"]\n                        r_data[\"duration\"] = duration\n\n                        logging.info(f\"[Recalc] Source {src}, round {r_num}, duration: {duration:.4f} s\")\n\n                        self.save_data(\n                            \"model_arrival_latency\",\n                            src,\n                            self._addr,\n                            num_round=r_num,\n                            current_round=current_round,\n                            latency=duration,\n                        )\n\n                    r_data[\"pending_sources\"].clear()\n                    r_data[\"pending_recalculation\"] = False\n\n    async def recollect_similarity(self, ure: UpdateReceivedEvent):\n        \"\"\"\n        Collect and analyze model similarity metrics.\n\n        Args:\n            ure: UpdateReceivedEvent containing model and metadata\n        \"\"\"\n        (decoded_model, weight, nei, round_num, local) = await ure.get_event_data()\n\n        if not (self._enabled and self._is_metric_enabled(\"model_similarity\")):\n            return\n\n        if not self._engine.config.participant[\"adaptive_args\"][\"model_similarity\"]:\n            return\n\n        if nei == self._addr:\n            return\n\n        logging.info(\"\ud83e\udd16  handle_model_message | Checking model similarity\")\n\n        local_model = self._engine.trainer.get_model_parameters()\n        similarity_values = self._calculate_all_similarity_metrics(local_model, decoded_model)\n\n        similarity_metrics = {\n            \"timestamp\": datetime.now(),\n            \"nei\": nei,\n            \"round\": round_num,\n            \"current_round\": await self._engine.get_round(),\n            **similarity_values\n        }\n\n        self._store_similarity_metrics(nei, similarity_metrics)\n        self._check_similarity_threshold(nei, similarity_values[\"cosine\"])\n\n    def _calculate_all_similarity_metrics(self, local_model: dict, received_model: dict) -&gt; dict:\n        \"\"\"Calculate all similarity metrics between two models.\"\"\"\n        if not local_model or not received_model:\n            return {\n                \"cosine\": 0.0,\n                \"euclidean\": 0.0,\n                \"manhattan\": 0.0,\n                \"pearson_correlation\": 0.0,\n                \"jaccard\": 0.0,\n                \"minkowski\": 0.0,\n            }\n\n        similarity_functions = [\n            (\"cosine\", cosine_metric),\n            (\"euclidean\", euclidean_metric),\n            (\"manhattan\", manhattan_metric),\n            (\"pearson_correlation\", pearson_correlation_metric),\n            (\"jaccard\", jaccard_metric),\n        ]\n\n        similarity_values = {}\n\n        for name, metric_func in similarity_functions:\n            try:\n                similarity_values[name] = metric_func(local_model, received_model, similarity=True)\n            except Exception:\n                similarity_values[name] = 0.0\n\n        try:\n            similarity_values[\"minkowski\"] = minkowski_metric(\n                local_model, received_model, p=2, similarity=True\n            )\n        except Exception:\n            similarity_values[\"minkowski\"] = 0.0\n\n        return similarity_values\n\n    def _store_similarity_metrics(self, nei: str, similarity_metrics: dict):\n        \"\"\"Store similarity metrics for the given neighbor.\"\"\"\n        if nei not in self.connection_metrics:\n            self.connection_metrics[nei] = Metrics()\n\n        self.connection_metrics[nei].similarity.append(similarity_metrics)\n\n    def _check_similarity_threshold(self, nei: str, cosine_value: float):\n        \"\"\"Check if cosine similarity is below threshold and mark node if necessary.\"\"\"\n        if cosine_value &lt; self.SIMILARITY_THRESHOLD:\n            logging.info(\"\ud83e\udd16  handle_model_message | Model similarity is less than threshold\")\n            self.rejected_nodes.add(nei)\n\n    async def recollect_number_message(self, source, message):\n        \"\"\"Record a number message from a source.\"\"\"\n        await self._record_message_data(source)\n\n    async def recollect_duplicated_number_message(self, dme: DuplicatedMessageEvent):\n        \"\"\"Record a duplicated message event.\"\"\"\n        event_data = await dme.get_event_data()\n        if isinstance(event_data, tuple):\n            source = event_data[0]\n        else:\n            source = event_data\n        await self._record_message_data(source)\n\n    async def _record_message_data(self, source: str):\n        \"\"\"Record message data for the given source if it's not the current address.\"\"\"\n        if source != self._addr:\n            current_time = time.time()\n            if current_time:\n                self.save_data(\n                    \"number_message\",\n                    source,\n                    self._addr,\n                    time=current_time,\n                    current_round=await self._engine.get_round(),\n                )\n\n    async def recollect_fraction_of_parameters_changed(self, ure: UpdateReceivedEvent):\n        \"\"\"\n        Collect and analyze the fraction of parameters that changed between models.\n\n        Args:\n            ure: UpdateReceivedEvent containing model and metadata\n        \"\"\"\n        (decoded_model, weight, source, round_num, local) = await ure.get_event_data()\n\n        current_round = await self._engine.get_round()\n        parameters_local = self._engine.trainer.get_model_parameters()\n\n        prev_threshold = self._get_previous_threshold(source, current_round)\n        differences = self._calculate_parameter_differences(parameters_local, decoded_model)\n        current_threshold = self._calculate_threshold(differences, prev_threshold)\n\n        changed_params, total_params, changes_record = self._count_changed_parameters(\n            parameters_local, decoded_model, current_threshold\n        )\n\n        fraction_changed = changed_params / total_params if total_params &gt; 0 else 0.0\n\n        self._store_fraction_data(source, current_round, {\n            \"fraction_changed\": fraction_changed,\n            \"total_params\": total_params,\n            \"changed_params\": changed_params,\n            \"threshold\": current_threshold,\n            \"changes_record\": changes_record,\n        })\n\n        self.save_data(\n            \"fraction_of_params_changed\",\n            source,\n            self._addr,\n            current_round=current_round,\n            fraction_changed=fraction_changed,\n            threshold=current_threshold,\n        )\n\n    def _get_previous_threshold(self, source: str, current_round: int) -&gt; float:\n        \"\"\"Get the threshold from the previous round for the given source.\"\"\"\n        if (source in self.fraction_of_params_changed and \n            current_round - 1 in self.fraction_of_params_changed[source]):\n            return self.fraction_of_params_changed[source][current_round - 1][-1][\"threshold\"]\n        return None\n\n    def _calculate_parameter_differences(self, local_params: dict, received_params: dict) -&gt; list:\n        \"\"\"Calculate absolute differences between local and received parameters.\"\"\"\n        differences = []\n        for key in local_params.keys():\n            if key in received_params:\n                local_tensor = local_params[key].cpu()\n                received_tensor = received_params[key].cpu()\n                diff = torch.abs(local_tensor - received_tensor)\n                differences.extend(diff.flatten().tolist())\n        return differences\n\n    def _calculate_threshold(self, differences: list, prev_threshold: float) -&gt; float:\n        \"\"\"Calculate the threshold for determining parameter changes.\"\"\"\n        if not differences:\n            return 0\n\n        mean_threshold = torch.mean(torch.tensor(differences)).item()\n        if prev_threshold is not None:\n            return (prev_threshold + mean_threshold) / 2\n        return mean_threshold\n\n    def _count_changed_parameters(self, local_params: dict, received_params: dict, threshold: float) -&gt; tuple:\n        \"\"\"Count the number of parameters that changed above the threshold.\"\"\"\n        total_params = 0\n        changed_params = 0\n        changes_record = {}\n\n        for key in local_params.keys():\n            if key in received_params:\n                local_tensor = local_params[key].cpu()\n                received_tensor = received_params[key].cpu()\n                diff = torch.abs(local_tensor - received_tensor)\n                total_params += diff.numel()\n\n                num_changed = torch.sum(diff &gt; threshold).item()\n                changed_params += num_changed\n\n                if num_changed &gt; 0:\n                    changes_record[key] = num_changed\n\n        return changed_params, total_params, changes_record\n\n    def _store_fraction_data(self, source: str, current_round: int, data: dict):\n        \"\"\"Store fraction data in the internal data structure.\"\"\"\n        if source not in self.fraction_of_params_changed:\n            self.fraction_of_params_changed[source] = {}\n        if current_round not in self.fraction_of_params_changed[source]:\n            self.fraction_of_params_changed[source][current_round] = []\n\n        self.fraction_of_params_changed[source][current_round].append(data)\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.__init__","title":"<code>__init__(engine, config)</code>","text":"<p>Initialize the Reputation system.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>The engine instance providing the runtime context.</p> required <code>config</code> <code>Config</code> <p>The configuration object with participant settings.</p> required Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>def __init__(self, engine: \"Engine\", config: \"Config\"):\n    \"\"\"\n    Initialize the Reputation system.\n\n    Args:\n        engine (Engine): The engine instance providing the runtime context.\n        config (Config): The configuration object with participant settings.\n    \"\"\"\n    self._engine = engine\n    self._config = config\n    self._addr = engine.addr\n    self._log_dir = engine.log_dir\n    self._idx = engine.idx\n\n    self._initialize_data_structures()\n    self._configure_constants()\n    self._load_configuration()\n    self._setup_connection_metrics()\n    self._configure_metric_weights()\n    self._log_initialization_info()\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.analyze_anomalies","title":"<code>analyze_anomalies(addr, nei, current_round, fraction_changed, threshold)</code>","text":"<p>Analyze anomalies in the fraction of parameters changed.</p> <p>Returns:</p> Name Type Description <code>float</code> <p>The fraction score between 0 and 1.</p> Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>def analyze_anomalies(\n    self,\n    addr,\n    nei,\n    current_round,\n    fraction_changed,\n    threshold,\n):\n    \"\"\"\n    Analyze anomalies in the fraction of parameters changed.\n\n    Returns:\n        float: The fraction score between 0 and 1.\n    \"\"\"\n    try:\n        key = (addr, nei, current_round)\n        self._initialize_fraction_history_entry(key, fraction_changed, threshold)\n\n        if current_round == 0:\n            return self._handle_initial_round_anomalies(key, fraction_changed, threshold)\n        else:\n            return self._handle_subsequent_round_anomalies(key, addr, nei, current_round, fraction_changed, threshold)\n\n    except Exception:\n        logging.exception(\"Error analyzing anomalies\")\n        return -1\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.calculate_reputation","title":"<code>calculate_reputation(ae)</code>  <code>async</code>","text":"<p>Calculate the reputation of the node based on the active metrics.</p> <p>Parameters:</p> Name Type Description Default <code>ae</code> <code>AggregationEvent</code> <p>The aggregation event.</p> required Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>async def calculate_reputation(self, ae: AggregationEvent):\n    \"\"\"\n    Calculate the reputation of the node based on the active metrics.\n\n    Args:\n        ae (AggregationEvent): The aggregation event.\n    \"\"\"\n    if not self._enabled:\n        return\n\n    (updates, _, _) = await ae.get_event_data()\n    await self._log_reputation_calculation_start()\n\n    neighbors = set(await self._engine._cm.get_addrs_current_connections(only_direct=True))\n\n    await self._process_neighbor_metrics(neighbors)\n    await self._calculate_reputation_by_factor(neighbors)\n    await self._handle_initial_reputation()\n    await self._process_feedback()\n    await self._finalize_reputation_calculation(updates, neighbors)\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.calculate_similarity_from_metrics","title":"<code>calculate_similarity_from_metrics(nei, current_round)</code>","text":"<p>Calculate the similarity value from stored similarity metrics.</p> <p>Parameters:</p> Name Type Description Default <code>nei</code> <code>str</code> <p>The neighbor identifier</p> required <code>current_round</code> <code>int</code> <p>The current round number</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The computed similarity value (0.0 if no metrics found)</p> Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>def calculate_similarity_from_metrics(self, nei: str, current_round: int) -&gt; float:\n    \"\"\"\n    Calculate the similarity value from stored similarity metrics.\n\n    Args:\n        nei: The neighbor identifier\n        current_round: The current round number\n\n    Returns:\n        float: The computed similarity value (0.0 if no metrics found)\n    \"\"\"\n    try:\n        metrics_instance = self.connection_metrics.get(nei)\n        if not metrics_instance:\n            return 0.0\n\n        relevant_metrics = [\n            metric for metric in metrics_instance.similarity \n            if metric.get(\"nei\") == nei and metric.get(\"current_round\") == current_round\n        ]\n\n        if not relevant_metrics:\n            relevant_metrics = [\n                metric for metric in metrics_instance.similarity \n                if metric.get(\"nei\") == nei\n            ]\n\n        if not relevant_metrics:\n            return 0.0\n        neighbor_metric = relevant_metrics[-1]\n\n        similarity_weights = {\n            \"cosine\": 0.25,\n            \"euclidean\": 0.25, \n            \"manhattan\": 0.25,\n            \"pearson_correlation\": 0.25,\n        }\n\n        similarity_value = sum(\n            similarity_weights[metric_name] * float(neighbor_metric.get(metric_name, 0))\n            for metric_name in similarity_weights\n        )\n\n        return max(0.0, min(1.0, similarity_value))\n\n    except Exception:\n        return 0.0\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.calculate_value_metrics","title":"<code>calculate_value_metrics(addr, nei, metrics_active=None)</code>  <code>async</code>","text":"<p>Calculate the reputation of each participant based on the data stored in self.connection_metrics.</p> <p>Parameters:</p> Name Type Description Default <code>addr</code> <code>str</code> <p>Source IP address.</p> required <code>nei</code> <code>str</code> <p>Destination IP address.</p> required <code>metrics_active</code> <code>dict</code> <p>The active metrics.</p> <code>None</code> Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>async def calculate_value_metrics(self, addr, nei, metrics_active=None):\n    \"\"\"\n    Calculate the reputation of each participant based on the data stored in self.connection_metrics.\n\n    Args:\n        addr (str): Source IP address.\n        nei (str): Destination IP address.\n        metrics_active (dict): The active metrics.\n    \"\"\"\n    try:\n        current_round = await self._engine.get_round()\n        metrics_instance = self.connection_metrics.get(nei)\n\n        if not metrics_instance:\n            logging.warning(f\"No metrics found for neighbor {nei}\")\n            return self._get_default_metric_values()\n\n        metric_results = {\n            \"messages\": self._process_num_messages_metric(metrics_instance, addr, nei, current_round, metrics_active),\n            \"fraction\": self._process_fraction_parameters_metric(metrics_instance, addr, nei, current_round, metrics_active),\n            \"latency\": self._process_model_arrival_latency_metric(metrics_instance, addr, nei, current_round, metrics_active),\n            \"similarity\": self._process_model_similarity_metric(nei, current_round, metrics_active)\n        }\n\n        self._log_metrics_graphics(metric_results, addr, nei, current_round)\n\n        return (\n            metric_results[\"messages\"][\"avg\"],\n            metric_results[\"similarity\"],\n            metric_results[\"fraction\"],\n            metric_results[\"latency\"]\n        )\n\n    except Exception as e:\n        logging.exception(f\"Error calculating reputation. Type: {type(e).__name__}\")\n        return 0, 0, 0, 0\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.calculate_weighted_values","title":"<code>calculate_weighted_values(avg_messages_number_message_normalized, similarity_reputation, fraction_score_asign, avg_model_arrival_latency, history_data, current_round, addr, nei, reputation_metrics)</code>","text":"<p>Calculate the weighted values for each metric.</p> Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>def calculate_weighted_values(\n    self,\n    avg_messages_number_message_normalized,\n    similarity_reputation,\n    fraction_score_asign,\n    avg_model_arrival_latency,\n    history_data,\n    current_round,\n    addr,\n    nei,\n    reputation_metrics,\n):\n    \"\"\"\n    Calculate the weighted values for each metric.\n    \"\"\"\n    if current_round is None:\n        return\n\n    self._ensure_history_data_structure(history_data)\n    active_metrics = self._get_active_metrics(\n        avg_messages_number_message_normalized,\n        similarity_reputation,\n        fraction_score_asign,\n        avg_model_arrival_latency,\n        reputation_metrics\n    )\n    self._add_current_metrics_to_history(active_metrics, history_data, current_round, addr, nei)\n\n    if current_round &gt;= self.INITIAL_ROUND_FOR_REPUTATION and len(active_metrics) &gt; 0:\n        adjusted_weights = self._calculate_dynamic_weights(active_metrics, history_data)\n    else:\n        adjusted_weights = self._calculate_uniform_weights(active_metrics)\n\n    self._update_history_with_weights(active_metrics, history_data, adjusted_weights, current_round, nei)\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.create_graphic_reputation","title":"<code>create_graphic_reputation(addr, round_num)</code>","text":"<p>Log reputation data for visualization.</p> <p>Parameters:</p> Name Type Description Default <code>addr</code> <code>str</code> <p>The node address</p> required <code>round_num</code> <code>int</code> <p>The round number for logging step</p> required Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>def create_graphic_reputation(self, addr: str, round_num: int):\n    \"\"\"\n    Log reputation data for visualization.\n\n    Args:\n        addr: The node address\n        round_num: The round number for logging step\n    \"\"\"\n    try:\n        valid_reputations = {\n            node_id: float(data[\"reputation\"])\n            for node_id, data in self.reputation.items()\n            if data.get(\"reputation\") is not None\n        }\n\n        if valid_reputations:\n            reputation_data = {f\"Reputation/{addr}\": valid_reputations}\n            self._engine.trainer._logger.log_data(reputation_data, step=round_num)\n\n    except Exception:\n        logging.exception(\"Error creating reputation graphic\")\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.create_graphics_to_metrics","title":"<code>create_graphics_to_metrics(number_message_count, number_message_norm, similarity, fraction, model_arrival_latency, addr, nei, current_round, total_rounds)</code>","text":"<p>Create and log graphics for reputation metrics.</p> <p>Parameters:</p> Name Type Description Default <code>number_message_count</code> <code>float</code> <p>Count of messages for logging</p> required <code>number_message_norm</code> <code>float</code> <p>Normalized message metric</p> required <code>similarity</code> <code>float</code> <p>Similarity metric value</p> required <code>fraction</code> <code>float</code> <p>Fraction of parameters changed metric</p> required <code>model_arrival_latency</code> <code>float</code> <p>Model arrival latency metric</p> required <code>addr</code> <code>str</code> <p>Address identifier</p> required <code>nei</code> <code>str</code> <p>Neighbor identifier</p> required <code>current_round</code> <code>int</code> <p>Current round number</p> required <code>total_rounds</code> <code>int</code> <p>Total number of rounds</p> required Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>def create_graphics_to_metrics(\n    self,\n    number_message_count: float,\n    number_message_norm: float,\n    similarity: float,\n    fraction: float,\n    model_arrival_latency: float,\n    addr: str,\n    nei: str,\n    current_round: int,\n    total_rounds: int,\n):\n    \"\"\"\n    Create and log graphics for reputation metrics.\n\n    Args:\n        number_message_count: Count of messages for logging\n        number_message_norm: Normalized message metric\n        similarity: Similarity metric value\n        fraction: Fraction of parameters changed metric\n        model_arrival_latency: Model arrival latency metric\n        addr: Address identifier\n        nei: Neighbor identifier\n        current_round: Current round number\n        total_rounds: Total number of rounds\n    \"\"\"\n    if current_round is None or current_round &gt;= total_rounds:\n        return\n\n    self.engine.trainer._logger.log_data(\n        {f\"R-Model_arrival_latency_reputation/{addr}\": {nei: model_arrival_latency}}, \n        step=current_round\n    )\n    self.engine.trainer._logger.log_data(\n        {f\"R-Count_messages_number_message_reputation/{addr}\": {nei: number_message_count}}, \n        step=current_round\n    )\n    self.engine.trainer._logger.log_data(\n        {f\"R-number_message_reputation/{addr}\": {nei: number_message_norm}}, \n        step=current_round\n    )\n    self.engine.trainer._logger.log_data(\n        {f\"R-Similarity_reputation/{addr}\": {nei: similarity}}, \n        step=current_round\n    )\n    self.engine.trainer._logger.log_data(\n        {f\"R-Fraction_reputation/{addr}\": {nei: fraction}}, \n        step=current_round\n    )\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.include_feedback_in_reputation","title":"<code>include_feedback_in_reputation()</code>  <code>async</code>","text":"<p>Include feedback of neighbors in the reputation.</p> Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>async def include_feedback_in_reputation(self):\n    \"\"\"\n    Include feedback of neighbors in the reputation.\n    \"\"\"\n    weight_current_reputation = self.REPUTATION_CURRENT_WEIGHT\n    weight_feedback = self.REPUTATION_FEEDBACK_WEIGHT\n\n    if self.reputation_with_all_feedback is None:\n        logging.info(\"No feedback received.\")\n        return False\n\n    updated = False\n\n    for (current_node, node_ip, round_num), scores in self.reputation_with_all_feedback.items():\n        if not scores:\n            logging.info(f\"No feedback received for node {node_ip} in round {round_num}\")\n            continue\n\n        if node_ip not in self.reputation:\n            logging.info(f\"No reputation for node {node_ip}\")\n            continue\n\n        if (\n            \"last_feedback_round\" in self.reputation[node_ip]\n            and self.reputation[node_ip][\"last_feedback_round\"] &gt;= round_num\n        ):\n            continue\n\n        avg_feedback = sum(scores) / len(scores)\n        logging.info(f\"Receive feedback to node {node_ip} with average score {avg_feedback}\")\n\n        current_reputation = self.reputation[node_ip][\"reputation\"]\n        if current_reputation is None:\n            logging.info(f\"No reputation calculate for node {node_ip}.\")\n            continue\n\n        combined_reputation = (current_reputation * weight_current_reputation) + (avg_feedback * weight_feedback)\n        logging.info(f\"Combined reputation for node {node_ip} in round {round_num}: {combined_reputation}\")\n\n        self.reputation[node_ip] = {\n            \"reputation\": combined_reputation,\n            \"round\": await self._engine.get_round(),\n            \"last_feedback_round\": round_num,\n        }\n        updated = True\n        logging.info(f\"Updated self.reputation for {node_ip}: {self.reputation[node_ip]}\")\n\n    if updated:\n        return True\n    else:\n        return False\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.init_reputation","title":"<code>init_reputation(federation_nodes=None, round_num=None, last_feedback_round=None, init_reputation=None)</code>  <code>async</code>","text":"<p>Initialize the reputation system.</p> <p>Parameters:</p> Name Type Description Default <code>federation_nodes</code> <p>List of federation node identifiers</p> <code>None</code> <code>round_num</code> <p>Current round number  </p> <code>None</code> <code>last_feedback_round</code> <p>Last round that received feedback</p> <code>None</code> <code>init_reputation</code> <p>Initial reputation value to assign</p> <code>None</code> Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>async def init_reputation(\n    self, federation_nodes=None, round_num=None, last_feedback_round=None, init_reputation=None\n):\n    \"\"\"\n    Initialize the reputation system.\n\n    Args:\n        federation_nodes: List of federation node identifiers\n        round_num: Current round number  \n        last_feedback_round: Last round that received feedback\n        init_reputation: Initial reputation value to assign\n    \"\"\"\n    if not self._enabled:\n        return\n\n    if not self._validate_init_parameters(federation_nodes, round_num, init_reputation):\n        return\n\n    neighbors = self._validate_federation_nodes(federation_nodes)\n    if not neighbors:\n        logging.error(\"init_reputation | No valid neighbors found\")\n        return\n\n    await self._initialize_neighbor_reputations(neighbors, round_num, last_feedback_round, init_reputation)\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.manage_metric_number_message","title":"<code>manage_metric_number_message(messages_number_message, addr, nei, current_round, metric_active=True)</code>","text":"<p>Manage the number of messages metric for a specific neighbor.</p> <p>Parameters:</p> Name Type Description Default <code>messages_number_message</code> <code>list</code> <p>List of message data</p> required <code>addr</code> <code>str</code> <p>Source address</p> required <code>nei</code> <code>str</code> <p>Neighbor address</p> required <code>current_round</code> <code>int</code> <p>Current round number</p> required <code>metric_active</code> <code>bool</code> <p>Whether the metric is active</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple[float, int]</code> <p>Tuple of (normalized_messages, messages_count)</p> Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>def manage_metric_number_message(\n    self, messages_number_message: list, addr: str, nei: str, current_round: int, metric_active: bool = True\n) -&gt; tuple[float, int]:\n    \"\"\"\n    Manage the number of messages metric for a specific neighbor.\n\n    Args:\n        messages_number_message: List of message data\n        addr: Source address\n        nei: Neighbor address\n        current_round: Current round number\n        metric_active: Whether the metric is active\n\n    Returns:\n        Tuple of (normalized_messages, messages_count)\n    \"\"\"\n    try:\n        if current_round == 0 or not metric_active:\n            return 0.0, 0\n\n        messages_count = self._count_relevant_messages(messages_number_message, addr, nei, current_round)\n        neighbor_stats = self._calculate_neighbor_statistics(messages_number_message, current_round)\n\n        normalized_messages = self._calculate_normalized_messages(messages_count, neighbor_stats)\n\n        normalized_messages = self._apply_historical_penalty(\n            normalized_messages, addr, nei, current_round\n        )\n\n        self._store_message_history(addr, nei, current_round, normalized_messages)\n        normalized_messages = max(0.001, normalized_messages)\n\n        return normalized_messages, messages_count\n\n    except Exception:\n        logging.exception(\"Error managing metric number_message\")\n        return 0.0, 0\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.manage_model_arrival_latency","title":"<code>manage_model_arrival_latency(addr, nei, latency, current_round, round_num)</code>","text":"<p>Manage the model_arrival_latency metric using latency.</p> <p>Parameters:</p> Name Type Description Default <code>addr</code> <code>str</code> <p>Source IP address.</p> required <code>nei</code> <code>str</code> <p>Destination IP address.</p> required <code>latency</code> <code>float</code> <p>Latency value for the current model_arrival_latency.</p> required <code>current_round</code> <code>int</code> <p>The current round of the program.</p> required <code>round_num</code> <code>int</code> <p>The round number of the model_arrival_latency.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>Normalized score between 0 and 1 for model_arrival_latency.</p> Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>def manage_model_arrival_latency(self, addr, nei, latency, current_round, round_num):\n    \"\"\"\n    Manage the model_arrival_latency metric using latency.\n\n    Args:\n        addr (str): Source IP address.\n        nei (str): Destination IP address.\n        latency (float): Latency value for the current model_arrival_latency.\n        current_round (int): The current round of the program.\n        round_num (int): The round number of the model_arrival_latency.\n\n    Returns:\n        float: Normalized score between 0 and 1 for model_arrival_latency.\n    \"\"\"\n    try:\n        current_key = nei\n\n        self._initialize_latency_round_entry(current_round, current_key, latency)\n\n        if current_round &gt;= 1:\n            score = self._calculate_latency_score(current_round, current_key, latency)\n            self._update_latency_entry_with_score(current_round, current_key, score)\n        else:\n            score = 0\n\n        return score\n\n    except Exception as e:\n        logging.exception(f\"Error managing model_arrival_latency: {e}\")\n        return 0\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.on_round_start","title":"<code>on_round_start(rse)</code>  <code>async</code>","text":"<p>Handle the start of a new round and initialize the round timing information.</p> Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>async def on_round_start(self, rse: RoundStartEvent):\n    \"\"\"\n    Handle the start of a new round and initialize the round timing information.\n    \"\"\"\n    (round_id, start_time, expected_nodes) = await rse.get_event_data()\n    if round_id not in self.round_timing_info:\n        self.round_timing_info[round_id] = {}\n    self.round_timing_info[round_id][\"start_time\"] = start_time\n    expected_nodes.difference_update(self.rejected_nodes)\n    expected_nodes = list(expected_nodes)\n    self._recalculate_pending_latencies(round_id)\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.recollect_duplicated_number_message","title":"<code>recollect_duplicated_number_message(dme)</code>  <code>async</code>","text":"<p>Record a duplicated message event.</p> Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>async def recollect_duplicated_number_message(self, dme: DuplicatedMessageEvent):\n    \"\"\"Record a duplicated message event.\"\"\"\n    event_data = await dme.get_event_data()\n    if isinstance(event_data, tuple):\n        source = event_data[0]\n    else:\n        source = event_data\n    await self._record_message_data(source)\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.recollect_fraction_of_parameters_changed","title":"<code>recollect_fraction_of_parameters_changed(ure)</code>  <code>async</code>","text":"<p>Collect and analyze the fraction of parameters that changed between models.</p> <p>Parameters:</p> Name Type Description Default <code>ure</code> <code>UpdateReceivedEvent</code> <p>UpdateReceivedEvent containing model and metadata</p> required Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>async def recollect_fraction_of_parameters_changed(self, ure: UpdateReceivedEvent):\n    \"\"\"\n    Collect and analyze the fraction of parameters that changed between models.\n\n    Args:\n        ure: UpdateReceivedEvent containing model and metadata\n    \"\"\"\n    (decoded_model, weight, source, round_num, local) = await ure.get_event_data()\n\n    current_round = await self._engine.get_round()\n    parameters_local = self._engine.trainer.get_model_parameters()\n\n    prev_threshold = self._get_previous_threshold(source, current_round)\n    differences = self._calculate_parameter_differences(parameters_local, decoded_model)\n    current_threshold = self._calculate_threshold(differences, prev_threshold)\n\n    changed_params, total_params, changes_record = self._count_changed_parameters(\n        parameters_local, decoded_model, current_threshold\n    )\n\n    fraction_changed = changed_params / total_params if total_params &gt; 0 else 0.0\n\n    self._store_fraction_data(source, current_round, {\n        \"fraction_changed\": fraction_changed,\n        \"total_params\": total_params,\n        \"changed_params\": changed_params,\n        \"threshold\": current_threshold,\n        \"changes_record\": changes_record,\n    })\n\n    self.save_data(\n        \"fraction_of_params_changed\",\n        source,\n        self._addr,\n        current_round=current_round,\n        fraction_changed=fraction_changed,\n        threshold=current_threshold,\n    )\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.recollect_number_message","title":"<code>recollect_number_message(source, message)</code>  <code>async</code>","text":"<p>Record a number message from a source.</p> Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>async def recollect_number_message(self, source, message):\n    \"\"\"Record a number message from a source.\"\"\"\n    await self._record_message_data(source)\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.recollect_similarity","title":"<code>recollect_similarity(ure)</code>  <code>async</code>","text":"<p>Collect and analyze model similarity metrics.</p> <p>Parameters:</p> Name Type Description Default <code>ure</code> <code>UpdateReceivedEvent</code> <p>UpdateReceivedEvent containing model and metadata</p> required Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>async def recollect_similarity(self, ure: UpdateReceivedEvent):\n    \"\"\"\n    Collect and analyze model similarity metrics.\n\n    Args:\n        ure: UpdateReceivedEvent containing model and metadata\n    \"\"\"\n    (decoded_model, weight, nei, round_num, local) = await ure.get_event_data()\n\n    if not (self._enabled and self._is_metric_enabled(\"model_similarity\")):\n        return\n\n    if not self._engine.config.participant[\"adaptive_args\"][\"model_similarity\"]:\n        return\n\n    if nei == self._addr:\n        return\n\n    logging.info(\"\ud83e\udd16  handle_model_message | Checking model similarity\")\n\n    local_model = self._engine.trainer.get_model_parameters()\n    similarity_values = self._calculate_all_similarity_metrics(local_model, decoded_model)\n\n    similarity_metrics = {\n        \"timestamp\": datetime.now(),\n        \"nei\": nei,\n        \"round\": round_num,\n        \"current_round\": await self._engine.get_round(),\n        **similarity_values\n    }\n\n    self._store_similarity_metrics(nei, similarity_metrics)\n    self._check_similarity_threshold(nei, similarity_values[\"cosine\"])\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.save_data","title":"<code>save_data(type_data, nei, addr, num_round=None, time=None, current_round=None, fraction_changed=None, threshold=None, latency=None)</code>","text":"<p>Save data between nodes and aggregated models.</p> <p>Parameters:</p> Name Type Description Default <code>type_data</code> <code>str</code> <p>Type of data to save ('number_message', 'fraction_of_params_changed', 'model_arrival_latency')</p> required <code>nei</code> <code>str</code> <p>Neighbor identifier</p> required <code>addr</code> <code>str</code> <p>Address identifier</p> required <code>num_round</code> <code>int</code> <p>Round number</p> <code>None</code> <code>time</code> <code>float</code> <p>Timestamp</p> <code>None</code> <code>current_round</code> <code>int</code> <p>Current round number</p> <code>None</code> <code>fraction_changed</code> <code>float</code> <p>Fraction of parameters changed</p> <code>None</code> <code>threshold</code> <code>float</code> <p>Threshold value</p> <code>None</code> <code>latency</code> <code>float</code> <p>Latency value</p> <code>None</code> Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>def save_data(\n    self,\n    type_data: str,\n    nei: str,\n    addr: str,\n    num_round: int = None,\n    time: float = None,\n    current_round: int = None,\n    fraction_changed: float = None,\n    threshold: float = None,\n    latency: float = None,\n):\n    \"\"\"\n    Save data between nodes and aggregated models.\n\n    Args:\n        type_data: Type of data to save ('number_message', 'fraction_of_params_changed', 'model_arrival_latency')\n        nei: Neighbor identifier\n        addr: Address identifier\n        num_round: Round number\n        time: Timestamp\n        current_round: Current round number\n        fraction_changed: Fraction of parameters changed\n        threshold: Threshold value\n        latency: Latency value\n    \"\"\"\n    if addr == nei:\n        return\n\n    if nei not in self.connection_metrics:\n        logging.warning(f\"Neighbor {nei} not found in connection_metrics\")\n        return\n\n    try:\n        metrics_instance = self.connection_metrics[nei]\n\n        if type_data == \"number_message\":\n            message_data = {\"time\": time, \"current_round\": current_round}\n            if not isinstance(metrics_instance.messages, list):\n                metrics_instance.messages = []\n            metrics_instance.messages.append(message_data)\n        elif type_data == \"fraction_of_params_changed\":\n            fraction_data = {\n                \"fraction_changed\": fraction_changed,\n                \"threshold\": threshold,\n                \"current_round\": current_round,\n            }\n            metrics_instance.fraction_of_params_changed.update(fraction_data)\n        elif type_data == \"model_arrival_latency\":\n            latency_data = {\n                \"latency\": latency,\n                \"round\": num_round,\n                \"round_received\": current_round,\n            }\n            metrics_instance.model_arrival_latency.update(latency_data)\n        else:\n            logging.warning(f\"Unknown data type: {type_data}\")\n\n    except Exception:\n        logging.exception(f\"Error saving data for type {type_data} and neighbor {nei}\")\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.save_model_arrival_latency_history","title":"<code>save_model_arrival_latency_history(nei, model_arrival_latency, round_num)</code>","text":"<p>Save the model_arrival_latency history of a participant (addr) regarding its neighbor (nei) in memory. Use 3 rounds for the average. Args:     nei (str): The neighboring node involved.     model_arrival_latency (float): The model_arrival_latency value to be saved.     round_num (int): The current round number.</p> <p>Returns:</p> Name Type Description <code>float</code> <p>The smoothed average model_arrival_latency including the current round.</p> Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>def save_model_arrival_latency_history(self, nei, model_arrival_latency, round_num):\n    \"\"\"\n    Save the model_arrival_latency history of a participant (addr) regarding its neighbor (nei) in memory.\n    Use 3 rounds for the average.\n    Args:\n        nei (str): The neighboring node involved.\n        model_arrival_latency (float): The model_arrival_latency value to be saved.\n        round_num (int): The current round number.\n\n    Returns:\n        float: The smoothed average model_arrival_latency including the current round.\n    \"\"\"\n    try:\n        current_key = nei\n\n        self._initialize_latency_history_entry(round_num, current_key, model_arrival_latency)\n\n        if model_arrival_latency &gt; 0 and round_num &gt;= 1:\n            avg_model_arrival_latency = self._calculate_latency_weighted_average_positive(\n                round_num, current_key, model_arrival_latency\n            )\n        elif model_arrival_latency == 0 and round_num &gt;= 1:\n            avg_model_arrival_latency = self._calculate_latency_weighted_average_zero(\n                round_num, current_key\n            )\n        elif model_arrival_latency &lt; 0 and round_num &gt;= 1:\n            avg_model_arrival_latency = abs(model_arrival_latency) * self.NEGATIVE_LATENCY_PENALTY\n        else:\n            avg_model_arrival_latency = 0\n\n        self.model_arrival_latency_history[round_num][current_key][\"avg_model_arrival_latency\"] = (\n            avg_model_arrival_latency\n        )\n\n        return avg_model_arrival_latency\n\n    except Exception:\n        logging.exception(\"Error saving model_arrival_latency history\")\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.save_number_message_history","title":"<code>save_number_message_history(addr, nei, messages_number_message_normalized, current_round)</code>","text":"<p>Save the number_message history of a participant (addr) regarding its neighbor (nei) in memory. Uses a weighted average of the past 3 rounds to smooth the result.</p> <p>Returns:</p> Name Type Description <code>float</code> <p>The weighted average including the current round.</p> Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>def save_number_message_history(self, addr, nei, messages_number_message_normalized, current_round):\n    \"\"\"\n    Save the number_message history of a participant (addr) regarding its neighbor (nei) in memory.\n    Uses a weighted average of the past 3 rounds to smooth the result.\n\n    Returns:\n        float: The weighted average including the current round.\n    \"\"\"\n    try:\n        key = (addr, nei)\n\n        self._initialize_message_history_entry(key, current_round, messages_number_message_normalized)\n\n        if messages_number_message_normalized &gt; 0 and current_round &gt;= 1:\n            avg_number_message = self._calculate_weighted_average_positive(key, current_round, messages_number_message_normalized)\n        elif messages_number_message_normalized == 0 and current_round &gt;= 1:\n            avg_number_message = self._calculate_weighted_average_zero(key, current_round)\n        elif messages_number_message_normalized &lt; 0 and current_round &gt;= 1:\n            avg_number_message = abs(messages_number_message_normalized) * self.NEGATIVE_LATENCY_PENALTY\n        else:\n            avg_number_message = 0\n\n        self.number_message_history[key][current_round][\"avg_number_message\"] = avg_number_message\n        return avg_number_message\n\n    except Exception:\n        logging.exception(\"Error saving number_message history\")\n        return -1\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.save_reputation_history_in_memory","title":"<code>save_reputation_history_in_memory(addr, nei, reputation)</code>  <code>async</code>","text":"<p>Save reputation history and calculate weighted average.</p> <p>Parameters:</p> Name Type Description Default <code>addr</code> <code>str</code> <p>The node's identifier</p> required <code>nei</code> <code>str</code> <p>The neighboring node identifier  </p> required <code>reputation</code> <code>float</code> <p>The reputation value to save</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The weighted average reputation</p> Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>async def save_reputation_history_in_memory(self, addr: str, nei: str, reputation: float) -&gt; float:\n    \"\"\"\n    Save reputation history and calculate weighted average.\n\n    Args:\n        addr: The node's identifier\n        nei: The neighboring node identifier  \n        reputation: The reputation value to save\n\n    Returns:\n        float: The weighted average reputation\n    \"\"\"\n    try:\n        key = (addr, nei)\n        current_round = await self._engine.get_round()\n\n        if key not in self.reputation_history:\n            self.reputation_history[key] = {}\n\n        self.reputation_history[key][current_round] = reputation\n\n        rounds = sorted(self.reputation_history[key].keys(), reverse=True)[:2]\n\n        if len(rounds) &gt;= 2:\n            current_rep = self.reputation_history[key][rounds[0]]\n            previous_rep = self.reputation_history[key][rounds[1]]\n\n            current_weight = self.REPUTATION_CURRENT_WEIGHT\n            previous_weight = self.REPUTATION_FEEDBACK_WEIGHT\n            avg_reputation = (current_rep * current_weight) + (previous_rep * previous_weight)\n\n            logging.info(f\"Current reputation: {current_rep}, Previous reputation: {previous_rep}\")\n            logging.info(f\"Reputation ponderated: {avg_reputation}\")\n        else:\n            avg_reputation = reputation\n\n        return avg_reputation\n\n    except Exception:\n        logging.exception(\"Error saving reputation history\")\n        return -1\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.send_reputation_to_neighbors","title":"<code>send_reputation_to_neighbors(neighbors)</code>  <code>async</code>","text":"<p>Send the calculated reputation to the neighbors.</p> Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>async def send_reputation_to_neighbors(self, neighbors):\n    \"\"\"\n    Send the calculated reputation to the neighbors.\n    \"\"\"\n    for nei, data in self.reputation.items():\n        if data[\"reputation\"] is not None:\n            neighbors_to_send = [neighbor for neighbor in neighbors if neighbor != nei]\n\n            for neighbor in neighbors_to_send:\n                message = self._engine.cm.create_message(\n                    \"reputation\",\n                    \"share\",\n                    node_id=nei,\n                    score=float(data[\"reputation\"]),\n                    round=await self._engine.get_round(),\n                )\n                await self._engine.cm.send_message(neighbor, message)\n                logging.info(\n                    f\"Sending reputation to node {nei} from node {neighbor} with reputation {data['reputation']}\"\n                )\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.setup","title":"<code>setup()</code>  <code>async</code>","text":"<p>Set up the reputation system by subscribing to relevant events.</p> Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>async def setup(self):\n    \"\"\"Set up the reputation system by subscribing to relevant events.\"\"\"\n    if self._enabled:\n        await EventManager.get_instance().subscribe_node_event(RoundStartEvent, self.on_round_start)\n        await EventManager.get_instance().subscribe_node_event(AggregationEvent, self.calculate_reputation)\n        if self._is_metric_enabled(\"model_similarity\"):\n            await EventManager.get_instance().subscribe_node_event(UpdateReceivedEvent, self.recollect_similarity)\n        if self._is_metric_enabled(\"fraction_parameters_changed\"):\n            await EventManager.get_instance().subscribe_node_event(\n                UpdateReceivedEvent, self.recollect_fraction_of_parameters_changed\n            )\n        if self._is_metric_enabled(\"model_arrival_latency\"):\n            await EventManager.get_instance().subscribe_node_event(\n                UpdateReceivedEvent, self.recollect_model_arrival_latency\n            )\n        if self._is_metric_enabled(\"num_messages\"):\n            await EventManager.get_instance().subscribe((\"model\", \"update\"), self.recollect_number_message)\n            await EventManager.get_instance().subscribe((\"model\", \"initialization\"), self.recollect_number_message)\n            await EventManager.get_instance().subscribe((\"control\", \"alive\"), self.recollect_number_message)\n            await EventManager.get_instance().subscribe(\n                (\"federation\", \"federation_models_included\"), self.recollect_number_message\n            )\n            await EventManager.get_instance().subscribe_node_event(DuplicatedMessageEvent, self.recollect_duplicated_number_message)\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.update_process_aggregation","title":"<code>update_process_aggregation(updates)</code>  <code>async</code>","text":"<p>Update the process of aggregation by removing rejected nodes from the updates and scaling the weights of the models based on their reputation.</p> Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>async def update_process_aggregation(self, updates):\n    \"\"\"\n    Update the process of aggregation by removing rejected nodes from the updates and\n    scaling the weights of the models based on their reputation.\n    \"\"\"\n    for rn in self.rejected_nodes:\n        if rn in updates:\n            updates.pop(rn)\n\n    if await self.engine.get_round() &gt;= 1:\n        for nei in list(updates.keys()):\n            if nei in self.reputation:\n                rep = self.reputation[nei].get(\"reputation\", 0)\n                if rep &gt;= self.REPUTATION_SCALING_THRESHOLD:\n                    weight = (rep - self.REPUTATION_SCALING_THRESHOLD) / self.REPUTATION_SCALING_RANGE\n                    model_dict = updates[nei][0]\n                    extra_data = updates[nei][1]\n\n                    scaled_model = {k: v * weight for k, v in model_dict.items()}\n                    updates[nei] = (scaled_model, extra_data)\n\n                    logging.info(f\"\u2705 Nei {nei} with reputation {rep:.4f}, scaled model with weight {weight:.4f}\")\n                else:\n                    logging.info(f\"\u26d4 Nei {nei} with reputation {rep:.4f}, model rejected\")\n\n    logging.info(f\"Updates after rejected nodes: {list(updates.keys())}\")\n    logging.info(f\"Nodes rejected: {self.rejected_nodes}\")\n</code></pre>"},{"location":"api/addons/trustworthiness/","title":"Documentation for Trustworthiness Module","text":""},{"location":"api/addons/trustworthiness/calculation/","title":"Documentation for Calculation Module","text":""},{"location":"api/addons/trustworthiness/calculation/#nebula.addons.trustworthiness.calculation.check_properties","title":"<code>check_properties(*args)</code>","text":"<p>Check if all the arguments have values.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>list</code> <p>All the arguments.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>float</code> <p>The mean of arguments that have values.</p> Source code in <code>nebula/addons/trustworthiness/calculation.py</code> <pre><code>def check_properties(*args):\n    \"\"\"\n    Check if all the arguments have values.\n\n    Args:\n        args (list): All the arguments.\n\n    Returns:\n        float: The mean of arguments that have values.\n    \"\"\"\n\n    result = map(lambda x: x is not None and x != \"\", args)\n    return np.mean(list(result))\n</code></pre>"},{"location":"api/addons/trustworthiness/calculation/#nebula.addons.trustworthiness.calculation.get_avg_loss_accuracy","title":"<code>get_avg_loss_accuracy(scenario_name)</code>","text":"<p>Calculates the mean accuracy and loss models of the nodes.</p> <p>Parameters:</p> Name Type Description Default <code>loss_files</code> <code>list</code> <p>Files that contain the loss of the models of the nodes.</p> required <code>accuracy_files</code> <code>list</code> <p>Files that contain the acurracies of the models of the nodes.</p> required <p>Returns:</p> Type Description <p>3-tupla: The mean loss of the models, the mean accuracies of the models, the standard deviation of the accuracies of the models.</p> Source code in <code>nebula/addons/trustworthiness/calculation.py</code> <pre><code>def get_avg_loss_accuracy(scenario_name):\n    \"\"\"\n    Calculates the mean accuracy and loss models of the nodes.\n\n    Args:\n        loss_files (list): Files that contain the loss of the models of the nodes.\n        accuracy_files (list): Files that contain the acurracies of the models of the nodes.\n\n    Returns:\n        3-tupla: The mean loss of the models, the mean accuracies of the models, the standard deviation of the accuracies of the models.\n    \"\"\"\n    total_accuracy = 0\n    total_loss = 0\n\n    data_file = os.path.join(os.environ.get('NEBULA_LOGS_DIR'), scenario_name, \"trustworthiness\", \"data_results.csv\")\n\n    data = read_csv(data_file)\n\n    number_files = len(data)\n\n    total_loss = data[\"loss\"].sum()\n    total_accuracy = data[\"accuracy\"].sum()\n\n    avg_loss = total_loss / number_files\n    avg_accuracy = total_accuracy / number_files\n    std_accuracy = statistics.stdev(data[\"accuracy\"])\n\n    return avg_loss, avg_accuracy, std_accuracy\n</code></pre>"},{"location":"api/addons/trustworthiness/calculation/#nebula.addons.trustworthiness.calculation.get_bytes_models","title":"<code>get_bytes_models(models_files)</code>","text":"<p>Calculates the mean bytes of the final models of the nodes.</p> <p>Parameters:</p> Name Type Description Default <code>models_files</code> <code>list</code> <p>List of final models.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The mean bytes of the models.</p> Source code in <code>nebula/addons/trustworthiness/calculation.py</code> <pre><code>def get_bytes_models(models_files):\n    \"\"\"\n    Calculates the mean bytes of the final models of the nodes.\n\n    Args:\n        models_files (list): List of final models.\n\n    Returns:\n        float: The mean bytes of the models.\n    \"\"\"\n\n    total_models_size = 0\n    number_models = len(models_files)\n\n    for file in models_files:\n        model_size = os.path.getsize(file)\n        total_models_size += model_size\n\n    avg_model_size = total_models_size / number_models\n\n    return avg_model_size\n</code></pre>"},{"location":"api/addons/trustworthiness/calculation/#nebula.addons.trustworthiness.calculation.get_bytes_sent_recv","title":"<code>get_bytes_sent_recv(scenario_name)</code>","text":"<p>Calculates the mean bytes sent and received of the nodes.</p> <p>Parameters:</p> Name Type Description Default <code>bytes_sent_files</code> <code>list</code> <p>Files that contain the bytes sent of the nodes.</p> required <code>bytes_recv_files</code> <code>list</code> <p>Files that contain the bytes received of the nodes.</p> required <p>Returns:</p> Type Description <p>4-tupla: The total bytes sent, the total bytes received, the mean bytes sent and the mean bytes received of the nodes.</p> Source code in <code>nebula/addons/trustworthiness/calculation.py</code> <pre><code>def get_bytes_sent_recv(scenario_name):\n    \"\"\"\n    Calculates the mean bytes sent and received of the nodes.\n\n    Args:\n        bytes_sent_files (list): Files that contain the bytes sent of the nodes.\n        bytes_recv_files (list): Files that contain the bytes received of the nodes.\n\n    Returns:\n        4-tupla: The total bytes sent, the total bytes received, the mean bytes sent and the mean bytes received of the nodes.\n    \"\"\"\n    total_upload_bytes = 0\n    total_download_bytes = 0\n\n    data_file = os.path.join(os.environ.get('NEBULA_LOGS_DIR'), scenario_name, \"trustworthiness\", \"data_results.csv\")\n\n    data = read_csv(data_file)\n\n    number_files = len(data)\n\n    total_upload_bytes = int(data[\"bytes_sent\"].sum())\n    total_download_bytes = int(data[\"bytes_recv\"].sum())\n\n    avg_upload_bytes = total_upload_bytes / number_files\n    avg_download_bytes = total_download_bytes / number_files\n\n    return total_upload_bytes, total_download_bytes, avg_upload_bytes, avg_download_bytes\n</code></pre>"},{"location":"api/addons/trustworthiness/calculation/#nebula.addons.trustworthiness.calculation.get_clever_score","title":"<code>get_clever_score(model, test_sample, nb_classes, learning_rate)</code>","text":"<p>Calculates the CLEVER score.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>object</code> <p>The model.</p> required <code>test_sample</code> <code>object</code> <p>One test sample to calculate the CLEVER score.</p> required <code>nb_classes</code> <code>int</code> <p>The nb_classes of the model.</p> required <code>learning_rate</code> <code>float</code> <p>The learning rate of the model.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The CLEVER score.</p> Source code in <code>nebula/addons/trustworthiness/calculation.py</code> <pre><code>def get_clever_score(model, test_sample, nb_classes, learning_rate):\n    \"\"\"\n    Calculates the CLEVER score.\n\n    Args:\n        model (object): The model.\n        test_sample (object): One test sample to calculate the CLEVER score.\n        nb_classes (int): The nb_classes of the model.\n        learning_rate (float): The learning rate of the model.\n\n    Returns:\n        float: The CLEVER score.\n    \"\"\"\n\n    images, _ = test_sample\n    background = images[-1]\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), learning_rate)\n\n    # Create the ART classifier\n    classifier = PyTorchClassifier(\n        model=model,\n        loss=criterion,\n        optimizer=optimizer,\n        input_shape=(1, 28, 28),\n        nb_classes=nb_classes,\n    )\n\n    score_untargeted = clever_u(\n        classifier,\n        background.numpy(),\n        10,\n        5,\n        R_L2,\n        norm=2,\n        pool_factor=3,\n        verbose=False,\n    )\n    return score_untargeted\n</code></pre>"},{"location":"api/addons/trustworthiness/calculation/#nebula.addons.trustworthiness.calculation.get_cv","title":"<code>get_cv(list=None, std=None, mean=None)</code>","text":"<p>Get the coefficient of variation.</p> <p>Parameters:</p> Name Type Description Default <code>list</code> <code>list</code> <p>List in which the coefficient of variation will be calculated.</p> <code>None</code> <code>std</code> <code>float</code> <p>Standard deviation of a list.</p> <code>None</code> <code>mean</code> <code>float</code> <p>Mean of a list.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>float</code> <p>The coefficient of variation calculated.</p> Source code in <code>nebula/addons/trustworthiness/calculation.py</code> <pre><code>def get_cv(list=None, std=None, mean=None):\n    \"\"\"\n    Get the coefficient of variation.\n\n    Args:\n        list (list): List in which the coefficient of variation will be calculated.\n        std (float): Standard deviation of a list.\n        mean (float): Mean of a list.\n\n    Returns:\n        float: The coefficient of variation calculated.\n    \"\"\"\n    if std is not None and mean is not None:\n        return std / mean\n\n    if list is not None:\n        return np.std(list) / np.mean(list)\n\n    return 0\n</code></pre>"},{"location":"api/addons/trustworthiness/calculation/#nebula.addons.trustworthiness.calculation.get_elapsed_time","title":"<code>get_elapsed_time(start_time, end_time)</code>","text":"<p>Calculates the elapsed time during the execution of the scenario.</p> <p>Parameters:</p> Name Type Description Default <code>start_time</code> <code>datetime</code> <p>Start datetime.</p> required <code>end_time</code> <code>datetime</code> <p>End datetime.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The elapsed time.</p> Source code in <code>nebula/addons/trustworthiness/calculation.py</code> <pre><code>def get_elapsed_time(start_time, end_time):\n    \"\"\"\n    Calculates the elapsed time during the execution of the scenario.\n\n    Args:\n        start_time (datetime): Start datetime.\n        end_time (datetime): End datetime.\n\n    Returns:\n        float: The elapsed time.\n    \"\"\"\n    start_date = datetime.strptime(start_time, \"%d/%m/%Y %H:%M:%S\")\n    end_date = datetime.strptime(end_time, \"%d/%m/%Y %H:%M:%S\")\n\n    elapsed_time = (end_date - start_date).total_seconds() / 60\n\n    return elapsed_time\n</code></pre>"},{"location":"api/addons/trustworthiness/calculation/#nebula.addons.trustworthiness.calculation.get_feature_importance_cv","title":"<code>get_feature_importance_cv(model, test_sample)</code>","text":"<p>Calculates the coefficient of variation of the feature importance.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>object</code> <p>The model.</p> required <code>test_sample</code> <code>object</code> <p>One test sample to calculate the feature importance.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The coefficient of variation of the feature importance.</p> Source code in <code>nebula/addons/trustworthiness/calculation.py</code> <pre><code>def get_feature_importance_cv(model, test_sample):\n    \"\"\"\n    Calculates the coefficient of variation of the feature importance.\n\n    Args:\n        model (object): The model.\n        test_sample (object): One test sample to calculate the feature importance.\n\n    Returns:\n        float: The coefficient of variation of the feature importance.\n    \"\"\"\n\n    try:\n        cv = 0\n        batch_size = 10\n        device = \"cpu\"\n\n        if isinstance(model, torch.nn.Module):\n            batched_data, _ = test_sample\n\n            n = batch_size\n            m = math.floor(0.8 * n)\n\n            background = batched_data[:m].to(device)\n            test_data = batched_data[m:n].to(device)\n\n            e = shap.DeepExplainer(model, background)\n            shap_values = e.shap_values(test_data)\n            if shap_values is not None and len(shap_values) &gt; 0:\n                sums = np.array([shap_values[i].sum() for i in range(len(shap_values))])\n                abs_sums = np.absolute(sums)\n                cv = variation(abs_sums)\n    except Exception as e:\n        logger.warning(\"Could not compute feature importance CV with shap\")\n        cv = 1\n    if math.isnan(cv):\n        cv = 1\n    return cv\n</code></pre>"},{"location":"api/addons/trustworthiness/calculation/#nebula.addons.trustworthiness.calculation.get_global_privacy_risk","title":"<code>get_global_privacy_risk(dp, epsilon, n)</code>","text":"<p>Calculates the global privacy risk by epsilon and the number of clients.</p> <p>Parameters:</p> Name Type Description Default <code>dp</code> <code>bool</code> <p>Indicates if differential privacy is used or not.</p> required <code>epsilon</code> <code>int</code> <p>The epsilon value.</p> required <code>n</code> <code>int</code> <p>The number of clients in the scenario.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The global privacy risk.</p> Source code in <code>nebula/addons/trustworthiness/calculation.py</code> <pre><code>def get_global_privacy_risk(dp, epsilon, n):\n    \"\"\"\n    Calculates the global privacy risk by epsilon and the number of clients.\n\n    Args:\n        dp (bool): Indicates if differential privacy is used or not.\n        epsilon (int): The epsilon value.\n        n (int): The number of clients in the scenario.\n\n    Returns:\n        float: The global privacy risk.\n    \"\"\"\n\n    if dp is True and isinstance(epsilon, numbers.Number):\n        return 1 / (1 + (n - 1) * math.pow(e, -epsilon))\n    else:\n        return 1\n</code></pre>"},{"location":"api/addons/trustworthiness/calculation/#nebula.addons.trustworthiness.calculation.get_map_value_score","title":"<code>get_map_value_score(score_key, score_map)</code>","text":"<p>Finds the score by the score_key in the score_map and returns the value.</p> <p>Parameters:</p> Name Type Description Default <code>score_key</code> <code>string</code> <p>The key to look up in the score_map.</p> required <code>score_map</code> <code>dict</code> <p>The score map defined in the eval_metrics.json file.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The score obtained in the score_map.</p> Source code in <code>nebula/addons/trustworthiness/calculation.py</code> <pre><code>def get_map_value_score(score_key, score_map):\n    \"\"\"\n    Finds the score by the score_key in the score_map and returns the value.\n\n    Args:\n        score_key (string): The key to look up in the score_map.\n        score_map (dict): The score map defined in the eval_metrics.json file.\n\n    Returns:\n        float: The score obtained in the score_map.\n    \"\"\"\n    score = 0\n    if score_map is None:\n        logger.warning(\"Score map is missing\")\n    else:\n        score = score_map[score_key]\n    return score\n</code></pre>"},{"location":"api/addons/trustworthiness/calculation/#nebula.addons.trustworthiness.calculation.get_mapped_score","title":"<code>get_mapped_score(score_key, score_map)</code>","text":"<p>Finds the score by the score_key in the score_map.</p> <p>Parameters:</p> Name Type Description Default <code>score_key</code> <code>string</code> <p>The key to look up in the score_map.</p> required <code>score_map</code> <code>dict</code> <p>The score map defined in the eval_metrics.json file.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The normalized score of [0, 1].</p> Source code in <code>nebula/addons/trustworthiness/calculation.py</code> <pre><code>def get_mapped_score(score_key, score_map):\n    \"\"\"\n    Finds the score by the score_key in the score_map.\n\n    Args:\n        score_key (string): The key to look up in the score_map.\n        score_map (dict): The score map defined in the eval_metrics.json file.\n\n    Returns:\n        float: The normalized score of [0, 1].\n    \"\"\"\n    score = 0\n    if score_map is None:\n        logger.warning(\"Score map is missing\")\n    else:\n        keys = [key for key, value in score_map.items()]\n        scores = [value for key, value in score_map.items()]\n        normalized_scores = get_normalized_scores(scores)\n        normalized_score_map = dict(zip(keys, normalized_scores, strict=False))\n        score = normalized_score_map.get(score_key, np.nan)\n\n    return score\n</code></pre>"},{"location":"api/addons/trustworthiness/calculation/#nebula.addons.trustworthiness.calculation.get_normalized_scores","title":"<code>get_normalized_scores(scores)</code>","text":"<p>Calculates the normalized scores of a list.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>list</code> <p>The values that will be normalized.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>The normalized list.</p> Source code in <code>nebula/addons/trustworthiness/calculation.py</code> <pre><code>def get_normalized_scores(scores):\n    \"\"\"\n    Calculates the normalized scores of a list.\n\n    Args:\n        scores (list): The values that will be normalized.\n\n    Returns:\n        list: The normalized list.\n    \"\"\"\n    normalized = [(x - np.min(scores)) / (np.max(scores) - np.min(scores)) for x in scores]\n    return normalized\n</code></pre>"},{"location":"api/addons/trustworthiness/calculation/#nebula.addons.trustworthiness.calculation.get_range_score","title":"<code>get_range_score(value, ranges, direction='asc')</code>","text":"<p>Maps the value to a range and gets the score by the range and direction.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>int</code> <p>The input score.</p> required <code>ranges</code> <code>list</code> <p>The ranges defined.</p> required <code>direction</code> <code>string</code> <p>Asc means the higher the range the higher the score, desc means otherwise.</p> <code>'asc'</code> <p>Returns:</p> Name Type Description <code>float</code> <p>The normalized score of [0, 1].</p> Source code in <code>nebula/addons/trustworthiness/calculation.py</code> <pre><code>def get_range_score(value, ranges, direction=\"asc\"):\n    \"\"\"\n    Maps the value to a range and gets the score by the range and direction.\n\n    Args:\n        value (int): The input score.\n        ranges (list): The ranges defined.\n        direction (string): Asc means the higher the range the higher the score, desc means otherwise.\n\n    Returns:\n        float: The normalized score of [0, 1].\n    \"\"\"\n\n    if not (type(value) == int or type(value) == float):\n        logger.warning(\"Input value is not a number\")\n        logger.warning(f\"{value}\")\n        return 0\n    else:\n        score = 0\n        if ranges is None:\n            logger.warning(\"Score ranges are missing\")\n        else:\n            total_bins = len(ranges) + 1\n            bin = np.digitize(value, ranges, right=True)\n            score = 1 - (bin / total_bins) if direction == \"desc\" else bin / total_bins\n        return score\n</code></pre>"},{"location":"api/addons/trustworthiness/calculation/#nebula.addons.trustworthiness.calculation.get_scaled_score","title":"<code>get_scaled_score(value, scale, direction)</code>","text":"<p>Maps a score of a specific scale into the scale between zero and one.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>int or float</code> <p>The raw value of the metric.</p> required <code>scale</code> <code>list</code> <p>List containing the minimum and maximum value the value can fall in between.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The normalized score of [0, 1].</p> Source code in <code>nebula/addons/trustworthiness/calculation.py</code> <pre><code>def get_scaled_score(value, scale: list, direction: str):\n    \"\"\"\n    Maps a score of a specific scale into the scale between zero and one.\n\n    Args:\n        value (int or float): The raw value of the metric.\n        scale (list): List containing the minimum and maximum value the value can fall in between.\n\n    Returns:\n        float: The normalized score of [0, 1].\n    \"\"\"\n\n    score = 0\n    try:\n        value_min, value_max = scale[0], scale[1]\n    except Exception:\n        logger.warning(\"Score minimum or score maximum is missing. The minimum has been set to 0 and the maximum to 1\")\n        value_min, value_max = 0, 1\n    if not value:\n        logger.warning(\"Score value is missing. Set value to zero\")\n    else:\n        low, high = 0, 1\n        if value &gt;= value_max:\n            score = 1\n        elif value &lt;= value_min:\n            score = 0\n        else:\n            diff = value_max - value_min\n            diffScale = high - low\n            score = (float(value) - value_min) * (float(diffScale) / diff) + low\n        if direction == \"desc\":\n            score = high - score\n\n    return score\n</code></pre>"},{"location":"api/addons/trustworthiness/calculation/#nebula.addons.trustworthiness.calculation.get_true_score","title":"<code>get_true_score(value, direction)</code>","text":"<p>Returns the negative of the value if direction is 'desc', otherwise returns value.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>int</code> <p>The input score.</p> required <code>direction</code> <code>string</code> <p>Asc means the higher the range the higher the score, desc means otherwise.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The score obtained.</p> Source code in <code>nebula/addons/trustworthiness/calculation.py</code> <pre><code>def get_true_score(value, direction):\n    \"\"\"\n    Returns the negative of the value if direction is 'desc', otherwise returns value.\n\n    Args:\n        value (int): The input score.\n        direction (string): Asc means the higher the range the higher the score, desc means otherwise.\n\n    Returns:\n        float: The score obtained.\n    \"\"\"\n\n    if value is True:\n        return 1\n    elif value is False:\n        return 0\n    else:\n        if not (type(value) == int or type(value) == float):\n            logger.warning(\"Input value is not a number\")\n            logger.warning(f\"{value}.\")\n            return 0\n        else:\n            if direction == \"desc\":\n                return 1 - value\n            else:\n                return value\n</code></pre>"},{"location":"api/addons/trustworthiness/calculation/#nebula.addons.trustworthiness.calculation.get_value","title":"<code>get_value(value)</code>","text":"<p>Get the value of a metric.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>The value of the metric.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The value of the metric.</p> Source code in <code>nebula/addons/trustworthiness/calculation.py</code> <pre><code>def get_value(value):\n    \"\"\"\n    Get the value of a metric.\n\n    Args:\n        value (float): The value of the metric.\n\n    Returns:\n        float: The value of the metric.\n    \"\"\"\n\n    return value\n</code></pre>"},{"location":"api/addons/trustworthiness/calculation/#nebula.addons.trustworthiness.calculation.stop_emissions_tracking_and_save","title":"<code>stop_emissions_tracking_and_save(tracker, outdir, emissions_file, role, workload, sample_size=0)</code>","text":"<p>Stops emissions tracking object from CodeCarbon and saves relevant information to emissions.csv file.</p> <p>Parameters:</p> Name Type Description Default <code>tracker</code> <code>object</code> <p>The emissions tracker object holding information.</p> required <code>outdir</code> <code>str</code> <p>The path of the output directory of the experiment.</p> required <code>emissions_file</code> <code>str</code> <p>The path to the emissions file.</p> required <code>role</code> <code>str</code> <p>Either client or server depending on the role.</p> required <code>workload</code> <code>str</code> <p>Either aggregation or training depending on the workload.</p> required <code>sample_size</code> <code>int</code> <p>The number of samples used for training, if aggregation 0.</p> <code>0</code> Source code in <code>nebula/addons/trustworthiness/calculation.py</code> <pre><code>def stop_emissions_tracking_and_save(\n    tracker: EmissionsTracker,\n    outdir: str,\n    emissions_file: str,\n    role: str,\n    workload: str,\n    sample_size: int = 0,\n):\n    \"\"\"\n    Stops emissions tracking object from CodeCarbon and saves relevant information to emissions.csv file.\n\n    Args:\n        tracker (object): The emissions tracker object holding information.\n        outdir (str): The path of the output directory of the experiment.\n        emissions_file (str): The path to the emissions file.\n        role (str): Either client or server depending on the role.\n        workload (str): Either aggregation or training depending on the workload.\n        sample_size (int): The number of samples used for training, if aggregation 0.\n    \"\"\"\n\n    tracker.stop()\n\n    emissions_file = os.path.join(outdir, emissions_file)\n\n    if exists(emissions_file):\n        df = pd.read_csv(emissions_file)\n    else:\n        df = pd.DataFrame(\n            columns=[\n                \"role\",\n                \"energy_grid\",\n                \"emissions\",\n                \"workload\",\n                \"CPU_model\",\n                \"GPU_model\",\n            ]\n        )\n    try:\n        energy_grid = (tracker.final_emissions_data.emissions / tracker.final_emissions_data.energy_consumed) * 1000\n        df = pd.concat(\n            [\n                df,\n                pd.DataFrame({\n                    \"role\": role,\n                    \"energy_grid\": [energy_grid],\n                    \"emissions\": [tracker.final_emissions_data.emissions],\n                    \"workload\": workload,\n                    \"CPU_model\": tracker.final_emissions_data.cpu_model\n                    if tracker.final_emissions_data.cpu_model\n                    else \"None\",\n                    \"GPU_model\": tracker.final_emissions_data.gpu_model\n                    if tracker.final_emissions_data.gpu_model\n                    else \"None\",\n                    \"CPU_used\": True if tracker.final_emissions_data.cpu_energy else False,\n                    \"GPU_used\": True if tracker.final_emissions_data.gpu_energy else False,\n                    \"energy_consumed\": tracker.final_emissions_data.energy_consumed,\n                    \"sample_size\": sample_size,\n                }),\n            ],\n            ignore_index=True,\n        )\n        df.to_csv(emissions_file, encoding=\"utf-8\", index=False)\n    except Exception as e:\n        logger.warning(e)\n</code></pre>"},{"location":"api/addons/trustworthiness/factsheet/","title":"Documentation for Factsheet Module","text":""},{"location":"api/addons/trustworthiness/factsheet/#nebula.addons.trustworthiness.factsheet.Factsheet","title":"<code>Factsheet</code>","text":"Source code in <code>nebula/addons/trustworthiness/factsheet.py</code> <pre><code>class Factsheet:\n    def __init__(self):\n        \"\"\"\n        Manager class to populate the FactSheet\n        \"\"\"\n        self.factsheet_file_nm = \"factsheet.json\"\n        self.factsheet_template_file_nm = \"factsheet_template.json\"\n\n    def populate_factsheet_pre_train(self, data, scenario_name):\n        \"\"\"\n        Populates the factsheet with values before the training.\n\n        Args:\n            data (dict): Contains the data from the scenario.\n            scenario_name (string): The name of the scenario.\n        \"\"\"\n\n        factsheet_file = os.path.join(os.environ.get('NEBULA_LOGS_DIR'), scenario_name, \"trustworthiness\", self.factsheet_file_nm)\n\n        factsheet_template = os.path.join(dirname, \"configs\", self.factsheet_template_file_nm)\n\n        if not os.path.exists(factsheet_file):\n            shutil.copyfile(factsheet_template, factsheet_file)\n\n        with open(factsheet_file, \"r+\") as f:\n            factsheet = {}\n\n            try:\n                factsheet = json.load(f)\n\n                if data is not None:\n                    logging.info(\"FactSheet: Populating factsheet with pre training metrics\")\n\n                    federation = data[\"federation\"]\n                    n_nodes = int(data[\"n_nodes\"])\n                    dataset = data[\"dataset\"]\n                    algorithm = data[\"model\"]\n                    aggregation_algorithm = data[\"agg_algorithm\"]\n                    n_rounds = int(data[\"rounds\"])\n                    attack = data[\"attack_params\"][\"attacks\"]\n                    if attack != \"No Attack\":\n                        poisoned_node_percent = int(data[\"attack_params\"][\"poisoned_node_percent\"])\n                        poisoned_sample_percent = int(data[\"attack_params\"][\"poisoned_sample_percent\"])\n                        poisoned_noise_percent = int(data[\"attack_params\"][\"poisoned_noise_percent\"])\n                    else:\n                        poisoned_node_percent = 0\n                        poisoned_sample_percent = 0\n                        poisoned_noise_percent = 0\n                    with_reputation = data[\"reputation\"][\"enabled\"]\n                    is_dynamic_topology = False # data[\"is_dynamic_topology\"]\n                    is_dynamic_aggregation = False # data[\"is_dynamic_aggregation\"]\n                    target_aggregation = False # data[\"target_aggregation\"]\n\n                    if attack != \"No Attack\" and with_reputation == True and is_dynamic_aggregation == True:\n                        background = f\"For the project setup, the most important aspects are the following: The federation architecture is {federation}, involving {n_nodes} clients, the dataset used is {dataset}, the learning algorithm is {algorithm}, the aggregation algorithm is {aggregation_algorithm} and the number of rounds is {n_rounds}. In addition, the type of attack used against the clients is {attack}, where the percentage of attacked nodes is {poisoned_node_percent}, the percentage of attacked samples of each node is {poisoned_sample_percent}, and the percent of poisoned noise is {poisoned_noise_percent}. A reputation-based defence with a dynamic aggregation based on the aggregation algorithm {target_aggregation} is used, and the trustworthiness of the project is desired.\"\n\n                    elif attack != \"No Attack\" and with_reputation == True and is_dynamic_topology == True:\n                        background = f\"For the project setup, the most important aspects are the following: The federation architecture is {federation}, involving {n_nodes} clients, the dataset used is {dataset}, the learning algorithm is {algorithm}, the aggregation algorithm is {aggregation_algorithm} and the number of rounds is {n_rounds}. In addition, the type of attack used against the clients is {attack}, where the percentage of attacked nodes is {poisoned_node_percent}, the percentage of attacked samples of each node is {poisoned_sample_percent}, and the percent of poisoned noise is {poisoned_noise_percent}. A reputation-based defence with a dynamic topology is used, and the trustworthiness of the project is desired.\"\n\n                    elif attack != \"No Attack\" and with_reputation == False:\n                        background = f\"For the project setup, the most important aspects are the following: The federation architecture is {federation}, involving {n_nodes} clients, the dataset used is {dataset}, the learning algorithm is {algorithm}, the aggregation algorithm is {aggregation_algorithm} and the number of rounds is {n_rounds}. In addition, the type of attack used against the clients is {attack}, where the percentage of attacked nodes is {poisoned_node_percent}, the percentage of attacked samples of each node is {poisoned_sample_percent}, and the percent of poisoned noise is {poisoned_noise_percent}. No defence mechanism is used, and the trustworthiness of the project is desired.\"\n\n                    elif attack == \"No Attack\":\n                        background = f\"For the project setup, the most important aspects are the following: The federation architecture is {federation}, involving {n_nodes} clients, the dataset used is {dataset}, the learning algorithm is {algorithm}, the aggregation algorithm is {aggregation_algorithm} and the number of rounds is {n_rounds}. No attacks against clients are used, and the trustworthiness of the project is desired.\"\n\n                    # Set project specifications\n                    factsheet[\"project\"][\"overview\"] = data[\"scenario_title\"]\n                    factsheet[\"project\"][\"purpose\"] = data[\"scenario_description\"]\n                    factsheet[\"project\"][\"background\"] = background\n\n                    # Set data specifications\n                    factsheet[\"data\"][\"provenance\"] = data[\"dataset\"]\n                    factsheet[\"data\"][\"preprocessing\"] = data[\"topology\"]\n\n                    # Set participants\n                    factsheet[\"participants\"][\"client_num\"] = data[\"n_nodes\"] or \"\"\n                    factsheet[\"participants\"][\"sample_client_rate\"] = 1\n                    factsheet[\"participants\"][\"client_selector\"] = \"\"\n\n                    # Set configuration\n                    factsheet[\"configuration\"][\"aggregation_algorithm\"] = data[\"agg_algorithm\"] or \"\"\n                    factsheet[\"configuration\"][\"training_model\"] = data[\"model\"] or \"\"\n                    factsheet[\"configuration\"][\"personalization\"] = False\n                    factsheet[\"configuration\"][\"visualization\"] = True\n                    factsheet[\"configuration\"][\"total_round_num\"] = n_rounds\n\n                    if poisoned_noise_percent != 0:\n                        factsheet[\"configuration\"][\"differential_privacy\"] = True\n                        factsheet[\"configuration\"][\"dp_epsilon\"] = poisoned_noise_percent\n                    else:\n                        factsheet[\"configuration\"][\"differential_privacy\"] = False\n                        factsheet[\"configuration\"][\"dp_epsilon\"] = \"\"\n\n                    if dataset == \"MNIST\" and algorithm == \"MLP\":\n                        model = MNISTModelMLP()\n                    elif dataset == \"MNIST\" and algorithm == \"CNN\":\n                        model = MNISTModelCNN()\n                    # elif dataset == \"Syscall\" and algorithm == \"MLP\":\n                    #     model = SyscallModelMLP()\n                    # else:\n                    #     model = CIFAR10ModelCNN()\n\n                    factsheet[\"configuration\"][\"learning_rate\"] = model.get_learning_rate()\n                    factsheet[\"configuration\"][\"trainable_param_num\"] = model.count_parameters()\n                    factsheet[\"configuration\"][\"local_update_steps\"] = 1\n\n                    f.seek(0)\n                    f.truncate()\n                    json.dump(factsheet, f, indent=4)\n\n            except JSONDecodeError as e:\n                logging.warning(f\"{factsheet_file} is invalid\")\n                logging.error(e)\n\n    def populate_factsheet_post_train(self, scenario_name, start_time, end_time):\n        \"\"\"\n        Populates the factsheet with values after the training.\n\n        Args:\n            scenario (object): The scenario object.\n        \"\"\"\n        factsheet_file = os.path.join(f\"{os.environ.get('NEBULA_LOGS_DIR')}{scenario_name}/trustworthiness/{self.factsheet_file_nm}\")\n\n        logging.info(\"FactSheet: Populating factsheet with post training metrics\")\n\n        with open(factsheet_file, \"r+\") as f:\n            factsheet = {}\n            try:\n                factsheet = json.load(f)\n\n                dataset = factsheet[\"data\"][\"provenance\"]\n                model = factsheet[\"configuration\"][\"training_model\"]\n\n                files_dir = f\"{os.environ.get('NEBULA_LOGS_DIR')}/{scenario_name}/trustworthiness\"\n\n                models_files = glob.glob(os.path.join(files_dir, \"*final_model*\"))\n                #dataloaders_files = glob.glob(os.path.join(files_dir, \"*train_loader*\"))\n                test_dataloader_file = f\"{files_dir}/participant_1_test_loader.pk\"\n                train_model_file = f\"{files_dir}/participant_1_train_model.pk\"\n                emissions_file = os.path.join(files_dir, \"emissions.csv\")\n\n                # # Entropy\n                # i = 0\n                # for file in dataloaders_files:\n                #     with open(file, \"rb\") as file:\n                #         dataloader = pickle.load(file)\n                #     get_entropy(i, scenario_name, dataloader)\n                #     i += 1\n\n                get_all_data_entropy(scenario_name)\n\n                with open(f\"{files_dir}/entropy.json\", \"r\") as file:\n                    entropy_distribution = json.load(file)\n\n                values = np.array(list(entropy_distribution.values()))\n\n                normalized_values = (values - np.min(values)) / (np.max(values) - np.min(values))\n\n                avg_entropy = np.mean(normalized_values)\n\n                factsheet[\"data\"][\"avg_entropy\"] = avg_entropy\n\n                # Set performance data\n                result_avg_loss_accuracy = get_avg_loss_accuracy(scenario_name)\n                factsheet[\"performance\"][\"test_loss_avg\"] = result_avg_loss_accuracy[0]\n                factsheet[\"performance\"][\"test_acc_avg\"] = result_avg_loss_accuracy[1]\n                test_acc_cv = get_cv(std=result_avg_loss_accuracy[2], mean=result_avg_loss_accuracy[1])\n                factsheet[\"fairness\"][\"test_acc_cv\"] = 1 if test_acc_cv &gt; 1 else test_acc_cv\n\n                factsheet[\"system\"][\"avg_time_minutes\"] = get_elapsed_time(start_time, end_time)\n                factsheet[\"system\"][\"avg_model_size\"] = get_bytes_models(models_files)\n\n                result_bytes_sent_recv = get_bytes_sent_recv(scenario_name)\n                factsheet[\"system\"][\"total_upload_bytes\"] = result_bytes_sent_recv[0]\n                factsheet[\"system\"][\"total_download_bytes\"] = result_bytes_sent_recv[1]\n                factsheet[\"system\"][\"avg_upload_bytes\"] = result_bytes_sent_recv[2]\n                factsheet[\"system\"][\"avg_download_bytes\"] = result_bytes_sent_recv[3]\n\n                factsheet[\"fairness\"][\"selection_cv\"] = 1\n\n                count_all_class_samples(scenario_name)\n\n                with open(f\"{files_dir}/count_class.json\", \"r\") as file:\n                    class_distribution = json.load(file)\n\n                class_samples_sizes = [x for x in class_distribution.values()]\n                class_imbalance = get_cv(list=class_samples_sizes)\n                factsheet[\"fairness\"][\"class_imbalance\"] = 1 if class_imbalance &gt; 1 else class_imbalance\n\n                with open(train_model_file, \"rb\") as file:\n                    lightning_model = pickle.load(file)\n\n                if dataset == \"MNIST\" and model == \"MLP\":\n                    model = MNISTModelMLP()\n                elif dataset == \"MNIST\" and model == \"CNN\":\n                    model = MNISTModelCNN()\n                # elif dataset == \"Syscall\" and model == \"MLP\":\n                #     model = SyscallModelMLP()\n                # else:\n                #     model = CIFAR10ModelCNN()\n\n                model.load_state_dict(lightning_model.state_dict())\n\n                with open(test_dataloader_file, \"rb\") as file:\n                    test_dataloader = pickle.load(file)\n\n                test_sample = next(iter(test_dataloader))\n\n                lr = factsheet[\"configuration\"][\"learning_rate\"]\n                value_clever = get_clever_score(model, test_sample, 10, lr)\n\n                factsheet[\"performance\"][\"test_clever\"] = 1 if value_clever &gt; 1 else value_clever\n\n                feature_importance = get_feature_importance_cv(model, test_sample)\n\n                factsheet[\"performance\"][\"test_feature_importance_cv\"] = 1 if feature_importance &gt; 1 else feature_importance\n\n                # Set emissions metrics\n                emissions = None if emissions_file is None else read_csv(emissions_file)\n                if emissions is not None:\n                    logging.info(\"FactSheet: Populating emissions\")\n                    cpu_spez_df = pd.read_csv(os.path.join(os.path.dirname(__file__), \"benchmarks\", \"CPU_benchmarks_v4.csv\"), header=0)\n                    emissions[\"CPU_model\"] = emissions[\"CPU_model\"].astype(str).str.replace(r\"\\([^)]*\\)\", \"\", regex=True)\n                    emissions[\"CPU_model\"] = emissions[\"CPU_model\"].astype(str).str.replace(r\" CPU\", \"\", regex=True)\n                    emissions[\"GPU_model\"] = emissions[\"GPU_model\"].astype(str).str.replace(r\"[0-9] x \", \"\", regex=True)\n                    emissions = pd.merge(emissions, cpu_spez_df[[\"cpuName\", \"powerPerf\"]], left_on=\"CPU_model\", right_on=\"cpuName\", how=\"left\")\n                    gpu_spez_df = pd.read_csv(os.path.join(os.path.dirname(__file__), \"benchmarks\", \"GPU_benchmarks_v7.csv\"), header=0)\n                    emissions = pd.merge(emissions, gpu_spez_df[[\"gpuName\", \"powerPerformance\"]], left_on=\"GPU_model\", right_on=\"gpuName\", how=\"left\")\n\n                    emissions.drop(\"cpuName\", axis=1, inplace=True)\n                    emissions.drop(\"gpuName\", axis=1, inplace=True)\n                    emissions[\"powerPerf\"] = emissions[\"powerPerf\"].astype(float)\n                    emissions[\"powerPerformance\"] = emissions[\"powerPerformance\"].astype(float)\n                    client_emissions = emissions.loc[emissions[\"role\"] == \"trainer\"]\n                    client_avg_carbon_intensity = round(client_emissions[\"energy_grid\"].mean(), 2)\n                    factsheet[\"sustainability\"][\"avg_carbon_intensity_clients\"] = check_field_filled(factsheet, [\"sustainability\", \"avg_carbon_intensity_clients\"], client_avg_carbon_intensity, \"\")\n                    factsheet[\"sustainability\"][\"emissions_training\"] = check_field_filled(factsheet, [\"sustainability\", \"emissions_training\"], client_emissions[\"emissions\"].sum(), \"\")\n                    factsheet[\"participants\"][\"avg_dataset_size\"] = check_field_filled(factsheet, [\"participants\", \"avg_dataset_size\"], client_emissions[\"sample_size\"].mean(), \"\")\n                    GPU_powerperf = (client_emissions.loc[client_emissions[\"GPU_used\"] == True])[\"powerPerformance\"]\n                    CPU_powerperf = (client_emissions.loc[client_emissions[\"CPU_used\"] == True])[\"powerPerf\"]\n                    clients_power_performance = round(pd.concat([GPU_powerperf, CPU_powerperf]).mean(), 2)\n                    factsheet[\"sustainability\"][\"avg_power_performance_clients\"] = check_field_filled(factsheet, [\"sustainability\", \"avg_power_performance_clients\"], clients_power_performance, \"\")\n\n                    server_emissions = emissions.loc[emissions[\"role\"] == \"server\"]\n                    server_avg_carbon_intensity = round(server_emissions[\"energy_grid\"].mean(), 2)\n                    factsheet[\"sustainability\"][\"avg_carbon_intensity_server\"] = check_field_filled(factsheet, [\"sustainability\", \"avg_carbon_intensity_server\"], server_avg_carbon_intensity, \"\")\n                    factsheet[\"sustainability\"][\"emissions_aggregation\"] = check_field_filled(factsheet, [\"sustainability\", \"emissions_aggregation\"], server_emissions[\"emissions\"].sum(), \"\")\n                    GPU_powerperf = (server_emissions.loc[server_emissions[\"GPU_used\"] == True])[\"powerPerformance\"]\n                    CPU_powerperf = (server_emissions.loc[server_emissions[\"CPU_used\"] == True])[\"powerPerf\"]\n                    server_power_performance = round(pd.concat([GPU_powerperf, CPU_powerperf]).mean(), 2)\n                    factsheet[\"sustainability\"][\"avg_power_performance_server\"] = check_field_filled(factsheet, [\"sustainability\", \"avg_power_performance_server\"], server_power_performance, \"\")\n\n                    factsheet[\"sustainability\"][\"emissions_communication_uplink\"] = check_field_filled(factsheet, [\"sustainability\", \"emissions_communication_uplink\"], factsheet[\"system\"][\"total_upload_bytes\"] * 2.24e-10 * factsheet[\"sustainability\"][\"avg_carbon_intensity_clients\"], \"\")\n                    factsheet[\"sustainability\"][\"emissions_communication_downlink\"] = check_field_filled(factsheet, [\"sustainability\", \"emissions_communication_downlink\"], factsheet[\"system\"][\"total_download_bytes\"] * 2.24e-10 * factsheet[\"sustainability\"][\"avg_carbon_intensity_server\"], \"\")\n\n                f.seek(0)\n                f.truncate()\n                json.dump(factsheet, f, indent=4)\n\n            except JSONDecodeError as e:\n                logging.info(f\"{factsheet_file} is invalid\")\n                logging.error(e)\n</code></pre>"},{"location":"api/addons/trustworthiness/factsheet/#nebula.addons.trustworthiness.factsheet.Factsheet.__init__","title":"<code>__init__()</code>","text":"<p>Manager class to populate the FactSheet</p> Source code in <code>nebula/addons/trustworthiness/factsheet.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Manager class to populate the FactSheet\n    \"\"\"\n    self.factsheet_file_nm = \"factsheet.json\"\n    self.factsheet_template_file_nm = \"factsheet_template.json\"\n</code></pre>"},{"location":"api/addons/trustworthiness/factsheet/#nebula.addons.trustworthiness.factsheet.Factsheet.populate_factsheet_post_train","title":"<code>populate_factsheet_post_train(scenario_name, start_time, end_time)</code>","text":"<p>Populates the factsheet with values after the training.</p> <p>Parameters:</p> Name Type Description Default <code>scenario</code> <code>object</code> <p>The scenario object.</p> required Source code in <code>nebula/addons/trustworthiness/factsheet.py</code> <pre><code>def populate_factsheet_post_train(self, scenario_name, start_time, end_time):\n    \"\"\"\n    Populates the factsheet with values after the training.\n\n    Args:\n        scenario (object): The scenario object.\n    \"\"\"\n    factsheet_file = os.path.join(f\"{os.environ.get('NEBULA_LOGS_DIR')}{scenario_name}/trustworthiness/{self.factsheet_file_nm}\")\n\n    logging.info(\"FactSheet: Populating factsheet with post training metrics\")\n\n    with open(factsheet_file, \"r+\") as f:\n        factsheet = {}\n        try:\n            factsheet = json.load(f)\n\n            dataset = factsheet[\"data\"][\"provenance\"]\n            model = factsheet[\"configuration\"][\"training_model\"]\n\n            files_dir = f\"{os.environ.get('NEBULA_LOGS_DIR')}/{scenario_name}/trustworthiness\"\n\n            models_files = glob.glob(os.path.join(files_dir, \"*final_model*\"))\n            #dataloaders_files = glob.glob(os.path.join(files_dir, \"*train_loader*\"))\n            test_dataloader_file = f\"{files_dir}/participant_1_test_loader.pk\"\n            train_model_file = f\"{files_dir}/participant_1_train_model.pk\"\n            emissions_file = os.path.join(files_dir, \"emissions.csv\")\n\n            # # Entropy\n            # i = 0\n            # for file in dataloaders_files:\n            #     with open(file, \"rb\") as file:\n            #         dataloader = pickle.load(file)\n            #     get_entropy(i, scenario_name, dataloader)\n            #     i += 1\n\n            get_all_data_entropy(scenario_name)\n\n            with open(f\"{files_dir}/entropy.json\", \"r\") as file:\n                entropy_distribution = json.load(file)\n\n            values = np.array(list(entropy_distribution.values()))\n\n            normalized_values = (values - np.min(values)) / (np.max(values) - np.min(values))\n\n            avg_entropy = np.mean(normalized_values)\n\n            factsheet[\"data\"][\"avg_entropy\"] = avg_entropy\n\n            # Set performance data\n            result_avg_loss_accuracy = get_avg_loss_accuracy(scenario_name)\n            factsheet[\"performance\"][\"test_loss_avg\"] = result_avg_loss_accuracy[0]\n            factsheet[\"performance\"][\"test_acc_avg\"] = result_avg_loss_accuracy[1]\n            test_acc_cv = get_cv(std=result_avg_loss_accuracy[2], mean=result_avg_loss_accuracy[1])\n            factsheet[\"fairness\"][\"test_acc_cv\"] = 1 if test_acc_cv &gt; 1 else test_acc_cv\n\n            factsheet[\"system\"][\"avg_time_minutes\"] = get_elapsed_time(start_time, end_time)\n            factsheet[\"system\"][\"avg_model_size\"] = get_bytes_models(models_files)\n\n            result_bytes_sent_recv = get_bytes_sent_recv(scenario_name)\n            factsheet[\"system\"][\"total_upload_bytes\"] = result_bytes_sent_recv[0]\n            factsheet[\"system\"][\"total_download_bytes\"] = result_bytes_sent_recv[1]\n            factsheet[\"system\"][\"avg_upload_bytes\"] = result_bytes_sent_recv[2]\n            factsheet[\"system\"][\"avg_download_bytes\"] = result_bytes_sent_recv[3]\n\n            factsheet[\"fairness\"][\"selection_cv\"] = 1\n\n            count_all_class_samples(scenario_name)\n\n            with open(f\"{files_dir}/count_class.json\", \"r\") as file:\n                class_distribution = json.load(file)\n\n            class_samples_sizes = [x for x in class_distribution.values()]\n            class_imbalance = get_cv(list=class_samples_sizes)\n            factsheet[\"fairness\"][\"class_imbalance\"] = 1 if class_imbalance &gt; 1 else class_imbalance\n\n            with open(train_model_file, \"rb\") as file:\n                lightning_model = pickle.load(file)\n\n            if dataset == \"MNIST\" and model == \"MLP\":\n                model = MNISTModelMLP()\n            elif dataset == \"MNIST\" and model == \"CNN\":\n                model = MNISTModelCNN()\n            # elif dataset == \"Syscall\" and model == \"MLP\":\n            #     model = SyscallModelMLP()\n            # else:\n            #     model = CIFAR10ModelCNN()\n\n            model.load_state_dict(lightning_model.state_dict())\n\n            with open(test_dataloader_file, \"rb\") as file:\n                test_dataloader = pickle.load(file)\n\n            test_sample = next(iter(test_dataloader))\n\n            lr = factsheet[\"configuration\"][\"learning_rate\"]\n            value_clever = get_clever_score(model, test_sample, 10, lr)\n\n            factsheet[\"performance\"][\"test_clever\"] = 1 if value_clever &gt; 1 else value_clever\n\n            feature_importance = get_feature_importance_cv(model, test_sample)\n\n            factsheet[\"performance\"][\"test_feature_importance_cv\"] = 1 if feature_importance &gt; 1 else feature_importance\n\n            # Set emissions metrics\n            emissions = None if emissions_file is None else read_csv(emissions_file)\n            if emissions is not None:\n                logging.info(\"FactSheet: Populating emissions\")\n                cpu_spez_df = pd.read_csv(os.path.join(os.path.dirname(__file__), \"benchmarks\", \"CPU_benchmarks_v4.csv\"), header=0)\n                emissions[\"CPU_model\"] = emissions[\"CPU_model\"].astype(str).str.replace(r\"\\([^)]*\\)\", \"\", regex=True)\n                emissions[\"CPU_model\"] = emissions[\"CPU_model\"].astype(str).str.replace(r\" CPU\", \"\", regex=True)\n                emissions[\"GPU_model\"] = emissions[\"GPU_model\"].astype(str).str.replace(r\"[0-9] x \", \"\", regex=True)\n                emissions = pd.merge(emissions, cpu_spez_df[[\"cpuName\", \"powerPerf\"]], left_on=\"CPU_model\", right_on=\"cpuName\", how=\"left\")\n                gpu_spez_df = pd.read_csv(os.path.join(os.path.dirname(__file__), \"benchmarks\", \"GPU_benchmarks_v7.csv\"), header=0)\n                emissions = pd.merge(emissions, gpu_spez_df[[\"gpuName\", \"powerPerformance\"]], left_on=\"GPU_model\", right_on=\"gpuName\", how=\"left\")\n\n                emissions.drop(\"cpuName\", axis=1, inplace=True)\n                emissions.drop(\"gpuName\", axis=1, inplace=True)\n                emissions[\"powerPerf\"] = emissions[\"powerPerf\"].astype(float)\n                emissions[\"powerPerformance\"] = emissions[\"powerPerformance\"].astype(float)\n                client_emissions = emissions.loc[emissions[\"role\"] == \"trainer\"]\n                client_avg_carbon_intensity = round(client_emissions[\"energy_grid\"].mean(), 2)\n                factsheet[\"sustainability\"][\"avg_carbon_intensity_clients\"] = check_field_filled(factsheet, [\"sustainability\", \"avg_carbon_intensity_clients\"], client_avg_carbon_intensity, \"\")\n                factsheet[\"sustainability\"][\"emissions_training\"] = check_field_filled(factsheet, [\"sustainability\", \"emissions_training\"], client_emissions[\"emissions\"].sum(), \"\")\n                factsheet[\"participants\"][\"avg_dataset_size\"] = check_field_filled(factsheet, [\"participants\", \"avg_dataset_size\"], client_emissions[\"sample_size\"].mean(), \"\")\n                GPU_powerperf = (client_emissions.loc[client_emissions[\"GPU_used\"] == True])[\"powerPerformance\"]\n                CPU_powerperf = (client_emissions.loc[client_emissions[\"CPU_used\"] == True])[\"powerPerf\"]\n                clients_power_performance = round(pd.concat([GPU_powerperf, CPU_powerperf]).mean(), 2)\n                factsheet[\"sustainability\"][\"avg_power_performance_clients\"] = check_field_filled(factsheet, [\"sustainability\", \"avg_power_performance_clients\"], clients_power_performance, \"\")\n\n                server_emissions = emissions.loc[emissions[\"role\"] == \"server\"]\n                server_avg_carbon_intensity = round(server_emissions[\"energy_grid\"].mean(), 2)\n                factsheet[\"sustainability\"][\"avg_carbon_intensity_server\"] = check_field_filled(factsheet, [\"sustainability\", \"avg_carbon_intensity_server\"], server_avg_carbon_intensity, \"\")\n                factsheet[\"sustainability\"][\"emissions_aggregation\"] = check_field_filled(factsheet, [\"sustainability\", \"emissions_aggregation\"], server_emissions[\"emissions\"].sum(), \"\")\n                GPU_powerperf = (server_emissions.loc[server_emissions[\"GPU_used\"] == True])[\"powerPerformance\"]\n                CPU_powerperf = (server_emissions.loc[server_emissions[\"CPU_used\"] == True])[\"powerPerf\"]\n                server_power_performance = round(pd.concat([GPU_powerperf, CPU_powerperf]).mean(), 2)\n                factsheet[\"sustainability\"][\"avg_power_performance_server\"] = check_field_filled(factsheet, [\"sustainability\", \"avg_power_performance_server\"], server_power_performance, \"\")\n\n                factsheet[\"sustainability\"][\"emissions_communication_uplink\"] = check_field_filled(factsheet, [\"sustainability\", \"emissions_communication_uplink\"], factsheet[\"system\"][\"total_upload_bytes\"] * 2.24e-10 * factsheet[\"sustainability\"][\"avg_carbon_intensity_clients\"], \"\")\n                factsheet[\"sustainability\"][\"emissions_communication_downlink\"] = check_field_filled(factsheet, [\"sustainability\", \"emissions_communication_downlink\"], factsheet[\"system\"][\"total_download_bytes\"] * 2.24e-10 * factsheet[\"sustainability\"][\"avg_carbon_intensity_server\"], \"\")\n\n            f.seek(0)\n            f.truncate()\n            json.dump(factsheet, f, indent=4)\n\n        except JSONDecodeError as e:\n            logging.info(f\"{factsheet_file} is invalid\")\n            logging.error(e)\n</code></pre>"},{"location":"api/addons/trustworthiness/factsheet/#nebula.addons.trustworthiness.factsheet.Factsheet.populate_factsheet_pre_train","title":"<code>populate_factsheet_pre_train(data, scenario_name)</code>","text":"<p>Populates the factsheet with values before the training.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>Contains the data from the scenario.</p> required <code>scenario_name</code> <code>string</code> <p>The name of the scenario.</p> required Source code in <code>nebula/addons/trustworthiness/factsheet.py</code> <pre><code>def populate_factsheet_pre_train(self, data, scenario_name):\n    \"\"\"\n    Populates the factsheet with values before the training.\n\n    Args:\n        data (dict): Contains the data from the scenario.\n        scenario_name (string): The name of the scenario.\n    \"\"\"\n\n    factsheet_file = os.path.join(os.environ.get('NEBULA_LOGS_DIR'), scenario_name, \"trustworthiness\", self.factsheet_file_nm)\n\n    factsheet_template = os.path.join(dirname, \"configs\", self.factsheet_template_file_nm)\n\n    if not os.path.exists(factsheet_file):\n        shutil.copyfile(factsheet_template, factsheet_file)\n\n    with open(factsheet_file, \"r+\") as f:\n        factsheet = {}\n\n        try:\n            factsheet = json.load(f)\n\n            if data is not None:\n                logging.info(\"FactSheet: Populating factsheet with pre training metrics\")\n\n                federation = data[\"federation\"]\n                n_nodes = int(data[\"n_nodes\"])\n                dataset = data[\"dataset\"]\n                algorithm = data[\"model\"]\n                aggregation_algorithm = data[\"agg_algorithm\"]\n                n_rounds = int(data[\"rounds\"])\n                attack = data[\"attack_params\"][\"attacks\"]\n                if attack != \"No Attack\":\n                    poisoned_node_percent = int(data[\"attack_params\"][\"poisoned_node_percent\"])\n                    poisoned_sample_percent = int(data[\"attack_params\"][\"poisoned_sample_percent\"])\n                    poisoned_noise_percent = int(data[\"attack_params\"][\"poisoned_noise_percent\"])\n                else:\n                    poisoned_node_percent = 0\n                    poisoned_sample_percent = 0\n                    poisoned_noise_percent = 0\n                with_reputation = data[\"reputation\"][\"enabled\"]\n                is_dynamic_topology = False # data[\"is_dynamic_topology\"]\n                is_dynamic_aggregation = False # data[\"is_dynamic_aggregation\"]\n                target_aggregation = False # data[\"target_aggregation\"]\n\n                if attack != \"No Attack\" and with_reputation == True and is_dynamic_aggregation == True:\n                    background = f\"For the project setup, the most important aspects are the following: The federation architecture is {federation}, involving {n_nodes} clients, the dataset used is {dataset}, the learning algorithm is {algorithm}, the aggregation algorithm is {aggregation_algorithm} and the number of rounds is {n_rounds}. In addition, the type of attack used against the clients is {attack}, where the percentage of attacked nodes is {poisoned_node_percent}, the percentage of attacked samples of each node is {poisoned_sample_percent}, and the percent of poisoned noise is {poisoned_noise_percent}. A reputation-based defence with a dynamic aggregation based on the aggregation algorithm {target_aggregation} is used, and the trustworthiness of the project is desired.\"\n\n                elif attack != \"No Attack\" and with_reputation == True and is_dynamic_topology == True:\n                    background = f\"For the project setup, the most important aspects are the following: The federation architecture is {federation}, involving {n_nodes} clients, the dataset used is {dataset}, the learning algorithm is {algorithm}, the aggregation algorithm is {aggregation_algorithm} and the number of rounds is {n_rounds}. In addition, the type of attack used against the clients is {attack}, where the percentage of attacked nodes is {poisoned_node_percent}, the percentage of attacked samples of each node is {poisoned_sample_percent}, and the percent of poisoned noise is {poisoned_noise_percent}. A reputation-based defence with a dynamic topology is used, and the trustworthiness of the project is desired.\"\n\n                elif attack != \"No Attack\" and with_reputation == False:\n                    background = f\"For the project setup, the most important aspects are the following: The federation architecture is {federation}, involving {n_nodes} clients, the dataset used is {dataset}, the learning algorithm is {algorithm}, the aggregation algorithm is {aggregation_algorithm} and the number of rounds is {n_rounds}. In addition, the type of attack used against the clients is {attack}, where the percentage of attacked nodes is {poisoned_node_percent}, the percentage of attacked samples of each node is {poisoned_sample_percent}, and the percent of poisoned noise is {poisoned_noise_percent}. No defence mechanism is used, and the trustworthiness of the project is desired.\"\n\n                elif attack == \"No Attack\":\n                    background = f\"For the project setup, the most important aspects are the following: The federation architecture is {federation}, involving {n_nodes} clients, the dataset used is {dataset}, the learning algorithm is {algorithm}, the aggregation algorithm is {aggregation_algorithm} and the number of rounds is {n_rounds}. No attacks against clients are used, and the trustworthiness of the project is desired.\"\n\n                # Set project specifications\n                factsheet[\"project\"][\"overview\"] = data[\"scenario_title\"]\n                factsheet[\"project\"][\"purpose\"] = data[\"scenario_description\"]\n                factsheet[\"project\"][\"background\"] = background\n\n                # Set data specifications\n                factsheet[\"data\"][\"provenance\"] = data[\"dataset\"]\n                factsheet[\"data\"][\"preprocessing\"] = data[\"topology\"]\n\n                # Set participants\n                factsheet[\"participants\"][\"client_num\"] = data[\"n_nodes\"] or \"\"\n                factsheet[\"participants\"][\"sample_client_rate\"] = 1\n                factsheet[\"participants\"][\"client_selector\"] = \"\"\n\n                # Set configuration\n                factsheet[\"configuration\"][\"aggregation_algorithm\"] = data[\"agg_algorithm\"] or \"\"\n                factsheet[\"configuration\"][\"training_model\"] = data[\"model\"] or \"\"\n                factsheet[\"configuration\"][\"personalization\"] = False\n                factsheet[\"configuration\"][\"visualization\"] = True\n                factsheet[\"configuration\"][\"total_round_num\"] = n_rounds\n\n                if poisoned_noise_percent != 0:\n                    factsheet[\"configuration\"][\"differential_privacy\"] = True\n                    factsheet[\"configuration\"][\"dp_epsilon\"] = poisoned_noise_percent\n                else:\n                    factsheet[\"configuration\"][\"differential_privacy\"] = False\n                    factsheet[\"configuration\"][\"dp_epsilon\"] = \"\"\n\n                if dataset == \"MNIST\" and algorithm == \"MLP\":\n                    model = MNISTModelMLP()\n                elif dataset == \"MNIST\" and algorithm == \"CNN\":\n                    model = MNISTModelCNN()\n                # elif dataset == \"Syscall\" and algorithm == \"MLP\":\n                #     model = SyscallModelMLP()\n                # else:\n                #     model = CIFAR10ModelCNN()\n\n                factsheet[\"configuration\"][\"learning_rate\"] = model.get_learning_rate()\n                factsheet[\"configuration\"][\"trainable_param_num\"] = model.count_parameters()\n                factsheet[\"configuration\"][\"local_update_steps\"] = 1\n\n                f.seek(0)\n                f.truncate()\n                json.dump(factsheet, f, indent=4)\n\n        except JSONDecodeError as e:\n            logging.warning(f\"{factsheet_file} is invalid\")\n            logging.error(e)\n</code></pre>"},{"location":"api/addons/trustworthiness/graphics/","title":"Documentation for Graphics Module","text":""},{"location":"api/addons/trustworthiness/metric/","title":"Documentation for Metric Module","text":""},{"location":"api/addons/trustworthiness/metric/#nebula.addons.trustworthiness.metric.TrustMetricManager","title":"<code>TrustMetricManager</code>","text":"<p>Manager class to help store the output directory and handle calls from the FL framework.</p> Source code in <code>nebula/addons/trustworthiness/metric.py</code> <pre><code>class TrustMetricManager:\n    \"\"\"\n    Manager class to help store the output directory and handle calls from the FL framework.\n    \"\"\"\n\n    def __init__(self, scenario_start_time):\n        self.factsheet_file_nm = \"factsheet.json\"\n        self.eval_metrics_file_nm = \"eval_metrics.json\"\n        self.nebula_trust_results_nm = \"nebula_trust_results.json\"\n        self.scenario_start_time = scenario_start_time\n\n    def evaluate(self, experiment_name, weights, use_weights=False):\n        \"\"\"\n        Evaluates the trustworthiness score.\n\n        Args:\n            scenario (object): The scenario in whith the trustworthiness will be calculated.\n            weights (dict): The desired weghts of the pillars.\n            use_weights (bool): True to turn on the weights in the metric config file, default to False.\n        \"\"\"\n        # Get scenario name\n        scenario_name = experiment_name\n        factsheet_file = os.path.join(os.environ.get('NEBULA_LOGS_DIR'), scenario_name, \"trustworthiness\", self.factsheet_file_nm)\n        metrics_cfg_file = os.path.join(dirname, \"configs\", self.eval_metrics_file_nm)\n        results_file = os.path.join(os.environ.get('NEBULA_LOGS_DIR'), scenario_name, \"trustworthiness\", self.nebula_trust_results_nm)\n\n        if not os.path.exists(factsheet_file):\n            logger.error(f\"{factsheet_file} is missing! Please check documentation.\")\n            return\n\n        if not os.path.exists(metrics_cfg_file):\n            logger.error(f\"{metrics_cfg_file} is missing! Please check documentation.\")\n            return\n\n        with open(factsheet_file, \"r\") as f, open(metrics_cfg_file, \"r\") as m:\n            factsheet = json.load(f)\n            metrics_cfg = json.load(m)\n            metrics = metrics_cfg.items()\n            input_docs = {\"factsheet\": factsheet}\n\n            result_json = {\"trust_score\": 0, \"pillars\": []}\n            final_score = 0\n            result_print = []\n            for key, value in metrics:\n                pillar = TrustPillar(key, value, input_docs, use_weights)\n                score, result = pillar.evaluate()\n                weight = weights.get(key) / 100\n                final_score += weight * score\n                result_print.append([key, score])\n                result_json[\"pillars\"].append(result)\n            final_score = round(final_score, 2)\n            result_json[\"trust_score\"] = final_score\n            write_results_json(results_file, result_json)\n\n            graphics = Graphics(self.scenario_start_time, scenario_name)\n            graphics.graphics()\n</code></pre>"},{"location":"api/addons/trustworthiness/metric/#nebula.addons.trustworthiness.metric.TrustMetricManager.evaluate","title":"<code>evaluate(experiment_name, weights, use_weights=False)</code>","text":"<p>Evaluates the trustworthiness score.</p> <p>Parameters:</p> Name Type Description Default <code>scenario</code> <code>object</code> <p>The scenario in whith the trustworthiness will be calculated.</p> required <code>weights</code> <code>dict</code> <p>The desired weghts of the pillars.</p> required <code>use_weights</code> <code>bool</code> <p>True to turn on the weights in the metric config file, default to False.</p> <code>False</code> Source code in <code>nebula/addons/trustworthiness/metric.py</code> <pre><code>def evaluate(self, experiment_name, weights, use_weights=False):\n    \"\"\"\n    Evaluates the trustworthiness score.\n\n    Args:\n        scenario (object): The scenario in whith the trustworthiness will be calculated.\n        weights (dict): The desired weghts of the pillars.\n        use_weights (bool): True to turn on the weights in the metric config file, default to False.\n    \"\"\"\n    # Get scenario name\n    scenario_name = experiment_name\n    factsheet_file = os.path.join(os.environ.get('NEBULA_LOGS_DIR'), scenario_name, \"trustworthiness\", self.factsheet_file_nm)\n    metrics_cfg_file = os.path.join(dirname, \"configs\", self.eval_metrics_file_nm)\n    results_file = os.path.join(os.environ.get('NEBULA_LOGS_DIR'), scenario_name, \"trustworthiness\", self.nebula_trust_results_nm)\n\n    if not os.path.exists(factsheet_file):\n        logger.error(f\"{factsheet_file} is missing! Please check documentation.\")\n        return\n\n    if not os.path.exists(metrics_cfg_file):\n        logger.error(f\"{metrics_cfg_file} is missing! Please check documentation.\")\n        return\n\n    with open(factsheet_file, \"r\") as f, open(metrics_cfg_file, \"r\") as m:\n        factsheet = json.load(f)\n        metrics_cfg = json.load(m)\n        metrics = metrics_cfg.items()\n        input_docs = {\"factsheet\": factsheet}\n\n        result_json = {\"trust_score\": 0, \"pillars\": []}\n        final_score = 0\n        result_print = []\n        for key, value in metrics:\n            pillar = TrustPillar(key, value, input_docs, use_weights)\n            score, result = pillar.evaluate()\n            weight = weights.get(key) / 100\n            final_score += weight * score\n            result_print.append([key, score])\n            result_json[\"pillars\"].append(result)\n        final_score = round(final_score, 2)\n        result_json[\"trust_score\"] = final_score\n        write_results_json(results_file, result_json)\n\n        graphics = Graphics(self.scenario_start_time, scenario_name)\n        graphics.graphics()\n</code></pre>"},{"location":"api/addons/trustworthiness/pillar/","title":"Documentation for Pillar Module","text":""},{"location":"api/addons/trustworthiness/pillar/#nebula.addons.trustworthiness.pillar.TrustPillar","title":"<code>TrustPillar</code>","text":"<p>Class to represent a trust pillar.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>string</code> <p>Name of the pillar.</p> required <code>metrics</code> <code>dict</code> <p>Metric definitions for the pillar.</p> required <code>input_docs</code> <code>dict</code> <p>Input documents.</p> required <code>use_weights</code> <code>bool</code> <p>True to turn on the weights in the metric config file.</p> <code>False</code> Source code in <code>nebula/addons/trustworthiness/pillar.py</code> <pre><code>class TrustPillar:\n    \"\"\"\n    Class to represent a trust pillar.\n\n    Args:\n        name (string): Name of the pillar.\n        metrics (dict): Metric definitions for the pillar.\n        input_docs (dict): Input documents.\n        use_weights (bool): True to turn on the weights in the metric config file.\n\n    \"\"\"\n\n    def __init__(self, name, metrics, input_docs, use_weights=False):\n        self.name = name\n        self.input_docs = input_docs\n        self.metrics = metrics\n        self.result = []\n        self.use_weights = use_weights\n\n    def evaluate(self):\n        \"\"\"\n        Evaluate the trust score for the pillar.\n\n        Returns:\n            float: Score of [0, 1].\n        \"\"\"\n        score = 0\n        avg_weight = 1 / len(self.metrics)\n        for key, value in self.metrics.items():\n            weight = value.get(\"weight\", avg_weight) if self.use_weights else avg_weight\n            score += weight * self.get_notion_score(key, value.get(\"metrics\"))\n        score = round(score, 2)\n        return score, {self.name: {\"score\": score, \"notions\": self.result}}\n\n    def get_notion_score(self, name, metrics):\n        \"\"\"\n        Evaluate the trust score for the notion.\n\n        Args:\n            name (string): Name of the notion.\n            metrics (list): Metrics definitions of the notion.\n\n        Returns:\n            float: Score of [0, 1].\n        \"\"\"\n\n        notion_score = 0\n        avg_weight = 1 / len(metrics)\n        metrics_result = []\n        for key, value in metrics.items():\n            metric_score = self.get_metric_score(metrics_result, key, value)\n            weight = value.get(\"weight\", avg_weight) if self.use_weights else avg_weight\n            notion_score += weight * float(metric_score)\n        self.result.append({name: {\"score\": notion_score, \"metrics\": metrics_result}})\n        return notion_score\n\n    def get_metric_score(self, result, name, metric):\n        \"\"\"\n        Evaluate the trust score for the metric.\n\n        Args:\n            result (object): The result object\n            name (string): Name of the metric.\n            metrics (dict): The metric definition.\n\n        Returns:\n            float: Score of [0, 1].\n        \"\"\"\n\n        score = 0\n        try:\n            input_value = get_input_value(self.input_docs, metric.get(\"inputs\"), metric.get(\"operation\"))\n\n            score_type = metric.get(\"type\")\n            if input_value is None:\n                logger.warning(f\"{name} input value is null\")\n            else:\n                if score_type == \"true_score\":\n                    score = calculation.get_true_score(input_value, metric.get(\"direction\"))\n                elif score_type == \"score_mapping\":\n                    score = calculation.get_mapped_score(input_value, metric.get(\"score_map\"))\n                elif score_type == \"ranges\":\n                    score = calculation.get_range_score(input_value, metric.get(\"ranges\"), metric.get(\"direction\"))\n                elif score_type == \"score_map_value\":\n                    score = calculation.get_map_value_score(input_value, metric.get(\"score_map\"))\n                elif score_type == \"scaled_score\":\n                    score = calculation.get_scaled_score(input_value, metric.get(\"scale\"), metric.get(\"direction\"))\n                elif score_type == \"property_check\":\n                    score = 0 if input_value is None else input_value\n\n                else:\n                    logger.warning(f\"The score type {score_type} is not yet implemented.\")\n\n        except KeyError:\n            logger.warning(f\"Null input for {name} metric\")\n        score = round(score, 2)\n        result.append({name: {\"score\": score}})\n        return score\n</code></pre>"},{"location":"api/addons/trustworthiness/pillar/#nebula.addons.trustworthiness.pillar.TrustPillar.evaluate","title":"<code>evaluate()</code>","text":"<p>Evaluate the trust score for the pillar.</p> <p>Returns:</p> Name Type Description <code>float</code> <p>Score of [0, 1].</p> Source code in <code>nebula/addons/trustworthiness/pillar.py</code> <pre><code>def evaluate(self):\n    \"\"\"\n    Evaluate the trust score for the pillar.\n\n    Returns:\n        float: Score of [0, 1].\n    \"\"\"\n    score = 0\n    avg_weight = 1 / len(self.metrics)\n    for key, value in self.metrics.items():\n        weight = value.get(\"weight\", avg_weight) if self.use_weights else avg_weight\n        score += weight * self.get_notion_score(key, value.get(\"metrics\"))\n    score = round(score, 2)\n    return score, {self.name: {\"score\": score, \"notions\": self.result}}\n</code></pre>"},{"location":"api/addons/trustworthiness/pillar/#nebula.addons.trustworthiness.pillar.TrustPillar.get_metric_score","title":"<code>get_metric_score(result, name, metric)</code>","text":"<p>Evaluate the trust score for the metric.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>object</code> <p>The result object</p> required <code>name</code> <code>string</code> <p>Name of the metric.</p> required <code>metrics</code> <code>dict</code> <p>The metric definition.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>Score of [0, 1].</p> Source code in <code>nebula/addons/trustworthiness/pillar.py</code> <pre><code>def get_metric_score(self, result, name, metric):\n    \"\"\"\n    Evaluate the trust score for the metric.\n\n    Args:\n        result (object): The result object\n        name (string): Name of the metric.\n        metrics (dict): The metric definition.\n\n    Returns:\n        float: Score of [0, 1].\n    \"\"\"\n\n    score = 0\n    try:\n        input_value = get_input_value(self.input_docs, metric.get(\"inputs\"), metric.get(\"operation\"))\n\n        score_type = metric.get(\"type\")\n        if input_value is None:\n            logger.warning(f\"{name} input value is null\")\n        else:\n            if score_type == \"true_score\":\n                score = calculation.get_true_score(input_value, metric.get(\"direction\"))\n            elif score_type == \"score_mapping\":\n                score = calculation.get_mapped_score(input_value, metric.get(\"score_map\"))\n            elif score_type == \"ranges\":\n                score = calculation.get_range_score(input_value, metric.get(\"ranges\"), metric.get(\"direction\"))\n            elif score_type == \"score_map_value\":\n                score = calculation.get_map_value_score(input_value, metric.get(\"score_map\"))\n            elif score_type == \"scaled_score\":\n                score = calculation.get_scaled_score(input_value, metric.get(\"scale\"), metric.get(\"direction\"))\n            elif score_type == \"property_check\":\n                score = 0 if input_value is None else input_value\n\n            else:\n                logger.warning(f\"The score type {score_type} is not yet implemented.\")\n\n    except KeyError:\n        logger.warning(f\"Null input for {name} metric\")\n    score = round(score, 2)\n    result.append({name: {\"score\": score}})\n    return score\n</code></pre>"},{"location":"api/addons/trustworthiness/pillar/#nebula.addons.trustworthiness.pillar.TrustPillar.get_notion_score","title":"<code>get_notion_score(name, metrics)</code>","text":"<p>Evaluate the trust score for the notion.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>string</code> <p>Name of the notion.</p> required <code>metrics</code> <code>list</code> <p>Metrics definitions of the notion.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>Score of [0, 1].</p> Source code in <code>nebula/addons/trustworthiness/pillar.py</code> <pre><code>def get_notion_score(self, name, metrics):\n    \"\"\"\n    Evaluate the trust score for the notion.\n\n    Args:\n        name (string): Name of the notion.\n        metrics (list): Metrics definitions of the notion.\n\n    Returns:\n        float: Score of [0, 1].\n    \"\"\"\n\n    notion_score = 0\n    avg_weight = 1 / len(metrics)\n    metrics_result = []\n    for key, value in metrics.items():\n        metric_score = self.get_metric_score(metrics_result, key, value)\n        weight = value.get(\"weight\", avg_weight) if self.use_weights else avg_weight\n        notion_score += weight * float(metric_score)\n    self.result.append({name: {\"score\": notion_score, \"metrics\": metrics_result}})\n    return notion_score\n</code></pre>"},{"location":"api/addons/trustworthiness/trustworthiness/","title":"Documentation for Trustworthiness Module","text":""},{"location":"api/addons/trustworthiness/trustworthiness/#nebula.addons.trustworthiness.trustworthiness.Trustworthiness","title":"<code>Trustworthiness</code>","text":"Source code in <code>nebula/addons/trustworthiness/trustworthiness.py</code> <pre><code>class Trustworthiness():\n    def __init__(self, engine: Engine, config: Config):\n        config.reset_logging_configuration()\n        print_msg_box(\n            msg=f\"Name Trustworthiness Module\\nRole: {engine.rb.get_role_name()}\",\n            indent=2,\n        )\n        self._engine = engine\n        self._config = config\n        self._trust_config = self._config.participant[\"trust_args\"][\"scenario\"]\n        self._experiment_name = self._config.participant[\"scenario_args\"][\"name\"]\n        self._trust_dir_files = f\"/nebula/app/logs/{self._experiment_name}/trustworthiness\"\n        self._emissions_file = 'emissions.csv'\n        self._role: Role = engine.rb.get_role()\n        self._idx = self._config.participant[\"device_args\"][\"idx\"]\n        self._trust_workload: TrustWorkload = self._factory_trust_workload(self._role, self._engine, self._idx, self._trust_dir_files)       \n\n        # EmissionsTracker from codecarbon to measure the emissions during the aggregation step in the server\n        self._tracker= EmissionsTracker(tracking_mode='process', log_level='error', save_to_file=False)\n\n    @property\n    def tw(self):\n        \"\"\"TrustWorkload depending on the node Role\"\"\"\n        return self._trust_workload\n\n    async def start(self):\n        await self._create_trustworthiness_directory()\n        await self.tw.init(self._experiment_name)\n        await EventManager.get_instance().subscribe_node_event(ExperimentFinishEvent, self._process_experiment_finish_event)\n        self._tracker.start()\n\n    async def _create_trustworthiness_directory(self):\n        import os\n        trust_dir = os.path.join(os.environ.get(\"NEBULA_LOGS_DIR\"), self._experiment_name, \"trustworthiness\")\n        # Create a directory to save files to calcutate trust\n        os.makedirs(trust_dir, exist_ok=True)\n        os.chmod(trust_dir, 0o777)\n\n    async def _process_experiment_finish_event(self, efe: ExperimentFinishEvent):\n        from nebula.addons.trustworthiness.utils import save_class_count_per_participant\n        class_counter = self._engine.trainer.datamodule.get_samples_per_label()\n        save_class_count_per_participant(self._experiment_name, class_counter, self._idx)\n\n        await self.tw.finish_experiment_role_pre_actions()\n\n        last_loss, last_accuracy = self.tw.get_metrics()\n\n        # Get bytes send/received from reporter\n        bytes_sent = self._engine.reporter.acc_bytes_sent\n        bytes_recv = self._engine.reporter.acc_bytes_recv\n\n        # Get TrustWorkload info\n        workload = self.tw.get_workload()\n        sample_size = self.tw.get_sample_size()\n\n        # Last operations\n        save_results_csv(self._experiment_name, self._idx, bytes_sent, bytes_recv, last_loss, last_accuracy)\n        stop_emissions_tracking_and_save(self._tracker, self._trust_dir_files, self._emissions_file, self._role.value, workload, sample_size)\n\n        await self.tw.finish_experiment_role_post_actions(self._trust_config, self._experiment_name)\n\n    def _factory_trust_workload(self, role: Role, engine: Engine, idx, trust_files_route) -&gt; TrustWorkload:  \n        trust_workloads = {\n            Role.TRAINER: TrustWorkloadTrainer, \n            Role.SERVER: TrustWorkloadServer\n        }\n        trust_workload = trust_workloads.get(role)\n        if trust_workload:\n            return trust_workload(engine, idx, trust_files_route)\n        else:\n            raise TrustWorkloadException(f\"Trustworthiness workload for role {role} not defined\")\n</code></pre>"},{"location":"api/addons/trustworthiness/trustworthiness/#nebula.addons.trustworthiness.trustworthiness.Trustworthiness.tw","title":"<code>tw</code>  <code>property</code>","text":"<p>TrustWorkload depending on the node Role</p>"},{"location":"api/addons/trustworthiness/utils/","title":"Documentation for Utils Module","text":""},{"location":"api/addons/trustworthiness/utils/#nebula.addons.trustworthiness.utils.check_field_filled","title":"<code>check_field_filled(factsheet_dict, factsheet_path, value, empty='')</code>","text":"<p>Check if the field in the factsheet file is filled or not.</p> <p>Parameters:</p> Name Type Description Default <code>factsheet_dict</code> <code>dict</code> <p>The factshett dict.</p> required <code>factsheet_path</code> <code>list</code> <p>The factsheet field to check.</p> required <code>value</code> <code>float</code> <p>The value to add in the field.</p> required <code>empty</code> <code>string</code> <p>If the value could not be appended, the empty string is returned.</p> <code>''</code> <p>Returns:</p> Name Type Description <code>float</code> <p>The value added in the factsheet or empty if the value could not be appened</p> Source code in <code>nebula/addons/trustworthiness/utils.py</code> <pre><code>def check_field_filled(factsheet_dict, factsheet_path, value, empty=\"\"):\n    \"\"\"\n    Check if the field in the factsheet file is filled or not.\n\n    Args:\n        factsheet_dict (dict): The factshett dict.\n        factsheet_path (list): The factsheet field to check.\n        value (float): The value to add in the field.\n        empty (string): If the value could not be appended, the empty string is returned.\n\n    Returns:\n        float: The value added in the factsheet or empty if the value could not be appened\n\n    \"\"\"\n    if factsheet_dict[factsheet_path[0]][factsheet_path[1]]:\n        return factsheet_dict[factsheet_path[0]][factsheet_path[1]]\n    elif value != \"\" and value != \"nan\":\n        if type(value) != str and type(value) != list:\n            if math.isnan(value):\n                return 0\n            else:\n                return value\n        else:\n            return value\n    else:\n        return empty\n</code></pre>"},{"location":"api/addons/trustworthiness/utils/#nebula.addons.trustworthiness.utils.count_class_samples","title":"<code>count_class_samples(scenario_name, dataloaders_files, class_counter=None)</code>","text":"<p>Counts the number of samples by class.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_name</code> <code>string</code> <p>Name of the scenario.</p> required <code>dataloaders_files</code> <code>list</code> <p>Files that contain the dataloaders.</p> required Source code in <code>nebula/addons/trustworthiness/utils.py</code> <pre><code>def count_class_samples(scenario_name, dataloaders_files, class_counter: Counter = None):\n    \"\"\"\n    Counts the number of samples by class.\n\n    Args:\n        scenario_name (string): Name of the scenario.\n        dataloaders_files (list): Files that contain the dataloaders.\n\n    \"\"\"\n\n    result = {}\n    dataloaders = []\n\n    if class_counter:\n        result = {hashids.encode(int(class_id)): count for class_id, count in class_counter.items()}\n    else:\n        for file in dataloaders_files:\n            with open(file, \"rb\") as f:\n                dataloader = pickle.load(f)\n                dataloaders.append(dataloader)\n\n        for dataloader in dataloaders:\n            for batch, labels in dataloader:\n                for b, label in zip(batch, labels):\n                    l = hashids.encode(label.item())\n                    if l in result:\n                        result[l] += 1\n                    else:\n                        result[l] = 1\n\n    try:\n        name_file = os.path.join(os.environ.get('NEBULA_LOGS_DIR'), scenario_name, \"trustworthiness\", \"count_class.json\")\n    except:\n        name_file = os.path.join(\"nebula\", \"app\", \"logs\", scenario_name, \"trustworthiness\", \"count_class.json\")\n\n    with open(name_file, \"w\") as f:\n        json.dump(result, f)\n</code></pre>"},{"location":"api/addons/trustworthiness/utils/#nebula.addons.trustworthiness.utils.get_entropy","title":"<code>get_entropy(client_id, scenario_name, dataloader)</code>","text":"<p>Get the entropy of each client in the scenario.</p> <p>Parameters:</p> Name Type Description Default <code>client_id</code> <code>int</code> <p>The client id.</p> required <code>scenario_name</code> <code>string</code> <p>Name of the scenario.</p> required <code>dataloaders_files</code> <code>list</code> <p>Files that contain the dataloaders.</p> required Source code in <code>nebula/addons/trustworthiness/utils.py</code> <pre><code>def get_entropy(client_id, scenario_name, dataloader):\n    \"\"\"\n    Get the entropy of each client in the scenario.\n\n    Args:\n        client_id (int): The client id.\n        scenario_name (string): Name of the scenario.\n        dataloaders_files (list): Files that contain the dataloaders.\n\n    \"\"\"\n    result = {}\n    client_entropy = {}\n\n    name_file = os.path.join(os.environ.get('NEBULA_LOGS_DIR'), scenario_name, \"trustworthiness\", \"entropy.json\")\n\n    if os.path.exists(name_file):\n        logging.info(f\"entropy fiel already exists.. loading.\")\n        with open(name_file, \"r\") as f:\n            client_entropy = json.load(f)\n\n    client_id_hash = hashids.encode(client_id)\n\n    for batch, labels in dataloader:\n        for b, label in zip(batch, labels):\n            l = hashids.encode(label.item())\n            if l in result:\n                result[l] += 1\n            else:\n                result[l] = 1\n\n    n = len(dataloader)\n    entropy_value = entropy([x / n for x in result.values()], base=2)\n    client_entropy[client_id_hash] = entropy_value\n    with open(name_file, \"w\") as f:\n        json.dump(client_entropy, f)\n</code></pre>"},{"location":"api/addons/trustworthiness/utils/#nebula.addons.trustworthiness.utils.get_input_value","title":"<code>get_input_value(input_docs, inputs, operation)</code>","text":"<p>Gets the input value from input document and apply the metric operation on the value.</p> <p>Parameters:</p> Name Type Description Default <code>inputs_docs</code> <code>map</code> <p>The input document map.</p> required <code>inputs</code> <code>list</code> <p>All the inputs.</p> required <code>operation</code> <code>string</code> <p>The metric operation.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The metric value</p> Source code in <code>nebula/addons/trustworthiness/utils.py</code> <pre><code>def get_input_value(input_docs, inputs, operation):\n    \"\"\"\n    Gets the input value from input document and apply the metric operation on the value.\n\n    Args:\n        inputs_docs (map): The input document map.\n        inputs (list): All the inputs.\n        operation (string): The metric operation.\n\n    Returns:\n        float: The metric value\n\n    \"\"\"\n\n    input_value = None\n    args = []\n    for i in inputs:\n        source = i.get(\"source\", \"\")\n        field = i.get(\"field_path\", \"\")\n        input_doc = input_docs.get(source, None)\n        if input_doc is None:\n            logger.warning(f\"{source} is null\")\n        else:\n            input = get_value_from_path(input_doc, field)\n            args.append(input)\n    try:\n        operationFn = getattr(calculation, operation)\n        input_value = operationFn(*args)\n    except TypeError:\n        logger.warning(f\"{operation} is not valid\")\n\n    return input_value\n</code></pre>"},{"location":"api/addons/trustworthiness/utils/#nebula.addons.trustworthiness.utils.get_value_from_path","title":"<code>get_value_from_path(input_doc, path)</code>","text":"<p>Gets the input value from input document by path.</p> <p>Parameters:</p> Name Type Description Default <code>inputs_doc</code> <code>map</code> <p>The input document map.</p> required <code>path</code> <code>string</code> <p>The field name of the input value of interest.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The input value from the input document</p> Source code in <code>nebula/addons/trustworthiness/utils.py</code> <pre><code>def get_value_from_path(input_doc, path):\n    \"\"\"\n    Gets the input value from input document by path.\n\n    Args:\n        inputs_doc (map): The input document map.\n        path (string): The field name of the input value of interest.\n\n    Returns:\n        float: The input value from the input document\n\n    \"\"\"\n\n    d = input_doc\n    for nested_key in path.split(\"/\"):\n        temp = d.get(nested_key)\n        if isinstance(temp, dict):\n            d = d.get(nested_key)\n        else:\n            return temp\n    return None\n</code></pre>"},{"location":"api/addons/trustworthiness/utils/#nebula.addons.trustworthiness.utils.read_csv","title":"<code>read_csv(filename)</code>","text":"<p>Read a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>string</code> <p>Name of the file.</p> required <p>Returns:</p> Name Type Description <code>object</code> <p>The CSV readed.</p> Source code in <code>nebula/addons/trustworthiness/utils.py</code> <pre><code>def read_csv(filename):\n    \"\"\"\n    Read a CSV file.\n\n    Args:\n        filename (string): Name of the file.\n\n    Returns:\n        object: The CSV readed.\n\n    \"\"\"\n    if exists(filename):\n        return pd.read_csv(filename)\n</code></pre>"},{"location":"api/addons/trustworthiness/utils/#nebula.addons.trustworthiness.utils.write_results_json","title":"<code>write_results_json(out_file, dict)</code>","text":"<p>Writes the result to JSON.</p> <p>Parameters:</p> Name Type Description Default <code>out_file</code> <code>string</code> <p>The output file.</p> required <code>dict</code> <code>dict</code> <p>The object to be witten into JSON.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The input value from the input document</p> Source code in <code>nebula/addons/trustworthiness/utils.py</code> <pre><code>def write_results_json(out_file, dict):\n    \"\"\"\n    Writes the result to JSON.\n\n    Args:\n        out_file (string): The output file.\n        dict (dict): The object to be witten into JSON.\n\n    Returns:\n        float: The input value from the input document\n\n    \"\"\"\n\n    with open(out_file, \"a\") as f:\n        json.dump(dict, f, indent=4)\n</code></pre>"},{"location":"api/addons/waf/","title":"Documentation for Waf Module","text":""},{"location":"api/controller/","title":"Documentation for Controller Module","text":""},{"location":"api/controller/controller/","title":"Documentation for Controller Module","text":""},{"location":"api/controller/controller/#nebula.controller.controller.TermEscapeCodeFormatter","title":"<code>TermEscapeCodeFormatter</code>","text":"<p>               Bases: <code>Formatter</code></p> <p>Custom logging formatter that removes ANSI terminal escape codes from log messages.</p> <p>This formatter is useful when you want to clean up log outputs by stripping out any terminal color codes or formatting sequences before logging them to a file or other non-terminal output.</p> <p>Attributes:</p> Name Type Description <code>fmt</code> <code>str</code> <p>Format string for the log message.</p> <code>datefmt</code> <code>str</code> <p>Format string for the date in the log message.</p> <code>style</code> <code>str</code> <p>Formatting style (default is '%').</p> <code>validate</code> <code>bool</code> <p>Whether to validate the format string.</p> <p>Methods:</p> Name Description <code>format</code> <p>Strips ANSI escape codes from the log message and formats it.</p> Source code in <code>nebula/controller/controller.py</code> <pre><code>class TermEscapeCodeFormatter(logging.Formatter):\n    \"\"\"\n    Custom logging formatter that removes ANSI terminal escape codes from log messages.\n\n    This formatter is useful when you want to clean up log outputs by stripping out\n    any terminal color codes or formatting sequences before logging them to a file\n    or other non-terminal output.\n\n    Attributes:\n        fmt (str): Format string for the log message.\n        datefmt (str): Format string for the date in the log message.\n        style (str): Formatting style (default is '%').\n        validate (bool): Whether to validate the format string.\n\n    Methods:\n        format(record): Strips ANSI escape codes from the log message and formats it.\n    \"\"\"\n\n    def __init__(self, fmt=None, datefmt=None, style=\"%\", validate=True):\n        \"\"\"\n        Initializes the TermEscapeCodeFormatter.\n\n        Args:\n            fmt (str, optional): The format string for the log message.\n            datefmt (str, optional): The format string for the date.\n            style (str, optional): The formatting style. Defaults to '%'.\n            validate (bool, optional): Whether to validate the format string. Defaults to True.\n        \"\"\"\n        super().__init__(fmt, datefmt, style, validate)\n\n    def format(self, record):\n        \"\"\"\n        Formats the specified log record, stripping out any ANSI escape codes.\n\n        Args:\n            record (logging.LogRecord): The log record to be formatted.\n\n        Returns:\n            str: The formatted log message with escape codes removed.\n        \"\"\"\n        escape_re = re.compile(r\"\\x1b\\[[0-9;]*m\")\n        record.msg = re.sub(escape_re, \"\", str(record.msg))\n        return super().format(record)\n</code></pre>"},{"location":"api/controller/controller/#nebula.controller.controller.TermEscapeCodeFormatter.__init__","title":"<code>__init__(fmt=None, datefmt=None, style='%', validate=True)</code>","text":"<p>Initializes the TermEscapeCodeFormatter.</p> <p>Parameters:</p> Name Type Description Default <code>fmt</code> <code>str</code> <p>The format string for the log message.</p> <code>None</code> <code>datefmt</code> <code>str</code> <p>The format string for the date.</p> <code>None</code> <code>style</code> <code>str</code> <p>The formatting style. Defaults to '%'.</p> <code>'%'</code> <code>validate</code> <code>bool</code> <p>Whether to validate the format string. Defaults to True.</p> <code>True</code> Source code in <code>nebula/controller/controller.py</code> <pre><code>def __init__(self, fmt=None, datefmt=None, style=\"%\", validate=True):\n    \"\"\"\n    Initializes the TermEscapeCodeFormatter.\n\n    Args:\n        fmt (str, optional): The format string for the log message.\n        datefmt (str, optional): The format string for the date.\n        style (str, optional): The formatting style. Defaults to '%'.\n        validate (bool, optional): Whether to validate the format string. Defaults to True.\n    \"\"\"\n    super().__init__(fmt, datefmt, style, validate)\n</code></pre>"},{"location":"api/controller/controller/#nebula.controller.controller.TermEscapeCodeFormatter.format","title":"<code>format(record)</code>","text":"<p>Formats the specified log record, stripping out any ANSI escape codes.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>LogRecord</code> <p>The log record to be formatted.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The formatted log message with escape codes removed.</p> Source code in <code>nebula/controller/controller.py</code> <pre><code>def format(self, record):\n    \"\"\"\n    Formats the specified log record, stripping out any ANSI escape codes.\n\n    Args:\n        record (logging.LogRecord): The log record to be formatted.\n\n    Returns:\n        str: The formatted log message with escape codes removed.\n    \"\"\"\n    escape_re = re.compile(r\"\\x1b\\[[0-9;]*m\")\n    record.msg = re.sub(escape_re, \"\", str(record.msg))\n    return super().format(record)\n</code></pre>"},{"location":"api/controller/controller/#nebula.controller.controller.add_user_controller","title":"<code>add_user_controller(user=Body(...), password=Body(...), role=Body(...))</code>  <code>async</code>","text":"<p>Endpoint to add a new user to the database.</p> <p>Body Parameters: - user: Username. - password: Password for the new user. - role: Role assigned to the user (e.g., \"admin\", \"user\").</p> <p>Returns a success message or an error if the user could not be added.</p> Source code in <code>nebula/controller/controller.py</code> <pre><code>@app.post(\"/user/add\")\nasync def add_user_controller(user: str = Body(...), password: str = Body(...), role: str = Body(...)):\n    \"\"\"\n    Endpoint to add a new user to the database.\n\n    Body Parameters:\n    - user: Username.\n    - password: Password for the new user.\n    - role: Role assigned to the user (e.g., \"admin\", \"user\").\n\n    Returns a success message or an error if the user could not be added.\n    \"\"\"\n    from nebula.controller.database import add_user\n\n    try:\n        add_user(user, password, role)\n        return {\"detail\": \"User added successfully\"}\n    except Exception as e:\n        logging.exception(f\"Error adding user: {e}\")\n        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f\"Error adding user: {e}\")\n</code></pre>"},{"location":"api/controller/controller/#nebula.controller.controller.check_scenario","title":"<code>check_scenario(role, scenario_name)</code>  <code>async</code>","text":"<p>Checks if a scenario is allowed for a specific role.</p> <p>Parameters:</p> Name Type Description Default <code>role</code> <code>str</code> <p>Role to validate.</p> required <code>scenario_name</code> <code>str</code> <p>Name of the scenario.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Whether the scenario is allowed for the role.</p> Source code in <code>nebula/controller/controller.py</code> <pre><code>@app.get(\"/scenarios/check/{role}/{scenario_name}\")\nasync def check_scenario(\n    role: Annotated[str, Path(regex=\"^[a-zA-Z0-9_-]+$\", min_length=1, max_length=50, description=\"Valid role\")],\n    scenario_name: Annotated[\n        str, Path(regex=\"^[a-zA-Z0-9_-]+$\", min_length=1, max_length=50, description=\"Valid scenario name\")\n    ],\n):\n    \"\"\"\n    Checks if a scenario is allowed for a specific role.\n\n    Args:\n        role (str): Role to validate.\n        scenario_name (str): Name of the scenario.\n\n    Returns:\n        dict: Whether the scenario is allowed for the role.\n    \"\"\"\n    from nebula.controller.database import check_scenario_with_role\n\n    try:\n        allowed = check_scenario_with_role(role, scenario_name)\n        return {\"allowed\": allowed}\n    except Exception as e:\n        logging.exception(f\"Error checking scenario with role: {e}\")\n        raise HTTPException(status_code=500, detail=\"Internal server error\")\n</code></pre>"},{"location":"api/controller/controller/#nebula.controller.controller.configure_logger","title":"<code>configure_logger(controller_log)</code>","text":"<p>Configures the logging system for the controller.</p> <ul> <li>Sets a format for console and file logging.</li> <li>Creates a console handler with INFO level.</li> <li>Creates a file handler for 'controller.log' with INFO level.</li> <li>Configures specific Uvicorn loggers to use the file handler   without duplicating log messages.</li> </ul> Source code in <code>nebula/controller/controller.py</code> <pre><code>def configure_logger(controller_log):\n    \"\"\"\n    Configures the logging system for the controller.\n\n    - Sets a format for console and file logging.\n    - Creates a console handler with INFO level.\n    - Creates a file handler for 'controller.log' with INFO level.\n    - Configures specific Uvicorn loggers to use the file handler\n      without duplicating log messages.\n    \"\"\"\n    log_console_format = \"[%(asctime)s] [%(name)s] [%(levelname)s] %(message)s\"\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.INFO)\n    console_handler.setFormatter(TermEscapeCodeFormatter(log_console_format))\n    console_handler_file = logging.FileHandler(os.path.join(controller_log), mode=\"w\")\n    console_handler_file.setLevel(logging.INFO)\n    console_handler_file.setFormatter(logging.Formatter(\"[%(asctime)s] [%(name)s] [%(levelname)s] %(message)s\"))\n    logging.basicConfig(\n        level=logging.DEBUG,\n        handlers=[\n            console_handler,\n            console_handler_file,\n        ],\n    )\n    uvicorn_loggers = [\"uvicorn\", \"uvicorn.error\", \"uvicorn.access\"]\n    for logger_name in uvicorn_loggers:\n        logger = logging.getLogger(logger_name)\n        logger.handlers = []  # Remove existing handlers\n        logger.propagate = False  # Prevent duplicate logs\n        handler = logging.FileHandler(os.path.join(controller_log), mode=\"a\")\n        handler.setFormatter(logging.Formatter(\"[%(asctime)s] [%(name)s] [%(levelname)s] %(message)s\"))\n        logger.addHandler(handler)\n</code></pre>"},{"location":"api/controller/controller/#nebula.controller.controller.discover_vpn","title":"<code>discover_vpn()</code>  <code>async</code>","text":"<p>Calls the Tailscale CLI to fetch the current status in JSON format, extracts all IPv4 addresses (by filtering out any address containing \u201c:\u201d), and returns them as a JSON object {\"ips\": [...]}.</p> Source code in <code>nebula/controller/controller.py</code> <pre><code>@app.get(\"/discover-vpn\")\nasync def discover_vpn():\n    \"\"\"\n    Calls the Tailscale CLI to fetch the current status in JSON format,\n    extracts all IPv4 addresses (by filtering out any address containing \u201c:\u201d),\n    and returns them as a JSON object {\"ips\": [...]}.\n    \"\"\"\n    try:\n        # 1) Launch the `tailscale status --json` subprocess\n        proc = await asyncio.create_subprocess_exec(\n            \"tailscale\", \"status\", \"--json\",\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE,\n        )\n\n        # 2) Wait for it to finish and capture stdout/stderr\n        out, err = await proc.communicate()\n        if proc.returncode != 0:\n            # If the CLI returned an error, raise to be caught below\n            raise RuntimeError(err.decode())\n\n        # 3) Parse the JSON output\n        data = json.loads(out.decode())\n\n        # 4) Collect only the IPv4 addresses from each peer\n        ips = []\n        for peer in data.get(\"Peer\", {}).values():\n            for ip in peer.get(\"TailscaleIPs\", []):\n                if \":\" not in ip:  \n                    # Skip IPv6 entries (they contain colons)\n                    ips.append(ip)\n\n        # 5) Return the list of IPv4s\n        return {\"ips\": ips}\n\n    except Exception as e:\n        # 6) Log any failure and respond with HTTP 500\n        logging.error(f\"Error discovering VPN devices: {e}\")\n        raise HTTPException(status_code=500, detail=\"No devices discovered\")\n</code></pre>"},{"location":"api/controller/controller/#nebula.controller.controller.get_available_gpu","title":"<code>get_available_gpu()</code>  <code>async</code>","text":"<p>Get the list of GPUs with memory usage below 5%.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary with a list of GPU indices that are mostly free (usage &lt; 5%).</p> Source code in <code>nebula/controller/controller.py</code> <pre><code>@app.get(\"/available_gpus/\")\nasync def get_available_gpu():\n    \"\"\"\n    Get the list of GPUs with memory usage below 5%.\n\n    Returns:\n        dict: A dictionary with a list of GPU indices that are mostly free (usage &lt; 5%).\n    \"\"\"\n    available_gpus = []\n\n    if importlib.util.find_spec(\"pynvml\") is not None:\n        try:\n            import pynvml\n\n            await asyncio.to_thread(pynvml.nvmlInit)\n            devices = await asyncio.to_thread(pynvml.nvmlDeviceGetCount)\n\n            # Obtain GPU info\n            for i in range(devices):\n                handle = await asyncio.to_thread(pynvml.nvmlDeviceGetHandleByIndex, i)\n                memory_info = await asyncio.to_thread(pynvml.nvmlDeviceGetMemoryInfo, handle)\n                memory_used_percent = (memory_info.used / memory_info.total) * 100\n\n                # Obtain available GPUs\n                if memory_used_percent &lt; 5:\n                    available_gpus.append(i)\n\n            return {\n                \"available_gpus\": available_gpus,\n            }\n        except Exception:  # noqa: S110\n            pass\n</code></pre>"},{"location":"api/controller/controller/#nebula.controller.controller.get_least_memory_gpu","title":"<code>get_least_memory_gpu()</code>  <code>async</code>","text":"<p>Identify the GPU with the highest memory usage above a threshold (50%).</p> Note <p>Despite the name, this function returns the GPU using the most memory above 50% usage.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary with the index of the GPU using the most memory above the threshold,   or None if no such GPU is found.</p> Source code in <code>nebula/controller/controller.py</code> <pre><code>@app.get(\"/least_memory_gpu\")\nasync def get_least_memory_gpu():\n    \"\"\"\n    Identify the GPU with the highest memory usage above a threshold (50%).\n\n    Note:\n        Despite the name, this function returns the GPU using the **most**\n        memory above 50% usage.\n\n    Returns:\n        dict: A dictionary with the index of the GPU using the most memory above the threshold,\n              or None if no such GPU is found.\n    \"\"\"\n    gpu_with_least_memory_index = None\n\n    if importlib.util.find_spec(\"pynvml\") is not None:\n        max_memory_used_percent = 50\n        try:\n            import pynvml\n\n            await asyncio.to_thread(pynvml.nvmlInit)\n            devices = await asyncio.to_thread(pynvml.nvmlDeviceGetCount)\n\n            # Obtain GPU info\n            for i in range(devices):\n                handle = await asyncio.to_thread(pynvml.nvmlDeviceGetHandleByIndex, i)\n                memory_info = await asyncio.to_thread(pynvml.nvmlDeviceGetMemoryInfo, handle)\n                memory_used_percent = (memory_info.used / memory_info.total) * 100\n\n                # Obtain GPU with less memory available\n                if memory_used_percent &gt; max_memory_used_percent:\n                    max_memory_used_percent = memory_used_percent\n                    gpu_with_least_memory_index = i\n\n        except Exception:  # noqa: S110\n            pass\n\n    return {\n        \"gpu_with_least_memory_index\": gpu_with_least_memory_index,\n    }\n</code></pre>"},{"location":"api/controller/controller/#nebula.controller.controller.get_notes_by_scenario_name","title":"<code>get_notes_by_scenario_name(scenario_name)</code>  <code>async</code>","text":"<p>Endpoint to retrieve notes associated with a scenario.</p> <p>Path Parameters: - scenario_name: Name of the scenario.</p> <p>Returns the notes or raises an HTTPException on error.</p> Source code in <code>nebula/controller/controller.py</code> <pre><code>@app.get(\"/notes/{scenario_name}\")\nasync def get_notes_by_scenario_name(\n    scenario_name: Annotated[\n        str, Path(regex=\"^[a-zA-Z0-9_-]+$\", min_length=1, max_length=50, description=\"Valid scenario name\")\n    ],\n):\n    \"\"\"\n    Endpoint to retrieve notes associated with a scenario.\n\n    Path Parameters:\n    - scenario_name: Name of the scenario.\n\n    Returns the notes or raises an HTTPException on error.\n    \"\"\"\n    from nebula.controller.database import get_notes\n\n    try:\n        notes = get_notes(scenario_name)\n    except Exception as e:\n        logging.exception(f\"Error obtaining notes {notes}: {e}\")\n        raise HTTPException(status_code=500, detail=\"Internal server error\")\n\n    return notes\n</code></pre>"},{"location":"api/controller/controller/#nebula.controller.controller.get_physical_node_state","title":"<code>get_physical_node_state(ip)</code>  <code>async</code>","text":"<p>Query a single Raspberry Pi (or other node) for its training state.</p>"},{"location":"api/controller/controller/#nebula.controller.controller.get_physical_node_state--parameters","title":"Parameters","text":"<p>ip : str     IP address or hostname of the node.</p>"},{"location":"api/controller/controller/#nebula.controller.controller.get_physical_node_state--returns","title":"Returns","text":"<p>dict     \u2022 running (bool) \u2013 True if a training process is active.     \u2022 error   (str)  \u2013 Optional error message when the node is unreachable                         or returns a non-200 HTTP status.</p> Source code in <code>nebula/controller/controller.py</code> <pre><code>@app.get(\"/physical/state/{ip}\", tags=[\"physical\"])\nasync def get_physical_node_state(ip: str):\n    \"\"\"\n    Query a single Raspberry Pi (or other node) for its training state.\n\n    Parameters\n    ----------\n    ip : str\n        IP address or hostname of the node.\n\n    Returns\n    -------\n    dict\n        \u2022 running (bool) \u2013 True if a training process is active.  \n        \u2022 error   (str)  \u2013 Optional error message when the node is unreachable\n                            or returns a non-200 HTTP status.\n    \"\"\"\n    # Short global timeout so a dead node doesn't block the whole request\n    timeout = aiohttp.ClientTimeout(total=3)            # seconds\n\n    try:\n        async with aiohttp.ClientSession(timeout=timeout) as session:\n            async with session.get(f\"http://{ip}/state/\") as resp:\n                if resp.status == 200:\n                    # Forward the node's own JSON, expected to be {\"running\": bool}\n                    return await resp.json()\n                # Node responded but with an HTTP error code\n                return {\"running\": False,\n                        \"error\": f\"HTTP {resp.status}\"}\n    except Exception as exc:\n        # Network errors, timeouts, DNS failures, \u2026\n        return {\"running\": False, \"error\": str(exc)}\n</code></pre>"},{"location":"api/controller/controller/#nebula.controller.controller.get_physical_scenario_state","title":"<code>get_physical_scenario_state(scenario_name)</code>  <code>async</code>","text":"<p>Check the training state of every physical node assigned to a scenario.</p>"},{"location":"api/controller/controller/#nebula.controller.controller.get_physical_scenario_state--parameters","title":"Parameters","text":"<p>scenario_name : str     Scenario identifier.</p>"},{"location":"api/controller/controller/#nebula.controller.controller.get_physical_scenario_state--returns","title":"Returns","text":"<p>dict     {       \"running\":       bool,            # True  \u21e2 at least one node is training       \"nodes_state\":   { ip: {...} },   # result from each /state/ call       \"all_available\": bool             # True  \u21e2 every node responded and                                         #          none is training     }</p> Source code in <code>nebula/controller/controller.py</code> <pre><code>@app.get(\"/physical/scenario-state/{scenario_name}\", tags=[\"physical\"])\nasync def get_physical_scenario_state(scenario_name: str):\n    \"\"\"\n    Check the training state of *every* physical node assigned to a scenario.\n\n    Parameters\n    ----------\n    scenario_name : str\n        Scenario identifier.\n\n    Returns\n    -------\n    dict\n        {\n          \"running\":       bool,            # True  \u21e2 at least one node is training\n          \"nodes_state\":   { ip: {...} },   # result from each /state/ call\n          \"all_available\": bool             # True  \u21e2 every node responded and\n                                            #          none is training\n        }\n    \"\"\"\n    # 1) Retrieve scenario metadata and node list from the DB\n    scenario = await get_scenario_by_name(scenario_name)\n    if not scenario:\n        raise HTTPException(status_code=404, detail=\"Scenario not found\")\n\n    nodes = await list_nodes_by_scenario_name(scenario_name)\n    if not nodes:\n        raise HTTPException(status_code=404, detail=\"No nodes found for scenario\")\n\n    # 2) Probe all nodes concurrently\n    ips   = [n[\"ip\"] for n in nodes]\n    tasks = [get_physical_node_state(ip) for ip in ips]\n    states = await asyncio.gather(*tasks)               # parallel HTTP calls\n\n    # 3) Aggregate results\n    nodes_state  = dict(zip(ips, states))\n    any_running  = any(s.get(\"running\") for s in states)\n    # 'all_available' is true only if *every* node answered with running=False\n    # *and* without an error field.\n    all_available = all(\n        (not s.get(\"running\")) and (not s.get(\"error\")) for s in states\n    )\n\n    return {\n        \"running\": any_running,\n        \"nodes_state\": nodes_state,\n        \"all_available\": all_available,\n    }\n</code></pre>"},{"location":"api/controller/controller/#nebula.controller.controller.get_resources","title":"<code>get_resources()</code>  <code>async</code>","text":"<p>Get system resource usage including RAM and GPU memory usage.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing: - gpus (int): Number of GPUs detected. - memory_percent (float): Percentage of used RAM. - gpu_memory_percent (List[float]): List of GPU memory usage percentages.</p> Source code in <code>nebula/controller/controller.py</code> <pre><code>@app.get(\"/resources\")\nasync def get_resources():\n    \"\"\"\n    Get system resource usage including RAM and GPU memory usage.\n\n    Returns:\n        dict: A dictionary containing:\n            - gpus (int): Number of GPUs detected.\n            - memory_percent (float): Percentage of used RAM.\n            - gpu_memory_percent (List[float]): List of GPU memory usage percentages.\n    \"\"\"\n    devices = 0\n    gpu_memory_percent = []\n\n    # Obtain available RAM\n    memory_info = await asyncio.to_thread(psutil.virtual_memory)\n\n    if importlib.util.find_spec(\"pynvml\") is not None:\n        try:\n            import pynvml\n\n            await asyncio.to_thread(pynvml.nvmlInit)\n            devices = await asyncio.to_thread(pynvml.nvmlDeviceGetCount)\n\n            # Obtain GPU info\n            for i in range(devices):\n                handle = await asyncio.to_thread(pynvml.nvmlDeviceGetHandleByIndex, i)\n                memory_info_gpu = await asyncio.to_thread(pynvml.nvmlDeviceGetMemoryInfo, handle)\n                memory_used_percent = (memory_info_gpu.used / memory_info_gpu.total) * 100\n                gpu_memory_percent.append(memory_used_percent)\n\n        except Exception:  # noqa: S110\n            pass\n\n    return {\n        # \"cpu_percent\": psutil.cpu_percent(),\n        \"gpus\": devices,\n        \"memory_percent\": memory_info.percent,\n        \"gpu_memory_percent\": gpu_memory_percent,\n    }\n</code></pre>"},{"location":"api/controller/controller/#nebula.controller.controller.get_running_scenario","title":"<code>get_running_scenario(get_all=False)</code>  <code>async</code>","text":"<p>Retrieves the currently running scenario(s).</p> <p>Parameters:</p> Name Type Description Default <code>get_all</code> <code>bool</code> <p>If True, retrieves all running scenarios.</p> <code>False</code> <p>Returns:</p> Type Description <p>dict or list: Running scenario(s) information.</p> Source code in <code>nebula/controller/controller.py</code> <pre><code>@app.get(\"/scenarios/running\")\nasync def get_running_scenario(get_all: bool = False):\n    \"\"\"\n    Retrieves the currently running scenario(s).\n\n    Args:\n        get_all (bool): If True, retrieves all running scenarios.\n\n    Returns:\n        dict or list: Running scenario(s) information.\n    \"\"\"\n    from nebula.controller.database import get_running_scenario\n\n    try:\n        return get_running_scenario(get_all=get_all)\n    except Exception as e:\n        logging.exception(f\"Error obtaining running scenario: {e}\")\n        raise HTTPException(status_code=500, detail=\"Internal server error\")\n</code></pre>"},{"location":"api/controller/controller/#nebula.controller.controller.get_scenario_by_name","title":"<code>get_scenario_by_name(scenario_name)</code>  <code>async</code>","text":"<p>Fetches a scenario by its name.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_name</code> <code>str</code> <p>The name of the scenario.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>The scenario data.</p> Source code in <code>nebula/controller/controller.py</code> <pre><code>@app.get(\"/scenarios/{scenario_name}\")\nasync def get_scenario_by_name(\n    scenario_name: Annotated[\n        str, Path(regex=\"^[a-zA-Z0-9_-]+$\", min_length=1, max_length=50, description=\"Valid scenario name\")\n    ],\n):\n    \"\"\"\n    Fetches a scenario by its name.\n\n    Args:\n        scenario_name (str): The name of the scenario.\n\n    Returns:\n        dict: The scenario data.\n    \"\"\"\n    from nebula.controller.database import get_scenario_by_name\n\n    try:\n        scenario = get_scenario_by_name(scenario_name)\n    except Exception as e:\n        logging.exception(f\"Error obtaining scenario {scenario_name}: {e}\")\n        raise HTTPException(status_code=500, detail=\"Internal server error\")\n\n    return scenario\n</code></pre>"},{"location":"api/controller/controller/#nebula.controller.controller.get_scenarios","title":"<code>get_scenarios(user, role)</code>  <code>async</code>","text":"<p>Retrieves all scenarios associated with a given user and role.</p> <p>Parameters:</p> Name Type Description Default <code>user</code> <code>str</code> <p>Username to filter scenarios.</p> required <code>role</code> <code>str</code> <p>Role of the user (e.g., \"admin\").</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A list of scenarios and the currently running scenario.</p> Source code in <code>nebula/controller/controller.py</code> <pre><code>@app.get(\"/scenarios/{user}/{role}\")\nasync def get_scenarios(\n    user: Annotated[str, Path(regex=\"^[a-zA-Z0-9_-]+$\", min_length=1, max_length=50, description=\"Valid username\")],\n    role: Annotated[str, Path(regex=\"^[a-zA-Z0-9_-]+$\", min_length=1, max_length=50, description=\"Valid role\")],\n):\n    \"\"\"\n    Retrieves all scenarios associated with a given user and role.\n\n    Args:\n        user (str): Username to filter scenarios.\n        role (str): Role of the user (e.g., \"admin\").\n\n    Returns:\n        dict: A list of scenarios and the currently running scenario.\n    \"\"\"\n    from nebula.controller.database import get_all_scenarios_and_check_completed, get_running_scenario\n\n    try:\n        scenarios = get_all_scenarios_and_check_completed(username=user, role=role)\n        if role == \"admin\":\n            scenario_running = get_running_scenario()\n        else:\n            scenario_running = get_running_scenario(username=user)\n    except Exception as e:\n        logging.exception(f\"Error obtaining scenarios: {e}\")\n        raise HTTPException(status_code=500, detail=\"Internal server error\")\n\n    return {\"scenarios\": scenarios, \"scenario_running\": scenario_running}\n</code></pre>"},{"location":"api/controller/controller/#nebula.controller.controller.get_status","title":"<code>get_status()</code>  <code>async</code>","text":"<p>Check the status of the NEBULA Controller API.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A status message confirming the API is running.</p> Source code in <code>nebula/controller/controller.py</code> <pre><code>@app.get(\"/status\")\nasync def get_status():\n    \"\"\"\n    Check the status of the NEBULA Controller API.\n\n    Returns:\n        dict: A status message confirming the API is running.\n    \"\"\"\n    return {\"status\": \"NEBULA Controller API is running\"}\n</code></pre>"},{"location":"api/controller/controller/#nebula.controller.controller.get_user_by_scenario_name","title":"<code>get_user_by_scenario_name(scenario_name)</code>  <code>async</code>","text":"<p>Endpoint to retrieve the user assigned to a scenario.</p> <p>Path Parameters: - scenario_name: Name of the scenario.</p> <p>Returns user info or raises an HTTPException on error.</p> Source code in <code>nebula/controller/controller.py</code> <pre><code>@app.get(\"/user/{scenario_name}\")\nasync def get_user_by_scenario_name(\n    scenario_name: Annotated[\n        str, Path(regex=\"^[a-zA-Z0-9_-]+$\", min_length=1, max_length=50, description=\"Valid scenario name\")\n    ],\n):\n    \"\"\"\n    Endpoint to retrieve the user assigned to a scenario.\n\n    Path Parameters:\n    - scenario_name: Name of the scenario.\n\n    Returns user info or raises an HTTPException on error.\n    \"\"\"\n    from nebula.controller.database import get_user_by_scenario_name\n\n    try:\n        user = get_user_by_scenario_name(scenario_name)\n    except Exception as e:\n        logging.exception(f\"Error obtaining user {user}: {e}\")\n        raise HTTPException(status_code=500, detail=\"Internal server error\")\n\n    return user\n</code></pre>"},{"location":"api/controller/controller/#nebula.controller.controller.list_nodes_by_scenario_name","title":"<code>list_nodes_by_scenario_name(scenario_name)</code>  <code>async</code>","text":"<p>Lists all nodes associated with a specific scenario.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_name</code> <code>str</code> <p>Name of the scenario.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>List of nodes.</p> Source code in <code>nebula/controller/controller.py</code> <pre><code>@app.get(\"/nodes/{scenario_name}\")\nasync def list_nodes_by_scenario_name(\n    scenario_name: Annotated[\n        str, Path(regex=\"^[a-zA-Z0-9_-]+$\", min_length=1, max_length=50, description=\"Valid scenario name\")\n    ],\n):\n    \"\"\"\n    Lists all nodes associated with a specific scenario.\n\n    Args:\n        scenario_name (str): Name of the scenario.\n\n    Returns:\n        list: List of nodes.\n    \"\"\"\n    from nebula.controller.database import list_nodes_by_scenario_name\n\n    try:\n        nodes = list_nodes_by_scenario_name(scenario_name)\n    except Exception as e:\n        logging.exception(f\"Error obtaining nodes: {e}\")\n        raise HTTPException(status_code=500, detail=\"Internal server error\")\n\n    return nodes\n</code></pre>"},{"location":"api/controller/controller/#nebula.controller.controller.list_users_controller","title":"<code>list_users_controller(all_info=False)</code>  <code>async</code>","text":"<p>Endpoint to list all users in the database.</p> <p>Query Parameters: - all_info (bool): If True, returns full user info as dictionaries.</p> <p>Returns a list of users or raises an HTTPException on error.</p> Source code in <code>nebula/controller/controller.py</code> <pre><code>@app.get(\"/user/list\")\nasync def list_users_controller(all_info: bool = False):\n    \"\"\"\n    Endpoint to list all users in the database.\n\n    Query Parameters:\n    - all_info (bool): If True, returns full user info as dictionaries.\n\n    Returns a list of users or raises an HTTPException on error.\n    \"\"\"\n    from nebula.controller.database import list_users\n\n    try:\n        user_list = list_users(all_info)\n        if all_info:\n            # Convert each sqlite3.Row to a dictionary so that it is JSON serializable.\n            user_list = [dict(user) for user in user_list]\n        return {\"users\": user_list}\n    except Exception as e:\n        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f\"Error retrieving users: {e}\")\n</code></pre>"},{"location":"api/controller/controller/#nebula.controller.controller.node_done","title":"<code>node_done(scenario_name, request)</code>  <code>async</code>","text":"<p>Endpoint to forward node status to the frontend.</p> <p>Receives a JSON payload and forwards it to the frontend's /node/done route for the given scenario.</p> <p>Parameters: - scenario_name: Name of the scenario. - request: HTTP request with JSON body.</p> <p>Returns the response from the frontend or raises an HTTPException if it fails.</p> Source code in <code>nebula/controller/controller.py</code> <pre><code>@app.post(\"/nodes/{scenario_name}/done\")\nasync def node_done(\n    scenario_name: Annotated[\n        str,\n        Path(regex=\"^[a-zA-Z0-9_-]+$\", min_length=1, max_length=50, description=\"Valid scenario name\"),\n    ],\n    request: Request,\n):\n    \"\"\"\n    Endpoint to forward node status to the frontend.\n\n    Receives a JSON payload and forwards it to the frontend's /node/done route\n    for the given scenario.\n\n    Parameters:\n    - scenario_name: Name of the scenario.\n    - request: HTTP request with JSON body.\n\n    Returns the response from the frontend or raises an HTTPException if it fails.\n    \"\"\"\n    url = f\"http://{os.environ['NEBULA_CONTROLLER_NAME']}_nebula-frontend/platform/dashboard/{scenario_name}/node/done\"\n\n    data = await request.json()\n\n    async with aiohttp.ClientSession() as session:\n        async with session.post(url, json=data) as response:\n            if response.status == 200:\n                return await response.json()\n            else:\n                raise HTTPException(status_code=response.status, detail=\"Error posting data\")\n\n    return {\"message\": \"Nodes done\"}\n</code></pre>"},{"location":"api/controller/controller/#nebula.controller.controller.read_root","title":"<code>read_root()</code>  <code>async</code>","text":"<p>Root endpoint of the NEBULA Controller API.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A welcome message indicating the API is accessible.</p> Source code in <code>nebula/controller/controller.py</code> <pre><code>@app.get(\"/\")\nasync def read_root():\n    \"\"\"\n    Root endpoint of the NEBULA Controller API.\n\n    Returns:\n        dict: A welcome message indicating the API is accessible.\n    \"\"\"\n    return {\"message\": \"Welcome to the NEBULA Controller API\"}\n</code></pre>"},{"location":"api/controller/controller/#nebula.controller.controller.remove_nodes_by_scenario_name","title":"<code>remove_nodes_by_scenario_name(scenario_name=Body(..., embed=True))</code>  <code>async</code>","text":"<p>Endpoint to remove all nodes associated with a scenario.</p> <p>Body Parameters: - scenario_name: Name of the scenario whose nodes should be removed.</p> <p>Returns a success message or an error if something goes wrong.</p> Source code in <code>nebula/controller/controller.py</code> <pre><code>@app.post(\"/nodes/remove\")\nasync def remove_nodes_by_scenario_name(scenario_name: str = Body(..., embed=True)):\n    \"\"\"\n    Endpoint to remove all nodes associated with a scenario.\n\n    Body Parameters:\n    - scenario_name: Name of the scenario whose nodes should be removed.\n\n    Returns a success message or an error if something goes wrong.\n    \"\"\"\n    from nebula.controller.database import remove_nodes_by_scenario_name\n\n    try:\n        remove_nodes_by_scenario_name(scenario_name)\n    except Exception as e:\n        logging.exception(f\"Error removing nodes: {e}\")\n        raise HTTPException(status_code=500, detail=\"Internal server error\")\n\n    return {\"message\": f\"Nodes for scenario {scenario_name} removed successfully\"}\n</code></pre>"},{"location":"api/controller/controller/#nebula.controller.controller.remove_notes_by_scenario_name","title":"<code>remove_notes_by_scenario_name(scenario_name=Body(..., embed=True))</code>  <code>async</code>","text":"<p>Endpoint to remove notes associated with a scenario.</p> <p>Body Parameters: - scenario_name: Name of the scenario.</p> <p>Returns a success message or an error if something goes wrong.</p> Source code in <code>nebula/controller/controller.py</code> <pre><code>@app.post(\"/notes/remove\")\nasync def remove_notes_by_scenario_name(scenario_name: str = Body(..., embed=True)):\n    \"\"\"\n    Endpoint to remove notes associated with a scenario.\n\n    Body Parameters:\n    - scenario_name: Name of the scenario.\n\n    Returns a success message or an error if something goes wrong.\n    \"\"\"\n    from nebula.controller.database import remove_note\n\n    try:\n        remove_note(scenario_name)\n    except Exception as e:\n        logging.exception(f\"Error removing notes: {e}\")\n        raise HTTPException(status_code=500, detail=\"Internal server error\")\n\n    return {\"message\": f\"Notes for scenario {scenario_name} removed successfully\"}\n</code></pre>"},{"location":"api/controller/controller/#nebula.controller.controller.remove_scenario","title":"<code>remove_scenario(scenario_name=Body(..., embed=True))</code>  <code>async</code>","text":"<p>Removes a scenario from the database by its name.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_name</code> <code>str</code> <p>Name of the scenario to remove.</p> <code>Body(..., embed=True)</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A message indicating successful removal.</p> Source code in <code>nebula/controller/controller.py</code> <pre><code>@app.post(\"/scenarios/remove\")\nasync def remove_scenario(\n    scenario_name: str = Body(..., embed=True),\n):\n    \"\"\"\n    Removes a scenario from the database by its name.\n\n    Args:\n        scenario_name (str): Name of the scenario to remove.\n\n    Returns:\n        dict: A message indicating successful removal.\n    \"\"\"\n    from nebula.controller.database import remove_scenario_by_name\n    from nebula.controller.scenarios import ScenarioManagement\n\n    try:\n        remove_scenario_by_name(scenario_name)\n        ScenarioManagement.remove_files_by_scenario(scenario_name)\n    except Exception as e:\n        logging.exception(f\"Error removing scenario {scenario_name}: {e}\")\n        raise HTTPException(status_code=500, detail=\"Internal server error\")\n\n    return {\"message\": f\"Scenario {scenario_name} removed successfully\"}\n</code></pre>"},{"location":"api/controller/controller/#nebula.controller.controller.remove_user_controller","title":"<code>remove_user_controller(user=Body(..., embed=True))</code>  <code>async</code>","text":"<p>Controller endpoint that inserts a new user into the database.</p> <p>Parameters: - user: The username for the new user.</p> <p>Returns a success message if the user is deleted, or an HTTP error if an exception occurs.</p> Source code in <code>nebula/controller/controller.py</code> <pre><code>@app.post(\"/user/delete\")\nasync def remove_user_controller(user: str = Body(..., embed=True)):\n    \"\"\"\n    Controller endpoint that inserts a new user into the database.\n\n    Parameters:\n    - user: The username for the new user.\n\n    Returns a success message if the user is deleted, or an HTTP error if an exception occurs.\n    \"\"\"\n    from nebula.controller.database import delete_user_from_db\n\n    try:\n        delete_user_from_db(user)\n        return {\"detail\": \"User deleted successfully\"}\n    except Exception as e:\n        logging.exception(f\"Error deleting user: {e}\")\n        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f\"Error deleting user: {e}\")\n</code></pre>"},{"location":"api/controller/controller/#nebula.controller.controller.run_scenario","title":"<code>run_scenario(scenario_data=Body(..., embed=True), role=Body(..., embed=True), user=Body(..., embed=True))</code>  <code>async</code>","text":"<p>Launches a new scenario based on the provided configuration.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_data</code> <code>dict</code> <p>The complete configuration of the scenario to be executed.</p> <code>Body(..., embed=True)</code> <code>role</code> <code>str</code> <p>The role of the user initiating the scenario.</p> <code>Body(..., embed=True)</code> <code>user</code> <code>str</code> <p>The username of the user initiating the scenario.</p> <code>Body(..., embed=True)</code> <p>Returns:</p> Name Type Description <code>str</code> <p>The name of the scenario that was started.</p> Source code in <code>nebula/controller/controller.py</code> <pre><code>@app.post(\"/scenarios/run\")\nasync def run_scenario(\n    scenario_data: dict = Body(..., embed=True), role: str = Body(..., embed=True), user: str = Body(..., embed=True)\n):\n    \"\"\"\n    Launches a new scenario based on the provided configuration.\n\n    Args:\n        scenario_data (dict): The complete configuration of the scenario to be executed.\n        role (str): The role of the user initiating the scenario.\n        user (str): The username of the user initiating the scenario.\n\n    Returns:\n        str: The name of the scenario that was started.\n    \"\"\"\n\n    import subprocess\n\n    from nebula.controller.scenarios import ScenarioManagement\n\n    validate_physical_fields(scenario_data)\n\n    # Manager for the actual scenario\n    scenarioManagement = ScenarioManagement(scenario_data, user)\n\n    await update_scenario(\n        scenario_name=scenarioManagement.scenario_name,\n        start_time=scenarioManagement.start_date_scenario,\n        end_time=\"\",\n        scenario=scenario_data,\n        status=\"running\",\n        role=role,\n        username=user,\n    )\n\n    # Run the actual scenario\n    try:\n        if scenarioManagement.scenario.mobility:\n            additional_participants = scenario_data[\"additional_participants\"]\n            schema_additional_participants = scenario_data[\"schema_additional_participants\"]\n            await scenarioManagement.load_configurations_and_start_nodes(\n                additional_participants, schema_additional_participants\n            )\n        else:\n            await scenarioManagement.load_configurations_and_start_nodes()\n    except subprocess.CalledProcessError as e:\n        logging.exception(f\"Error docker-compose up: {e}\")\n        return\n\n    return scenarioManagement.scenario_name\n</code></pre>"},{"location":"api/controller/controller/#nebula.controller.controller.set_scenario_status_to_finished","title":"<code>set_scenario_status_to_finished(scenario_name=Body(..., embed=True), all=Body(False, embed=True))</code>  <code>async</code>","text":"<p>Sets the status of a scenario (or all scenarios) to 'finished'.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_name</code> <code>str</code> <p>Name of the scenario to mark as finished.</p> <code>Body(..., embed=True)</code> <code>all</code> <code>bool</code> <p>If True, sets all scenarios to finished.</p> <code>Body(False, embed=True)</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A message confirming the operation.</p> Source code in <code>nebula/controller/controller.py</code> <pre><code>@app.post(\"/scenarios/set_status_to_finished\")\nasync def set_scenario_status_to_finished(\n    scenario_name: str = Body(..., embed=True), all: bool = Body(False, embed=True)\n):\n    \"\"\"\n    Sets the status of a scenario (or all scenarios) to 'finished'.\n\n    Args:\n        scenario_name (str): Name of the scenario to mark as finished.\n        all (bool): If True, sets all scenarios to finished.\n\n    Returns:\n        dict: A message confirming the operation.\n    \"\"\"\n    from nebula.controller.database import scenario_set_all_status_to_finished, scenario_set_status_to_finished\n\n    try:\n        if all:\n            scenario_set_all_status_to_finished()\n        else:\n            scenario_set_status_to_finished(scenario_name)\n    except Exception as e:\n        logging.exception(f\"Error setting scenario {scenario_name} to finished: {e}\")\n        raise HTTPException(status_code=500, detail=\"Internal server error\")\n\n    return {\"message\": f\"Scenario {scenario_name} status set to finished successfully\"}\n</code></pre>"},{"location":"api/controller/controller/#nebula.controller.controller.stop_scenario","title":"<code>stop_scenario(scenario_name=Body(..., embed=True), username=Body(..., embed=True), all=Body(False, embed=True))</code>  <code>async</code>","text":"<p>Stops the execution of a federated learning scenario and performs cleanup operations.</p> This endpoint <ul> <li>Stops all participant containers associated with the specified scenario.</li> <li>Removes Docker containers and network resources tied to the scenario and user.</li> <li>Sets the scenario's status to \"finished\" in the database.</li> <li>Optionally finalizes all active scenarios if the 'all' flag is set.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>scenario_name</code> <code>str</code> <p>Name of the scenario to stop.</p> <code>Body(..., embed=True)</code> <code>username</code> <code>str</code> <p>User who initiated the stop operation.</p> <code>Body(..., embed=True)</code> <code>all</code> <code>bool</code> <p>Whether to stop all running scenarios instead of just one (default: False).</p> <code>Body(False, embed=True)</code> <p>Raises:</p> Type Description <code>HTTPException</code> <p>Returns a 500 status code if any step fails.</p> Note <p>This function does not currently trigger statistics generation.</p> Source code in <code>nebula/controller/controller.py</code> <pre><code>@app.post(\"/scenarios/stop\")\nasync def stop_scenario(\n    scenario_name: str = Body(..., embed=True),\n    username: str = Body(..., embed=True),\n    all: bool = Body(False, embed=True),\n):\n    \"\"\"\n    Stops the execution of a federated learning scenario and performs cleanup operations.\n\n    This endpoint:\n        - Stops all participant containers associated with the specified scenario.\n        - Removes Docker containers and network resources tied to the scenario and user.\n        - Sets the scenario's status to \"finished\" in the database.\n        - Optionally finalizes all active scenarios if the 'all' flag is set.\n\n    Args:\n        scenario_name (str): Name of the scenario to stop.\n        username (str): User who initiated the stop operation.\n        all (bool): Whether to stop all running scenarios instead of just one (default: False).\n\n    Raises:\n        HTTPException: Returns a 500 status code if any step fails.\n\n    Note:\n        This function does not currently trigger statistics generation.\n    \"\"\"\n    from nebula.controller.scenarios import ScenarioManagement\n\n    # ScenarioManagement.stop_participants(scenario_name)\n    DockerUtils.remove_containers_by_prefix(f\"{os.environ.get('NEBULA_CONTROLLER_NAME')}_{username}-participant\")\n    DockerUtils.remove_docker_network(\n        f\"{(os.environ.get('NEBULA_CONTROLLER_NAME'))}_{str(username).lower()}-nebula-net-scenario\"\n    )\n    try:\n        if all:\n            scenario_set_all_status_to_finished()\n        else:\n            scenario_set_status_to_finished(scenario_name)\n    except Exception as e:\n        logging.exception(f\"Error setting scenario {scenario_name} to finished: {e}\")\n        raise HTTPException(status_code=500, detail=\"Internal server error\")\n</code></pre>"},{"location":"api/controller/controller/#nebula.controller.controller.update_nodes","title":"<code>update_nodes(scenario_name, request)</code>  <code>async</code>","text":"<p>Updates the configuration of a node in the database and notifies the frontend.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_name</code> <code>str</code> <p>The scenario to which the node belongs.</p> required <code>request</code> <code>Request</code> <p>The HTTP request containing the node data.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Confirmation or response from the frontend.</p> Source code in <code>nebula/controller/controller.py</code> <pre><code>@app.post(\"/nodes/{scenario_name}/update\")\nasync def update_nodes(\n    scenario_name: Annotated[\n        str,\n        Path(regex=\"^[a-zA-Z0-9_-]+$\", min_length=1, max_length=50, description=\"Valid scenario name\"),\n    ],\n    request: Request,\n):\n    \"\"\"\n    Updates the configuration of a node in the database and notifies the frontend.\n\n    Args:\n        scenario_name (str): The scenario to which the node belongs.\n        request (Request): The HTTP request containing the node data.\n\n    Returns:\n        dict: Confirmation or response from the frontend.\n    \"\"\"\n    from nebula.controller.database import update_node_record\n\n    try:\n        config = await request.json()\n        timestamp = datetime.datetime.now()\n        # Update the node in database\n        await update_node_record(\n            str(config[\"device_args\"][\"uid\"]),\n            str(config[\"device_args\"][\"idx\"]),\n            str(config[\"network_args\"][\"ip\"]),\n            str(config[\"network_args\"][\"port\"]),\n            str(config[\"device_args\"][\"role\"]),\n            str(config[\"network_args\"][\"neighbors\"]),\n            str(config[\"mobility_args\"][\"latitude\"]),\n            str(config[\"mobility_args\"][\"longitude\"]),\n            str(timestamp),\n            str(config[\"scenario_args\"][\"federation\"]),\n            str(config[\"federation_args\"][\"round\"]),\n            str(config[\"scenario_args\"][\"name\"]),\n            str(config[\"tracking_args\"][\"run_hash\"]),\n            str(config[\"device_args\"][\"malicious\"]),\n        )\n    except Exception as e:\n        logging.exception(f\"Error updating nodes: {e}\")\n        raise HTTPException(status_code=500, detail=\"Internal server error\")\n\n    url = (\n        f\"http://{os.environ['NEBULA_CONTROLLER_NAME']}_nebula-frontend/platform/dashboard/{scenario_name}/node/update\"\n    )\n\n    config[\"timestamp\"] = str(timestamp)\n\n    async with aiohttp.ClientSession() as session:\n        async with session.post(url, json=config) as response:\n            if response.status == 200:\n                return await response.json()\n            else:\n                raise HTTPException(status_code=response.status, detail=\"Error posting data\")\n\n    return {\"message\": \"Nodes updated successfully in the database\"}\n</code></pre>"},{"location":"api/controller/controller/#nebula.controller.controller.update_notes_by_scenario_name","title":"<code>update_notes_by_scenario_name(scenario_name=Body(..., embed=True), notes=Body(..., embed=True))</code>  <code>async</code>","text":"<p>Endpoint to update notes for a given scenario.</p> <p>Body Parameters: - scenario_name: Name of the scenario. - notes: Text content to store as notes.</p> <p>Returns a success message or an error if something goes wrong.</p> Source code in <code>nebula/controller/controller.py</code> <pre><code>@app.post(\"/notes/update\")\nasync def update_notes_by_scenario_name(scenario_name: str = Body(..., embed=True), notes: str = Body(..., embed=True)):\n    \"\"\"\n    Endpoint to update notes for a given scenario.\n\n    Body Parameters:\n    - scenario_name: Name of the scenario.\n    - notes: Text content to store as notes.\n\n    Returns a success message or an error if something goes wrong.\n    \"\"\"\n    from nebula.controller.database import save_notes\n\n    try:\n        save_notes(scenario_name, notes)\n    except Exception as e:\n        logging.exception(f\"Error updating notes: {e}\")\n        raise HTTPException(status_code=500, detail=\"Internal server error\")\n\n    return {\"message\": f\"Notes for scenario {scenario_name} updated successfully\"}\n</code></pre>"},{"location":"api/controller/controller/#nebula.controller.controller.update_scenario","title":"<code>update_scenario(scenario_name=Body(..., embed=True), start_time=Body(..., embed=True), end_time=Body(..., embed=True), scenario=Body(..., embed=True), status=Body(..., embed=True), role=Body(..., embed=True), username=Body(..., embed=True))</code>  <code>async</code>","text":"<p>Updates the status and metadata of a scenario.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_name</code> <code>str</code> <p>Name of the scenario.</p> <code>Body(..., embed=True)</code> <code>start_time</code> <code>str</code> <p>Start time of the scenario.</p> <code>Body(..., embed=True)</code> <code>end_time</code> <code>str</code> <p>End time of the scenario.</p> <code>Body(..., embed=True)</code> <code>scenario</code> <code>dict</code> <p>Scenario configuration.</p> <code>Body(..., embed=True)</code> <code>status</code> <code>str</code> <p>New status of the scenario (e.g., \"running\", \"finished\").</p> <code>Body(..., embed=True)</code> <code>role</code> <code>str</code> <p>Role associated with the scenario.</p> <code>Body(..., embed=True)</code> <code>username</code> <code>str</code> <p>User performing the update.</p> <code>Body(..., embed=True)</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A message confirming the update.</p> Source code in <code>nebula/controller/controller.py</code> <pre><code>@app.post(\"/scenarios/update\")\nasync def update_scenario(\n    scenario_name: str = Body(..., embed=True),\n    start_time: str = Body(..., embed=True),\n    end_time: str = Body(..., embed=True),\n    scenario: dict = Body(..., embed=True),\n    status: str = Body(..., embed=True),\n    role: str = Body(..., embed=True),\n    username: str = Body(..., embed=True),\n):\n    \"\"\"\n    Updates the status and metadata of a scenario.\n\n    Args:\n        scenario_name (str): Name of the scenario.\n        start_time (str): Start time of the scenario.\n        end_time (str): End time of the scenario.\n        scenario (dict): Scenario configuration.\n        status (str): New status of the scenario (e.g., \"running\", \"finished\").\n        role (str): Role associated with the scenario.\n        username (str): User performing the update.\n\n    Returns:\n        dict: A message confirming the update.\n    \"\"\"\n    from nebula.controller.database import scenario_update_record\n    from nebula.controller.scenarios import Scenario\n\n    try:\n        scenario = Scenario.from_dict(scenario)\n        scenario_update_record(scenario_name, start_time, end_time, scenario, status, role, username)\n    except Exception as e:\n        logging.exception(f\"Error updating scenario {scenario_name}: {e}\")\n        raise HTTPException(status_code=500, detail=\"Internal server error\")\n\n    return {\"message\": f\"Scenario {scenario_name} updated successfully\"}\n</code></pre>"},{"location":"api/controller/controller/#nebula.controller.controller.update_user_controller","title":"<code>update_user_controller(user=Body(...), password=Body(...), role=Body(...))</code>  <code>async</code>","text":"<p>Controller endpoint that modifies a user of the database.</p> <p>Parameters: - user: The username of the user. - password: The user's password. - role: The role of the user.</p> <p>Returns a success message if the user is updated, or an HTTP error if an exception occurs.</p> Source code in <code>nebula/controller/controller.py</code> <pre><code>@app.post(\"/user/update\")\nasync def update_user_controller(user: str = Body(...), password: str = Body(...), role: str = Body(...)):\n    \"\"\"\n    Controller endpoint that modifies a user of the database.\n\n    Parameters:\n    - user: The username of the user.\n    - password: The user's password.\n    - role: The role of the user.\n\n    Returns a success message if the user is updated, or an HTTP error if an exception occurs.\n    \"\"\"\n    from nebula.controller.database import update_user\n\n    try:\n        update_user(user, password, role)\n        return {\"detail\": \"User updated successfully\"}\n    except Exception as e:\n        logging.exception(f\"Error updating user: {e}\")\n        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f\"Error updating user: {e}\")\n</code></pre>"},{"location":"api/controller/controller/#nebula.controller.controller.verify_user_controller","title":"<code>verify_user_controller(user=Body(...), password=Body(...))</code>  <code>async</code>","text":"<p>Endpoint to verify user credentials.</p> <p>Body Parameters: - user: Username. - password: Password.</p> <p>Returns the user role on success or raises an error on failure.</p> Source code in <code>nebula/controller/controller.py</code> <pre><code>@app.post(\"/user/verify\")\nasync def verify_user_controller(user: str = Body(...), password: str = Body(...)):\n    \"\"\"\n    Endpoint to verify user credentials.\n\n    Body Parameters:\n    - user: Username.\n    - password: Password.\n\n    Returns the user role on success or raises an error on failure.\n    \"\"\"\n    from nebula.controller.database import get_user_info, list_users, verify\n\n    try:\n        user_submitted = user.upper()\n        if (user_submitted in list_users()) and verify(user_submitted, password):\n            user_info = get_user_info(user_submitted)\n            return {\"user\": user_submitted, \"role\": user_info[2]}\n        else:\n            raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED)\n    except Exception as e:\n        logging.exception(f\"Error verifying user: {e}\")\n        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f\"Error verifying user: {e}\")\n</code></pre>"},{"location":"api/controller/database/","title":"Documentation for Database Module","text":""},{"location":"api/controller/database/#nebula.controller.database.add_user","title":"<code>add_user(user, password, role)</code>","text":"<p>Adds a new user to the users database with a hashed password.</p> <p>Parameters:</p> Name Type Description Default <code>user</code> <code>str</code> <p>The username to add (stored in uppercase).</p> required <code>password</code> <code>str</code> <p>The plain text password to hash and store.</p> required <code>role</code> <code>str</code> <p>The role assigned to the user.</p> required Source code in <code>nebula/controller/database.py</code> <pre><code>def add_user(user, password, role):\n    \"\"\"\n    Adds a new user to the users database with a hashed password.\n\n    Args:\n        user (str): The username to add (stored in uppercase).\n        password (str): The plain text password to hash and store.\n        role (str): The role assigned to the user.\n    \"\"\"\n    ph = PasswordHasher()\n    with sqlite3.connect(user_db_file_location) as conn:\n        c = conn.cursor()\n        c.execute(\n            \"INSERT INTO users VALUES (?, ?, ?)\",\n            (user.upper(), ph.hash(password), role),\n        )\n</code></pre>"},{"location":"api/controller/database/#nebula.controller.database.check_scenario_federation_completed","title":"<code>check_scenario_federation_completed(scenario_name)</code>","text":"<p>Check if all nodes in a given scenario have completed the required federation rounds.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_name</code> <code>str</code> <p>The unique name identifier of the scenario to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if all nodes have completed the total rounds specified for the scenario, False otherwise or if an error occurs.</p> Behavior <ul> <li>Retrieves the total number of rounds defined for the scenario.</li> <li>Fetches the current round progress of all nodes in that scenario.</li> <li>Returns True only if every node has reached the total rounds.</li> <li>Handles database errors and missing scenario cases gracefully.</li> </ul> Source code in <code>nebula/controller/database.py</code> <pre><code>def check_scenario_federation_completed(scenario_name):\n    \"\"\"\n    Check if all nodes in a given scenario have completed the required federation rounds.\n\n    Parameters:\n        scenario_name (str): The unique name identifier of the scenario to check.\n\n    Returns:\n        bool: True if all nodes have completed the total rounds specified for the scenario, False otherwise or if an error occurs.\n\n    Behavior:\n        - Retrieves the total number of rounds defined for the scenario.\n        - Fetches the current round progress of all nodes in that scenario.\n        - Returns True only if every node has reached the total rounds.\n        - Handles database errors and missing scenario cases gracefully.\n    \"\"\"\n    try:\n        # Connect to the scenario database to get the total rounds for the scenario\n        with sqlite3.connect(scenario_db_file_location) as conn:\n            conn.row_factory = sqlite3.Row\n            c = conn.cursor()\n            c.execute(\"SELECT rounds FROM scenarios WHERE name = ?;\", (scenario_name,))\n            scenario = c.fetchone()\n\n            if not scenario:\n                raise ValueError(f\"Scenario '{scenario_name}' not found.\")\n\n            total_rounds = scenario[\"rounds\"]\n\n        # Connect to the node database to check the rounds for each node\n        with sqlite3.connect(node_db_file_location) as conn:\n            conn.row_factory = sqlite3.Row\n            c = conn.cursor()\n            c.execute(\"SELECT round FROM nodes WHERE scenario = ?;\", (scenario_name,))\n            nodes = c.fetchall()\n\n            if len(nodes) == 0:\n                return False\n\n            # Check if all nodes have completed the total rounds\n            total_rounds_str = str(total_rounds)\n            return all(str(node[\"round\"]) == total_rounds_str for node in nodes)\n\n    except sqlite3.Error as e:\n        print(f\"Database error: {e}\")\n        return False\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return False\n</code></pre>"},{"location":"api/controller/database/#nebula.controller.database.check_scenario_with_role","title":"<code>check_scenario_with_role(role, scenario_name)</code>","text":"<p>Verify if a scenario exists with a specific role and name.</p> <p>Parameters:</p> Name Type Description Default <code>role</code> <code>str</code> <p>The role associated with the scenario (e.g., \"admin\", \"user\").</p> required <code>scenario_name</code> <code>str</code> <p>The unique name identifier of the scenario.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if a scenario with the given role and name exists, False otherwise.</p> Source code in <code>nebula/controller/database.py</code> <pre><code>def check_scenario_with_role(role, scenario_name):\n    \"\"\"\n    Verify if a scenario exists with a specific role and name.\n\n    Parameters:\n        role (str): The role associated with the scenario (e.g., \"admin\", \"user\").\n        scenario_name (str): The unique name identifier of the scenario.\n\n    Returns:\n        bool: True if a scenario with the given role and name exists, False otherwise.\n    \"\"\"\n    with sqlite3.connect(scenario_db_file_location) as conn:\n        conn.row_factory = sqlite3.Row\n        c = conn.cursor()\n        c.execute(\n            \"SELECT * FROM scenarios WHERE role = ? AND name = ?;\",\n            (\n                role,\n                scenario_name,\n            ),\n        )\n        result = c.fetchone()\n\n    return result is not None\n</code></pre>"},{"location":"api/controller/database/#nebula.controller.database.delete_user_from_db","title":"<code>delete_user_from_db(user)</code>","text":"<p>Deletes a user record from the users database.</p> <p>Parameters:</p> Name Type Description Default <code>user</code> <code>str</code> <p>The username of the user to be deleted.</p> required Source code in <code>nebula/controller/database.py</code> <pre><code>def delete_user_from_db(user):\n    \"\"\"\n    Deletes a user record from the users database.\n\n    Args:\n        user (str): The username of the user to be deleted.\n    \"\"\"\n    with sqlite3.connect(user_db_file_location) as conn:\n        c = conn.cursor()\n        c.execute(\"DELETE FROM users WHERE user = ?\", (user,))\n</code></pre>"},{"location":"api/controller/database/#nebula.controller.database.ensure_columns","title":"<code>ensure_columns(conn, table_name, desired_columns)</code>  <code>async</code>","text":"<p>Ensures that a table contains all the desired columns, adding any that are missing.</p> This function <ul> <li>Retrieves the current columns of the specified table.</li> <li>Compares them with the desired columns.</li> <li>Adds any missing columns to the table using ALTER TABLE statements.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>conn</code> <code>Connection</code> <p>Active connection to the SQLite database.</p> required <code>table_name</code> <code>str</code> <p>Name of the table to check and modify.</p> required <code>desired_columns</code> <code>dict</code> <p>Dictionary mapping column names to their SQL definitions.</p> required Note <p>This operation is committed immediately after all changes are applied.</p> Source code in <code>nebula/controller/database.py</code> <pre><code>async def ensure_columns(conn, table_name, desired_columns):\n    \"\"\"\n    Ensures that a table contains all the desired columns, adding any that are missing.\n\n    This function:\n        - Retrieves the current columns of the specified table.\n        - Compares them with the desired columns.\n        - Adds any missing columns to the table using ALTER TABLE statements.\n\n    Args:\n        conn (aiosqlite.Connection): Active connection to the SQLite database.\n        table_name (str): Name of the table to check and modify.\n        desired_columns (dict): Dictionary mapping column names to their SQL definitions.\n\n    Note:\n        This operation is committed immediately after all changes are applied.\n    \"\"\"\n    _c = await conn.execute(f\"PRAGMA table_info({table_name});\")\n    existing_columns = [row[1] for row in await _c.fetchall()]\n    for column_name, column_definition in desired_columns.items():\n        if column_name not in existing_columns:\n            await conn.execute(f\"ALTER TABLE {table_name} ADD COLUMN {column_name} {column_definition};\")\n    await conn.commit()\n</code></pre>"},{"location":"api/controller/database/#nebula.controller.database.get_all_scenarios","title":"<code>get_all_scenarios(username, role, sort_by='start_time')</code>","text":"<p>Retrieve all scenarios from the database filtered by user role and sorted by a specified field.</p> <p>Parameters:</p> Name Type Description Default <code>username</code> <code>str</code> <p>The username of the requesting user.</p> required <code>role</code> <code>str</code> <p>The role of the user, e.g., \"admin\" or regular user.</p> required <code>sort_by</code> <code>str</code> <p>The field name to sort the results by. Defaults to \"start_time\".</p> <code>'start_time'</code> <p>Returns:</p> Type Description <p>list[sqlite3.Row]: A list of scenario records as SQLite Row objects.</p> Behavior <ul> <li>Admin users retrieve all scenarios.</li> <li>Non-admin users retrieve only scenarios associated with their username.</li> <li>Sorting by \"start_time\" applies custom datetime ordering.</li> <li>Other sort fields are applied directly in the ORDER BY clause.</li> </ul> Source code in <code>nebula/controller/database.py</code> <pre><code>def get_all_scenarios(username, role, sort_by=\"start_time\"):\n    \"\"\"\n    Retrieve all scenarios from the database filtered by user role and sorted by a specified field.\n\n    Parameters:\n        username (str): The username of the requesting user.\n        role (str): The role of the user, e.g., \"admin\" or regular user.\n        sort_by (str, optional): The field name to sort the results by. Defaults to \"start_time\".\n\n    Returns:\n        list[sqlite3.Row]: A list of scenario records as SQLite Row objects.\n\n    Behavior:\n        - Admin users retrieve all scenarios.\n        - Non-admin users retrieve only scenarios associated with their username.\n        - Sorting by \"start_time\" applies custom datetime ordering.\n        - Other sort fields are applied directly in the ORDER BY clause.\n    \"\"\"\n    with sqlite3.connect(scenario_db_file_location) as conn:\n        conn.row_factory = sqlite3.Row\n        c = conn.cursor()\n        if role == \"admin\":\n            if sort_by == \"start_time\":\n                command = \"\"\"\n                SELECT * FROM scenarios\n                ORDER BY strftime('%Y-%m-%d %H:%M:%S', substr(start_time, 7, 4) || '-' || substr(start_time, 4, 2) || '-' || substr(start_time, 1, 2) || ' ' || substr(start_time, 12, 8));\n                \"\"\"\n                c.execute(command)\n            else:\n                command = \"SELECT * FROM scenarios ORDER BY ?;\"\n                c.execute(command, (sort_by,))\n        else:\n            if sort_by == \"start_time\":\n                command = \"\"\"\n                SELECT * FROM scenarios\n                WHERE username = ?\n                ORDER BY strftime('%Y-%m-%d %H:%M:%S', substr(start_time, 7, 4) || '-' || substr(start_time, 4, 2) || '-' || substr(start_time, 1, 2) || ' ' || substr(start_time, 12, 8));\n                \"\"\"\n                c.execute(command, (username,))\n            else:\n                command = \"SELECT * FROM scenarios WHERE username = ? ORDER BY ?;\"\n                c.execute(\n                    command,\n                    (\n                        username,\n                        sort_by,\n                    ),\n                )\n        result = c.fetchall()\n\n    return result\n</code></pre>"},{"location":"api/controller/database/#nebula.controller.database.get_all_scenarios_and_check_completed","title":"<code>get_all_scenarios_and_check_completed(username, role, sort_by='start_time')</code>","text":"<p>Retrieve all scenarios with detailed fields and update the status of running scenarios if their federation is completed.</p> <p>Parameters:</p> Name Type Description Default <code>username</code> <code>str</code> <p>The username of the requesting user.</p> required <code>role</code> <code>str</code> <p>The role of the user, e.g., \"admin\" or regular user.</p> required <code>sort_by</code> <code>str</code> <p>The field name to sort the results by. Defaults to \"start_time\".</p> <code>'start_time'</code> <p>Returns:</p> Type Description <p>list[sqlite3.Row]: A list of scenario records including name, username, title, start_time, model, dataset, rounds, and status.</p> Behavior <ul> <li>Admin users retrieve all scenarios.</li> <li>Non-admin users retrieve only scenarios associated with their username.</li> <li>Scenarios are sorted by start_time with special handling for null or empty values.</li> <li>For scenarios with status \"running\", checks if federation is completed:<ul> <li>If completed, updates the scenario status to \"completed\".</li> <li>Refreshes the returned scenario list after updates.</li> </ul> </li> </ul> Source code in <code>nebula/controller/database.py</code> <pre><code>def get_all_scenarios_and_check_completed(username, role, sort_by=\"start_time\"):\n    \"\"\"\n    Retrieve all scenarios with detailed fields and update the status of running scenarios if their federation is completed.\n\n    Parameters:\n        username (str): The username of the requesting user.\n        role (str): The role of the user, e.g., \"admin\" or regular user.\n        sort_by (str, optional): The field name to sort the results by. Defaults to \"start_time\".\n\n    Returns:\n        list[sqlite3.Row]: A list of scenario records including name, username, title, start_time, model, dataset, rounds, and status.\n\n    Behavior:\n        - Admin users retrieve all scenarios.\n        - Non-admin users retrieve only scenarios associated with their username.\n        - Scenarios are sorted by start_time with special handling for null or empty values.\n        - For scenarios with status \"running\", checks if federation is completed:\n            - If completed, updates the scenario status to \"completed\".\n            - Refreshes the returned scenario list after updates.\n    \"\"\"\n    with sqlite3.connect(scenario_db_file_location) as conn:\n        conn.row_factory = sqlite3.Row\n        c = conn.cursor()\n\n        if role == \"admin\":\n            if sort_by == \"start_time\":\n                command = \"\"\"\n                SELECT name, username, title, start_time, model, dataset, rounds, status FROM scenarios\n                ORDER BY\n                    CASE\n                        WHEN start_time IS NULL OR start_time = '' THEN 1\n                        ELSE 0\n                    END,\n                    strftime(\n                        '%Y-%m-%d %H:%M:%S',\n                        substr(start_time, 7, 4) || '-' || substr(start_time, 4, 2) || '-' || substr(start_time, 1, 2) || ' ' || substr(start_time, 12, 8)\n                    );\n                \"\"\"\n                c.execute(command)\n            else:\n                command = \"SELECT name, username, title, start_time, model, dataset, rounds, status FROM scenarios ORDER BY ?;\"\n                c.execute(command, (sort_by,))\n            result = c.fetchall()\n        else:\n            if sort_by == \"start_time\":\n                command = \"\"\"\n                SELECT name, username, title, start_time, model, dataset, rounds, status FROM scenarios\n                WHERE username = ?\n                ORDER BY\n                    CASE\n                        WHEN start_time IS NULL OR start_time = '' THEN 1\n                        ELSE 0\n                    END,\n                    strftime(\n                        '%Y-%m-%d %H:%M:%S',\n                        substr(start_time, 7, 4) || '-' || substr(start_time, 4, 2) || '-' || substr(start_time, 1, 2) || ' ' || substr(start_time, 12, 8)\n                    );\n                \"\"\"\n                c.execute(command, (username,))\n            else:\n                command = \"SELECT name, username, title, start_time, model, dataset, rounds, status FROM scenarios WHERE username = ? ORDER BY ?;\"\n                c.execute(\n                    command,\n                    (\n                        username,\n                        sort_by,\n                    ),\n                )\n            result = c.fetchall()\n\n        for scenario in result:\n            if scenario[\"status\"] == \"running\":\n                if check_scenario_federation_completed(scenario[\"name\"]):\n                    scenario_set_status_to_completed(scenario[\"name\"])\n                    result = get_all_scenarios(username, role)\n\n    return result\n</code></pre>"},{"location":"api/controller/database/#nebula.controller.database.get_completed_scenario","title":"<code>get_completed_scenario()</code>","text":"<p>Retrieve a single scenario with status \"completed\" from the database.</p> <p>Returns:</p> Type Description <p>sqlite3.Row: A scenario record with status \"completed\", or None if no such scenario exists.</p> Behavior <ul> <li>Fetches the first scenario found with status \"completed\".</li> </ul> Source code in <code>nebula/controller/database.py</code> <pre><code>def get_completed_scenario():\n    \"\"\"\n    Retrieve a single scenario with status \"completed\" from the database.\n\n    Returns:\n        sqlite3.Row: A scenario record with status \"completed\", or None if no such scenario exists.\n\n    Behavior:\n        - Fetches the first scenario found with status \"completed\".\n    \"\"\"\n    with sqlite3.connect(scenario_db_file_location) as conn:\n        conn.row_factory = sqlite3.Row\n        c = conn.cursor()\n        command = \"SELECT * FROM scenarios WHERE status = ?;\"\n        c.execute(command, (\"completed\",))\n        result = c.fetchone()\n\n    return result\n</code></pre>"},{"location":"api/controller/database/#nebula.controller.database.get_notes","title":"<code>get_notes(scenario)</code>","text":"<p>Retrieve notes associated with a specific scenario.</p> <p>Parameters:</p> Name Type Description Default <code>scenario</code> <code>str</code> <p>The unique identifier of the scenario.</p> required <p>Returns:</p> Type Description <p>sqlite3.Row or None: The notes record for the given scenario, or None if no notes exist.</p> Source code in <code>nebula/controller/database.py</code> <pre><code>def get_notes(scenario):\n    \"\"\"\n    Retrieve notes associated with a specific scenario.\n\n    Parameters:\n        scenario (str): The unique identifier of the scenario.\n\n    Returns:\n        sqlite3.Row or None: The notes record for the given scenario, or None if no notes exist.\n    \"\"\"\n    with sqlite3.connect(notes_db_file_location) as conn:\n        conn.row_factory = sqlite3.Row\n        c = conn.cursor()\n        c.execute(\"SELECT * FROM notes WHERE scenario = ?;\", (scenario,))\n        result = c.fetchone()\n\n    return result\n</code></pre>"},{"location":"api/controller/database/#nebula.controller.database.get_running_scenario","title":"<code>get_running_scenario(username=None, get_all=False)</code>","text":"<p>Retrieve running or completed scenarios from the database, optionally filtered by username.</p> <p>Parameters:</p> Name Type Description Default <code>username</code> <code>str</code> <p>The username to filter scenarios by. If None, no user filter is applied.</p> <code>None</code> <code>get_all</code> <code>bool</code> <p>If True, returns all matching scenarios; otherwise returns only one. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>sqlite3.Row or list[sqlite3.Row]: A single scenario record or a list of scenario records matching the criteria.</p> Behavior <ul> <li>Filters scenarios with status \"running\".</li> <li>Applies username filter if provided.</li> <li>Returns either one or all matching records depending on get_all.</li> </ul> Source code in <code>nebula/controller/database.py</code> <pre><code>def get_running_scenario(username=None, get_all=False):\n    \"\"\"\n    Retrieve running or completed scenarios from the database, optionally filtered by username.\n\n    Parameters:\n        username (str, optional): The username to filter scenarios by. If None, no user filter is applied.\n        get_all (bool, optional): If True, returns all matching scenarios; otherwise returns only one. Defaults to False.\n\n    Returns:\n        sqlite3.Row or list[sqlite3.Row]: A single scenario record or a list of scenario records matching the criteria.\n\n    Behavior:\n        - Filters scenarios with status \"running\".\n        - Applies username filter if provided.\n        - Returns either one or all matching records depending on get_all.\n    \"\"\"\n    with sqlite3.connect(scenario_db_file_location) as conn:\n        conn.row_factory = sqlite3.Row\n        c = conn.cursor()\n\n        if username:\n            command = \"\"\"\n                SELECT * FROM scenarios\n                WHERE (status = ?) AND username = ?;\n            \"\"\"\n            c.execute(command, (\"running\", username))\n\n            result = c.fetchone()\n        else:\n            command = \"SELECT * FROM scenarios WHERE status = ?;\"\n            c.execute(command, (\"running\",))\n            if get_all:\n                result = c.fetchall()\n            else:\n                result = c.fetchone()\n\n    return result\n</code></pre>"},{"location":"api/controller/database/#nebula.controller.database.get_scenario_by_name","title":"<code>get_scenario_by_name(scenario_name)</code>","text":"<p>Retrieve a scenario record by its unique name.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_name</code> <code>str</code> <p>The unique name identifier of the scenario.</p> required <p>Returns:</p> Type Description <p>sqlite3.Row: The scenario record matching the given name, or None if not found.</p> Source code in <code>nebula/controller/database.py</code> <pre><code>def get_scenario_by_name(scenario_name):\n    \"\"\"\n    Retrieve a scenario record by its unique name.\n\n    Parameters:\n        scenario_name (str): The unique name identifier of the scenario.\n\n    Returns:\n        sqlite3.Row: The scenario record matching the given name, or None if not found.\n    \"\"\"\n    with sqlite3.connect(scenario_db_file_location) as conn:\n        conn.row_factory = sqlite3.Row\n        c = conn.cursor()\n        c.execute(\"SELECT * FROM scenarios WHERE name = ?;\", (scenario_name,))\n        result = c.fetchone()\n\n    return result\n</code></pre>"},{"location":"api/controller/database/#nebula.controller.database.get_user_by_scenario_name","title":"<code>get_user_by_scenario_name(scenario_name)</code>","text":"<p>Retrieve the username associated with a given scenario name.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_name</code> <code>str</code> <p>The unique name identifier of the scenario.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The username linked to the specified scenario, or None if not found.</p> Source code in <code>nebula/controller/database.py</code> <pre><code>def get_user_by_scenario_name(scenario_name):\n    \"\"\"\n    Retrieve the username associated with a given scenario name.\n\n    Parameters:\n        scenario_name (str): The unique name identifier of the scenario.\n\n    Returns:\n        str: The username linked to the specified scenario, or None if not found.\n    \"\"\"\n    with sqlite3.connect(scenario_db_file_location) as conn:\n        conn.row_factory = sqlite3.Row\n        c = conn.cursor()\n        c.execute(\"SELECT username FROM scenarios WHERE name = ?;\", (scenario_name,))\n        result = c.fetchone()\n\n    return result[\"username\"]\n</code></pre>"},{"location":"api/controller/database/#nebula.controller.database.get_user_info","title":"<code>get_user_info(user)</code>","text":"<p>Fetches detailed information for a specific user from the users database.</p> <p>Parameters:</p> Name Type Description Default <code>user</code> <code>str</code> <p>The username to retrieve information for.</p> required <p>Returns:</p> Type Description <p>sqlite3.Row or None: A row containing the user's information if found, otherwise None.</p> Source code in <code>nebula/controller/database.py</code> <pre><code>def get_user_info(user):\n    \"\"\"\n    Fetches detailed information for a specific user from the users database.\n\n    Args:\n        user (str): The username to retrieve information for.\n\n    Returns:\n        sqlite3.Row or None: A row containing the user's information if found, otherwise None.\n    \"\"\"\n    with sqlite3.connect(user_db_file_location) as conn:\n        conn.row_factory = sqlite3.Row\n        c = conn.cursor()\n\n        command = \"SELECT * FROM users WHERE user = ?\"\n        c.execute(command, (user,))\n        result = c.fetchone()\n\n    return result\n</code></pre>"},{"location":"api/controller/database/#nebula.controller.database.initialize_databases","title":"<code>initialize_databases(databases_dir)</code>  <code>async</code>","text":"<p>Initializes all required SQLite databases and their corresponding tables for the system.</p> This function <ul> <li>Defines paths for user, node, scenario, and notes databases based on the provided directory.</li> <li>Sets up each database with appropriate PRAGMA settings.</li> <li>Creates necessary tables if they do not exist.</li> <li>Ensures all expected columns are present in each table, adding any missing ones.</li> <li>Creates a default admin user if no users are present.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>databases_dir</code> <code>str</code> <p>Path to the directory where the database files will be created or accessed.</p> required Note <p>Default credentials (username and password) are taken from environment variables: - NEBULA_DEFAULT_USER - NEBULA_DEFAULT_PASSWORD</p> Source code in <code>nebula/controller/database.py</code> <pre><code>async def initialize_databases(databases_dir):\n    \"\"\"\n    Initializes all required SQLite databases and their corresponding tables for the system.\n\n    This function:\n        - Defines paths for user, node, scenario, and notes databases based on the provided directory.\n        - Sets up each database with appropriate PRAGMA settings.\n        - Creates necessary tables if they do not exist.\n        - Ensures all expected columns are present in each table, adding any missing ones.\n        - Creates a default admin user if no users are present.\n\n    Args:\n        databases_dir (str): Path to the directory where the database files will be created or accessed.\n\n    Note:\n        Default credentials (username and password) are taken from environment variables:\n        - NEBULA_DEFAULT_USER\n        - NEBULA_DEFAULT_PASSWORD\n    \"\"\"\n    global user_db_file_location, node_db_file_location, scenario_db_file_location, notes_db_file_location\n\n    user_db_file_location = os.path.join(databases_dir, \"users.db\")\n    node_db_file_location = os.path.join(databases_dir, \"nodes.db\")\n    scenario_db_file_location = os.path.join(databases_dir, \"scenarios.db\")\n    notes_db_file_location = os.path.join(databases_dir, \"notes.db\")\n\n    await setup_database(user_db_file_location)\n    await setup_database(node_db_file_location)\n    await setup_database(scenario_db_file_location)\n    await setup_database(notes_db_file_location)\n\n    async with aiosqlite.connect(user_db_file_location) as conn:\n        await conn.execute(\n            \"\"\"\n            CREATE TABLE IF NOT EXISTS users (\n                user TEXT PRIMARY KEY,\n                password TEXT,\n                role TEXT\n            );\n            \"\"\"\n        )\n        desired_columns = {\"user\": \"TEXT PRIMARY KEY\", \"password\": \"TEXT\", \"role\": \"TEXT\"}\n        await ensure_columns(conn, \"users\", desired_columns)\n\n    async with aiosqlite.connect(node_db_file_location) as conn:\n        await conn.execute(\n            \"\"\"\n            CREATE TABLE IF NOT EXISTS nodes (\n                uid TEXT PRIMARY KEY,\n                idx TEXT,\n                ip TEXT,\n                port TEXT,\n                role TEXT,\n                neighbors TEXT,\n                latitude TEXT,\n                longitude TEXT,\n                timestamp TEXT,\n                federation TEXT,\n                round TEXT,\n                scenario TEXT,\n                hash TEXT,\n                malicious TEXT\n            );\n            \"\"\"\n        )\n        desired_columns = {\n            \"uid\": \"TEXT PRIMARY KEY\",\n            \"idx\": \"TEXT\",\n            \"ip\": \"TEXT\",\n            \"port\": \"TEXT\",\n            \"role\": \"TEXT\",\n            \"neighbors\": \"TEXT\",\n            \"latitude\": \"TEXT\",\n            \"longitude\": \"TEXT\",\n            \"timestamp\": \"TEXT\",\n            \"federation\": \"TEXT\",\n            \"round\": \"TEXT\",\n            \"scenario\": \"TEXT\",\n            \"hash\": \"TEXT\",\n            \"malicious\": \"TEXT\",\n        }\n        await ensure_columns(conn, \"nodes\", desired_columns)\n\n    async with aiosqlite.connect(scenario_db_file_location) as conn:\n        await conn.execute(\n            \"\"\"\n            CREATE TABLE IF NOT EXISTS scenarios (\n                name TEXT PRIMARY KEY,\n                start_time TEXT,\n                end_time TEXT,\n                title TEXT,\n                description TEXT,\n                deployment TEXT,\n                federation TEXT,\n                topology TEXT,\n                nodes TEXT,\n                nodes_graph TEXT,\n                n_nodes TEXT,\n                matrix TEXT,\n                random_topology_probability TEXT,\n                dataset TEXT,\n                iid TEXT,\n                partition_selection TEXT,\n                partition_parameter TEXT,\n                model TEXT,\n                agg_algorithm TEXT,\n                rounds TEXT,\n                logginglevel TEXT,\n                report_status_data_queue TEXT,\n                accelerator TEXT,\n                network_subnet TEXT,\n                network_gateway TEXT,\n                epochs TEXT,\n                attack_params TEXT,\n                reputation TEXT,\n                random_geo TEXT,\n                latitude TEXT,\n                longitude TEXT,\n                mobility TEXT,\n                mobility_type TEXT,\n                radius_federation TEXT,\n                scheme_mobility TEXT,\n                round_frequency TEXT,\n                mobile_participants_percent TEXT,\n                additional_participants TEXT,\n                schema_additional_participants TEXT,\n                status TEXT,\n                role TEXT,\n                username TEXT,\n                gpu_id TEXT\n            );\n            \"\"\"\n        )\n        desired_columns = {\n            \"name\": \"TEXT PRIMARY KEY\",\n            \"start_time\": \"TEXT\",\n            \"end_time\": \"TEXT\",\n            \"title\": \"TEXT\",\n            \"description\": \"TEXT\",\n            \"deployment\": \"TEXT\",\n            \"federation\": \"TEXT\",\n            \"topology\": \"TEXT\",\n            \"nodes\": \"TEXT\",\n            \"nodes_graph\": \"TEXT\",\n            \"n_nodes\": \"TEXT\",\n            \"matrix\": \"TEXT\",\n            \"random_topology_probability\": \"TEXT\",\n            \"dataset\": \"TEXT\",\n            \"iid\": \"TEXT\",\n            \"partition_selection\": \"TEXT\",\n            \"partition_parameter\": \"TEXT\",\n            \"model\": \"TEXT\",\n            \"agg_algorithm\": \"TEXT\",\n            \"rounds\": \"TEXT\",\n            \"logginglevel\": \"TEXT\",\n            \"report_status_data_queue\": \"TEXT\",\n            \"accelerator\": \"TEXT\",\n            \"gpu_id\": \"TEXT\",\n            \"network_subnet\": \"TEXT\",\n            \"network_gateway\": \"TEXT\",\n            \"epochs\": \"TEXT\",\n            \"attack_params\": \"TEXT\",\n            \"reputation\": \"TEXT\",\n            \"random_geo\": \"TEXT\",\n            \"latitude\": \"TEXT\",\n            \"longitude\": \"TEXT\",\n            \"mobility\": \"TEXT\",\n            \"mobility_type\": \"TEXT\",\n            \"radius_federation\": \"TEXT\",\n            \"scheme_mobility\": \"TEXT\",\n            \"round_frequency\": \"TEXT\",\n            \"mobile_participants_percent\": \"TEXT\",\n            \"additional_participants\": \"TEXT\",\n            \"schema_additional_participants\": \"TEXT\",\n            \"status\": \"TEXT\",\n            \"role\": \"TEXT\",\n            \"username\": \"TEXT\",\n        }\n        await ensure_columns(conn, \"scenarios\", desired_columns)\n\n    async with aiosqlite.connect(notes_db_file_location) as conn:\n        await conn.execute(\n            \"\"\"\n            CREATE TABLE IF NOT EXISTS notes (\n                scenario TEXT PRIMARY KEY,\n                scenario_notes TEXT\n            );\n            \"\"\"\n        )\n        desired_columns = {\"scenario\": \"TEXT PRIMARY KEY\", \"scenario_notes\": \"TEXT\"}\n        await ensure_columns(conn, \"notes\", desired_columns)\n\n    username = os.environ.get(\"NEBULA_DEFAULT_USER\", \"admin\")\n    password = os.environ.get(\"NEBULA_DEFAULT_PASSWORD\", \"admin\")\n    if not list_users():\n        add_user(username, password, \"admin\")\n    if not verify_hash_algorithm(username):\n        update_user(username, password, \"admin\")\n</code></pre>"},{"location":"api/controller/database/#nebula.controller.database.list_nodes","title":"<code>list_nodes(scenario_name=None, sort_by='idx')</code>","text":"<p>Retrieves a list of nodes from the nodes database, optionally filtered by scenario and sorted.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_name</code> <code>str</code> <p>Name of the scenario to filter nodes by. If None, returns all nodes.</p> <code>None</code> <code>sort_by</code> <code>str</code> <p>Column name to sort the results by. Defaults to \"idx\".</p> <code>'idx'</code> <p>Returns:</p> Type Description <p>list or None: A list of sqlite3.Row objects representing nodes, or None if an error occurs.</p> Source code in <code>nebula/controller/database.py</code> <pre><code>def list_nodes(scenario_name=None, sort_by=\"idx\"):\n    \"\"\"\n    Retrieves a list of nodes from the nodes database, optionally filtered by scenario and sorted.\n\n    Args:\n        scenario_name (str, optional): Name of the scenario to filter nodes by. If None, returns all nodes.\n        sort_by (str): Column name to sort the results by. Defaults to \"idx\".\n\n    Returns:\n        list or None: A list of sqlite3.Row objects representing nodes, or None if an error occurs.\n    \"\"\"\n    try:\n        with sqlite3.connect(node_db_file_location) as conn:\n            conn.row_factory = sqlite3.Row\n            c = conn.cursor()\n\n            if scenario_name:\n                command = \"SELECT * FROM nodes WHERE scenario = ? ORDER BY \" + sort_by + \";\"\n                c.execute(command, (scenario_name,))\n            else:\n                command = \"SELECT * FROM nodes ORDER BY \" + sort_by + \";\"\n                c.execute(command)\n\n            result = c.fetchall()\n\n            return result\n    except sqlite3.Error as e:\n        print(f\"Error occurred while listing nodes: {e}\")\n        return None\n</code></pre>"},{"location":"api/controller/database/#nebula.controller.database.list_nodes_by_scenario_name","title":"<code>list_nodes_by_scenario_name(scenario_name)</code>","text":"<p>Fetches all nodes associated with a specific scenario, ordered by their index as integers.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_name</code> <code>str</code> <p>The name of the scenario to filter nodes by.</p> required <p>Returns:</p> Type Description <p>list or None: A list of sqlite3.Row objects for nodes in the scenario, or None if an error occurs.</p> Source code in <code>nebula/controller/database.py</code> <pre><code>def list_nodes_by_scenario_name(scenario_name):\n    \"\"\"\n    Fetches all nodes associated with a specific scenario, ordered by their index as integers.\n\n    Args:\n        scenario_name (str): The name of the scenario to filter nodes by.\n\n    Returns:\n        list or None: A list of sqlite3.Row objects for nodes in the scenario, or None if an error occurs.\n    \"\"\"\n    try:\n        with sqlite3.connect(node_db_file_location) as conn:\n            conn.row_factory = sqlite3.Row\n            c = conn.cursor()\n\n            command = \"SELECT * FROM nodes WHERE scenario = ? ORDER BY CAST(idx AS INTEGER) ASC;\"\n            c.execute(command, (scenario_name,))\n            result = c.fetchall()\n\n            return result\n    except sqlite3.Error as e:\n        print(f\"Error occurred while listing nodes by scenario name: {e}\")\n        return None\n</code></pre>"},{"location":"api/controller/database/#nebula.controller.database.list_users","title":"<code>list_users(all_info=False)</code>","text":"<p>Retrieves a list of users from the users database.</p> <p>Parameters:</p> Name Type Description Default <code>all_info</code> <code>bool</code> <p>If True, returns full user records; otherwise, returns only usernames. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of usernames or full user records depending on the all_info flag.</p> Source code in <code>nebula/controller/database.py</code> <pre><code>def list_users(all_info=False):\n    \"\"\"\n    Retrieves a list of users from the users database.\n\n    Args:\n        all_info (bool): If True, returns full user records; otherwise, returns only usernames. Default is False.\n\n    Returns:\n        list: A list of usernames or full user records depending on the all_info flag.\n    \"\"\"\n    with sqlite3.connect(user_db_file_location) as conn:\n        conn.row_factory = sqlite3.Row\n        c = conn.cursor()\n        c.execute(\"SELECT * FROM users\")\n        result = c.fetchall()\n\n    if not all_info:\n        result = [user[\"user\"] for user in result]\n\n    return result\n</code></pre>"},{"location":"api/controller/database/#nebula.controller.database.remove_all_nodes","title":"<code>remove_all_nodes()</code>","text":"<p>Deletes all node records from the nodes database.</p> <p>This operation removes every entry in the nodes table.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>nebula/controller/database.py</code> <pre><code>def remove_all_nodes():\n    \"\"\"\n    Deletes all node records from the nodes database.\n\n    This operation removes every entry in the nodes table.\n\n    Returns:\n        None\n    \"\"\"\n    with sqlite3.connect(node_db_file_location) as conn:\n        c = conn.cursor()\n        command = \"DELETE FROM nodes;\"\n        c.execute(command)\n</code></pre>"},{"location":"api/controller/database/#nebula.controller.database.remove_nodes_by_scenario_name","title":"<code>remove_nodes_by_scenario_name(scenario_name)</code>","text":"<p>Deletes all nodes associated with a specific scenario from the database.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_name</code> <code>str</code> <p>The name of the scenario whose nodes should be removed.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>nebula/controller/database.py</code> <pre><code>def remove_nodes_by_scenario_name(scenario_name):\n    \"\"\"\n    Deletes all nodes associated with a specific scenario from the database.\n\n    Args:\n        scenario_name (str): The name of the scenario whose nodes should be removed.\n\n    Returns:\n        None\n    \"\"\"\n    with sqlite3.connect(node_db_file_location) as conn:\n        c = conn.cursor()\n        command = \"DELETE FROM nodes WHERE scenario = ?;\"\n        c.execute(command, (scenario_name,))\n</code></pre>"},{"location":"api/controller/database/#nebula.controller.database.remove_note","title":"<code>remove_note(scenario)</code>","text":"<p>Delete the note associated with a specific scenario.</p> <p>Parameters:</p> Name Type Description Default <code>scenario</code> <code>str</code> <p>The unique identifier of the scenario whose note should be removed.</p> required Source code in <code>nebula/controller/database.py</code> <pre><code>def remove_note(scenario):\n    \"\"\"\n    Delete the note associated with a specific scenario.\n\n    Parameters:\n        scenario (str): The unique identifier of the scenario whose note should be removed.\n    \"\"\"\n    with sqlite3.connect(notes_db_file_location) as conn:\n        c = conn.cursor()\n        c.execute(\"DELETE FROM notes WHERE scenario = ?;\", (scenario,))\n        conn.commit()\n</code></pre>"},{"location":"api/controller/database/#nebula.controller.database.remove_scenario_by_name","title":"<code>remove_scenario_by_name(scenario_name)</code>","text":"<p>Delete a scenario from the database by its unique name.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_name</code> <code>str</code> <p>The unique name identifier of the scenario to be removed.</p> required Behavior <ul> <li>Removes the scenario record matching the given name.</li> <li>Commits the deletion to the database.</li> </ul> Source code in <code>nebula/controller/database.py</code> <pre><code>def remove_scenario_by_name(scenario_name):\n    \"\"\"\n    Delete a scenario from the database by its unique name.\n\n    Parameters:\n        scenario_name (str): The unique name identifier of the scenario to be removed.\n\n    Behavior:\n        - Removes the scenario record matching the given name.\n        - Commits the deletion to the database.\n    \"\"\"\n    with sqlite3.connect(scenario_db_file_location) as conn:\n        conn.row_factory = sqlite3.Row\n        c = conn.cursor()\n        c.execute(\"DELETE FROM scenarios WHERE name = ?;\", (scenario_name,))\n        conn.commit()\n</code></pre>"},{"location":"api/controller/database/#nebula.controller.database.save_notes","title":"<code>save_notes(scenario, notes)</code>","text":"<p>Save or update notes associated with a specific scenario.</p> <p>Parameters:</p> Name Type Description Default <code>scenario</code> <code>str</code> <p>The unique identifier of the scenario.</p> required <code>notes</code> <code>str</code> <p>The textual notes to be saved for the scenario.</p> required Behavior <ul> <li>Inserts new notes if the scenario does not exist in the database.</li> <li>Updates existing notes if the scenario already has notes saved.</li> <li>Handles SQLite integrity and general database errors gracefully.</li> </ul> Source code in <code>nebula/controller/database.py</code> <pre><code>def save_notes(scenario, notes):\n    \"\"\"\n    Save or update notes associated with a specific scenario.\n\n    Parameters:\n        scenario (str): The unique identifier of the scenario.\n        notes (str): The textual notes to be saved for the scenario.\n\n    Behavior:\n        - Inserts new notes if the scenario does not exist in the database.\n        - Updates existing notes if the scenario already has notes saved.\n        - Handles SQLite integrity and general database errors gracefully.\n    \"\"\"\n    try:\n        with sqlite3.connect(notes_db_file_location) as conn:\n            c = conn.cursor()\n            c.execute(\n                \"\"\"\n                INSERT INTO notes (scenario, scenario_notes) VALUES (?, ?)\n                ON CONFLICT(scenario) DO UPDATE SET scenario_notes = excluded.scenario_notes;\n                \"\"\",\n                (scenario, notes),\n            )\n            conn.commit()\n    except sqlite3.IntegrityError as e:\n        print(f\"SQLite integrity error: {e}\")\n    except sqlite3.Error as e:\n        print(f\"SQLite error: {e}\")\n</code></pre>"},{"location":"api/controller/database/#nebula.controller.database.scenario_set_all_status_to_finished","title":"<code>scenario_set_all_status_to_finished()</code>","text":"<p>Set the status of all currently running scenarios to \"finished\" and update their end time to the current datetime.</p> Behavior <ul> <li>Finds all scenarios with status \"running\".</li> <li>Updates their status to \"finished\".</li> <li>Sets the end_time to the current timestamp.</li> <li>Commits the changes to the database.</li> </ul> Source code in <code>nebula/controller/database.py</code> <pre><code>def scenario_set_all_status_to_finished():\n    \"\"\"\n    Set the status of all currently running scenarios to \"finished\" and update their end time to the current datetime.\n\n    Behavior:\n        - Finds all scenarios with status \"running\".\n        - Updates their status to \"finished\".\n        - Sets the end_time to the current timestamp.\n        - Commits the changes to the database.\n    \"\"\"\n    with sqlite3.connect(scenario_db_file_location) as conn:\n        conn.row_factory = sqlite3.Row\n        c = conn.cursor()\n        current_time = str(datetime.datetime.now())\n        c.execute(\"UPDATE scenarios SET status = 'finished', end_time = ? WHERE status = 'running';\", (current_time,))\n        conn.commit()\n</code></pre>"},{"location":"api/controller/database/#nebula.controller.database.scenario_set_status_to_completed","title":"<code>scenario_set_status_to_completed(scenario_name)</code>","text":"<p>Set the status of a specific scenario to \"completed\".</p> <p>Parameters:</p> Name Type Description Default <code>scenario_name</code> <code>str</code> <p>The unique name identifier of the scenario to update.</p> required Behavior <ul> <li>Updates the scenario's status to \"completed\".</li> <li>Commits the change to the database.</li> </ul> Source code in <code>nebula/controller/database.py</code> <pre><code>def scenario_set_status_to_completed(scenario_name):\n    \"\"\"\n    Set the status of a specific scenario to \"completed\".\n\n    Parameters:\n        scenario_name (str): The unique name identifier of the scenario to update.\n\n    Behavior:\n        - Updates the scenario's status to \"completed\".\n        - Commits the change to the database.\n    \"\"\"\n    with sqlite3.connect(scenario_db_file_location) as conn:\n        conn.row_factory = sqlite3.Row\n        c = conn.cursor()\n        c.execute(\"UPDATE scenarios SET status = 'completed' WHERE name = ?;\", (scenario_name,))\n        conn.commit()\n</code></pre>"},{"location":"api/controller/database/#nebula.controller.database.scenario_set_status_to_finished","title":"<code>scenario_set_status_to_finished(scenario_name)</code>","text":"<p>Set the status of a specific scenario to \"finished\" and update its end time to the current datetime.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_name</code> <code>str</code> <p>The unique name identifier of the scenario to update.</p> required Behavior <ul> <li>Updates the scenario's status to \"finished\".</li> <li>Sets the end_time to the current timestamp.</li> <li>Commits the update to the database.</li> </ul> Source code in <code>nebula/controller/database.py</code> <pre><code>def scenario_set_status_to_finished(scenario_name):\n    \"\"\"\n    Set the status of a specific scenario to \"finished\" and update its end time to the current datetime.\n\n    Parameters:\n        scenario_name (str): The unique name identifier of the scenario to update.\n\n    Behavior:\n        - Updates the scenario's status to \"finished\".\n        - Sets the end_time to the current timestamp.\n        - Commits the update to the database.\n    \"\"\"\n    with sqlite3.connect(scenario_db_file_location) as conn:\n        conn.row_factory = sqlite3.Row\n        c = conn.cursor()\n        current_time = str(datetime.datetime.now())\n        c.execute(\n            \"UPDATE scenarios SET status = 'finished', end_time = ? WHERE name = ?;\", (current_time, scenario_name)\n        )\n        conn.commit()\n</code></pre>"},{"location":"api/controller/database/#nebula.controller.database.scenario_update_record","title":"<code>scenario_update_record(name, start_time, end_time, scenario, status, role, username)</code>","text":"<p>Insert a new scenario record or update an existing one in the database based on the scenario name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The unique name identifier of the scenario.</p> required <code>start_time</code> <code>str</code> <p>The start time of the scenario.</p> required <code>end_time</code> <code>str</code> <p>The end time of the scenario.</p> required <code>scenario</code> <code>object</code> <p>An object containing detailed scenario attributes.</p> required <code>status</code> <code>str</code> <p>The current status of the scenario.</p> required <code>role</code> <code>str</code> <p>The role of the user performing the operation.</p> required <code>username</code> <code>str</code> <p>The username of the user performing the operation.</p> required Behavior <ul> <li>Checks if a scenario with the given name exists.</li> <li>If not, inserts a new record with all scenario details.</li> <li>If exists, updates the existing record with the provided data.</li> <li>Commits the transaction to persist changes.</li> </ul> Source code in <code>nebula/controller/database.py</code> <pre><code>def scenario_update_record(name, start_time, end_time, scenario, status, role, username):\n    \"\"\"\n    Insert a new scenario record or update an existing one in the database based on the scenario name.\n\n    Parameters:\n        name (str): The unique name identifier of the scenario.\n        start_time (str): The start time of the scenario.\n        end_time (str): The end time of the scenario.\n        scenario (object): An object containing detailed scenario attributes.\n        status (str): The current status of the scenario.\n        role (str): The role of the user performing the operation.\n        username (str): The username of the user performing the operation.\n\n    Behavior:\n        - Checks if a scenario with the given name exists.\n        - If not, inserts a new record with all scenario details.\n        - If exists, updates the existing record with the provided data.\n        - Commits the transaction to persist changes.\n    \"\"\"\n    with sqlite3.connect(scenario_db_file_location) as conn:\n        conn.row_factory = sqlite3.Row\n        c = conn.cursor()\n\n        select_command = \"SELECT * FROM scenarios WHERE name = ?;\"\n        c.execute(select_command, (name,))\n        result = c.fetchone()\n\n        if result is None:\n            insert_command = \"\"\"\n                INSERT INTO scenarios (\n                    name,\n                    start_time,\n                    end_time,\n                    title,\n                    description,\n                    deployment,\n                    federation,\n                    topology,\n                    nodes,\n                    nodes_graph,\n                    n_nodes,\n                    matrix,\n                    random_topology_probability,\n                    dataset,\n                    iid,\n                    partition_selection,\n                    partition_parameter,\n                    model,\n                    agg_algorithm,\n                    rounds,\n                    logginglevel,\n                    report_status_data_queue,\n                    accelerator,\n                    gpu_id,\n                    network_subnet,\n                    network_gateway,\n                    epochs,\n                    attack_params,\n                    reputation,\n                    random_geo,\n                    latitude,\n                    longitude,\n                    mobility,\n                    mobility_type,\n                    radius_federation,\n                    scheme_mobility,\n                    round_frequency,\n                    mobile_participants_percent,\n                    additional_participants,\n                    schema_additional_participants,\n                    status,\n                    role,\n                    username\n                ) VALUES (\n                    ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?\n                );\n            \"\"\"\n            c.execute(\n                insert_command,\n                (\n                    name,\n                    start_time,\n                    end_time,\n                    scenario.scenario_title,\n                    scenario.scenario_description,\n                    scenario.deployment,\n                    scenario.federation,\n                    scenario.topology,\n                    json.dumps(scenario.nodes),\n                    json.dumps(scenario.nodes_graph),\n                    scenario.n_nodes,\n                    json.dumps(scenario.matrix),\n                    scenario.random_topology_probability,\n                    scenario.dataset,\n                    scenario.iid,\n                    scenario.partition_selection,\n                    scenario.partition_parameter,\n                    scenario.model,\n                    scenario.agg_algorithm,\n                    scenario.rounds,\n                    scenario.logginglevel,\n                    scenario.report_status_data_queue,\n                    scenario.accelerator,\n                    json.dumps(scenario.gpu_id),\n                    scenario.network_subnet,\n                    scenario.network_gateway,\n                    scenario.epochs,\n                    json.dumps(scenario.attack_params),\n                    json.dumps(scenario.reputation),\n                    scenario.random_geo,\n                    scenario.latitude,\n                    scenario.longitude,\n                    scenario.mobility,\n                    scenario.mobility_type,\n                    scenario.radius_federation,\n                    scenario.scheme_mobility,\n                    scenario.round_frequency,\n                    scenario.mobile_participants_percent,\n                    json.dumps(scenario.additional_participants),\n                    scenario.schema_additional_participants,\n                    status,\n                    role,\n                    username,\n                ),\n            )\n        else:\n            update_command = \"\"\"\n                UPDATE scenarios SET\n                    start_time = ?,\n                    end_time = ?,\n                    title = ?,\n                    description = ?,\n                    deployment = ?,\n                    federation = ?,\n                    topology = ?,\n                    nodes = ?,\n                    nodes_graph = ?,\n                    n_nodes = ?,\n                    matrix = ?,\n                    random_topology_probability = ?,\n                    dataset = ?,\n                    iid = ?,\n                    partition_selection = ?,\n                    partition_parameter = ?,\n                    model = ?,\n                    agg_algorithm = ?,\n                    rounds = ?,\n                    logginglevel = ?,\n                    report_status_data_queue = ?,\n                    accelerator = ?,\n                    gpu_id = ?,\n                    network_subnet = ?,\n                    network_gateway = ?,\n                    epochs = ?,\n                    attack_params = ?,\n                    reputation = ?,\n                    random_geo = ?,\n                    latitude = ?,\n                    longitude = ?,\n                    mobility = ?,\n                    mobility_type = ?,\n                    radius_federation = ?,\n                    scheme_mobility = ?,\n                    round_frequency = ?,\n                    mobile_participants_percent = ?,\n                    additional_participants = ?,\n                    schema_additional_participants = ?,\n                    status = ?,\n                    role = ?,\n                    username = ?\n                WHERE name = ?;\n            \"\"\"\n            c.execute(\n                update_command,\n                (\n                    start_time,\n                    end_time,\n                    scenario.scenario_title,\n                    scenario.scenario_description,\n                    scenario.deployment,\n                    scenario.federation,\n                    scenario.topology,\n                    json.dumps(scenario.nodes),\n                    json.dumps(scenario.nodes_graph),\n                    scenario.n_nodes,\n                    json.dumps(scenario.matrix),\n                    scenario.random_topology_probability,\n                    scenario.dataset,\n                    scenario.iid,\n                    scenario.partition_selection,\n                    scenario.partition_parameter,\n                    scenario.model,\n                    scenario.agg_algorithm,\n                    scenario.rounds,\n                    scenario.logginglevel,\n                    scenario.report_status_data_queue,\n                    scenario.accelerator,\n                    json.dumps(scenario.gpu_id),\n                    scenario.network_subnet,\n                    scenario.network_gateway,\n                    scenario.epochs,\n                    json.dumps(scenario.attack_params),\n                    json.dumps(scenario.reputation),\n                    scenario.random_geo,\n                    scenario.latitude,\n                    scenario.longitude,\n                    scenario.mobility,\n                    scenario.mobility_type,\n                    scenario.radius_federation,\n                    scenario.scheme_mobility,\n                    scenario.round_frequency,\n                    scenario.mobile_participants_percent,\n                    json.dumps(scenario.additional_participants),\n                    scenario.schema_additional_participants,\n                    status,\n                    role,\n                    username,\n                    name,\n                ),\n            )\n\n        conn.commit()\n</code></pre>"},{"location":"api/controller/database/#nebula.controller.database.setup_database","title":"<code>setup_database(db_file_location)</code>  <code>async</code>","text":"<p>Initializes the SQLite database with the required PRAGMA settings.</p> This function <ul> <li>Connects asynchronously to the specified SQLite database file.</li> <li>Applies a predefined list of PRAGMA settings to configure the database.</li> <li>Commits the changes after applying the settings.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>db_file_location</code> <code>str</code> <p>Path to the SQLite database file.</p> required <p>Raises:</p> Type Description <code>PermissionError</code> <p>Logged if the application lacks permission to create or modify the database file.</p> <code>Exception</code> <p>Logs any other unexpected error that occurs during setup.</p> Source code in <code>nebula/controller/database.py</code> <pre><code>async def setup_database(db_file_location):\n    \"\"\"\n    Initializes the SQLite database with the required PRAGMA settings.\n\n    This function:\n        - Connects asynchronously to the specified SQLite database file.\n        - Applies a predefined list of PRAGMA settings to configure the database.\n        - Commits the changes after applying the settings.\n\n    Args:\n        db_file_location (str): Path to the SQLite database file.\n\n    Exceptions:\n        PermissionError: Logged if the application lacks permission to create or modify the database file.\n        Exception: Logs any other unexpected error that occurs during setup.\n    \"\"\"\n    try:\n        async with aiosqlite.connect(db_file_location) as db:\n            for pragma in PRAGMA_SETTINGS:\n                await db.execute(pragma)\n            await db.commit()\n    except PermissionError:\n        logging.info(\"No permission to create the databases. Change the default databases directory\")\n    except Exception as e:\n        logging.exception(f\"An error has ocurred during setup_database: {e}\")\n</code></pre>"},{"location":"api/controller/database/#nebula.controller.database.update_node_record","title":"<code>update_node_record(node_uid, idx, ip, port, role, neighbors, latitude, longitude, timestamp, federation, federation_round, scenario, run_hash, malicious)</code>  <code>async</code>","text":"<p>Inserts or updates a node record in the database for a given scenario, ensuring thread-safe access.</p> <p>Parameters:</p> Name Type Description Default <code>node_uid</code> <code>str</code> <p>Unique identifier of the node.</p> required <code>idx</code> <code>str</code> <p>Index or identifier within the scenario.</p> required <code>ip</code> <code>str</code> <p>IP address of the node.</p> required <code>port</code> <code>str</code> <p>Port used by the node.</p> required <code>role</code> <code>str</code> <p>Role of the node in the federation.</p> required <code>neighbors</code> <code>str</code> <p>Neighbors of the node (serialized).</p> required <code>latitude</code> <code>str</code> <p>Geographic latitude of the node.</p> required <code>longitude</code> <code>str</code> <p>Geographic longitude of the node.</p> required <code>timestamp</code> <code>str</code> <p>Timestamp of the last update.</p> required <code>federation</code> <code>str</code> <p>Federation identifier the node belongs to.</p> required <code>federation_round</code> <code>str</code> <p>Current federation round.</p> required <code>scenario</code> <code>str</code> <p>Scenario name the node is part of.</p> required <code>run_hash</code> <code>str</code> <p>Hash of the current run/state.</p> required <code>malicious</code> <code>str</code> <p>Indicator if the node is malicious.</p> required <p>Returns:</p> Type Description <p>dict or None: The updated or inserted node record as a dictionary, or None if insertion/update failed.</p> Source code in <code>nebula/controller/database.py</code> <pre><code>async def update_node_record(\n    node_uid,\n    idx,\n    ip,\n    port,\n    role,\n    neighbors,\n    latitude,\n    longitude,\n    timestamp,\n    federation,\n    federation_round,\n    scenario,\n    run_hash,\n    malicious,\n):\n    \"\"\"\n    Inserts or updates a node record in the database for a given scenario, ensuring thread-safe access.\n\n    Args:\n        node_uid (str): Unique identifier of the node.\n        idx (str): Index or identifier within the scenario.\n        ip (str): IP address of the node.\n        port (str): Port used by the node.\n        role (str): Role of the node in the federation.\n        neighbors (str): Neighbors of the node (serialized).\n        latitude (str): Geographic latitude of the node.\n        longitude (str): Geographic longitude of the node.\n        timestamp (str): Timestamp of the last update.\n        federation (str): Federation identifier the node belongs to.\n        federation_round (str): Current federation round.\n        scenario (str): Scenario name the node is part of.\n        run_hash (str): Hash of the current run/state.\n        malicious (str): Indicator if the node is malicious.\n\n    Returns:\n        dict or None: The updated or inserted node record as a dictionary, or None if insertion/update failed.\n    \"\"\"\n    global _node_lock\n    async with _node_lock:\n        async with aiosqlite.connect(node_db_file_location) as conn:\n            conn.row_factory = aiosqlite.Row\n            _c = await conn.cursor()\n\n            # Check if the node already exists\n            await _c.execute(\"SELECT * FROM nodes WHERE uid = ? AND scenario = ?;\", (node_uid, scenario))\n            result = await _c.fetchone()\n\n            if result is None:\n                # Insert new node\n                await _c.execute(\n                    \"INSERT INTO nodes (uid, idx, ip, port, role, neighbors, latitude, longitude, timestamp, federation, round, scenario, hash, malicious) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?);\",\n                    (\n                        node_uid,\n                        idx,\n                        ip,\n                        port,\n                        role,\n                        neighbors,\n                        latitude,\n                        longitude,\n                        timestamp,\n                        federation,\n                        federation_round,\n                        scenario,\n                        run_hash,\n                        malicious,\n                    ),\n                )\n            else:\n                # Update existing node\n                await _c.execute(\n                    \"UPDATE nodes SET idx = ?, ip = ?, port = ?, role = ?, neighbors = ?, latitude = ?, longitude = ?, timestamp = ?, federation = ?, round = ?, hash = ?, malicious = ? WHERE uid = ? AND scenario = ?;\",\n                    (\n                        idx,\n                        ip,\n                        port,\n                        role,\n                        neighbors,\n                        latitude,\n                        longitude,\n                        timestamp,\n                        federation,\n                        federation_round,\n                        run_hash,\n                        malicious,\n                        node_uid,\n                        scenario,\n                    ),\n                )\n\n            await conn.commit()\n\n            # Fetch the updated or newly inserted row\n            await _c.execute(\"SELECT * FROM nodes WHERE uid = ? AND scenario = ?;\", (node_uid, scenario))\n            updated_row = await _c.fetchone()\n            return dict(updated_row) if updated_row else None\n</code></pre>"},{"location":"api/controller/database/#nebula.controller.database.update_user","title":"<code>update_user(user, password, role)</code>","text":"<p>Updates the password and role of an existing user in the users database.</p> <p>Parameters:</p> Name Type Description Default <code>user</code> <code>str</code> <p>The username to update (case-insensitive, stored as uppercase).</p> required <code>password</code> <code>str</code> <p>The new plain text password to hash and store.</p> required <code>role</code> <code>str</code> <p>The new role to assign to the user.</p> required Source code in <code>nebula/controller/database.py</code> <pre><code>def update_user(user, password, role):\n    \"\"\"\n    Updates the password and role of an existing user in the users database.\n\n    Args:\n        user (str): The username to update (case-insensitive, stored as uppercase).\n        password (str): The new plain text password to hash and store.\n        role (str): The new role to assign to the user.\n    \"\"\"\n    ph = PasswordHasher()\n    with sqlite3.connect(user_db_file_location) as conn:\n        c = conn.cursor()\n        c.execute(\n            \"UPDATE users SET password = ?, role = ? WHERE user = ?\",\n            (ph.hash(password), role, user.upper()),\n        )\n</code></pre>"},{"location":"api/controller/database/#nebula.controller.database.verify","title":"<code>verify(user, password)</code>","text":"<p>Verifies whether the provided password matches the stored hashed password for a user.</p> <p>Parameters:</p> Name Type Description Default <code>user</code> <code>str</code> <p>The username to verify.</p> required <code>password</code> <code>str</code> <p>The plain text password to check against the stored hash.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the password is correct, False otherwise.</p> Source code in <code>nebula/controller/database.py</code> <pre><code>def verify(user, password):\n    \"\"\"\n    Verifies whether the provided password matches the stored hashed password for a user.\n\n    Args:\n        user (str): The username to verify.\n        password (str): The plain text password to check against the stored hash.\n\n    Returns:\n        bool: True if the password is correct, False otherwise.\n    \"\"\"\n    ph = PasswordHasher()\n    with sqlite3.connect(user_db_file_location) as conn:\n        c = conn.cursor()\n\n        c.execute(\"SELECT password FROM users WHERE user = ?\", (user,))\n        result = c.fetchone()\n        if result:\n            try:\n                return ph.verify(result[0], password)\n            except:\n                return False\n    return False\n</code></pre>"},{"location":"api/controller/database/#nebula.controller.database.verify_hash_algorithm","title":"<code>verify_hash_algorithm(user)</code>","text":"<p>Checks if the stored password hash for a user uses a supported Argon2 algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>user</code> <code>str</code> <p>The username to check (case-insensitive, converted to uppercase).</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the password hash starts with a valid Argon2 prefix, False otherwise.</p> Source code in <code>nebula/controller/database.py</code> <pre><code>def verify_hash_algorithm(user):\n    \"\"\"\n    Checks if the stored password hash for a user uses a supported Argon2 algorithm.\n\n    Args:\n        user (str): The username to check (case-insensitive, converted to uppercase).\n\n    Returns:\n        bool: True if the password hash starts with a valid Argon2 prefix, False otherwise.\n    \"\"\"\n    user = user.upper()\n    argon2_prefixes = (\"$argon2i$\", \"$argon2id$\")\n\n    with sqlite3.connect(user_db_file_location) as conn:\n        conn.row_factory = sqlite3.Row\n        c = conn.cursor()\n\n        c.execute(\"SELECT password FROM users WHERE user = ?\", (user,))\n        result = c.fetchone()\n        if result:\n            password_hash = result[\"password\"]\n            return password_hash.startswith(argon2_prefixes)\n\n    return False\n</code></pre>"},{"location":"api/controller/http_helpers/","title":"Documentation for Http_helpers Module","text":""},{"location":"api/controller/scenarios/","title":"Documentation for Scenarios Module","text":""},{"location":"api/controller/scenarios/#nebula.controller.scenarios.Scenario","title":"<code>Scenario</code>","text":"<p>Class to define a scenario for the NEBULA platform. It contains all the parameters needed to create a scenario and run it on the platform.</p> Source code in <code>nebula/controller/scenarios.py</code> <pre><code>class Scenario:\n    \"\"\"\n    Class to define a scenario for the NEBULA platform.\n    It contains all the parameters needed to create a scenario and run it on the platform.\n    \"\"\"\n\n    def __init__(\n        self,\n        scenario_title,\n        scenario_description,\n        deployment,\n        federation,\n        topology,\n        nodes,\n        nodes_graph,\n        n_nodes,\n        matrix,\n        dataset,\n        iid,\n        partition_selection,\n        partition_parameter,\n        model,\n        agg_algorithm,\n        rounds,\n        logginglevel,\n        report_status_data_queue,\n        accelerator,\n        gpu_id,\n        network_subnet,\n        network_gateway,\n        epochs,\n        attack_params,\n        reputation,\n        random_geo,\n        latitude,\n        longitude,\n        mobility,\n        network_simulation,\n        mobility_type,\n        radius_federation,\n        scheme_mobility,\n        round_frequency,\n        mobile_participants_percent,\n        additional_participants,\n        schema_additional_participants,\n        with_trustworthiness,\n        robustness_pillar,\n        resilience_to_attacks,\n        algorithm_robustness,\n        client_reliability,\n        privacy_pillar,\n        technique,\n        uncertainty,\n        indistinguishability,\n        fairness_pillar,\n        selection_fairness,\n        performance_fairness,\n        class_distribution,\n        explainability_pillar,\n        interpretability,\n        post_hoc_methods,\n        accountability_pillar,\n        factsheet_completeness,\n        architectural_soundness_pillar,\n        client_management,\n        optimization,\n        sustainability_pillar,\n        energy_source,\n        hardware_efficiency,\n        federation_complexity,\n        random_topology_probability,\n        with_sa,\n        strict_topology,\n        sad_candidate_selector,\n        sad_model_handler,\n        sar_arbitration_policy,\n        sar_neighbor_policy,\n        sar_training,\n        sar_training_policy,\n        physical_ips=None,\n    ):\n        \"\"\"\n        Initialize a Scenario instance.\n\n        Args:\n            scenario_title (str): Title of the scenario.\n            scenario_description (str): Description of the scenario.\n            deployment (str): Type of deployment.\n            federation (str): Type of federation.\n            topology (str): Type of topology.\n            nodes (dict): Dictionary of nodes.\n            nodes_graph (dict): Dictionary of nodes for graph representation.\n            n_nodes (int): Number of nodes.\n            matrix (list): Adjacency matrix.\n            dataset (str): Name of the dataset.\n            iid (bool): Whether the data is IID.\n            partition_selection (str): Type of partition selection.\n            partition_parameter (float): Parameter for partition selection.\n            model (str): Name of the model.\n            agg_algorithm (str): Aggregation algorithm.\n            rounds (int): Number of rounds.\n            logginglevel (bool): Whether to log.\n            report_status_data_queue (bool): Whether to report status data.\n            accelerator (str): Type of accelerator.\n            gpu_id (str): ID of the GPU.\n            network_subnet (str): Network subnet.\n            network_gateway (str): Network gateway.\n            epochs (int): Number of epochs.\n            attack_params (dict): Dictionary containing attack parameters.\n            reputation (dict): Dictionary containing reputation configuration.\n            random_geo (bool): Indicator if random geo is used.\n            latitude (float): Latitude for mobility.\n            longitude (float): Longitude for mobility.\n            mobility (bool): Whether mobility is enabled.\n            network_simulation (bool): Whether network simulation is enabled.\n            mobility_type (str): Type of mobility.\n            radius_federation (float): Radius of federation.\n            scheme_mobility (str): Scheme of mobility.\n            round_frequency (int): Frequency of rounds.\n            mobile_participants_percent (float): Percentage of mobile participants.\n            additional_participants (list): List of additional participants.\n            schema_additional_participants (str): Schema for additional participants.\n            random_topology_probability (float): Probability for random topology.\n            with_sa (bool): Whether situational awareness is enabled.\n            strict_topology (bool): Whether strict topology is enabled.\n            sad_candidate_selector (str): Candidate selector for SAD.\n            sad_model_handler (str): Model handler for SAD.\n            sar_arbitration_policy (str): Arbitration policy for SAR.\n            sar_neighbor_policy (str): Neighbor policy for SAR.\n            sar_training (bool): Wheter SAR training is enabled.\n            sar_training_policy (str): Training policy for SAR.\n            physical_ips (list, optional): List of physical IPs for nodes. Defaults to None.\n        \"\"\"\n        self.scenario_title = scenario_title\n        self.scenario_description = scenario_description\n        self.deployment = deployment\n        self.federation = federation\n        self.topology = topology\n        self.nodes = nodes\n        self.nodes_graph = nodes_graph\n        self.n_nodes = n_nodes\n        self.matrix = matrix\n        self.dataset = dataset\n        self.iid = iid\n        self.partition_selection = partition_selection\n        self.partition_parameter = partition_parameter\n        self.model = model\n        self.agg_algorithm = agg_algorithm\n        self.rounds = rounds\n        self.logginglevel = logginglevel\n        self.report_status_data_queue = report_status_data_queue\n        self.accelerator = accelerator\n        self.gpu_id = gpu_id\n        self.network_subnet = network_subnet\n        self.network_gateway = network_gateway\n        self.epochs = epochs\n        self.attack_params = attack_params\n        self.reputation = reputation\n        self.random_geo = random_geo\n        self.latitude = latitude\n        self.longitude = longitude\n        self.mobility = mobility\n        self.network_simulation = network_simulation\n        self.mobility_type = mobility_type\n        self.radius_federation = radius_federation\n        self.scheme_mobility = scheme_mobility\n        self.round_frequency = round_frequency\n        self.mobile_participants_percent = mobile_participants_percent\n        self.additional_participants = additional_participants\n        self.with_trustworthiness = with_trustworthiness\n        self.robustness_pillar = robustness_pillar,\n        self.resilience_to_attacks = resilience_to_attacks,\n        self.algorithm_robustness = algorithm_robustness,\n        self.client_reliability = client_reliability,\n        self.privacy_pillar = privacy_pillar,\n        self.technique = technique,\n        self.uncertainty = uncertainty,\n        self.indistinguishability = indistinguishability,\n        self.fairness_pillar = fairness_pillar,\n        self.selection_fairness = selection_fairness,\n        self.performance_fairness = performance_fairness,\n        self.class_distribution = class_distribution,\n        self.explainability_pillar = explainability_pillar,\n        self.interpretability = interpretability,\n        self.post_hoc_methods = post_hoc_methods,\n        self.accountability_pillar = accountability_pillar,\n        self.factsheet_completeness = factsheet_completeness,\n        self.architectural_soundness_pillar = architectural_soundness_pillar,\n        self.client_management = client_management,\n        self.optimization = optimization,\n        self.sustainability_pillar = sustainability_pillar,\n        self.energy_source = energy_source,\n        self.hardware_efficiency = hardware_efficiency,\n        self.federation_complexity = federation_complexity,\n        self.schema_additional_participants = schema_additional_participants\n        self.random_topology_probability = random_topology_probability\n        self.with_sa = with_sa\n        self.strict_topology = strict_topology\n        self.sad_candidate_selector = sad_candidate_selector\n        self.sad_model_handler = sad_model_handler\n        self.sar_arbitration_policy = sar_arbitration_policy\n        self.sar_neighbor_policy = sar_neighbor_policy\n        self.sar_training = sar_training\n        self.sar_training_policy = sar_training_policy\n        self.physical_ips = physical_ips\n\n    def attack_node_assign(\n        self,\n        nodes,\n        federation,\n        poisoned_node_percent,\n        poisoned_sample_percent,\n        poisoned_noise_percent,\n        attack_params,\n    ):\n        \"\"\"\n        Assign and configure attack parameters to nodes within a federated learning network.\n\n        This method:\n            - Validates input attack parameters and percentages.\n            - Determines which nodes will be marked as malicious based on the specified\n              poisoned node percentage and attack type.\n            - Assigns attack roles and parameters to selected nodes.\n            - Supports multiple attack types such as Label Flipping, Sample Poisoning,\n              Model Poisoning, GLL Neuron Inversion, Swapping Weights, Delayer, and Flooding.\n            - Ensures proper validation and setting of attack-specific parameters, including\n              targeting, noise types, delays, intervals, and attack rounds.\n            - Updates nodes' malicious status, reputation, and attack parameters accordingly.\n\n        Args:\n            nodes (dict): Dictionary of nodes with their current attributes.\n            federation (str): Type of federated learning framework (e.g., \"DFL\").\n            poisoned_node_percent (float): Percentage of nodes to be poisoned (0-100).\n            poisoned_sample_percent (float): Percentage of samples to be poisoned (0-100).\n            poisoned_noise_percent (float): Percentage of noise to apply in poisoning (0-100).\n            attack_params (dict): Dictionary containing attack type and associated parameters.\n\n        Returns:\n            dict: Updated nodes dictionary with assigned malicious roles and attack parameters.\n\n        Raises:\n            ValueError: If any input parameter is invalid or attack type is unrecognized.\n        \"\"\"\n        import logging\n        import math\n        import random\n\n        # Validate input parameters\n        def validate_percentage(value, name):\n            \"\"\"\n            Validate that a given value is a float percentage between 0 and 100.\n\n            Args:\n                value: The value to validate, expected to be convertible to float.\n                name (str): Name of the parameter, used for error messages.\n\n            Returns:\n                float: The validated percentage value.\n\n            Raises:\n                ValueError: If the value is not a float or not within the range [0, 100].\n            \"\"\"\n            try:\n                value = float(value)\n                if not 0 &lt;= value &lt;= 100:\n                    raise ValueError(f\"{name} must be between 0 and 100\")\n                return value\n            except (TypeError, ValueError) as e:\n                raise ValueError(f\"Invalid {name}: {e!s}\")\n\n        def validate_positive_int(value, name):\n            \"\"\"\n            Validate that a given value is a positive integer (including zero).\n\n            Args:\n                value: The value to validate, expected to be convertible to int.\n                name (str): Name of the parameter, used for error messages.\n\n            Returns:\n                int: The validated positive integer value.\n\n            Raises:\n                ValueError: If the value is not an integer or is negative.\n            \"\"\"\n            try:\n                value = int(value)\n                if value &lt; 0:\n                    raise ValueError(f\"{name} must be positive\")\n                return value\n            except (TypeError, ValueError) as e:\n                raise ValueError(f\"Invalid {name}: {e!s}\")\n\n        # Validate attack type\n        valid_attacks = {\n            \"No Attack\",\n            \"Label Flipping\",\n            \"Sample Poisoning\",\n            \"Model Poisoning\",\n            \"GLL Neuron Inversion\",\n            \"Swapping Weights\",\n            \"Delayer\",\n            \"Flooding\",\n        }\n\n        # Get attack type from attack_params\n        if attack_params and \"attacks\" in attack_params:\n            attack = attack_params[\"attacks\"]\n\n        # Handle attack parameter which can be either a string or None\n        if attack is None:\n            attack = \"No Attack\"\n        elif not isinstance(attack, str):\n            raise ValueError(f\"Invalid attack type: {attack}. Expected string or None.\")\n\n        if attack not in valid_attacks:\n            raise ValueError(f\"Invalid attack type: {attack}. Must be one of {valid_attacks}\")\n\n        # Get attack parameters from attack_params\n        poisoned_node_percent = attack_params.get(\"poisoned_node_percent\", poisoned_node_percent)\n        poisoned_sample_percent = attack_params.get(\"poisoned_sample_percent\", poisoned_sample_percent)\n        poisoned_noise_percent = attack_params.get(\"poisoned_noise_percent\", poisoned_noise_percent)\n\n        # Validate percentage parameters\n        poisoned_node_percent = validate_percentage(poisoned_node_percent, \"poisoned_node_percent\")\n        poisoned_sample_percent = validate_percentage(poisoned_sample_percent, \"poisoned_sample_percent\")\n        poisoned_noise_percent = validate_percentage(poisoned_noise_percent, \"poisoned_noise_percent\")\n\n        nodes_index = []\n        # Get the nodes index\n        if federation == \"DFL\":\n            nodes_index = list(nodes.keys())\n        else:\n            for node in nodes:\n                if nodes[node][\"role\"] != \"server\":\n                    nodes_index.append(node)\n\n        logging.info(f\"Nodes index: {nodes_index}\")\n        logging.info(f\"Attack type: {attack}\")\n        logging.info(f\"Poisoned node percent: {poisoned_node_percent}\")\n\n        mal_nodes_defined = any(nodes[node][\"malicious\"] for node in nodes)\n        logging.info(f\"Malicious nodes already defined: {mal_nodes_defined}\")\n\n        attacked_nodes = []\n\n        if not mal_nodes_defined and attack != \"No Attack\":\n            n_nodes = len(nodes_index)\n            # Number of attacked nodes, round up\n            num_attacked = int(math.ceil(poisoned_node_percent / 100 * n_nodes))\n            if num_attacked &gt; n_nodes:\n                num_attacked = n_nodes\n\n            # Get the index of attacked nodes\n            attacked_nodes = random.sample(nodes_index, num_attacked)\n            logging.info(f\"Number of nodes to attack: {num_attacked}\")\n            logging.info(f\"Attacked nodes: {attacked_nodes}\")\n\n        # Assign the role of each node\n        for node in nodes:\n            node_att = \"No Attack\"\n            malicious = False\n            node_reputation = self.reputation.copy() if self.reputation else None\n\n            if node in attacked_nodes or nodes[node][\"malicious\"]:\n                malicious = True\n                node_reputation = None\n                node_att = attack\n                logging.info(f\"Node {node} marked as malicious with attack {attack}\")\n\n                # Initialize attack parameters with defaults\n                node_attack_params = attack_params.copy() if attack_params else {}\n\n                # Set attack-specific parameters\n                if attack == \"Label Flipping\":\n                    node_attack_params[\"poisoned_node_percent\"] = poisoned_node_percent\n                    node_attack_params[\"poisoned_sample_percent\"] = poisoned_sample_percent\n                    node_attack_params[\"targeted\"] = attack_params.get(\"targeted\", False)\n                    if node_attack_params[\"targeted\"]:\n                        node_attack_params[\"target_label\"] = validate_positive_int(\n                            attack_params.get(\"target_label\", 4), \"target_label\"\n                        )\n                        node_attack_params[\"target_changed_label\"] = validate_positive_int(\n                            attack_params.get(\"target_changed_label\", 7), \"target_changed_label\"\n                        )\n\n                elif attack == \"Sample Poisoning\":\n                    node_attack_params[\"poisoned_node_percent\"] = poisoned_node_percent\n                    node_attack_params[\"poisoned_sample_percent\"] = poisoned_sample_percent\n                    node_attack_params[\"poisoned_noise_percent\"] = poisoned_noise_percent\n                    node_attack_params[\"noise_type\"] = attack_params.get(\"noise_type\", \"Gaussian\")\n                    node_attack_params[\"targeted\"] = attack_params.get(\"targeted\", False)\n                    if node_attack_params[\"targeted\"]:\n                        node_attack_params[\"target_label\"] = validate_positive_int(\n                            attack_params.get(\"target_label\", 4), \"target_label\"\n                        )\n\n                elif attack == \"Model Poisoning\":\n                    node_attack_params[\"poisoned_node_percent\"] = poisoned_node_percent\n                    node_attack_params[\"poisoned_noise_percent\"] = poisoned_noise_percent\n                    node_attack_params[\"noise_type\"] = attack_params.get(\"noise_type\", \"Gaussian\")\n\n                elif attack == \"GLL Neuron Inversion\":\n                    node_attack_params[\"poisoned_node_percent\"] = poisoned_node_percent\n\n                elif attack == \"Swapping Weights\":\n                    node_attack_params[\"poisoned_node_percent\"] = poisoned_node_percent\n                    node_attack_params[\"layer_idx\"] = validate_positive_int(\n                        attack_params.get(\"layer_idx\", 0), \"layer_idx\"\n                    )\n\n                elif attack == \"Delayer\":\n                    node_attack_params[\"poisoned_node_percent\"] = poisoned_node_percent\n                    node_attack_params[\"delay\"] = validate_positive_int(attack_params.get(\"delay\", 10), \"delay\")\n                    node_attack_params[\"target_percentage\"] = validate_percentage(\n                        attack_params.get(\"target_percentage\", 100), \"target_percentage\"\n                    )\n                    node_attack_params[\"selection_interval\"] = validate_positive_int(\n                        attack_params.get(\"selection_interval\", 1), \"selection_interval\"\n                    )\n\n                elif attack == \"Flooding\":\n                    node_attack_params[\"poisoned_node_percent\"] = poisoned_node_percent\n                    node_attack_params[\"flooding_factor\"] = validate_positive_int(\n                        attack_params.get(\"flooding_factor\", 100), \"flooding_factor\"\n                    )\n                    node_attack_params[\"target_percentage\"] = validate_percentage(\n                        attack_params.get(\"target_percentage\", 100), \"target_percentage\"\n                    )\n                    node_attack_params[\"selection_interval\"] = validate_positive_int(\n                        attack_params.get(\"selection_interval\", 1), \"selection_interval\"\n                    )\n\n                # Add common attack parameters\n                node_attack_params[\"round_start_attack\"] = validate_positive_int(\n                    attack_params.get(\"round_start_attack\", 1), \"round_start_attack\"\n                )\n                node_attack_params[\"round_stop_attack\"] = validate_positive_int(\n                    attack_params.get(\"round_stop_attack\", 10), \"round_stop_attack\"\n                )\n                node_attack_params[\"attack_interval\"] = validate_positive_int(\n                    attack_params.get(\"attack_interval\", 1), \"attack_interval\"\n                )\n\n                # Validate round parameters\n                if node_attack_params[\"round_start_attack\"] &gt;= node_attack_params[\"round_stop_attack\"]:\n                    raise ValueError(\"round_start_attack must be less than round_stop_attack\")\n\n                node_attack_params[\"attacks\"] = node_att\n                nodes[node][\"malicious\"] = True\n                nodes[node][\"attack_params\"] = node_attack_params\n                nodes[node][\"fake_behavior\"] = nodes[node][\"role\"]\n                nodes[node][\"role\"] = \"malicious\"\n            else:\n                nodes[node][\"attack_params\"] = {\"attacks\": \"No Attack\"}\n\n            nodes[node][\"reputation\"] = node_reputation\n\n            logging.info(\n                f\"Node {node} final configuration - malicious: {nodes[node]['malicious']}, attack: {nodes[node]['attack_params']['attacks']}\"\n            )\n\n        return nodes\n\n    def mobility_assign(self, nodes, mobile_participants_percent):\n        \"\"\"\n        Assign mobility status to a subset of nodes based on a specified percentage.\n\n        This method:\n            - Calculates the number of mobile nodes by applying the given percentage.\n            - Randomly selects nodes to be marked as mobile.\n            - Updates each node's \"mobility\" attribute to True or False accordingly.\n\n        Args:\n            nodes (dict): Dictionary of nodes with their current attributes.\n            mobile_participants_percent (float): Percentage of nodes to be assigned mobility (0-100).\n\n        Returns:\n            dict: Updated nodes dictionary with mobility status assigned.\n        \"\"\"\n        import random\n\n        # Number of mobile nodes, round down\n        num_mobile = math.floor(mobile_participants_percent / 100 * len(nodes))\n        if num_mobile &gt; len(nodes):\n            num_mobile = len(nodes)\n\n        # Get the index of mobile nodes\n        mobile_nodes = random.sample(list(nodes.keys()), num_mobile)\n\n        # Assign the role of each node\n        for node in nodes:\n            node_mob = False\n            if node in mobile_nodes:\n                node_mob = True\n            nodes[node][\"mobility\"] = node_mob\n        return nodes\n\n    @classmethod\n    def from_dict(cls, data):\n        \"\"\"\n        Create an instance of the class from a dictionary of attributes.\n\n        This class method:\n            - Copies the input dictionary to prevent modification of the original data.\n            - Instantiates the class using the dictionary unpacked as keyword arguments.\n\n        Args:\n            data (dict): Dictionary containing attributes to initialize the class instance.\n\n        Returns:\n            cls: An instance of the class initialized with the provided data.\n        \"\"\"\n        # Create a copy of the data to avoid modifying the original\n        scenario_data = data.copy()\n\n        # Create the scenario object\n        scenario = cls(**scenario_data)\n\n        return scenario\n</code></pre>"},{"location":"api/controller/scenarios/#nebula.controller.scenarios.Scenario.__init__","title":"<code>__init__(scenario_title, scenario_description, deployment, federation, topology, nodes, nodes_graph, n_nodes, matrix, dataset, iid, partition_selection, partition_parameter, model, agg_algorithm, rounds, logginglevel, report_status_data_queue, accelerator, gpu_id, network_subnet, network_gateway, epochs, attack_params, reputation, random_geo, latitude, longitude, mobility, network_simulation, mobility_type, radius_federation, scheme_mobility, round_frequency, mobile_participants_percent, additional_participants, schema_additional_participants, with_trustworthiness, robustness_pillar, resilience_to_attacks, algorithm_robustness, client_reliability, privacy_pillar, technique, uncertainty, indistinguishability, fairness_pillar, selection_fairness, performance_fairness, class_distribution, explainability_pillar, interpretability, post_hoc_methods, accountability_pillar, factsheet_completeness, architectural_soundness_pillar, client_management, optimization, sustainability_pillar, energy_source, hardware_efficiency, federation_complexity, random_topology_probability, with_sa, strict_topology, sad_candidate_selector, sad_model_handler, sar_arbitration_policy, sar_neighbor_policy, sar_training, sar_training_policy, physical_ips=None)</code>","text":"<p>Initialize a Scenario instance.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_title</code> <code>str</code> <p>Title of the scenario.</p> required <code>scenario_description</code> <code>str</code> <p>Description of the scenario.</p> required <code>deployment</code> <code>str</code> <p>Type of deployment.</p> required <code>federation</code> <code>str</code> <p>Type of federation.</p> required <code>topology</code> <code>str</code> <p>Type of topology.</p> required <code>nodes</code> <code>dict</code> <p>Dictionary of nodes.</p> required <code>nodes_graph</code> <code>dict</code> <p>Dictionary of nodes for graph representation.</p> required <code>n_nodes</code> <code>int</code> <p>Number of nodes.</p> required <code>matrix</code> <code>list</code> <p>Adjacency matrix.</p> required <code>dataset</code> <code>str</code> <p>Name of the dataset.</p> required <code>iid</code> <code>bool</code> <p>Whether the data is IID.</p> required <code>partition_selection</code> <code>str</code> <p>Type of partition selection.</p> required <code>partition_parameter</code> <code>float</code> <p>Parameter for partition selection.</p> required <code>model</code> <code>str</code> <p>Name of the model.</p> required <code>agg_algorithm</code> <code>str</code> <p>Aggregation algorithm.</p> required <code>rounds</code> <code>int</code> <p>Number of rounds.</p> required <code>logginglevel</code> <code>bool</code> <p>Whether to log.</p> required <code>report_status_data_queue</code> <code>bool</code> <p>Whether to report status data.</p> required <code>accelerator</code> <code>str</code> <p>Type of accelerator.</p> required <code>gpu_id</code> <code>str</code> <p>ID of the GPU.</p> required <code>network_subnet</code> <code>str</code> <p>Network subnet.</p> required <code>network_gateway</code> <code>str</code> <p>Network gateway.</p> required <code>epochs</code> <code>int</code> <p>Number of epochs.</p> required <code>attack_params</code> <code>dict</code> <p>Dictionary containing attack parameters.</p> required <code>reputation</code> <code>dict</code> <p>Dictionary containing reputation configuration.</p> required <code>random_geo</code> <code>bool</code> <p>Indicator if random geo is used.</p> required <code>latitude</code> <code>float</code> <p>Latitude for mobility.</p> required <code>longitude</code> <code>float</code> <p>Longitude for mobility.</p> required <code>mobility</code> <code>bool</code> <p>Whether mobility is enabled.</p> required <code>network_simulation</code> <code>bool</code> <p>Whether network simulation is enabled.</p> required <code>mobility_type</code> <code>str</code> <p>Type of mobility.</p> required <code>radius_federation</code> <code>float</code> <p>Radius of federation.</p> required <code>scheme_mobility</code> <code>str</code> <p>Scheme of mobility.</p> required <code>round_frequency</code> <code>int</code> <p>Frequency of rounds.</p> required <code>mobile_participants_percent</code> <code>float</code> <p>Percentage of mobile participants.</p> required <code>additional_participants</code> <code>list</code> <p>List of additional participants.</p> required <code>schema_additional_participants</code> <code>str</code> <p>Schema for additional participants.</p> required <code>random_topology_probability</code> <code>float</code> <p>Probability for random topology.</p> required <code>with_sa</code> <code>bool</code> <p>Whether situational awareness is enabled.</p> required <code>strict_topology</code> <code>bool</code> <p>Whether strict topology is enabled.</p> required <code>sad_candidate_selector</code> <code>str</code> <p>Candidate selector for SAD.</p> required <code>sad_model_handler</code> <code>str</code> <p>Model handler for SAD.</p> required <code>sar_arbitration_policy</code> <code>str</code> <p>Arbitration policy for SAR.</p> required <code>sar_neighbor_policy</code> <code>str</code> <p>Neighbor policy for SAR.</p> required <code>sar_training</code> <code>bool</code> <p>Wheter SAR training is enabled.</p> required <code>sar_training_policy</code> <code>str</code> <p>Training policy for SAR.</p> required <code>physical_ips</code> <code>list</code> <p>List of physical IPs for nodes. Defaults to None.</p> <code>None</code> Source code in <code>nebula/controller/scenarios.py</code> <pre><code>def __init__(\n    self,\n    scenario_title,\n    scenario_description,\n    deployment,\n    federation,\n    topology,\n    nodes,\n    nodes_graph,\n    n_nodes,\n    matrix,\n    dataset,\n    iid,\n    partition_selection,\n    partition_parameter,\n    model,\n    agg_algorithm,\n    rounds,\n    logginglevel,\n    report_status_data_queue,\n    accelerator,\n    gpu_id,\n    network_subnet,\n    network_gateway,\n    epochs,\n    attack_params,\n    reputation,\n    random_geo,\n    latitude,\n    longitude,\n    mobility,\n    network_simulation,\n    mobility_type,\n    radius_federation,\n    scheme_mobility,\n    round_frequency,\n    mobile_participants_percent,\n    additional_participants,\n    schema_additional_participants,\n    with_trustworthiness,\n    robustness_pillar,\n    resilience_to_attacks,\n    algorithm_robustness,\n    client_reliability,\n    privacy_pillar,\n    technique,\n    uncertainty,\n    indistinguishability,\n    fairness_pillar,\n    selection_fairness,\n    performance_fairness,\n    class_distribution,\n    explainability_pillar,\n    interpretability,\n    post_hoc_methods,\n    accountability_pillar,\n    factsheet_completeness,\n    architectural_soundness_pillar,\n    client_management,\n    optimization,\n    sustainability_pillar,\n    energy_source,\n    hardware_efficiency,\n    federation_complexity,\n    random_topology_probability,\n    with_sa,\n    strict_topology,\n    sad_candidate_selector,\n    sad_model_handler,\n    sar_arbitration_policy,\n    sar_neighbor_policy,\n    sar_training,\n    sar_training_policy,\n    physical_ips=None,\n):\n    \"\"\"\n    Initialize a Scenario instance.\n\n    Args:\n        scenario_title (str): Title of the scenario.\n        scenario_description (str): Description of the scenario.\n        deployment (str): Type of deployment.\n        federation (str): Type of federation.\n        topology (str): Type of topology.\n        nodes (dict): Dictionary of nodes.\n        nodes_graph (dict): Dictionary of nodes for graph representation.\n        n_nodes (int): Number of nodes.\n        matrix (list): Adjacency matrix.\n        dataset (str): Name of the dataset.\n        iid (bool): Whether the data is IID.\n        partition_selection (str): Type of partition selection.\n        partition_parameter (float): Parameter for partition selection.\n        model (str): Name of the model.\n        agg_algorithm (str): Aggregation algorithm.\n        rounds (int): Number of rounds.\n        logginglevel (bool): Whether to log.\n        report_status_data_queue (bool): Whether to report status data.\n        accelerator (str): Type of accelerator.\n        gpu_id (str): ID of the GPU.\n        network_subnet (str): Network subnet.\n        network_gateway (str): Network gateway.\n        epochs (int): Number of epochs.\n        attack_params (dict): Dictionary containing attack parameters.\n        reputation (dict): Dictionary containing reputation configuration.\n        random_geo (bool): Indicator if random geo is used.\n        latitude (float): Latitude for mobility.\n        longitude (float): Longitude for mobility.\n        mobility (bool): Whether mobility is enabled.\n        network_simulation (bool): Whether network simulation is enabled.\n        mobility_type (str): Type of mobility.\n        radius_federation (float): Radius of federation.\n        scheme_mobility (str): Scheme of mobility.\n        round_frequency (int): Frequency of rounds.\n        mobile_participants_percent (float): Percentage of mobile participants.\n        additional_participants (list): List of additional participants.\n        schema_additional_participants (str): Schema for additional participants.\n        random_topology_probability (float): Probability for random topology.\n        with_sa (bool): Whether situational awareness is enabled.\n        strict_topology (bool): Whether strict topology is enabled.\n        sad_candidate_selector (str): Candidate selector for SAD.\n        sad_model_handler (str): Model handler for SAD.\n        sar_arbitration_policy (str): Arbitration policy for SAR.\n        sar_neighbor_policy (str): Neighbor policy for SAR.\n        sar_training (bool): Wheter SAR training is enabled.\n        sar_training_policy (str): Training policy for SAR.\n        physical_ips (list, optional): List of physical IPs for nodes. Defaults to None.\n    \"\"\"\n    self.scenario_title = scenario_title\n    self.scenario_description = scenario_description\n    self.deployment = deployment\n    self.federation = federation\n    self.topology = topology\n    self.nodes = nodes\n    self.nodes_graph = nodes_graph\n    self.n_nodes = n_nodes\n    self.matrix = matrix\n    self.dataset = dataset\n    self.iid = iid\n    self.partition_selection = partition_selection\n    self.partition_parameter = partition_parameter\n    self.model = model\n    self.agg_algorithm = agg_algorithm\n    self.rounds = rounds\n    self.logginglevel = logginglevel\n    self.report_status_data_queue = report_status_data_queue\n    self.accelerator = accelerator\n    self.gpu_id = gpu_id\n    self.network_subnet = network_subnet\n    self.network_gateway = network_gateway\n    self.epochs = epochs\n    self.attack_params = attack_params\n    self.reputation = reputation\n    self.random_geo = random_geo\n    self.latitude = latitude\n    self.longitude = longitude\n    self.mobility = mobility\n    self.network_simulation = network_simulation\n    self.mobility_type = mobility_type\n    self.radius_federation = radius_federation\n    self.scheme_mobility = scheme_mobility\n    self.round_frequency = round_frequency\n    self.mobile_participants_percent = mobile_participants_percent\n    self.additional_participants = additional_participants\n    self.with_trustworthiness = with_trustworthiness\n    self.robustness_pillar = robustness_pillar,\n    self.resilience_to_attacks = resilience_to_attacks,\n    self.algorithm_robustness = algorithm_robustness,\n    self.client_reliability = client_reliability,\n    self.privacy_pillar = privacy_pillar,\n    self.technique = technique,\n    self.uncertainty = uncertainty,\n    self.indistinguishability = indistinguishability,\n    self.fairness_pillar = fairness_pillar,\n    self.selection_fairness = selection_fairness,\n    self.performance_fairness = performance_fairness,\n    self.class_distribution = class_distribution,\n    self.explainability_pillar = explainability_pillar,\n    self.interpretability = interpretability,\n    self.post_hoc_methods = post_hoc_methods,\n    self.accountability_pillar = accountability_pillar,\n    self.factsheet_completeness = factsheet_completeness,\n    self.architectural_soundness_pillar = architectural_soundness_pillar,\n    self.client_management = client_management,\n    self.optimization = optimization,\n    self.sustainability_pillar = sustainability_pillar,\n    self.energy_source = energy_source,\n    self.hardware_efficiency = hardware_efficiency,\n    self.federation_complexity = federation_complexity,\n    self.schema_additional_participants = schema_additional_participants\n    self.random_topology_probability = random_topology_probability\n    self.with_sa = with_sa\n    self.strict_topology = strict_topology\n    self.sad_candidate_selector = sad_candidate_selector\n    self.sad_model_handler = sad_model_handler\n    self.sar_arbitration_policy = sar_arbitration_policy\n    self.sar_neighbor_policy = sar_neighbor_policy\n    self.sar_training = sar_training\n    self.sar_training_policy = sar_training_policy\n    self.physical_ips = physical_ips\n</code></pre>"},{"location":"api/controller/scenarios/#nebula.controller.scenarios.Scenario.attack_node_assign","title":"<code>attack_node_assign(nodes, federation, poisoned_node_percent, poisoned_sample_percent, poisoned_noise_percent, attack_params)</code>","text":"<p>Assign and configure attack parameters to nodes within a federated learning network.</p> This method <ul> <li>Validates input attack parameters and percentages.</li> <li>Determines which nodes will be marked as malicious based on the specified   poisoned node percentage and attack type.</li> <li>Assigns attack roles and parameters to selected nodes.</li> <li>Supports multiple attack types such as Label Flipping, Sample Poisoning,   Model Poisoning, GLL Neuron Inversion, Swapping Weights, Delayer, and Flooding.</li> <li>Ensures proper validation and setting of attack-specific parameters, including   targeting, noise types, delays, intervals, and attack rounds.</li> <li>Updates nodes' malicious status, reputation, and attack parameters accordingly.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>dict</code> <p>Dictionary of nodes with their current attributes.</p> required <code>federation</code> <code>str</code> <p>Type of federated learning framework (e.g., \"DFL\").</p> required <code>poisoned_node_percent</code> <code>float</code> <p>Percentage of nodes to be poisoned (0-100).</p> required <code>poisoned_sample_percent</code> <code>float</code> <p>Percentage of samples to be poisoned (0-100).</p> required <code>poisoned_noise_percent</code> <code>float</code> <p>Percentage of noise to apply in poisoning (0-100).</p> required <code>attack_params</code> <code>dict</code> <p>Dictionary containing attack type and associated parameters.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Updated nodes dictionary with assigned malicious roles and attack parameters.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any input parameter is invalid or attack type is unrecognized.</p> Source code in <code>nebula/controller/scenarios.py</code> <pre><code>def attack_node_assign(\n    self,\n    nodes,\n    federation,\n    poisoned_node_percent,\n    poisoned_sample_percent,\n    poisoned_noise_percent,\n    attack_params,\n):\n    \"\"\"\n    Assign and configure attack parameters to nodes within a federated learning network.\n\n    This method:\n        - Validates input attack parameters and percentages.\n        - Determines which nodes will be marked as malicious based on the specified\n          poisoned node percentage and attack type.\n        - Assigns attack roles and parameters to selected nodes.\n        - Supports multiple attack types such as Label Flipping, Sample Poisoning,\n          Model Poisoning, GLL Neuron Inversion, Swapping Weights, Delayer, and Flooding.\n        - Ensures proper validation and setting of attack-specific parameters, including\n          targeting, noise types, delays, intervals, and attack rounds.\n        - Updates nodes' malicious status, reputation, and attack parameters accordingly.\n\n    Args:\n        nodes (dict): Dictionary of nodes with their current attributes.\n        federation (str): Type of federated learning framework (e.g., \"DFL\").\n        poisoned_node_percent (float): Percentage of nodes to be poisoned (0-100).\n        poisoned_sample_percent (float): Percentage of samples to be poisoned (0-100).\n        poisoned_noise_percent (float): Percentage of noise to apply in poisoning (0-100).\n        attack_params (dict): Dictionary containing attack type and associated parameters.\n\n    Returns:\n        dict: Updated nodes dictionary with assigned malicious roles and attack parameters.\n\n    Raises:\n        ValueError: If any input parameter is invalid or attack type is unrecognized.\n    \"\"\"\n    import logging\n    import math\n    import random\n\n    # Validate input parameters\n    def validate_percentage(value, name):\n        \"\"\"\n        Validate that a given value is a float percentage between 0 and 100.\n\n        Args:\n            value: The value to validate, expected to be convertible to float.\n            name (str): Name of the parameter, used for error messages.\n\n        Returns:\n            float: The validated percentage value.\n\n        Raises:\n            ValueError: If the value is not a float or not within the range [0, 100].\n        \"\"\"\n        try:\n            value = float(value)\n            if not 0 &lt;= value &lt;= 100:\n                raise ValueError(f\"{name} must be between 0 and 100\")\n            return value\n        except (TypeError, ValueError) as e:\n            raise ValueError(f\"Invalid {name}: {e!s}\")\n\n    def validate_positive_int(value, name):\n        \"\"\"\n        Validate that a given value is a positive integer (including zero).\n\n        Args:\n            value: The value to validate, expected to be convertible to int.\n            name (str): Name of the parameter, used for error messages.\n\n        Returns:\n            int: The validated positive integer value.\n\n        Raises:\n            ValueError: If the value is not an integer or is negative.\n        \"\"\"\n        try:\n            value = int(value)\n            if value &lt; 0:\n                raise ValueError(f\"{name} must be positive\")\n            return value\n        except (TypeError, ValueError) as e:\n            raise ValueError(f\"Invalid {name}: {e!s}\")\n\n    # Validate attack type\n    valid_attacks = {\n        \"No Attack\",\n        \"Label Flipping\",\n        \"Sample Poisoning\",\n        \"Model Poisoning\",\n        \"GLL Neuron Inversion\",\n        \"Swapping Weights\",\n        \"Delayer\",\n        \"Flooding\",\n    }\n\n    # Get attack type from attack_params\n    if attack_params and \"attacks\" in attack_params:\n        attack = attack_params[\"attacks\"]\n\n    # Handle attack parameter which can be either a string or None\n    if attack is None:\n        attack = \"No Attack\"\n    elif not isinstance(attack, str):\n        raise ValueError(f\"Invalid attack type: {attack}. Expected string or None.\")\n\n    if attack not in valid_attacks:\n        raise ValueError(f\"Invalid attack type: {attack}. Must be one of {valid_attacks}\")\n\n    # Get attack parameters from attack_params\n    poisoned_node_percent = attack_params.get(\"poisoned_node_percent\", poisoned_node_percent)\n    poisoned_sample_percent = attack_params.get(\"poisoned_sample_percent\", poisoned_sample_percent)\n    poisoned_noise_percent = attack_params.get(\"poisoned_noise_percent\", poisoned_noise_percent)\n\n    # Validate percentage parameters\n    poisoned_node_percent = validate_percentage(poisoned_node_percent, \"poisoned_node_percent\")\n    poisoned_sample_percent = validate_percentage(poisoned_sample_percent, \"poisoned_sample_percent\")\n    poisoned_noise_percent = validate_percentage(poisoned_noise_percent, \"poisoned_noise_percent\")\n\n    nodes_index = []\n    # Get the nodes index\n    if federation == \"DFL\":\n        nodes_index = list(nodes.keys())\n    else:\n        for node in nodes:\n            if nodes[node][\"role\"] != \"server\":\n                nodes_index.append(node)\n\n    logging.info(f\"Nodes index: {nodes_index}\")\n    logging.info(f\"Attack type: {attack}\")\n    logging.info(f\"Poisoned node percent: {poisoned_node_percent}\")\n\n    mal_nodes_defined = any(nodes[node][\"malicious\"] for node in nodes)\n    logging.info(f\"Malicious nodes already defined: {mal_nodes_defined}\")\n\n    attacked_nodes = []\n\n    if not mal_nodes_defined and attack != \"No Attack\":\n        n_nodes = len(nodes_index)\n        # Number of attacked nodes, round up\n        num_attacked = int(math.ceil(poisoned_node_percent / 100 * n_nodes))\n        if num_attacked &gt; n_nodes:\n            num_attacked = n_nodes\n\n        # Get the index of attacked nodes\n        attacked_nodes = random.sample(nodes_index, num_attacked)\n        logging.info(f\"Number of nodes to attack: {num_attacked}\")\n        logging.info(f\"Attacked nodes: {attacked_nodes}\")\n\n    # Assign the role of each node\n    for node in nodes:\n        node_att = \"No Attack\"\n        malicious = False\n        node_reputation = self.reputation.copy() if self.reputation else None\n\n        if node in attacked_nodes or nodes[node][\"malicious\"]:\n            malicious = True\n            node_reputation = None\n            node_att = attack\n            logging.info(f\"Node {node} marked as malicious with attack {attack}\")\n\n            # Initialize attack parameters with defaults\n            node_attack_params = attack_params.copy() if attack_params else {}\n\n            # Set attack-specific parameters\n            if attack == \"Label Flipping\":\n                node_attack_params[\"poisoned_node_percent\"] = poisoned_node_percent\n                node_attack_params[\"poisoned_sample_percent\"] = poisoned_sample_percent\n                node_attack_params[\"targeted\"] = attack_params.get(\"targeted\", False)\n                if node_attack_params[\"targeted\"]:\n                    node_attack_params[\"target_label\"] = validate_positive_int(\n                        attack_params.get(\"target_label\", 4), \"target_label\"\n                    )\n                    node_attack_params[\"target_changed_label\"] = validate_positive_int(\n                        attack_params.get(\"target_changed_label\", 7), \"target_changed_label\"\n                    )\n\n            elif attack == \"Sample Poisoning\":\n                node_attack_params[\"poisoned_node_percent\"] = poisoned_node_percent\n                node_attack_params[\"poisoned_sample_percent\"] = poisoned_sample_percent\n                node_attack_params[\"poisoned_noise_percent\"] = poisoned_noise_percent\n                node_attack_params[\"noise_type\"] = attack_params.get(\"noise_type\", \"Gaussian\")\n                node_attack_params[\"targeted\"] = attack_params.get(\"targeted\", False)\n                if node_attack_params[\"targeted\"]:\n                    node_attack_params[\"target_label\"] = validate_positive_int(\n                        attack_params.get(\"target_label\", 4), \"target_label\"\n                    )\n\n            elif attack == \"Model Poisoning\":\n                node_attack_params[\"poisoned_node_percent\"] = poisoned_node_percent\n                node_attack_params[\"poisoned_noise_percent\"] = poisoned_noise_percent\n                node_attack_params[\"noise_type\"] = attack_params.get(\"noise_type\", \"Gaussian\")\n\n            elif attack == \"GLL Neuron Inversion\":\n                node_attack_params[\"poisoned_node_percent\"] = poisoned_node_percent\n\n            elif attack == \"Swapping Weights\":\n                node_attack_params[\"poisoned_node_percent\"] = poisoned_node_percent\n                node_attack_params[\"layer_idx\"] = validate_positive_int(\n                    attack_params.get(\"layer_idx\", 0), \"layer_idx\"\n                )\n\n            elif attack == \"Delayer\":\n                node_attack_params[\"poisoned_node_percent\"] = poisoned_node_percent\n                node_attack_params[\"delay\"] = validate_positive_int(attack_params.get(\"delay\", 10), \"delay\")\n                node_attack_params[\"target_percentage\"] = validate_percentage(\n                    attack_params.get(\"target_percentage\", 100), \"target_percentage\"\n                )\n                node_attack_params[\"selection_interval\"] = validate_positive_int(\n                    attack_params.get(\"selection_interval\", 1), \"selection_interval\"\n                )\n\n            elif attack == \"Flooding\":\n                node_attack_params[\"poisoned_node_percent\"] = poisoned_node_percent\n                node_attack_params[\"flooding_factor\"] = validate_positive_int(\n                    attack_params.get(\"flooding_factor\", 100), \"flooding_factor\"\n                )\n                node_attack_params[\"target_percentage\"] = validate_percentage(\n                    attack_params.get(\"target_percentage\", 100), \"target_percentage\"\n                )\n                node_attack_params[\"selection_interval\"] = validate_positive_int(\n                    attack_params.get(\"selection_interval\", 1), \"selection_interval\"\n                )\n\n            # Add common attack parameters\n            node_attack_params[\"round_start_attack\"] = validate_positive_int(\n                attack_params.get(\"round_start_attack\", 1), \"round_start_attack\"\n            )\n            node_attack_params[\"round_stop_attack\"] = validate_positive_int(\n                attack_params.get(\"round_stop_attack\", 10), \"round_stop_attack\"\n            )\n            node_attack_params[\"attack_interval\"] = validate_positive_int(\n                attack_params.get(\"attack_interval\", 1), \"attack_interval\"\n            )\n\n            # Validate round parameters\n            if node_attack_params[\"round_start_attack\"] &gt;= node_attack_params[\"round_stop_attack\"]:\n                raise ValueError(\"round_start_attack must be less than round_stop_attack\")\n\n            node_attack_params[\"attacks\"] = node_att\n            nodes[node][\"malicious\"] = True\n            nodes[node][\"attack_params\"] = node_attack_params\n            nodes[node][\"fake_behavior\"] = nodes[node][\"role\"]\n            nodes[node][\"role\"] = \"malicious\"\n        else:\n            nodes[node][\"attack_params\"] = {\"attacks\": \"No Attack\"}\n\n        nodes[node][\"reputation\"] = node_reputation\n\n        logging.info(\n            f\"Node {node} final configuration - malicious: {nodes[node]['malicious']}, attack: {nodes[node]['attack_params']['attacks']}\"\n        )\n\n    return nodes\n</code></pre>"},{"location":"api/controller/scenarios/#nebula.controller.scenarios.Scenario.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create an instance of the class from a dictionary of attributes.</p> This class method <ul> <li>Copies the input dictionary to prevent modification of the original data.</li> <li>Instantiates the class using the dictionary unpacked as keyword arguments.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>Dictionary containing attributes to initialize the class instance.</p> required <p>Returns:</p> Name Type Description <code>cls</code> <p>An instance of the class initialized with the provided data.</p> Source code in <code>nebula/controller/scenarios.py</code> <pre><code>@classmethod\ndef from_dict(cls, data):\n    \"\"\"\n    Create an instance of the class from a dictionary of attributes.\n\n    This class method:\n        - Copies the input dictionary to prevent modification of the original data.\n        - Instantiates the class using the dictionary unpacked as keyword arguments.\n\n    Args:\n        data (dict): Dictionary containing attributes to initialize the class instance.\n\n    Returns:\n        cls: An instance of the class initialized with the provided data.\n    \"\"\"\n    # Create a copy of the data to avoid modifying the original\n    scenario_data = data.copy()\n\n    # Create the scenario object\n    scenario = cls(**scenario_data)\n\n    return scenario\n</code></pre>"},{"location":"api/controller/scenarios/#nebula.controller.scenarios.Scenario.mobility_assign","title":"<code>mobility_assign(nodes, mobile_participants_percent)</code>","text":"<p>Assign mobility status to a subset of nodes based on a specified percentage.</p> This method <ul> <li>Calculates the number of mobile nodes by applying the given percentage.</li> <li>Randomly selects nodes to be marked as mobile.</li> <li>Updates each node's \"mobility\" attribute to True or False accordingly.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>dict</code> <p>Dictionary of nodes with their current attributes.</p> required <code>mobile_participants_percent</code> <code>float</code> <p>Percentage of nodes to be assigned mobility (0-100).</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Updated nodes dictionary with mobility status assigned.</p> Source code in <code>nebula/controller/scenarios.py</code> <pre><code>def mobility_assign(self, nodes, mobile_participants_percent):\n    \"\"\"\n    Assign mobility status to a subset of nodes based on a specified percentage.\n\n    This method:\n        - Calculates the number of mobile nodes by applying the given percentage.\n        - Randomly selects nodes to be marked as mobile.\n        - Updates each node's \"mobility\" attribute to True or False accordingly.\n\n    Args:\n        nodes (dict): Dictionary of nodes with their current attributes.\n        mobile_participants_percent (float): Percentage of nodes to be assigned mobility (0-100).\n\n    Returns:\n        dict: Updated nodes dictionary with mobility status assigned.\n    \"\"\"\n    import random\n\n    # Number of mobile nodes, round down\n    num_mobile = math.floor(mobile_participants_percent / 100 * len(nodes))\n    if num_mobile &gt; len(nodes):\n        num_mobile = len(nodes)\n\n    # Get the index of mobile nodes\n    mobile_nodes = random.sample(list(nodes.keys()), num_mobile)\n\n    # Assign the role of each node\n    for node in nodes:\n        node_mob = False\n        if node in mobile_nodes:\n            node_mob = True\n        nodes[node][\"mobility\"] = node_mob\n    return nodes\n</code></pre>"},{"location":"api/controller/scenarios/#nebula.controller.scenarios.ScenarioManagement","title":"<code>ScenarioManagement</code>","text":"<p>Initialize the scenario management.</p> <p>Parameters:</p> Name Type Description Default <code>scenario</code> <code>dict</code> <p>Dictionary containing the scenario configuration.</p> required <code>user</code> <code>str</code> <p>User identifier. Defaults to None.</p> <code>None</code> <p>Functionality: - Loads the scenario from a dictionary. - Sets up names and paths for configuration and log storage. - Creates necessary directories with proper permissions. - Saves the scenario configuration and management settings as JSON files. - Assigns malicious and mobile nodes according to scenario parameters. - Configures each node individually with parameters for networking, device,   attacks, defense, mobility, reporting, trustworthiness, and situational awareness.</p> Source code in <code>nebula/controller/scenarios.py</code> <pre><code>class ScenarioManagement:\n    \"\"\"\n    Initialize the scenario management.\n\n    Args:\n        scenario (dict): Dictionary containing the scenario configuration.\n        user (str, optional): User identifier. Defaults to None.\n\n    Functionality:\n    - Loads the scenario from a dictionary.\n    - Sets up names and paths for configuration and log storage.\n    - Creates necessary directories with proper permissions.\n    - Saves the scenario configuration and management settings as JSON files.\n    - Assigns malicious and mobile nodes according to scenario parameters.\n    - Configures each node individually with parameters for networking, device,\n      attacks, defense, mobility, reporting, trustworthiness, and situational awareness.\n    \"\"\"\n\n    def __init__(self, scenario, user=None):\n        # Current scenario\n        self.scenario = Scenario.from_dict(scenario)\n        # Uid of the user\n        self.user = user\n        # Scenario management settings\n        self.start_date_scenario = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n        self.scenario_name = f\"nebula_{self.scenario.federation}_{datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}\"\n        self.root_path = os.environ.get(\"NEBULA_ROOT_HOST\")\n        self.host_platform = os.environ.get(\"NEBULA_HOST_PLATFORM\")\n        self.config_dir = os.path.join(os.environ.get(\"NEBULA_CONFIG_DIR\"), self.scenario_name)\n        self.log_dir = os.environ.get(\"NEBULA_LOGS_DIR\")\n        self.cert_dir = os.environ.get(\"NEBULA_CERTS_DIR\")\n        self.advanced_analytics = os.environ.get(\"NEBULA_ADVANCED_ANALYTICS\", \"False\") == \"True\"\n        self.config = Config(entity=\"scenarioManagement\")\n\n        # If physical set the neighbours correctly\n        if self.scenario.deployment == \"physical\" and self.scenario.physical_ips:\n            for idx, ip in enumerate(self.scenario.physical_ips):\n                node_key = str(idx)\n                if node_key in self.scenario.nodes:\n                    self.scenario.nodes[node_key][\"ip\"] = ip\n\n        # Assign the controller endpoint\n        if self.scenario.deployment == \"docker\":\n            self.controller = f\"{os.environ.get('NEBULA_CONTROLLER_HOST')}:{os.environ.get('NEBULA_CONTROLLER_PORT')}\"\n        elif self.scenario.deployment == \"physical\":\n            self.controller = \"100.120.46.10:49152\"\n        else:\n            self.controller = f\"127.0.0.1:{os.environ.get('NEBULA_CONTROLLER_PORT')}\"\n\n        self.topologymanager = None\n        self.env_path = None\n\n        # Create Scenario management dirs\n        os.makedirs(self.config_dir, exist_ok=True)\n        os.makedirs(os.path.join(self.log_dir, self.scenario_name), exist_ok=True)\n        os.makedirs(self.cert_dir, exist_ok=True)\n\n        # Give permissions to the directories\n        os.chmod(self.config_dir, 0o777)\n        os.chmod(os.path.join(self.log_dir, self.scenario_name), 0o777)\n        os.chmod(self.cert_dir, 0o777)\n\n        # Save the scenario configuration\n        scenario_file = os.path.join(self.config_dir, \"scenario.json\")\n        with open(scenario_file, \"w\") as f:\n            json.dump(scenario, f, sort_keys=False, indent=2)\n\n        os.chmod(scenario_file, 0o777)\n\n        # Save management settings\n        settings = {\n            \"scenario_name\": self.scenario_name,\n            \"root_path\": self.root_path,\n            \"config_dir\": self.config_dir,\n            \"log_dir\": self.log_dir,\n            \"cert_dir\": self.cert_dir,\n            \"env\": None,\n        }\n\n        settings_file = os.path.join(self.config_dir, \"settings.json\")\n        with open(settings_file, \"w\") as f:\n            json.dump(settings, f, sort_keys=False, indent=2)\n\n        os.chmod(settings_file, 0o777)\n\n        # Get attack parameters from attack_params\n        poisoned_node_percent = self.scenario.attack_params.get(\"poisoned_node_percent\", 0)\n        poisoned_sample_percent = self.scenario.attack_params.get(\"poisoned_sample_percent\", 0)\n        poisoned_noise_percent = self.scenario.attack_params.get(\"poisoned_noise_percent\", 0)\n\n        self.scenario.nodes = self.scenario.attack_node_assign(\n            self.scenario.nodes,\n            self.scenario.federation,\n            int(poisoned_node_percent),\n            int(poisoned_sample_percent),\n            int(poisoned_noise_percent),\n            self.scenario.attack_params,\n        )\n\n        if self.scenario.mobility:\n            mobile_participants_percent = int(self.scenario.mobile_participants_percent)\n            self.scenario.nodes = self.scenario.mobility_assign(self.scenario.nodes, mobile_participants_percent)\n        else:\n            self.scenario.nodes = self.scenario.mobility_assign(self.scenario.nodes, 0)\n\n        # Save node settings\n        for node in self.scenario.nodes:\n            node_config = self.scenario.nodes[node]\n            participant_file = os.path.join(self.config_dir, f\"participant_{node_config['id']}.json\")\n            os.makedirs(os.path.dirname(participant_file), exist_ok=True)\n            shutil.copy(\n                os.path.join(\n                    os.path.dirname(__file__),\n                    \"../frontend/config/participant.json.example\",\n                ),\n                participant_file,\n            )\n            os.chmod(participant_file, 0o777)\n            with open(participant_file) as f:\n                participant_config = json.load(f)\n\n            participant_config[\"network_args\"][\"ip\"] = node_config[\"ip\"]\n            if self.scenario.deployment == \"physical\":\n                participant_config[\"network_args\"][\"port\"] = 8000\n            else:\n                participant_config[\"network_args\"][\"port\"] = int(node_config[\"port\"])\n            participant_config[\"network_args\"][\"simulation\"] = self.scenario.network_simulation\n            participant_config[\"device_args\"][\"idx\"] = node_config[\"id\"]\n            participant_config[\"device_args\"][\"start\"] = node_config[\"start\"]\n            participant_config[\"device_args\"][\"role\"] = node_config[\"role\"]\n            participant_config[\"device_args\"][\"proxy\"] = node_config[\"proxy\"]\n            participant_config[\"device_args\"][\"malicious\"] = node_config[\"malicious\"]\n            participant_config[\"scenario_args\"][\"rounds\"] = int(self.scenario.rounds)\n            participant_config[\"data_args\"][\"dataset\"] = self.scenario.dataset\n            participant_config[\"data_args\"][\"iid\"] = self.scenario.iid\n            participant_config[\"data_args\"][\"partition_selection\"] = self.scenario.partition_selection\n            participant_config[\"data_args\"][\"partition_parameter\"] = self.scenario.partition_parameter\n            participant_config[\"model_args\"][\"model\"] = self.scenario.model\n            participant_config[\"training_args\"][\"epochs\"] = int(self.scenario.epochs)\n            participant_config[\"device_args\"][\"accelerator\"] = self.scenario.accelerator\n            participant_config[\"device_args\"][\"gpu_id\"] = self.scenario.gpu_id\n            participant_config[\"device_args\"][\"logging\"] = self.scenario.logginglevel\n            participant_config[\"aggregator_args\"][\"algorithm\"] = self.scenario.agg_algorithm\n            # To be sure that benign nodes have no attack parameters\n            if node_config[\"role\"] == \"malicious\":\n                participant_config[\"adversarial_args\"][\"fake_behavior\"] = node_config[\"fake_behavior\"]\n                participant_config[\"adversarial_args\"][\"attack_params\"] = node_config[\"attack_params\"]\n            else:\n                participant_config[\"adversarial_args\"][\"attack_params\"] = {\"attacks\": \"No Attack\"}\n                participant_config[\"defense_args\"][\"reputation\"] = self.scenario.reputation\n\n            participant_config[\"mobility_args\"][\"random_geo\"] = self.scenario.random_geo\n            participant_config[\"mobility_args\"][\"latitude\"] = self.scenario.latitude\n            participant_config[\"mobility_args\"][\"longitude\"] = self.scenario.longitude\n            participant_config[\"mobility_args\"][\"mobility\"] = node_config[\"mobility\"]\n            participant_config[\"mobility_args\"][\"mobility_type\"] = self.scenario.mobility_type\n            participant_config[\"mobility_args\"][\"radius_federation\"] = self.scenario.radius_federation\n            participant_config[\"mobility_args\"][\"scheme_mobility\"] = self.scenario.scheme_mobility\n            participant_config[\"mobility_args\"][\"round_frequency\"] = self.scenario.round_frequency\n            participant_config[\"reporter_args\"][\"report_status_data_queue\"] = self.scenario.report_status_data_queue\n            participant_config[\"mobility_args\"][\"topology_type\"] = self.scenario.topology\n            if self.scenario.with_sa:\n                participant_config[\"situational_awareness\"] = {\n                    \"strict_topology\": self.scenario.strict_topology,\n                    \"sa_discovery\": {\n                        \"candidate_selector\": self.scenario.sad_candidate_selector,\n                        \"model_handler\": self.scenario.sad_model_handler,\n                        \"verbose\": True,\n                    },\n                    \"sa_reasoner\": {\n                        \"arbitration_policy\": self.scenario.sar_arbitration_policy,\n                        \"verbose\": True,\n                        \"sar_components\": {\"sa_network\": True, \"sa_training\": self.scenario.sar_training},\n                        \"sa_network\": {\"neighbor_policy\": self.scenario.sar_neighbor_policy, \"verbose\": True},\n                        \"sa_training\": {\"training_policy\": self.scenario.sar_training_policy, \"verbose\": True},\n                    },\n                }\n            participant_config[\"trustworthiness\"] = self.scenario.with_trustworthiness\n            if self.scenario.with_trustworthiness:\n                participant_config[\"trust_args\"] = {\n                    \"robustness_pillar\": self.scenario.robustness_pillar,\n                    \"resilience_to_attacks\": self.scenario.resilience_to_attacks,\n                    \"algorithm_robustness\": self.scenario.algorithm_robustness,\n                    \"client_reliability\": self.scenario.client_reliability,\n                    \"privacy_pillar\": self.scenario.privacy_pillar,\n                    \"technique\": self.scenario.technique,\n                    \"uncertainty\": self.scenario.uncertainty,\n                    \"indistinguishability\": self.scenario.indistinguishability,\n                    \"fairness_pillar\": self.scenario.fairness_pillar,\n                    \"selection_fairness\": self.scenario.selection_fairness,\n                    \"performance_fairness\": self.scenario.performance_fairness,\n                    \"class_distribution\": self.scenario.class_distribution,\n                    \"explainability_pillar\": self.scenario.explainability_pillar,\n                    \"interpretability\": self.scenario.interpretability,\n                    \"post_hoc_methods\": self.scenario.post_hoc_methods,\n                    \"accountability_pillar\": self.scenario.accountability_pillar,\n                    \"factsheet_completeness\": self.scenario.factsheet_completeness,\n                    \"architectural_soundness_pillar\": self.scenario.architectural_soundness_pillar,\n                    \"client_management\": self.scenario.client_management,\n                    \"optimization\": self.scenario.optimization,\n                    \"sustainability_pillar\": self.scenario.sustainability_pillar,\n                    \"energy_source\": self.scenario.energy_source,\n                    \"hardware_efficiency\": self.scenario.hardware_efficiency,\n                    \"federation_complexity\": self.scenario.federation_complexity,\n                    \"scenario\": scenario,\n                }\n\n            with open(participant_file, \"w\") as f:\n                json.dump(participant_config, f, sort_keys=False, indent=2)\n\n    @staticmethod\n    def stop_participants(scenario_name=None):\n        \"\"\"\n        Stop running participant nodes by removing the scenario command files.\n\n        This method deletes the 'current_scenario_commands.sh' (or '.ps1' on Windows)\n        file associated with a scenario. Removing this file signals the nodes to stop\n        by terminating their processes.\n\n        Args:\n            scenario_name (str, optional): The name of the scenario to stop. If None,\n                all scenarios' command files will be removed.\n\n        Notes:\n            - If the environment variable NEBULA_CONFIG_DIR is not set, a default\n              configuration directory path is used.\n            - Supports both Linux/macOS ('.sh') and Windows ('.ps1') script files.\n            - Any errors during file removal are logged with the traceback.\n        \"\"\"\n        # When stopping the nodes, we need to remove the current_scenario_commands.sh file -&gt; it will cause the nodes to stop using PIDs\n        try:\n            nebula_config_dir = os.environ.get(\"NEBULA_CONFIG_DIR\")\n            if not nebula_config_dir:\n                current_dir = os.path.dirname(__file__)\n                nebula_base_dir = os.path.abspath(os.path.join(current_dir, \"..\", \"..\"))\n                nebula_config_dir = os.path.join(nebula_base_dir, \"app\", \"config\")\n                logging.info(f\"NEBULA_CONFIG_DIR not found. Using default path: {nebula_config_dir}\")\n\n            if scenario_name:\n                if os.environ.get(\"NEBULA_HOST_PLATFORM\") == \"windows\":\n                    scenario_commands_file = os.path.join(\n                        nebula_config_dir, scenario_name, \"current_scenario_commands.ps1\"\n                    )\n                else:\n                    scenario_commands_file = os.path.join(\n                        nebula_config_dir, scenario_name, \"current_scenario_commands.sh\"\n                    )\n                if os.path.exists(scenario_commands_file):\n                    os.remove(scenario_commands_file)\n            else:\n                if os.environ.get(\"NEBULA_HOST_PLATFORM\") == \"windows\":\n                    files = glob.glob(\n                        os.path.join(nebula_config_dir, \"**/current_scenario_commands.ps1\"), recursive=True\n                    )\n                else:\n                    files = glob.glob(\n                        os.path.join(nebula_config_dir, \"**/current_scenario_commands.sh\"), recursive=True\n                    )\n                for file in files:\n                    os.remove(file)\n        except Exception as e:\n            logging.exception(f\"Error while removing current_scenario_commands.sh file: {e}\")\n\n    @staticmethod\n    def stop_nodes():\n        \"\"\"\n        Stop all running NEBULA nodes.\n\n        This method logs the shutdown action and calls the stop_participants\n        method to remove all scenario command files, which signals nodes to stop.\n        \"\"\"\n        logging.info(\"Closing NEBULA nodes... Please wait\")\n        ScenarioManagement.stop_participants()\n\n    async def load_configurations_and_start_nodes(\n        self, additional_participants=None, schema_additional_participants=None\n    ):\n        \"\"\"\n        Load participant configurations, generate certificates, setup topology, split datasets,\n        and start nodes according to the scenario deployment type.\n\n        This method:\n        - Generates CA and node certificates.\n        - Loads and updates participant configuration files.\n        - Creates the network topology and updates participant roles.\n        - Handles additional participants if provided.\n        - Initializes and partitions the dataset based on the scenario.\n        - Starts nodes using the specified deployment method (docker, physical, or process).\n\n        Args:\n            additional_participants (list, optional): List of additional participant configurations to add.\n            schema_additional_participants (optional): Schema for additional participants (currently unused).\n\n        Raises:\n            ValueError: If no participant files found, multiple start nodes detected, no start node found,\n                        unsupported dataset or unknown deployment type.\n        \"\"\"\n        logging.info(f\"Generating the scenario {self.scenario_name} at {self.start_date_scenario}\")\n\n        # Generate CA certificate\n        generate_ca_certificate(dir_path=self.cert_dir)\n\n        # Get participants configurations\n        participant_files = glob.glob(f\"{self.config_dir}/participant_*.json\")\n        participant_files.sort()\n        if len(participant_files) == 0:\n            raise ValueError(\"No participant files found in config folder\")\n\n        self.config.set_participants_config(participant_files)\n        self.n_nodes = len(participant_files)\n        logging.info(f\"Number of nodes: {self.n_nodes}\")\n\n        self.topologymanager = (\n            self.create_topology(matrix=self.scenario.matrix) if self.scenario.matrix else self.create_topology()\n        )\n\n        # Update participants configuration\n        is_start_node = False\n        config_participants = []\n        # ap = len(additional_participants) if additional_participants else 0\n        additional_nodes = len(additional_participants) if additional_participants else 0\n        logging.info(f\"######## nodes: {self.n_nodes} + additionals: {additional_nodes} ######\")\n\n        # Sort participant files by index to ensure correct order\n        participant_files.sort(key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))\n\n        for i in range(self.n_nodes):\n            with open(f\"{self.config_dir}/participant_\" + str(i) + \".json\") as f:\n                participant_config = json.load(f)\n            participant_config[\"scenario_args\"][\"federation\"] = self.scenario.federation\n            participant_config[\"scenario_args\"][\"n_nodes\"] = self.n_nodes + additional_nodes\n            participant_config[\"network_args\"][\"neighbors\"] = self.topologymanager.get_neighbors_string(i)\n            participant_config[\"scenario_args\"][\"name\"] = self.scenario_name\n            participant_config[\"scenario_args\"][\"start_time\"] = self.start_date_scenario\n            participant_config[\"device_args\"][\"idx\"] = i\n            participant_config[\"device_args\"][\"uid\"] = hashlib.sha1(\n                (\n                    str(participant_config[\"network_args\"][\"ip\"])\n                    + str(participant_config[\"network_args\"][\"port\"])\n                    + str(self.scenario_name)\n                ).encode()\n            ).hexdigest()\n            if participant_config[\"mobility_args\"][\"random_geo\"]:\n                (\n                    participant_config[\"mobility_args\"][\"latitude\"],\n                    participant_config[\"mobility_args\"][\"longitude\"],\n                ) = TopologyManager.get_coordinates(random_geo=True)\n            else:\n                participant_config[\"mobility_args\"][\"latitude\"] = self.scenario.latitude\n                participant_config[\"mobility_args\"][\"longitude\"] = self.scenario.longitude\n            # If not, use the given coordinates in the frontend\n            participant_config[\"tracking_args\"][\"local_tracking\"] = \"advanced\" if self.advanced_analytics else \"basic\"\n            participant_config[\"tracking_args\"][\"log_dir\"] = self.log_dir\n            participant_config[\"tracking_args\"][\"config_dir\"] = self.config_dir\n\n            # Generate node certificate\n            keyfile_path, certificate_path = generate_certificate(\n                dir_path=self.cert_dir,\n                node_id=f\"participant_{i}\",\n                ip=participant_config[\"network_args\"][\"ip\"],\n            )\n\n            participant_config[\"security_args\"][\"certfile\"] = certificate_path\n            participant_config[\"security_args\"][\"keyfile\"] = keyfile_path\n\n            if participant_config[\"device_args\"][\"start\"]:\n                if not is_start_node:\n                    is_start_node = True\n                else:\n                    raise ValueError(\"Only one node can be start node\")\n\n            with open(f\"{self.config_dir}/participant_\" + str(i) + \".json\", \"w\") as f:\n                json.dump(participant_config, f, sort_keys=False, indent=2)\n\n            config_participants.append((\n                participant_config[\"network_args\"][\"ip\"],\n                participant_config[\"network_args\"][\"port\"],\n                participant_config[\"device_args\"][\"role\"],\n            ))\n        if not is_start_node:\n            raise ValueError(\"No start node found\")\n        self.config.set_participants_config(participant_files)\n\n        # Add role to the topology (visualization purposes)\n        self.topologymanager.update_nodes(config_participants)\n        self.topologymanager.draw_graph(path=f\"{self.config_dir}/topology.png\", plot=False)\n\n        # Include additional participants (if any) as copies of the last participant\n        additional_participants_files = []\n        if additional_participants:\n            last_participant_file = participant_files[-1]\n            last_participant_index = len(participant_files)\n\n            for i, additional_participant in enumerate(additional_participants):\n                additional_participant_file = f\"{self.config_dir}/participant_{last_participant_index + i}.json\"\n                shutil.copy(last_participant_file, additional_participant_file)\n\n                with open(additional_participant_file) as f:\n                    participant_config = json.load(f)\n\n                logging.info(f\"Configuration | additional nodes |  participant: {self.n_nodes + i + 1}\")\n                last_ip = participant_config[\"network_args\"][\"ip\"]\n                logging.info(f\"Valores de la ultima ip: ({last_ip})\")\n                participant_config[\"scenario_args\"][\"n_nodes\"] = self.n_nodes + additional_nodes  # self.n_nodes + i + 1\n                participant_config[\"device_args\"][\"idx\"] = last_participant_index + i\n                participant_config[\"network_args\"][\"neighbors\"] = \"\"\n                participant_config[\"network_args\"][\"ip\"] = (\n                    participant_config[\"network_args\"][\"ip\"].rsplit(\".\", 1)[0]\n                    + \".\"\n                    + str(int(participant_config[\"network_args\"][\"ip\"].rsplit(\".\", 1)[1]) + i + 1)\n                )\n                participant_config[\"device_args\"][\"uid\"] = hashlib.sha1(\n                    (\n                        str(participant_config[\"network_args\"][\"ip\"])\n                        + str(participant_config[\"network_args\"][\"port\"])\n                        + str(self.scenario_name)\n                    ).encode()\n                ).hexdigest()\n                participant_config[\"mobility_args\"][\"additional_node\"][\"status\"] = True\n                participant_config[\"mobility_args\"][\"additional_node\"][\"time_start\"] = additional_participant[\n                    \"time_start\"\n                ]\n\n                # used for late creation nodes\n                participant_config[\"mobility_args\"][\"late_creation\"] = True\n\n                with open(additional_participant_file, \"w\") as f:\n                    json.dump(participant_config, f, sort_keys=False, indent=2)\n\n                additional_participants_files.append(additional_participant_file)\n\n        if additional_participants_files:\n            self.config.add_participants_config(additional_participants_files)\n\n        if additional_participants:\n            self.n_nodes += len(additional_participants)\n\n        # Splitting dataset\n        dataset_name = self.scenario.dataset\n        dataset = None\n        if dataset_name == \"MNIST\":\n            dataset = MNISTDataset(\n                num_classes=10,\n                partitions_number=self.n_nodes,\n                iid=self.scenario.iid,\n                partition=self.scenario.partition_selection,\n                partition_parameter=self.scenario.partition_parameter,\n                seed=42,\n                config_dir=self.config_dir,\n            )\n        elif dataset_name == \"FashionMNIST\":\n            dataset = FashionMNISTDataset(\n                num_classes=10,\n                partitions_number=self.n_nodes,\n                iid=self.scenario.iid,\n                partition=self.scenario.partition_selection,\n                partition_parameter=self.scenario.partition_parameter,\n                seed=42,\n                config_dir=self.config_dir,\n            )\n        elif dataset_name == \"EMNIST\":\n            dataset = EMNISTDataset(\n                num_classes=47,\n                partitions_number=self.n_nodes,\n                iid=self.scenario.iid,\n                partition=self.scenario.partition_selection,\n                partition_parameter=self.scenario.partition_parameter,\n                seed=42,\n                config_dir=self.config_dir,\n            )\n        elif dataset_name == \"CIFAR10\":\n            dataset = CIFAR10Dataset(\n                num_classes=10,\n                partitions_number=self.n_nodes,\n                iid=self.scenario.iid,\n                partition=self.scenario.partition_selection,\n                partition_parameter=self.scenario.partition_parameter,\n                seed=42,\n                config_dir=self.config_dir,\n            )\n        elif dataset_name == \"CIFAR100\":\n            dataset = CIFAR100Dataset(\n                num_classes=100,\n                partitions_number=self.n_nodes,\n                iid=self.scenario.iid,\n                partition=self.scenario.partition_selection,\n                partition_parameter=self.scenario.partition_parameter,\n                seed=42,\n                config_dir=self.config_dir,\n            )\n        else:\n            raise ValueError(f\"Dataset {dataset_name} not supported\")\n\n        logging.info(f\"Splitting {dataset_name} dataset...\")\n        dataset.initialize_dataset()\n        logging.info(f\"Splitting {dataset_name} dataset... Done\")\n\n        if self.scenario.deployment in [\"docker\", \"process\", \"physical\"]:\n            if self.scenario.deployment == \"docker\":\n                self.start_nodes_docker()\n            elif self.scenario.deployment == \"physical\":\n                self.start_nodes_physical()\n            elif self.scenario.deployment == \"process\":\n                self.start_nodes_process()\n            else:\n                raise ValueError(f\"Unknown deployment type: {self.scenario.deployment}\")\n        else:\n            logging.info(\n                f\"Virtualization mode is disabled for scenario '{self.scenario_name}' with {self.n_nodes} nodes. Waiting for nodes to start manually...\"\n            )\n\n    def create_topology(self, matrix=None):\n        \"\"\"\n        Create and return a network topology manager based on the scenario's topology settings or a given adjacency matrix.\n\n        Supports multiple topology types:\n        - Random: Generates an Erd\u0151s-R\u00e9nyi random graph with specified connection probability.\n        - Matrix: Uses a provided adjacency matrix to define the topology.\n        - Fully: Creates a fully connected network.\n        - Ring: Creates a ring-structured network with partial connectivity.\n        - Star: Creates a centralized star topology (only for CFL federation).\n\n        The method assigns IP and port information to nodes and returns the configured TopologyManager instance.\n\n        Args:\n            matrix (optional): Adjacency matrix to define custom topology. If provided, overrides scenario topology.\n\n        Raises:\n            ValueError: If an unknown topology type is specified in the scenario.\n\n        Returns:\n            TopologyManager: Configured topology manager with nodes assigned.\n        \"\"\"\n        import numpy as np\n\n        if self.scenario.topology == \"Random\":\n            # Create network topology using topology manager (random)\n            probability = float(self.scenario.random_topology_probability)\n            logging.info(\n                f\"Creating random network topology using erdos_renyi_graph: nodes={self.n_nodes}, probability={probability}\"\n            )\n            topologymanager = TopologyManager(\n                scenario_name=self.scenario_name,\n                n_nodes=self.n_nodes,\n                b_symmetric=True,\n                undirected_neighbor_num=3,\n            )\n            topologymanager.generate_random_topology(probability)\n        elif matrix is not None:\n            if self.n_nodes &gt; 2:\n                topologymanager = TopologyManager(\n                    topology=np.array(matrix),\n                    scenario_name=self.scenario_name,\n                    n_nodes=self.n_nodes,\n                    b_symmetric=True,\n                    undirected_neighbor_num=self.n_nodes - 1,\n                )\n            else:\n                topologymanager = TopologyManager(\n                    topology=np.array(matrix),\n                    scenario_name=self.scenario_name,\n                    n_nodes=self.n_nodes,\n                    b_symmetric=True,\n                    undirected_neighbor_num=2,\n                )\n        elif self.scenario.topology == \"Fully\":\n            # Create a fully connected network\n            topologymanager = TopologyManager(\n                scenario_name=self.scenario_name,\n                n_nodes=self.n_nodes,\n                b_symmetric=True,\n                undirected_neighbor_num=self.n_nodes - 1,\n            )\n            topologymanager.generate_topology()\n        elif self.scenario.topology == \"Ring\":\n            # Create a partially connected network (ring-structured network)\n            topologymanager = TopologyManager(scenario_name=self.scenario_name, n_nodes=self.n_nodes, b_symmetric=True)\n            topologymanager.generate_ring_topology(increase_convergence=True)\n        elif self.scenario.topology == \"Star\" and self.scenario.federation == \"CFL\":\n            # Create a centralized network\n            topologymanager = TopologyManager(scenario_name=self.scenario_name, n_nodes=self.n_nodes, b_symmetric=True)\n            topologymanager.generate_server_topology()\n        else:\n            raise ValueError(f\"Unknown topology type: {self.scenario.topology}\")\n\n        # Assign nodes to topology\n        nodes_ip_port = []\n        self.config.participants.sort(key=lambda x: int(x[\"device_args\"][\"idx\"]))\n        for i, node in enumerate(self.config.participants):\n            nodes_ip_port.append((\n                node[\"network_args\"][\"ip\"],\n                node[\"network_args\"][\"port\"],\n                \"undefined\",\n            ))\n\n        topologymanager.add_nodes(nodes_ip_port)\n        return topologymanager\n\n    def start_nodes_docker(self):\n        \"\"\"\n        Starts participant nodes as Docker containers using Docker SDK.\n\n        This method performs the following steps:\n        - Logs the beginning of the Docker container startup process.\n        - Creates a Docker network specific to the current user and scenario.\n        - Sorts participant nodes by their index.\n        - For each participant node:\n            - Sets up environment variables and host configuration,\n              enabling GPU support if required.\n            - Prepares Docker volume bindings and static network IP assignment.\n            - Updates the node configuration, replacing IP addresses as needed,\n              and writes the configuration to a JSON file.\n            - Creates and starts the Docker container for the node.\n            - Logs any exceptions encountered during container creation or startup.\n\n        Raises:\n            docker.errors.DockerException: If there are issues communicating with the Docker daemon.\n            OSError: If there are issues accessing file system paths for volume binding.\n            Exception: For any other unexpected errors during container creation or startup.\n\n        Note:\n            - The method assumes Docker and NVIDIA runtime are properly installed and configured.\n            - IP addresses in node configurations are replaced with network base dynamically.\n        \"\"\"\n        logging.info(\"Starting nodes using Docker Compose...\")\n        logging.info(f\"env path: {self.env_path}\")\n\n        network_name = f\"{os.environ.get('NEBULA_CONTROLLER_NAME')}_{str(self.user).lower()}-nebula-net-scenario\"\n\n        # Create the Docker network\n        base = DockerUtils.create_docker_network(network_name)\n\n        client = docker.from_env()\n\n        self.config.participants.sort(key=lambda x: x[\"device_args\"][\"idx\"])\n        i = 2\n        container_ids = []\n        for idx, node in enumerate(self.config.participants):\n            image = \"nebula-core\"\n            name = f\"{os.environ.get('NEBULA_CONTROLLER_NAME')}_{self.user}-participant{node['device_args']['idx']}\"\n\n            if node[\"device_args\"][\"accelerator\"] == \"gpu\":\n                environment = {\n                    \"NVIDIA_DISABLE_REQUIRE\": True,\n                    \"NEBULA_LOGS_DIR\": \"/nebula/app/logs/\",\n                    \"NEBULA_CONFIG_DIR\": \"/nebula/app/config/\",\n                }\n                host_config = client.api.create_host_config(\n                    binds=[f\"{self.root_path}:/nebula\", \"/var/run/docker.sock:/var/run/docker.sock\"],\n                    privileged=True,\n                    device_requests=[docker.types.DeviceRequest(driver=\"nvidia\", count=-1, capabilities=[[\"gpu\"]])],\n                    extra_hosts={\"host.docker.internal\": \"host-gateway\"},\n                )\n            else:\n                environment = {\"NEBULA_LOGS_DIR\": \"/nebula/app/logs/\", \"NEBULA_CONFIG_DIR\": \"/nebula/app/config/\"}\n                host_config = client.api.create_host_config(\n                    binds=[f\"{self.root_path}:/nebula\", \"/var/run/docker.sock:/var/run/docker.sock\"],\n                    privileged=True,\n                    device_requests=[],\n                    extra_hosts={\"host.docker.internal\": \"host-gateway\"},\n                )\n\n            volumes = [\"/nebula\", \"/var/run/docker.sock\"]\n\n            start_command = \"sleep 10\" if node[\"device_args\"][\"start\"] else \"sleep 0\"\n            command = [\n                \"/bin/bash\",\n                \"-c\",\n                f\"{start_command} &amp;&amp; ifconfig &amp;&amp; echo '{base}.1 host.docker.internal' &gt;&gt; /etc/hosts &amp;&amp; python /nebula/nebula/core/node.py /nebula/app/config/{self.scenario_name}/participant_{node['device_args']['idx']}.json\",\n            ]\n\n            networking_config = client.api.create_networking_config({\n                f\"{network_name}\": client.api.create_endpoint_config(\n                    ipv4_address=f\"{base}.{i}\",\n                ),\n                f\"{os.environ.get('NEBULA_CONTROLLER_NAME')}_nebula-net-base\": client.api.create_endpoint_config(),\n            })\n\n            node[\"tracking_args\"][\"log_dir\"] = \"/nebula/app/logs\"\n            node[\"tracking_args\"][\"config_dir\"] = f\"/nebula/app/config/{self.scenario_name}\"\n            node[\"scenario_args\"][\"controller\"] = self.controller\n            node[\"scenario_args\"][\"deployment\"] = self.scenario.deployment\n            node[\"security_args\"][\"certfile\"] = f\"/nebula/app/certs/participant_{node['device_args']['idx']}_cert.pem\"\n            node[\"security_args\"][\"keyfile\"] = f\"/nebula/app/certs/participant_{node['device_args']['idx']}_key.pem\"\n            node[\"security_args\"][\"cafile\"] = \"/nebula/app/certs/ca_cert.pem\"\n            node = json.loads(json.dumps(node).replace(\"192.168.50.\", f\"{base}.\"))  # TODO change this\n\n            # Write the config file in config directory\n            with open(f\"{self.config_dir}/participant_{node['device_args']['idx']}.json\", \"w\") as f:\n                json.dump(node, f, indent=4)\n\n            try:\n                container_id = client.api.create_container(\n                    image=image,\n                    name=name,\n                    detach=True,\n                    volumes=volumes,\n                    environment=environment,\n                    command=command,\n                    host_config=host_config,\n                    networking_config=networking_config,\n                )\n            except Exception as e:\n                logging.exception(f\"Creating container {name}: {e}\")\n\n            try:\n                client.api.start(container_id)\n                container_ids.append(container_id)\n            except Exception as e:\n                logging.exception(f\"Starting participant {name} error: {e}\")\n            i += 1\n\n    def start_nodes_process(self):\n        \"\"\"\n        Starts participant nodes as independent background processes on the host machine.\n\n        This method performs the following steps:\n        - Updates each participant's configuration with paths for logs, config, certificates,\n          and scenario parameters.\n        - Writes the updated configuration for each participant to a JSON file.\n        - Generates and writes a platform-specific script to start all participant nodes:\n            - On Windows, it creates a PowerShell script that launches each node as a background\n              process, redirects output and error streams to log files, and records process IDs.\n            - On Unix-like systems, it creates a bash script that launches each node in the\n              background, redirects output, and stores PIDs in a file.\n        - Sets executable permissions for the generated script.\n\n        Raises:\n            Exception: If any error occurs during the script generation or file operations.\n\n        Notes:\n            - The generated script must be executed separately by the user to actually start the nodes.\n            - Sleep intervals are added before starting nodes depending on their 'start' flag.\n            - Logs and PIDs are stored under the configured directories for monitoring and management.\n        \"\"\"\n        self.processes_root_path = os.path.join(os.path.dirname(__file__), \"..\", \"..\")\n        logging.info(\"Starting nodes as processes...\")\n        logging.info(f\"env path: {self.env_path}\")\n\n        # Include additional config to the participants\n        for idx, node in enumerate(self.config.participants):\n            node[\"tracking_args\"][\"log_dir\"] = os.path.join(self.root_path, \"app\", \"logs\")\n            node[\"tracking_args\"][\"config_dir\"] = os.path.join(self.root_path, \"app\", \"config\", self.scenario_name)\n            node[\"scenario_args\"][\"controller\"] = self.controller\n            node[\"scenario_args\"][\"deployment\"] = self.scenario.deployment\n            node[\"security_args\"][\"certfile\"] = os.path.join(\n                self.root_path, \"app\", \"certs\", f\"participant_{node['device_args']['idx']}_cert.pem\"\n            )\n            node[\"security_args\"][\"keyfile\"] = os.path.join(\n                self.root_path, \"app\", \"certs\", f\"participant_{node['device_args']['idx']}_key.pem\"\n            )\n            node[\"security_args\"][\"cafile\"] = os.path.join(self.root_path, \"app\", \"certs\", \"ca_cert.pem\")\n\n            # Write the config file in config directory\n            with open(f\"{self.config_dir}/participant_{node['device_args']['idx']}.json\", \"w\") as f:\n                json.dump(node, f, indent=4)\n\n        try:\n            if self.host_platform == \"windows\":\n                commands = \"\"\"\n                $ParentDir = Split-Path -Parent $PSScriptRoot\n                $PID_FILE = \"$PSScriptRoot\\\\current_scenario_pids.txt\"\n                New-Item -Path $PID_FILE -Force -ItemType File\n\n                \"\"\"\n                sorted_participants = sorted(\n                    self.config.participants,\n                    key=lambda node: node[\"device_args\"][\"idx\"],\n                    reverse=True,\n                )\n                for node in sorted_participants:\n                    if node[\"device_args\"][\"start\"]:\n                        commands += \"Start-Sleep -Seconds 10\\n\"\n                    else:\n                        commands += \"Start-Sleep -Seconds 2\\n\"\n\n                    commands += f'Write-Host \"Running node {node[\"device_args\"][\"idx\"]}...\"\\n'\n                    commands += f'$OUT_FILE = \"{self.root_path}\\\\app\\\\logs\\\\{self.scenario_name}\\\\participant_{node[\"device_args\"][\"idx\"]}.out\"\\n'\n                    commands += f'$ERROR_FILE = \"{self.root_path}\\\\app\\\\logs\\\\{self.scenario_name}\\\\participant_{node[\"device_args\"][\"idx\"]}.err\"\\n'\n\n                    # Use Start-Process for executing Python in background and capture PID\n                    commands += f\"\"\"$process = Start-Process -FilePath \"python\" -ArgumentList \"{self.root_path}\\\\nebula\\\\core\\\\node.py {self.root_path}\\\\app\\\\config\\\\{self.scenario_name}\\\\participant_{node[\"device_args\"][\"idx\"]}.json\" -PassThru -NoNewWindow -RedirectStandardOutput $OUT_FILE -RedirectStandardError $ERROR_FILE\n                Add-Content -Path $PID_FILE -Value $process.Id\n                \"\"\"\n\n                commands += 'Write-Host \"All nodes started. PIDs stored in $PID_FILE\"\\n'\n\n                with open(f\"{self.config_dir}/current_scenario_commands.ps1\", \"w\") as f:\n                    f.write(commands)\n                os.chmod(f\"{self.config_dir}/current_scenario_commands.ps1\", 0o755)\n            else:\n                commands = '#!/bin/bash\\n\\nPID_FILE=\"$(dirname \"$0\")/current_scenario_pids.txt\"\\n\\n&gt; $PID_FILE\\n\\n'\n                sorted_participants = sorted(\n                    self.config.participants,\n                    key=lambda node: node[\"device_args\"][\"idx\"],\n                    reverse=True,\n                )\n                for node in sorted_participants:\n                    if node[\"device_args\"][\"start\"]:\n                        commands += \"sleep 10\\n\"\n                    else:\n                        commands += \"sleep 2\\n\"\n                    commands += f'echo \"Running node {node[\"device_args\"][\"idx\"]}...\"\\n'\n                    commands += f\"OUT_FILE={self.root_path}/app/logs/{self.scenario_name}/participant_{node['device_args']['idx']}.out\\n\"\n                    commands += f\"python {self.root_path}/nebula/core/node.py {self.root_path}/app/config/{self.scenario_name}/participant_{node['device_args']['idx']}.json &amp;\\n\"\n                    commands += \"echo $! &gt;&gt; $PID_FILE\\n\\n\"\n\n                commands += 'echo \"All nodes started. PIDs stored in $PID_FILE\"\\n'\n\n                with open(f\"{self.config_dir}/current_scenario_commands.sh\", \"w\") as f:\n                    f.write(commands)\n                os.chmod(f\"{self.config_dir}/current_scenario_commands.sh\", 0o755)\n\n        except Exception as e:\n            raise Exception(f\"Error starting nodes as processes: {e}\")\n\n    async def _upload_and_start(self, node_cfg: dict) -&gt; None:\n        ip = node_cfg[\"network_args\"][\"ip\"]\n        port = node_cfg[\"network_args\"][\"port\"]\n        host = f\"{ip}:{port}\"\n        idx = node_cfg[\"device_args\"][\"idx\"]\n\n        cfg_dir = self.config_dir\n        config_path = f\"{cfg_dir}/participant_{idx}.json\"\n        global_test_path = f\"{cfg_dir}/global_test.h5\"\n        train_set_path = f\"{cfg_dir}/participant_{idx}_train.h5\"\n\n        # ---------- multipart/form-data ------------------------\n        form = FormData()\n        form.add_field(\n            \"config\", open(config_path, \"rb\"), filename=os.path.basename(config_path), content_type=\"application/json\"\n        )\n        form.add_field(\n            \"global_test\",\n            open(global_test_path, \"rb\"),\n            filename=os.path.basename(global_test_path),\n            content_type=\"application/octet-stream\",\n        )\n        form.add_field(\n            \"train_set\",\n            open(train_set_path, \"rb\"),\n            filename=os.path.basename(train_set_path),\n            content_type=\"application/octet-stream\",\n        )\n\n        # ---------- /physical/setup/ (PUT) ---------------------\n        setup_ep = f\"/physical/setup/{quote(host, safe='')}\"\n        st, data = await remote_post_form(self.controller, setup_ep, form, method=\"PUT\")\n        if st != 201:\n            raise RuntimeError(f\"[{host}] setup failed {st}: {data}\")\n\n        # ---------- /physical/run/ (GET) ------------------------\n        run_ep = f\"/physical/run/{quote(host, safe='')}\"\n        st, data = await remote_get(self.controller, run_ep)\n        if st != 200:\n            raise RuntimeError(f\"[{host}] run failed {st}: {data}\")\n\n        logging.info(\"Node %s running: %s\", host, data)\n\n    async def start_nodes_physical(self):\n        \"\"\"\n        Placeholder method for starting nodes on physical devices.\n\n        Logs informational messages indicating that deployment on physical devices\n        is not implemented or supported publicly. Users are encouraged to use Docker\n        or process-based deployment methods instead.\n\n        Currently, this method does not perform any actions.\n        \"\"\"\n        logging.info(\"Starting nodes as physical devices...\")\n        logging.info(f\"env path: {self.env_path}\")\n\n        for idx, node in enumerate(self.config.participants):\n            pass\n\n        asyncio.create_task(self._upload_and_start(node))\n\n        logging.info(\n            \"Physical devices deployment is not implemented publicly. Please use docker or process deployment.\"\n        )\n\n    @classmethod\n    def remove_files_by_scenario(cls, scenario_name):\n        \"\"\"\n        Remove configuration, logs, and reputation files associated with a given scenario.\n\n        This method attempts to delete the directories related to the specified scenario\n        within the NEBULA_CONFIG_DIR and NEBULA_LOGS_DIR environment paths, as well as\n        the reputation folder inside the nebula core directory.\n\n        If files or directories are not found, a warning is logged but the method continues.\n        If a PermissionError occurs while removing log files, the files are moved to a temporary\n        folder inside the NEBULA_ROOT path to avoid permission issues.\n\n        Raises:\n            Exception: Re-raises any unexpected exceptions encountered during file operations.\n        \"\"\"\n        try:\n            shutil.rmtree(FileUtils.check_path(os.environ[\"NEBULA_CONFIG_DIR\"], scenario_name))\n        except FileNotFoundError:\n            logging.warning(\"Files not found, nothing to remove\")\n        except Exception:\n            logging.exception(\"Unknown error while removing files\")\n            raise\n        try:\n            shutil.rmtree(FileUtils.check_path(os.environ[\"NEBULA_LOGS_DIR\"], scenario_name))\n        except PermissionError:\n            # Avoid error if the user does not have enough permissions to remove the tf.events files\n            logging.warning(\"Not enough permissions to remove the files, moving them to tmp folder\")\n            os.makedirs(\n                FileUtils.check_path(os.environ[\"NEBULA_ROOT\"], os.path.join(\"app\", \"tmp\", scenario_name)),\n                exist_ok=True,\n            )\n            os.chmod(\n                FileUtils.check_path(os.environ[\"NEBULA_ROOT\"], os.path.join(\"app\", \"tmp\", scenario_name)),\n                0o777,\n            )\n            shutil.move(\n                FileUtils.check_path(os.environ[\"NEBULA_LOGS_DIR\"], scenario_name),\n                FileUtils.check_path(os.environ[\"NEBULA_ROOT\"], os.path.join(\"app\", \"tmp\", scenario_name)),\n            )\n        except FileNotFoundError:\n            logging.warning(\"Files not found, nothing to remove\")\n        except Exception:\n            logging.exception(\"Unknown error while removing files\")\n\n            raise\n\n        try:\n            nebula_reputation = os.path.join(\n                os.environ[\"NEBULA_LOGS_DIR\"], \"..\", \"..\", \"nebula\", \"core\", \"reputation\", scenario_name\n            )\n            if os.path.exists(nebula_reputation):\n                shutil.rmtree(nebula_reputation)\n                logging.info(f\"Reputation folder {nebula_reputation} removed successfully\")\n        except FileNotFoundError:\n            logging.warning(\"Files not found in reputation folder, nothing to remove\")\n        except Exception:\n            logging.exception(\"Unknown error while removing files from reputation folder\")\n            raise\n\n    def scenario_finished(self, timeout_seconds):\n        \"\"\"\n        Check if all Docker containers related to the current scenario have finished.\n\n        This method monitors the Docker containers whose names contain the scenario name.\n        It waits until all such containers have exited or until the specified timeout is reached.\n        If the timeout is exceeded, all running scenario containers are stopped.\n\n        Args:\n            timeout_seconds (int): Maximum number of seconds to wait for containers to finish.\n\n        Returns:\n            bool: True if all containers finished before the timeout, False if timeout was reached and containers were stopped.\n        \"\"\"\n        client = docker.from_env()\n        all_containers = client.containers.list(all=True)\n        containers = [container for container in all_containers if self.scenario_name.lower() in container.name.lower()]\n\n        start_time = datetime.now()\n        while True:\n            all_containers_finished = True\n            for container in containers:\n                container.reload()\n                if container.status != \"exited\":\n                    all_containers_finished = False\n                    break\n            if all_containers_finished:\n                return True\n\n            current_time = datetime.now()\n            elapsed_time = current_time - start_time\n            if elapsed_time.total_seconds() &gt;= timeout_seconds:\n                for container in containers:\n                    container.stop()\n                return False\n\n            time.sleep(5)\n</code></pre>"},{"location":"api/controller/scenarios/#nebula.controller.scenarios.ScenarioManagement.create_topology","title":"<code>create_topology(matrix=None)</code>","text":"<p>Create and return a network topology manager based on the scenario's topology settings or a given adjacency matrix.</p> <p>Supports multiple topology types: - Random: Generates an Erd\u0151s-R\u00e9nyi random graph with specified connection probability. - Matrix: Uses a provided adjacency matrix to define the topology. - Fully: Creates a fully connected network. - Ring: Creates a ring-structured network with partial connectivity. - Star: Creates a centralized star topology (only for CFL federation).</p> <p>The method assigns IP and port information to nodes and returns the configured TopologyManager instance.</p> <p>Parameters:</p> Name Type Description Default <code>matrix</code> <code>optional</code> <p>Adjacency matrix to define custom topology. If provided, overrides scenario topology.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unknown topology type is specified in the scenario.</p> <p>Returns:</p> Name Type Description <code>TopologyManager</code> <p>Configured topology manager with nodes assigned.</p> Source code in <code>nebula/controller/scenarios.py</code> <pre><code>def create_topology(self, matrix=None):\n    \"\"\"\n    Create and return a network topology manager based on the scenario's topology settings or a given adjacency matrix.\n\n    Supports multiple topology types:\n    - Random: Generates an Erd\u0151s-R\u00e9nyi random graph with specified connection probability.\n    - Matrix: Uses a provided adjacency matrix to define the topology.\n    - Fully: Creates a fully connected network.\n    - Ring: Creates a ring-structured network with partial connectivity.\n    - Star: Creates a centralized star topology (only for CFL federation).\n\n    The method assigns IP and port information to nodes and returns the configured TopologyManager instance.\n\n    Args:\n        matrix (optional): Adjacency matrix to define custom topology. If provided, overrides scenario topology.\n\n    Raises:\n        ValueError: If an unknown topology type is specified in the scenario.\n\n    Returns:\n        TopologyManager: Configured topology manager with nodes assigned.\n    \"\"\"\n    import numpy as np\n\n    if self.scenario.topology == \"Random\":\n        # Create network topology using topology manager (random)\n        probability = float(self.scenario.random_topology_probability)\n        logging.info(\n            f\"Creating random network topology using erdos_renyi_graph: nodes={self.n_nodes}, probability={probability}\"\n        )\n        topologymanager = TopologyManager(\n            scenario_name=self.scenario_name,\n            n_nodes=self.n_nodes,\n            b_symmetric=True,\n            undirected_neighbor_num=3,\n        )\n        topologymanager.generate_random_topology(probability)\n    elif matrix is not None:\n        if self.n_nodes &gt; 2:\n            topologymanager = TopologyManager(\n                topology=np.array(matrix),\n                scenario_name=self.scenario_name,\n                n_nodes=self.n_nodes,\n                b_symmetric=True,\n                undirected_neighbor_num=self.n_nodes - 1,\n            )\n        else:\n            topologymanager = TopologyManager(\n                topology=np.array(matrix),\n                scenario_name=self.scenario_name,\n                n_nodes=self.n_nodes,\n                b_symmetric=True,\n                undirected_neighbor_num=2,\n            )\n    elif self.scenario.topology == \"Fully\":\n        # Create a fully connected network\n        topologymanager = TopologyManager(\n            scenario_name=self.scenario_name,\n            n_nodes=self.n_nodes,\n            b_symmetric=True,\n            undirected_neighbor_num=self.n_nodes - 1,\n        )\n        topologymanager.generate_topology()\n    elif self.scenario.topology == \"Ring\":\n        # Create a partially connected network (ring-structured network)\n        topologymanager = TopologyManager(scenario_name=self.scenario_name, n_nodes=self.n_nodes, b_symmetric=True)\n        topologymanager.generate_ring_topology(increase_convergence=True)\n    elif self.scenario.topology == \"Star\" and self.scenario.federation == \"CFL\":\n        # Create a centralized network\n        topologymanager = TopologyManager(scenario_name=self.scenario_name, n_nodes=self.n_nodes, b_symmetric=True)\n        topologymanager.generate_server_topology()\n    else:\n        raise ValueError(f\"Unknown topology type: {self.scenario.topology}\")\n\n    # Assign nodes to topology\n    nodes_ip_port = []\n    self.config.participants.sort(key=lambda x: int(x[\"device_args\"][\"idx\"]))\n    for i, node in enumerate(self.config.participants):\n        nodes_ip_port.append((\n            node[\"network_args\"][\"ip\"],\n            node[\"network_args\"][\"port\"],\n            \"undefined\",\n        ))\n\n    topologymanager.add_nodes(nodes_ip_port)\n    return topologymanager\n</code></pre>"},{"location":"api/controller/scenarios/#nebula.controller.scenarios.ScenarioManagement.load_configurations_and_start_nodes","title":"<code>load_configurations_and_start_nodes(additional_participants=None, schema_additional_participants=None)</code>  <code>async</code>","text":"<p>Load participant configurations, generate certificates, setup topology, split datasets, and start nodes according to the scenario deployment type.</p> <p>This method: - Generates CA and node certificates. - Loads and updates participant configuration files. - Creates the network topology and updates participant roles. - Handles additional participants if provided. - Initializes and partitions the dataset based on the scenario. - Starts nodes using the specified deployment method (docker, physical, or process).</p> <p>Parameters:</p> Name Type Description Default <code>additional_participants</code> <code>list</code> <p>List of additional participant configurations to add.</p> <code>None</code> <code>schema_additional_participants</code> <code>optional</code> <p>Schema for additional participants (currently unused).</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no participant files found, multiple start nodes detected, no start node found,         unsupported dataset or unknown deployment type.</p> Source code in <code>nebula/controller/scenarios.py</code> <pre><code>async def load_configurations_and_start_nodes(\n    self, additional_participants=None, schema_additional_participants=None\n):\n    \"\"\"\n    Load participant configurations, generate certificates, setup topology, split datasets,\n    and start nodes according to the scenario deployment type.\n\n    This method:\n    - Generates CA and node certificates.\n    - Loads and updates participant configuration files.\n    - Creates the network topology and updates participant roles.\n    - Handles additional participants if provided.\n    - Initializes and partitions the dataset based on the scenario.\n    - Starts nodes using the specified deployment method (docker, physical, or process).\n\n    Args:\n        additional_participants (list, optional): List of additional participant configurations to add.\n        schema_additional_participants (optional): Schema for additional participants (currently unused).\n\n    Raises:\n        ValueError: If no participant files found, multiple start nodes detected, no start node found,\n                    unsupported dataset or unknown deployment type.\n    \"\"\"\n    logging.info(f\"Generating the scenario {self.scenario_name} at {self.start_date_scenario}\")\n\n    # Generate CA certificate\n    generate_ca_certificate(dir_path=self.cert_dir)\n\n    # Get participants configurations\n    participant_files = glob.glob(f\"{self.config_dir}/participant_*.json\")\n    participant_files.sort()\n    if len(participant_files) == 0:\n        raise ValueError(\"No participant files found in config folder\")\n\n    self.config.set_participants_config(participant_files)\n    self.n_nodes = len(participant_files)\n    logging.info(f\"Number of nodes: {self.n_nodes}\")\n\n    self.topologymanager = (\n        self.create_topology(matrix=self.scenario.matrix) if self.scenario.matrix else self.create_topology()\n    )\n\n    # Update participants configuration\n    is_start_node = False\n    config_participants = []\n    # ap = len(additional_participants) if additional_participants else 0\n    additional_nodes = len(additional_participants) if additional_participants else 0\n    logging.info(f\"######## nodes: {self.n_nodes} + additionals: {additional_nodes} ######\")\n\n    # Sort participant files by index to ensure correct order\n    participant_files.sort(key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))\n\n    for i in range(self.n_nodes):\n        with open(f\"{self.config_dir}/participant_\" + str(i) + \".json\") as f:\n            participant_config = json.load(f)\n        participant_config[\"scenario_args\"][\"federation\"] = self.scenario.federation\n        participant_config[\"scenario_args\"][\"n_nodes\"] = self.n_nodes + additional_nodes\n        participant_config[\"network_args\"][\"neighbors\"] = self.topologymanager.get_neighbors_string(i)\n        participant_config[\"scenario_args\"][\"name\"] = self.scenario_name\n        participant_config[\"scenario_args\"][\"start_time\"] = self.start_date_scenario\n        participant_config[\"device_args\"][\"idx\"] = i\n        participant_config[\"device_args\"][\"uid\"] = hashlib.sha1(\n            (\n                str(participant_config[\"network_args\"][\"ip\"])\n                + str(participant_config[\"network_args\"][\"port\"])\n                + str(self.scenario_name)\n            ).encode()\n        ).hexdigest()\n        if participant_config[\"mobility_args\"][\"random_geo\"]:\n            (\n                participant_config[\"mobility_args\"][\"latitude\"],\n                participant_config[\"mobility_args\"][\"longitude\"],\n            ) = TopologyManager.get_coordinates(random_geo=True)\n        else:\n            participant_config[\"mobility_args\"][\"latitude\"] = self.scenario.latitude\n            participant_config[\"mobility_args\"][\"longitude\"] = self.scenario.longitude\n        # If not, use the given coordinates in the frontend\n        participant_config[\"tracking_args\"][\"local_tracking\"] = \"advanced\" if self.advanced_analytics else \"basic\"\n        participant_config[\"tracking_args\"][\"log_dir\"] = self.log_dir\n        participant_config[\"tracking_args\"][\"config_dir\"] = self.config_dir\n\n        # Generate node certificate\n        keyfile_path, certificate_path = generate_certificate(\n            dir_path=self.cert_dir,\n            node_id=f\"participant_{i}\",\n            ip=participant_config[\"network_args\"][\"ip\"],\n        )\n\n        participant_config[\"security_args\"][\"certfile\"] = certificate_path\n        participant_config[\"security_args\"][\"keyfile\"] = keyfile_path\n\n        if participant_config[\"device_args\"][\"start\"]:\n            if not is_start_node:\n                is_start_node = True\n            else:\n                raise ValueError(\"Only one node can be start node\")\n\n        with open(f\"{self.config_dir}/participant_\" + str(i) + \".json\", \"w\") as f:\n            json.dump(participant_config, f, sort_keys=False, indent=2)\n\n        config_participants.append((\n            participant_config[\"network_args\"][\"ip\"],\n            participant_config[\"network_args\"][\"port\"],\n            participant_config[\"device_args\"][\"role\"],\n        ))\n    if not is_start_node:\n        raise ValueError(\"No start node found\")\n    self.config.set_participants_config(participant_files)\n\n    # Add role to the topology (visualization purposes)\n    self.topologymanager.update_nodes(config_participants)\n    self.topologymanager.draw_graph(path=f\"{self.config_dir}/topology.png\", plot=False)\n\n    # Include additional participants (if any) as copies of the last participant\n    additional_participants_files = []\n    if additional_participants:\n        last_participant_file = participant_files[-1]\n        last_participant_index = len(participant_files)\n\n        for i, additional_participant in enumerate(additional_participants):\n            additional_participant_file = f\"{self.config_dir}/participant_{last_participant_index + i}.json\"\n            shutil.copy(last_participant_file, additional_participant_file)\n\n            with open(additional_participant_file) as f:\n                participant_config = json.load(f)\n\n            logging.info(f\"Configuration | additional nodes |  participant: {self.n_nodes + i + 1}\")\n            last_ip = participant_config[\"network_args\"][\"ip\"]\n            logging.info(f\"Valores de la ultima ip: ({last_ip})\")\n            participant_config[\"scenario_args\"][\"n_nodes\"] = self.n_nodes + additional_nodes  # self.n_nodes + i + 1\n            participant_config[\"device_args\"][\"idx\"] = last_participant_index + i\n            participant_config[\"network_args\"][\"neighbors\"] = \"\"\n            participant_config[\"network_args\"][\"ip\"] = (\n                participant_config[\"network_args\"][\"ip\"].rsplit(\".\", 1)[0]\n                + \".\"\n                + str(int(participant_config[\"network_args\"][\"ip\"].rsplit(\".\", 1)[1]) + i + 1)\n            )\n            participant_config[\"device_args\"][\"uid\"] = hashlib.sha1(\n                (\n                    str(participant_config[\"network_args\"][\"ip\"])\n                    + str(participant_config[\"network_args\"][\"port\"])\n                    + str(self.scenario_name)\n                ).encode()\n            ).hexdigest()\n            participant_config[\"mobility_args\"][\"additional_node\"][\"status\"] = True\n            participant_config[\"mobility_args\"][\"additional_node\"][\"time_start\"] = additional_participant[\n                \"time_start\"\n            ]\n\n            # used for late creation nodes\n            participant_config[\"mobility_args\"][\"late_creation\"] = True\n\n            with open(additional_participant_file, \"w\") as f:\n                json.dump(participant_config, f, sort_keys=False, indent=2)\n\n            additional_participants_files.append(additional_participant_file)\n\n    if additional_participants_files:\n        self.config.add_participants_config(additional_participants_files)\n\n    if additional_participants:\n        self.n_nodes += len(additional_participants)\n\n    # Splitting dataset\n    dataset_name = self.scenario.dataset\n    dataset = None\n    if dataset_name == \"MNIST\":\n        dataset = MNISTDataset(\n            num_classes=10,\n            partitions_number=self.n_nodes,\n            iid=self.scenario.iid,\n            partition=self.scenario.partition_selection,\n            partition_parameter=self.scenario.partition_parameter,\n            seed=42,\n            config_dir=self.config_dir,\n        )\n    elif dataset_name == \"FashionMNIST\":\n        dataset = FashionMNISTDataset(\n            num_classes=10,\n            partitions_number=self.n_nodes,\n            iid=self.scenario.iid,\n            partition=self.scenario.partition_selection,\n            partition_parameter=self.scenario.partition_parameter,\n            seed=42,\n            config_dir=self.config_dir,\n        )\n    elif dataset_name == \"EMNIST\":\n        dataset = EMNISTDataset(\n            num_classes=47,\n            partitions_number=self.n_nodes,\n            iid=self.scenario.iid,\n            partition=self.scenario.partition_selection,\n            partition_parameter=self.scenario.partition_parameter,\n            seed=42,\n            config_dir=self.config_dir,\n        )\n    elif dataset_name == \"CIFAR10\":\n        dataset = CIFAR10Dataset(\n            num_classes=10,\n            partitions_number=self.n_nodes,\n            iid=self.scenario.iid,\n            partition=self.scenario.partition_selection,\n            partition_parameter=self.scenario.partition_parameter,\n            seed=42,\n            config_dir=self.config_dir,\n        )\n    elif dataset_name == \"CIFAR100\":\n        dataset = CIFAR100Dataset(\n            num_classes=100,\n            partitions_number=self.n_nodes,\n            iid=self.scenario.iid,\n            partition=self.scenario.partition_selection,\n            partition_parameter=self.scenario.partition_parameter,\n            seed=42,\n            config_dir=self.config_dir,\n        )\n    else:\n        raise ValueError(f\"Dataset {dataset_name} not supported\")\n\n    logging.info(f\"Splitting {dataset_name} dataset...\")\n    dataset.initialize_dataset()\n    logging.info(f\"Splitting {dataset_name} dataset... Done\")\n\n    if self.scenario.deployment in [\"docker\", \"process\", \"physical\"]:\n        if self.scenario.deployment == \"docker\":\n            self.start_nodes_docker()\n        elif self.scenario.deployment == \"physical\":\n            self.start_nodes_physical()\n        elif self.scenario.deployment == \"process\":\n            self.start_nodes_process()\n        else:\n            raise ValueError(f\"Unknown deployment type: {self.scenario.deployment}\")\n    else:\n        logging.info(\n            f\"Virtualization mode is disabled for scenario '{self.scenario_name}' with {self.n_nodes} nodes. Waiting for nodes to start manually...\"\n        )\n</code></pre>"},{"location":"api/controller/scenarios/#nebula.controller.scenarios.ScenarioManagement.remove_files_by_scenario","title":"<code>remove_files_by_scenario(scenario_name)</code>  <code>classmethod</code>","text":"<p>Remove configuration, logs, and reputation files associated with a given scenario.</p> <p>This method attempts to delete the directories related to the specified scenario within the NEBULA_CONFIG_DIR and NEBULA_LOGS_DIR environment paths, as well as the reputation folder inside the nebula core directory.</p> <p>If files or directories are not found, a warning is logged but the method continues. If a PermissionError occurs while removing log files, the files are moved to a temporary folder inside the NEBULA_ROOT path to avoid permission issues.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>Re-raises any unexpected exceptions encountered during file operations.</p> Source code in <code>nebula/controller/scenarios.py</code> <pre><code>@classmethod\ndef remove_files_by_scenario(cls, scenario_name):\n    \"\"\"\n    Remove configuration, logs, and reputation files associated with a given scenario.\n\n    This method attempts to delete the directories related to the specified scenario\n    within the NEBULA_CONFIG_DIR and NEBULA_LOGS_DIR environment paths, as well as\n    the reputation folder inside the nebula core directory.\n\n    If files or directories are not found, a warning is logged but the method continues.\n    If a PermissionError occurs while removing log files, the files are moved to a temporary\n    folder inside the NEBULA_ROOT path to avoid permission issues.\n\n    Raises:\n        Exception: Re-raises any unexpected exceptions encountered during file operations.\n    \"\"\"\n    try:\n        shutil.rmtree(FileUtils.check_path(os.environ[\"NEBULA_CONFIG_DIR\"], scenario_name))\n    except FileNotFoundError:\n        logging.warning(\"Files not found, nothing to remove\")\n    except Exception:\n        logging.exception(\"Unknown error while removing files\")\n        raise\n    try:\n        shutil.rmtree(FileUtils.check_path(os.environ[\"NEBULA_LOGS_DIR\"], scenario_name))\n    except PermissionError:\n        # Avoid error if the user does not have enough permissions to remove the tf.events files\n        logging.warning(\"Not enough permissions to remove the files, moving them to tmp folder\")\n        os.makedirs(\n            FileUtils.check_path(os.environ[\"NEBULA_ROOT\"], os.path.join(\"app\", \"tmp\", scenario_name)),\n            exist_ok=True,\n        )\n        os.chmod(\n            FileUtils.check_path(os.environ[\"NEBULA_ROOT\"], os.path.join(\"app\", \"tmp\", scenario_name)),\n            0o777,\n        )\n        shutil.move(\n            FileUtils.check_path(os.environ[\"NEBULA_LOGS_DIR\"], scenario_name),\n            FileUtils.check_path(os.environ[\"NEBULA_ROOT\"], os.path.join(\"app\", \"tmp\", scenario_name)),\n        )\n    except FileNotFoundError:\n        logging.warning(\"Files not found, nothing to remove\")\n    except Exception:\n        logging.exception(\"Unknown error while removing files\")\n\n        raise\n\n    try:\n        nebula_reputation = os.path.join(\n            os.environ[\"NEBULA_LOGS_DIR\"], \"..\", \"..\", \"nebula\", \"core\", \"reputation\", scenario_name\n        )\n        if os.path.exists(nebula_reputation):\n            shutil.rmtree(nebula_reputation)\n            logging.info(f\"Reputation folder {nebula_reputation} removed successfully\")\n    except FileNotFoundError:\n        logging.warning(\"Files not found in reputation folder, nothing to remove\")\n    except Exception:\n        logging.exception(\"Unknown error while removing files from reputation folder\")\n        raise\n</code></pre>"},{"location":"api/controller/scenarios/#nebula.controller.scenarios.ScenarioManagement.scenario_finished","title":"<code>scenario_finished(timeout_seconds)</code>","text":"<p>Check if all Docker containers related to the current scenario have finished.</p> <p>This method monitors the Docker containers whose names contain the scenario name. It waits until all such containers have exited or until the specified timeout is reached. If the timeout is exceeded, all running scenario containers are stopped.</p> <p>Parameters:</p> Name Type Description Default <code>timeout_seconds</code> <code>int</code> <p>Maximum number of seconds to wait for containers to finish.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if all containers finished before the timeout, False if timeout was reached and containers were stopped.</p> Source code in <code>nebula/controller/scenarios.py</code> <pre><code>def scenario_finished(self, timeout_seconds):\n    \"\"\"\n    Check if all Docker containers related to the current scenario have finished.\n\n    This method monitors the Docker containers whose names contain the scenario name.\n    It waits until all such containers have exited or until the specified timeout is reached.\n    If the timeout is exceeded, all running scenario containers are stopped.\n\n    Args:\n        timeout_seconds (int): Maximum number of seconds to wait for containers to finish.\n\n    Returns:\n        bool: True if all containers finished before the timeout, False if timeout was reached and containers were stopped.\n    \"\"\"\n    client = docker.from_env()\n    all_containers = client.containers.list(all=True)\n    containers = [container for container in all_containers if self.scenario_name.lower() in container.name.lower()]\n\n    start_time = datetime.now()\n    while True:\n        all_containers_finished = True\n        for container in containers:\n            container.reload()\n            if container.status != \"exited\":\n                all_containers_finished = False\n                break\n        if all_containers_finished:\n            return True\n\n        current_time = datetime.now()\n        elapsed_time = current_time - start_time\n        if elapsed_time.total_seconds() &gt;= timeout_seconds:\n            for container in containers:\n                container.stop()\n            return False\n\n        time.sleep(5)\n</code></pre>"},{"location":"api/controller/scenarios/#nebula.controller.scenarios.ScenarioManagement.start_nodes_docker","title":"<code>start_nodes_docker()</code>","text":"<p>Starts participant nodes as Docker containers using Docker SDK.</p> <p>This method performs the following steps: - Logs the beginning of the Docker container startup process. - Creates a Docker network specific to the current user and scenario. - Sorts participant nodes by their index. - For each participant node:     - Sets up environment variables and host configuration,       enabling GPU support if required.     - Prepares Docker volume bindings and static network IP assignment.     - Updates the node configuration, replacing IP addresses as needed,       and writes the configuration to a JSON file.     - Creates and starts the Docker container for the node.     - Logs any exceptions encountered during container creation or startup.</p> <p>Raises:</p> Type Description <code>DockerException</code> <p>If there are issues communicating with the Docker daemon.</p> <code>OSError</code> <p>If there are issues accessing file system paths for volume binding.</p> <code>Exception</code> <p>For any other unexpected errors during container creation or startup.</p> Note <ul> <li>The method assumes Docker and NVIDIA runtime are properly installed and configured.</li> <li>IP addresses in node configurations are replaced with network base dynamically.</li> </ul> Source code in <code>nebula/controller/scenarios.py</code> <pre><code>def start_nodes_docker(self):\n    \"\"\"\n    Starts participant nodes as Docker containers using Docker SDK.\n\n    This method performs the following steps:\n    - Logs the beginning of the Docker container startup process.\n    - Creates a Docker network specific to the current user and scenario.\n    - Sorts participant nodes by their index.\n    - For each participant node:\n        - Sets up environment variables and host configuration,\n          enabling GPU support if required.\n        - Prepares Docker volume bindings and static network IP assignment.\n        - Updates the node configuration, replacing IP addresses as needed,\n          and writes the configuration to a JSON file.\n        - Creates and starts the Docker container for the node.\n        - Logs any exceptions encountered during container creation or startup.\n\n    Raises:\n        docker.errors.DockerException: If there are issues communicating with the Docker daemon.\n        OSError: If there are issues accessing file system paths for volume binding.\n        Exception: For any other unexpected errors during container creation or startup.\n\n    Note:\n        - The method assumes Docker and NVIDIA runtime are properly installed and configured.\n        - IP addresses in node configurations are replaced with network base dynamically.\n    \"\"\"\n    logging.info(\"Starting nodes using Docker Compose...\")\n    logging.info(f\"env path: {self.env_path}\")\n\n    network_name = f\"{os.environ.get('NEBULA_CONTROLLER_NAME')}_{str(self.user).lower()}-nebula-net-scenario\"\n\n    # Create the Docker network\n    base = DockerUtils.create_docker_network(network_name)\n\n    client = docker.from_env()\n\n    self.config.participants.sort(key=lambda x: x[\"device_args\"][\"idx\"])\n    i = 2\n    container_ids = []\n    for idx, node in enumerate(self.config.participants):\n        image = \"nebula-core\"\n        name = f\"{os.environ.get('NEBULA_CONTROLLER_NAME')}_{self.user}-participant{node['device_args']['idx']}\"\n\n        if node[\"device_args\"][\"accelerator\"] == \"gpu\":\n            environment = {\n                \"NVIDIA_DISABLE_REQUIRE\": True,\n                \"NEBULA_LOGS_DIR\": \"/nebula/app/logs/\",\n                \"NEBULA_CONFIG_DIR\": \"/nebula/app/config/\",\n            }\n            host_config = client.api.create_host_config(\n                binds=[f\"{self.root_path}:/nebula\", \"/var/run/docker.sock:/var/run/docker.sock\"],\n                privileged=True,\n                device_requests=[docker.types.DeviceRequest(driver=\"nvidia\", count=-1, capabilities=[[\"gpu\"]])],\n                extra_hosts={\"host.docker.internal\": \"host-gateway\"},\n            )\n        else:\n            environment = {\"NEBULA_LOGS_DIR\": \"/nebula/app/logs/\", \"NEBULA_CONFIG_DIR\": \"/nebula/app/config/\"}\n            host_config = client.api.create_host_config(\n                binds=[f\"{self.root_path}:/nebula\", \"/var/run/docker.sock:/var/run/docker.sock\"],\n                privileged=True,\n                device_requests=[],\n                extra_hosts={\"host.docker.internal\": \"host-gateway\"},\n            )\n\n        volumes = [\"/nebula\", \"/var/run/docker.sock\"]\n\n        start_command = \"sleep 10\" if node[\"device_args\"][\"start\"] else \"sleep 0\"\n        command = [\n            \"/bin/bash\",\n            \"-c\",\n            f\"{start_command} &amp;&amp; ifconfig &amp;&amp; echo '{base}.1 host.docker.internal' &gt;&gt; /etc/hosts &amp;&amp; python /nebula/nebula/core/node.py /nebula/app/config/{self.scenario_name}/participant_{node['device_args']['idx']}.json\",\n        ]\n\n        networking_config = client.api.create_networking_config({\n            f\"{network_name}\": client.api.create_endpoint_config(\n                ipv4_address=f\"{base}.{i}\",\n            ),\n            f\"{os.environ.get('NEBULA_CONTROLLER_NAME')}_nebula-net-base\": client.api.create_endpoint_config(),\n        })\n\n        node[\"tracking_args\"][\"log_dir\"] = \"/nebula/app/logs\"\n        node[\"tracking_args\"][\"config_dir\"] = f\"/nebula/app/config/{self.scenario_name}\"\n        node[\"scenario_args\"][\"controller\"] = self.controller\n        node[\"scenario_args\"][\"deployment\"] = self.scenario.deployment\n        node[\"security_args\"][\"certfile\"] = f\"/nebula/app/certs/participant_{node['device_args']['idx']}_cert.pem\"\n        node[\"security_args\"][\"keyfile\"] = f\"/nebula/app/certs/participant_{node['device_args']['idx']}_key.pem\"\n        node[\"security_args\"][\"cafile\"] = \"/nebula/app/certs/ca_cert.pem\"\n        node = json.loads(json.dumps(node).replace(\"192.168.50.\", f\"{base}.\"))  # TODO change this\n\n        # Write the config file in config directory\n        with open(f\"{self.config_dir}/participant_{node['device_args']['idx']}.json\", \"w\") as f:\n            json.dump(node, f, indent=4)\n\n        try:\n            container_id = client.api.create_container(\n                image=image,\n                name=name,\n                detach=True,\n                volumes=volumes,\n                environment=environment,\n                command=command,\n                host_config=host_config,\n                networking_config=networking_config,\n            )\n        except Exception as e:\n            logging.exception(f\"Creating container {name}: {e}\")\n\n        try:\n            client.api.start(container_id)\n            container_ids.append(container_id)\n        except Exception as e:\n            logging.exception(f\"Starting participant {name} error: {e}\")\n        i += 1\n</code></pre>"},{"location":"api/controller/scenarios/#nebula.controller.scenarios.ScenarioManagement.start_nodes_physical","title":"<code>start_nodes_physical()</code>  <code>async</code>","text":"<p>Placeholder method for starting nodes on physical devices.</p> <p>Logs informational messages indicating that deployment on physical devices is not implemented or supported publicly. Users are encouraged to use Docker or process-based deployment methods instead.</p> <p>Currently, this method does not perform any actions.</p> Source code in <code>nebula/controller/scenarios.py</code> <pre><code>async def start_nodes_physical(self):\n    \"\"\"\n    Placeholder method for starting nodes on physical devices.\n\n    Logs informational messages indicating that deployment on physical devices\n    is not implemented or supported publicly. Users are encouraged to use Docker\n    or process-based deployment methods instead.\n\n    Currently, this method does not perform any actions.\n    \"\"\"\n    logging.info(\"Starting nodes as physical devices...\")\n    logging.info(f\"env path: {self.env_path}\")\n\n    for idx, node in enumerate(self.config.participants):\n        pass\n\n    asyncio.create_task(self._upload_and_start(node))\n\n    logging.info(\n        \"Physical devices deployment is not implemented publicly. Please use docker or process deployment.\"\n    )\n</code></pre>"},{"location":"api/controller/scenarios/#nebula.controller.scenarios.ScenarioManagement.start_nodes_process","title":"<code>start_nodes_process()</code>","text":"<p>Starts participant nodes as independent background processes on the host machine.</p> <p>This method performs the following steps: - Updates each participant's configuration with paths for logs, config, certificates,   and scenario parameters. - Writes the updated configuration for each participant to a JSON file. - Generates and writes a platform-specific script to start all participant nodes:     - On Windows, it creates a PowerShell script that launches each node as a background       process, redirects output and error streams to log files, and records process IDs.     - On Unix-like systems, it creates a bash script that launches each node in the       background, redirects output, and stores PIDs in a file. - Sets executable permissions for the generated script.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If any error occurs during the script generation or file operations.</p> Notes <ul> <li>The generated script must be executed separately by the user to actually start the nodes.</li> <li>Sleep intervals are added before starting nodes depending on their 'start' flag.</li> <li>Logs and PIDs are stored under the configured directories for monitoring and management.</li> </ul> Source code in <code>nebula/controller/scenarios.py</code> <pre><code>def start_nodes_process(self):\n    \"\"\"\n    Starts participant nodes as independent background processes on the host machine.\n\n    This method performs the following steps:\n    - Updates each participant's configuration with paths for logs, config, certificates,\n      and scenario parameters.\n    - Writes the updated configuration for each participant to a JSON file.\n    - Generates and writes a platform-specific script to start all participant nodes:\n        - On Windows, it creates a PowerShell script that launches each node as a background\n          process, redirects output and error streams to log files, and records process IDs.\n        - On Unix-like systems, it creates a bash script that launches each node in the\n          background, redirects output, and stores PIDs in a file.\n    - Sets executable permissions for the generated script.\n\n    Raises:\n        Exception: If any error occurs during the script generation or file operations.\n\n    Notes:\n        - The generated script must be executed separately by the user to actually start the nodes.\n        - Sleep intervals are added before starting nodes depending on their 'start' flag.\n        - Logs and PIDs are stored under the configured directories for monitoring and management.\n    \"\"\"\n    self.processes_root_path = os.path.join(os.path.dirname(__file__), \"..\", \"..\")\n    logging.info(\"Starting nodes as processes...\")\n    logging.info(f\"env path: {self.env_path}\")\n\n    # Include additional config to the participants\n    for idx, node in enumerate(self.config.participants):\n        node[\"tracking_args\"][\"log_dir\"] = os.path.join(self.root_path, \"app\", \"logs\")\n        node[\"tracking_args\"][\"config_dir\"] = os.path.join(self.root_path, \"app\", \"config\", self.scenario_name)\n        node[\"scenario_args\"][\"controller\"] = self.controller\n        node[\"scenario_args\"][\"deployment\"] = self.scenario.deployment\n        node[\"security_args\"][\"certfile\"] = os.path.join(\n            self.root_path, \"app\", \"certs\", f\"participant_{node['device_args']['idx']}_cert.pem\"\n        )\n        node[\"security_args\"][\"keyfile\"] = os.path.join(\n            self.root_path, \"app\", \"certs\", f\"participant_{node['device_args']['idx']}_key.pem\"\n        )\n        node[\"security_args\"][\"cafile\"] = os.path.join(self.root_path, \"app\", \"certs\", \"ca_cert.pem\")\n\n        # Write the config file in config directory\n        with open(f\"{self.config_dir}/participant_{node['device_args']['idx']}.json\", \"w\") as f:\n            json.dump(node, f, indent=4)\n\n    try:\n        if self.host_platform == \"windows\":\n            commands = \"\"\"\n            $ParentDir = Split-Path -Parent $PSScriptRoot\n            $PID_FILE = \"$PSScriptRoot\\\\current_scenario_pids.txt\"\n            New-Item -Path $PID_FILE -Force -ItemType File\n\n            \"\"\"\n            sorted_participants = sorted(\n                self.config.participants,\n                key=lambda node: node[\"device_args\"][\"idx\"],\n                reverse=True,\n            )\n            for node in sorted_participants:\n                if node[\"device_args\"][\"start\"]:\n                    commands += \"Start-Sleep -Seconds 10\\n\"\n                else:\n                    commands += \"Start-Sleep -Seconds 2\\n\"\n\n                commands += f'Write-Host \"Running node {node[\"device_args\"][\"idx\"]}...\"\\n'\n                commands += f'$OUT_FILE = \"{self.root_path}\\\\app\\\\logs\\\\{self.scenario_name}\\\\participant_{node[\"device_args\"][\"idx\"]}.out\"\\n'\n                commands += f'$ERROR_FILE = \"{self.root_path}\\\\app\\\\logs\\\\{self.scenario_name}\\\\participant_{node[\"device_args\"][\"idx\"]}.err\"\\n'\n\n                # Use Start-Process for executing Python in background and capture PID\n                commands += f\"\"\"$process = Start-Process -FilePath \"python\" -ArgumentList \"{self.root_path}\\\\nebula\\\\core\\\\node.py {self.root_path}\\\\app\\\\config\\\\{self.scenario_name}\\\\participant_{node[\"device_args\"][\"idx\"]}.json\" -PassThru -NoNewWindow -RedirectStandardOutput $OUT_FILE -RedirectStandardError $ERROR_FILE\n            Add-Content -Path $PID_FILE -Value $process.Id\n            \"\"\"\n\n            commands += 'Write-Host \"All nodes started. PIDs stored in $PID_FILE\"\\n'\n\n            with open(f\"{self.config_dir}/current_scenario_commands.ps1\", \"w\") as f:\n                f.write(commands)\n            os.chmod(f\"{self.config_dir}/current_scenario_commands.ps1\", 0o755)\n        else:\n            commands = '#!/bin/bash\\n\\nPID_FILE=\"$(dirname \"$0\")/current_scenario_pids.txt\"\\n\\n&gt; $PID_FILE\\n\\n'\n            sorted_participants = sorted(\n                self.config.participants,\n                key=lambda node: node[\"device_args\"][\"idx\"],\n                reverse=True,\n            )\n            for node in sorted_participants:\n                if node[\"device_args\"][\"start\"]:\n                    commands += \"sleep 10\\n\"\n                else:\n                    commands += \"sleep 2\\n\"\n                commands += f'echo \"Running node {node[\"device_args\"][\"idx\"]}...\"\\n'\n                commands += f\"OUT_FILE={self.root_path}/app/logs/{self.scenario_name}/participant_{node['device_args']['idx']}.out\\n\"\n                commands += f\"python {self.root_path}/nebula/core/node.py {self.root_path}/app/config/{self.scenario_name}/participant_{node['device_args']['idx']}.json &amp;\\n\"\n                commands += \"echo $! &gt;&gt; $PID_FILE\\n\\n\"\n\n            commands += 'echo \"All nodes started. PIDs stored in $PID_FILE\"\\n'\n\n            with open(f\"{self.config_dir}/current_scenario_commands.sh\", \"w\") as f:\n                f.write(commands)\n            os.chmod(f\"{self.config_dir}/current_scenario_commands.sh\", 0o755)\n\n    except Exception as e:\n        raise Exception(f\"Error starting nodes as processes: {e}\")\n</code></pre>"},{"location":"api/controller/scenarios/#nebula.controller.scenarios.ScenarioManagement.stop_nodes","title":"<code>stop_nodes()</code>  <code>staticmethod</code>","text":"<p>Stop all running NEBULA nodes.</p> <p>This method logs the shutdown action and calls the stop_participants method to remove all scenario command files, which signals nodes to stop.</p> Source code in <code>nebula/controller/scenarios.py</code> <pre><code>@staticmethod\ndef stop_nodes():\n    \"\"\"\n    Stop all running NEBULA nodes.\n\n    This method logs the shutdown action and calls the stop_participants\n    method to remove all scenario command files, which signals nodes to stop.\n    \"\"\"\n    logging.info(\"Closing NEBULA nodes... Please wait\")\n    ScenarioManagement.stop_participants()\n</code></pre>"},{"location":"api/controller/scenarios/#nebula.controller.scenarios.ScenarioManagement.stop_participants","title":"<code>stop_participants(scenario_name=None)</code>  <code>staticmethod</code>","text":"<p>Stop running participant nodes by removing the scenario command files.</p> <p>This method deletes the 'current_scenario_commands.sh' (or '.ps1' on Windows) file associated with a scenario. Removing this file signals the nodes to stop by terminating their processes.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_name</code> <code>str</code> <p>The name of the scenario to stop. If None, all scenarios' command files will be removed.</p> <code>None</code> Notes <ul> <li>If the environment variable NEBULA_CONFIG_DIR is not set, a default   configuration directory path is used.</li> <li>Supports both Linux/macOS ('.sh') and Windows ('.ps1') script files.</li> <li>Any errors during file removal are logged with the traceback.</li> </ul> Source code in <code>nebula/controller/scenarios.py</code> <pre><code>@staticmethod\ndef stop_participants(scenario_name=None):\n    \"\"\"\n    Stop running participant nodes by removing the scenario command files.\n\n    This method deletes the 'current_scenario_commands.sh' (or '.ps1' on Windows)\n    file associated with a scenario. Removing this file signals the nodes to stop\n    by terminating their processes.\n\n    Args:\n        scenario_name (str, optional): The name of the scenario to stop. If None,\n            all scenarios' command files will be removed.\n\n    Notes:\n        - If the environment variable NEBULA_CONFIG_DIR is not set, a default\n          configuration directory path is used.\n        - Supports both Linux/macOS ('.sh') and Windows ('.ps1') script files.\n        - Any errors during file removal are logged with the traceback.\n    \"\"\"\n    # When stopping the nodes, we need to remove the current_scenario_commands.sh file -&gt; it will cause the nodes to stop using PIDs\n    try:\n        nebula_config_dir = os.environ.get(\"NEBULA_CONFIG_DIR\")\n        if not nebula_config_dir:\n            current_dir = os.path.dirname(__file__)\n            nebula_base_dir = os.path.abspath(os.path.join(current_dir, \"..\", \"..\"))\n            nebula_config_dir = os.path.join(nebula_base_dir, \"app\", \"config\")\n            logging.info(f\"NEBULA_CONFIG_DIR not found. Using default path: {nebula_config_dir}\")\n\n        if scenario_name:\n            if os.environ.get(\"NEBULA_HOST_PLATFORM\") == \"windows\":\n                scenario_commands_file = os.path.join(\n                    nebula_config_dir, scenario_name, \"current_scenario_commands.ps1\"\n                )\n            else:\n                scenario_commands_file = os.path.join(\n                    nebula_config_dir, scenario_name, \"current_scenario_commands.sh\"\n                )\n            if os.path.exists(scenario_commands_file):\n                os.remove(scenario_commands_file)\n        else:\n            if os.environ.get(\"NEBULA_HOST_PLATFORM\") == \"windows\":\n                files = glob.glob(\n                    os.path.join(nebula_config_dir, \"**/current_scenario_commands.ps1\"), recursive=True\n                )\n            else:\n                files = glob.glob(\n                    os.path.join(nebula_config_dir, \"**/current_scenario_commands.sh\"), recursive=True\n                )\n            for file in files:\n                os.remove(file)\n    except Exception as e:\n        logging.exception(f\"Error while removing current_scenario_commands.sh file: {e}\")\n</code></pre>"},{"location":"api/core/","title":"Documentation for Core Module","text":""},{"location":"api/core/addonmanager/","title":"Documentation for Addonmanager Module","text":""},{"location":"api/core/addonmanager/#nebula.core.addonmanager.AddondManager","title":"<code>AddondManager</code>","text":"<p>Responsible for initializing and managing system add-ons.</p> <p>This class handles the lifecycle of optional services (add-ons) such as mobility simulation, GPS module, and network simulation. Add-ons are conditionally deployed based on the provided configuration.</p> Source code in <code>nebula/core/addonmanager.py</code> <pre><code>class AddondManager:\n    \"\"\"\n    Responsible for initializing and managing system add-ons.\n\n    This class handles the lifecycle of optional services (add-ons) such as mobility simulation,\n    GPS module, and network simulation. Add-ons are conditionally deployed based on the provided configuration.\n    \"\"\"\n\n    def __init__(self, engine: \"Engine\", config: Config):\n        \"\"\"\n        Initializes the AddondManager instance.\n\n        Args:\n            engine (Engine): Reference to the main engine instance of the system.\n            config (dict): Configuration object containing participant settings for enabling add-ons.\n\n        This constructor sets up the internal references to the engine and configuration, and\n        initializes the list of add-ons to be managed.\n        \"\"\"\n        self._engine = engine\n        self._config = config\n        self._addons = []\n\n    async def deploy_additional_services(self):\n        \"\"\"\n        Deploys and starts additional services based on the participant's configuration.\n\n        This method checks the configuration to determine which optional components should be\n        activated. It supports:\n            - Mobility simulation (e.g., moving node behavior).\n            - GPS module (e.g., geolocation updates).\n            - Network simulation (e.g., changing connectivity conditions).\n\n        All enabled add-ons are instantiated and started asynchronously.\n\n        Notes:\n            - Add-ons are stored in the internal list `_addons` for lifecycle management.\n            - Services are only launched if the corresponding configuration flags are set.\n        \"\"\"\n        print_msg_box(msg=\"Deploying Additional Services\", indent=2, title=\"Addons Manager\")\n        if self._config.participant[\"trustworthiness\"]:\n            from nebula.addons.trustworthiness.trustworthiness import Trustworthiness\n\n            trustworthiness = Trustworthiness(self._engine, self._config)\n            self._addons.append(trustworthiness)\n\n        if self._config.participant[\"mobility_args\"][\"mobility\"]:\n            mobility = Mobility(self._config, verbose=False)\n            self._addons.append(mobility)\n\n            update_interval = 5\n            gps = factory_gpsmodule(\"nebula\", self._config, self._engine.addr, update_interval, verbose=False)\n            self._addons.append(gps)\n\n        if self._config.participant[\"network_args\"][\"simulation\"]:\n            refresh_conditions_interval = 5\n            network_simulation = factory_network_simulator(\"nebula\", refresh_conditions_interval, \"eth0\", verbose=False)\n            self._addons.append(network_simulation)\n\n        for add in self._addons:\n            await add.start()\n\n    async def stop_additional_services(self):\n        \"\"\"\n        Stops all additional services.\n        \"\"\"\n        logging.info(\"\ud83d\uded1  Stopping additional services...\")\n        for add in self._addons:\n            try:\n                logging.info(f\"\ud83d\uded1  Stopping addon: {add.__class__.__name__}\")\n                await add.stop()\n                logging.info(f\"\u2705  Successfully stopped addon: {add.__class__.__name__}\")\n            except Exception as e:\n                logging.exception(f\"\u274c  Error stopping addon {add.__class__.__name__}: {e}\")\n        logging.info(\"\ud83d\uded1  Finished stopping additional services\")\n</code></pre>"},{"location":"api/core/addonmanager/#nebula.core.addonmanager.AddondManager.__init__","title":"<code>__init__(engine, config)</code>","text":"<p>Initializes the AddondManager instance.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>Reference to the main engine instance of the system.</p> required <code>config</code> <code>dict</code> <p>Configuration object containing participant settings for enabling add-ons.</p> required <p>This constructor sets up the internal references to the engine and configuration, and initializes the list of add-ons to be managed.</p> Source code in <code>nebula/core/addonmanager.py</code> <pre><code>def __init__(self, engine: \"Engine\", config: Config):\n    \"\"\"\n    Initializes the AddondManager instance.\n\n    Args:\n        engine (Engine): Reference to the main engine instance of the system.\n        config (dict): Configuration object containing participant settings for enabling add-ons.\n\n    This constructor sets up the internal references to the engine and configuration, and\n    initializes the list of add-ons to be managed.\n    \"\"\"\n    self._engine = engine\n    self._config = config\n    self._addons = []\n</code></pre>"},{"location":"api/core/addonmanager/#nebula.core.addonmanager.AddondManager.deploy_additional_services","title":"<code>deploy_additional_services()</code>  <code>async</code>","text":"<p>Deploys and starts additional services based on the participant's configuration.</p> <p>This method checks the configuration to determine which optional components should be activated. It supports:     - Mobility simulation (e.g., moving node behavior).     - GPS module (e.g., geolocation updates).     - Network simulation (e.g., changing connectivity conditions).</p> <p>All enabled add-ons are instantiated and started asynchronously.</p> Notes <ul> <li>Add-ons are stored in the internal list <code>_addons</code> for lifecycle management.</li> <li>Services are only launched if the corresponding configuration flags are set.</li> </ul> Source code in <code>nebula/core/addonmanager.py</code> <pre><code>async def deploy_additional_services(self):\n    \"\"\"\n    Deploys and starts additional services based on the participant's configuration.\n\n    This method checks the configuration to determine which optional components should be\n    activated. It supports:\n        - Mobility simulation (e.g., moving node behavior).\n        - GPS module (e.g., geolocation updates).\n        - Network simulation (e.g., changing connectivity conditions).\n\n    All enabled add-ons are instantiated and started asynchronously.\n\n    Notes:\n        - Add-ons are stored in the internal list `_addons` for lifecycle management.\n        - Services are only launched if the corresponding configuration flags are set.\n    \"\"\"\n    print_msg_box(msg=\"Deploying Additional Services\", indent=2, title=\"Addons Manager\")\n    if self._config.participant[\"trustworthiness\"]:\n        from nebula.addons.trustworthiness.trustworthiness import Trustworthiness\n\n        trustworthiness = Trustworthiness(self._engine, self._config)\n        self._addons.append(trustworthiness)\n\n    if self._config.participant[\"mobility_args\"][\"mobility\"]:\n        mobility = Mobility(self._config, verbose=False)\n        self._addons.append(mobility)\n\n        update_interval = 5\n        gps = factory_gpsmodule(\"nebula\", self._config, self._engine.addr, update_interval, verbose=False)\n        self._addons.append(gps)\n\n    if self._config.participant[\"network_args\"][\"simulation\"]:\n        refresh_conditions_interval = 5\n        network_simulation = factory_network_simulator(\"nebula\", refresh_conditions_interval, \"eth0\", verbose=False)\n        self._addons.append(network_simulation)\n\n    for add in self._addons:\n        await add.start()\n</code></pre>"},{"location":"api/core/addonmanager/#nebula.core.addonmanager.AddondManager.stop_additional_services","title":"<code>stop_additional_services()</code>  <code>async</code>","text":"<p>Stops all additional services.</p> Source code in <code>nebula/core/addonmanager.py</code> <pre><code>async def stop_additional_services(self):\n    \"\"\"\n    Stops all additional services.\n    \"\"\"\n    logging.info(\"\ud83d\uded1  Stopping additional services...\")\n    for add in self._addons:\n        try:\n            logging.info(f\"\ud83d\uded1  Stopping addon: {add.__class__.__name__}\")\n            await add.stop()\n            logging.info(f\"\u2705  Successfully stopped addon: {add.__class__.__name__}\")\n        except Exception as e:\n            logging.exception(f\"\u274c  Error stopping addon {add.__class__.__name__}: {e}\")\n    logging.info(\"\ud83d\uded1  Finished stopping additional services\")\n</code></pre>"},{"location":"api/core/engine/","title":"Documentation for Engine Module","text":""},{"location":"api/core/engine/#nebula.core.engine.Engine","title":"<code>Engine</code>","text":"Source code in <code>nebula/core/engine.py</code> <pre><code>class Engine:\n    def __init__(\n        self,\n        model,\n        datamodule,\n        config=Config,\n        trainer=Lightning,\n        security=False,\n    ):\n        self.config = config\n        self.idx = config.participant[\"device_args\"][\"idx\"]\n        self.experiment_name = config.participant[\"scenario_args\"][\"name\"]\n        self.ip = config.participant[\"network_args\"][\"ip\"]\n        self.port = config.participant[\"network_args\"][\"port\"]\n        self.addr = config.participant[\"network_args\"][\"addr\"]\n\n        self.name = config.participant[\"device_args\"][\"name\"]\n        self.client = docker.from_env()\n\n        print_banner()\n\n        self._trainer = None\n        self._aggregator = None\n        self.round = None\n        self.total_rounds = None\n        self.federation_nodes = set()\n        self._federation_nodes_lock = Locker(\"federation_nodes_lock\", async_lock=True)\n        self.initialized = False\n        self.log_dir = os.path.join(config.participant[\"tracking_args\"][\"log_dir\"], self.experiment_name)\n\n        self.security = security\n\n        self._trainer = trainer(model, datamodule, config=self.config)\n        self._aggregator = create_aggregator(config=self.config, engine=self)\n\n        self._secure_neighbors = []\n        self._is_malicious = self.config.participant[\"adversarial_args\"][\"attack_params\"][\"attacks\"] != \"No Attack\"\n\n        role = config.participant[\"device_args\"][\"role\"]\n        self._role_behavior: RoleBehavior = factory_role_behavior(role, self, config)\n        self._role_behavior_performance_lock = Locker(\"role_behavior_performance_lock\", async_lock=True)\n\n        print_msg_box(\n            msg=f\"Name {self.name}\\nRole: {self._role_behavior.get_role_name()}\",\n            indent=2,\n            title=\"Node information\",\n        )\n\n        msg = f\"Trainer: {self._trainer.__class__.__name__}\"\n        msg += f\"\\nDataset: {self.config.participant['data_args']['dataset']}\"\n        msg += f\"\\nIID: {self.config.participant['data_args']['iid']}\"\n        msg += f\"\\nModel: {model.__class__.__name__}\"\n        msg += f\"\\nAggregation algorithm: {self._aggregator.__class__.__name__}\"\n        msg += f\"\\nNode behavior: {'malicious' if self._is_malicious else 'benign'}\"\n        print_msg_box(msg=msg, indent=2, title=\"Scenario information\")\n        print_msg_box(\n            msg=f\"Logging type: {self._trainer.logger.__class__.__name__}\",\n            indent=2,\n            title=\"Logging information\",\n        )\n\n        self.learning_cycle_lock = Locker(name=\"learning_cycle_lock\", async_lock=True)\n        self.federation_setup_lock = Locker(name=\"federation_setup_lock\", async_lock=True)\n        self.federation_ready_lock = Locker(name=\"federation_ready_lock\", async_lock=True)\n        self.round_lock = Locker(name=\"round_lock\", async_lock=True)\n        self._round_in_process_lock = Locker(\"round_in_process_lock\", async_lock=True)\n        self.config.reload_config_file()\n\n        self._cm = CommunicationsManager(engine=self)\n\n        self._reporter = Reporter(config=self.config, trainer=self.trainer)\n\n        self._sinchronized_status = True\n        self.sinchronized_status_lock = Locker(name=\"sinchronized_status_lock\")\n\n        self.trainning_in_progress_lock = Locker(name=\"trainning_in_progress_lock\", async_lock=True)\n\n        event_manager = EventManager.get_instance(verbose=False)\n        self._addon_manager = AddondManager(self, self.config)\n\n        # Additional Components\n        if \"situational_awareness\" in self.config.participant:\n            self._situational_awareness = SituationalAwareness(self.config, self)\n        else:\n            self._situational_awareness = None\n\n        if self.config.participant[\"defense_args\"][\"reputation\"][\"enabled\"]:\n            self._reputation = Reputation(engine=self, config=self.config)\n\n    @property\n    def cm(self):\n        \"\"\"Communication Manager\"\"\"\n        return self._cm\n\n    @property\n    def reporter(self):\n        \"\"\"Reporter\"\"\"\n        return self._reporter\n\n    @property\n    def aggregator(self):\n        \"\"\"Aggregator\"\"\"\n        return self._aggregator\n\n    @property\n    def trainer(self):\n        \"\"\"Trainer\"\"\"\n        return self._trainer\n\n    @property\n    def rb(self):\n        \"\"\"Role Behavior\"\"\"\n        return self._role_behavior\n\n    @property\n    def sa(self):\n        \"\"\"Situational Awareness Module\"\"\"\n        return self._situational_awareness\n\n    def get_aggregator_type(self):\n        return type(self.aggregator)\n\n    def get_addr(self):\n        return self.addr\n\n    def get_config(self):\n        return self.config\n\n    async def get_federation_nodes(self):\n        async with self._federation_nodes_lock:\n            return self.federation_nodes.copy()\n\n    async def update_federation_nodes(self, federation_nodes):\n        async with self._federation_nodes_lock:\n            self.federation_nodes = federation_nodes\n\n    def get_initialization_status(self):\n        return self.initialized\n\n    def set_initialization_status(self, status):\n        self.initialized = status\n\n    async def get_round(self):\n        async with self.round_lock:\n            current_round = self.round\n        return current_round\n\n    def get_federation_ready_lock(self):\n        return self.federation_ready_lock\n\n    def get_federation_setup_lock(self):\n        return self.federation_setup_lock\n\n    def get_trainning_in_progress_lock(self):\n        return self.trainning_in_progress_lock\n\n    def get_round_lock(self):\n        return self.round_lock\n\n    def set_round(self, new_round):\n        logging.info(f\"\ud83e\udd16  Update round count | from: {self.round} | to round: {new_round}\")\n        self.round = new_round\n        self.trainer.set_current_round(new_round)\n\n    \"\"\"                                                     ##############################\n                                                            #       MODEL CALLBACKS      #\n                                                            ##############################\n    \"\"\"\n\n    async def model_initialization_callback(self, source, message):\n        logging.info(f\"\ud83e\udd16  handle_model_message | Received model initialization from {source}\")\n        try:\n            model = self.trainer.deserialize_model(message.parameters)\n            self.trainer.set_model_parameters(model, initialize=True)\n            logging.info(\"\ud83e\udd16  Init Model | Model Parameters Initialized\")\n            self.set_initialization_status(True)\n            await (\n                self.get_federation_ready_lock().release_async()\n            )  # Enable learning cycle once the initialization is done\n            try:\n                await (\n                    self.get_federation_ready_lock().release_async()\n                )  # Release the lock acquired at the beginning of the engine\n            except RuntimeError:\n                pass\n        except RuntimeError:\n            pass\n\n    async def model_update_callback(self, source, message):\n        logging.info(f\"\ud83e\udd16  handle_model_message | Received model update from {source} with round {message.round}\")\n        if not self.get_federation_ready_lock().locked() and len(await self.get_federation_nodes()) == 0:\n            logging.info(\"\ud83e\udd16  handle_model_message | There are no defined federation nodes\")\n            return\n        decoded_model = self.trainer.deserialize_model(message.parameters)\n        updt_received_event = UpdateReceivedEvent(decoded_model, message.weight, source, message.round)\n        await EventManager.get_instance().publish_node_event(updt_received_event)\n\n    \"\"\"                                                     ##############################\n                                                            #      General callbacks     #\n                                                            ##############################\n    \"\"\"\n\n    async def _discovery_discover_callback(self, source, message):\n        logging.info(\n            f\"\ud83d\udd0d  handle_discovery_message | Trigger | Received discovery message from {source} (network propagation)\"\n        )\n        current_connections = await self.cm.get_addrs_current_connections(myself=True)\n        if source not in current_connections:\n            logging.info(f\"\ud83d\udd0d  handle_discovery_message | Trigger | Connecting to {source} indirectly\")\n            await self.cm.connect(source, direct=False)\n        async with self.cm.get_connections_lock():\n            if source in self.cm.connections:\n                # Update the latitude and longitude of the node (if already connected)\n                if (\n                    message.latitude is not None\n                    and -90 &lt;= message.latitude &lt;= 90\n                    and message.longitude is not None\n                    and -180 &lt;= message.longitude &lt;= 180\n                ):\n                    self.cm.connections[source].update_geolocation(message.latitude, message.longitude)\n                else:\n                    logging.warning(\n                        f\"\ud83d\udd0d  Invalid geolocation received from {source}: latitude={message.latitude}, longitude={message.longitude}\"\n                    )\n\n    async def _control_alive_callback(self, source, message):\n        logging.info(f\"\ud83d\udd27  handle_control_message | Trigger | Received alive message from {source}\")\n        current_connections = await self.cm.get_addrs_current_connections(myself=True)\n        if source in current_connections:\n            try:\n                await self.cm.health.alive(source)\n            except Exception as e:\n                logging.exception(f\"Error updating alive status in connection: {e}\")\n        else:\n            logging.error(f\"\u2757\ufe0f  Connection {source} not found in connections...\")\n\n    async def _control_leadership_transfer_callback(self, source, message):\n        logging.info(f\"\ud83d\udd27  handle_control_message | Trigger | Received leadership transfer message from {source}\")\n\n        if await self._round_in_process_lock.locked_async():\n            logging.info(\"Learning cycle is executing, role behavior will be modified next round\")\n            await self.rb.set_next_role(Role.AGGREGATOR, source_to_notificate=source)\n        else:\n            try:\n                logging.info(\"Trying to modify Role behavior\")\n                lock_task = asyncio.create_task(self._round_in_process_lock.acquire_async())\n                await asyncio.wait_for(lock_task, timeout=3)\n                self._role_behavior = change_role_behavior(self.rb, Role.AGGREGATOR, self, self.config)\n                await self.rb.set_next_role(Role.AGGREGATOR, source_to_notificate=source)\n                await self.update_self_role()\n                await self._round_in_process_lock.release_async()\n            except TimeoutError:\n                logging.info(\"Learning cycle is locked, role behavior will be modified next round\")\n                await self.rb.set_next_role(Role.AGGREGATOR, source_to_notificate=source)\n\n    async def _control_leadership_transfer_ack_callback(self, source, message):\n        logging.info(f\"\ud83d\udd27  handle_control_message | Trigger | Received leadership transfer ack message from {source}\")\n        # No concurrence of difference ack received treated, be aware of that.\n        if await self._round_in_process_lock.locked_async():\n            logging.info(\"Learning cycle is executing, role behavior will be modified next round\")\n            await self.rb.set_next_role(Role.TRAINER)\n        else:\n            try:\n                lock_task = asyncio.create_task(self._round_in_process_lock.acquire_async())\n                await asyncio.wait_for(lock_task, timeout=3)\n\n                logging.info(\"Role behavior could be executed...\")\n                await self.rb.set_next_role(Role.TRAINER)\n                await self.update_self_role()\n\n                await self._round_in_process_lock.release_async()\n\n            except TimeoutError:\n                logging.info(\"Learning cycle is locked, role behavior will be modified next round\")\n                await self.rb.set_next_role(Role.TRAINER)\n\n\n    async def _connection_connect_callback(self, source, message):\n        logging.info(f\"\ud83d\udd17  handle_connection_message | Trigger | Received connection message from {source}\")\n        current_connections = await self.cm.get_addrs_current_connections(myself=True)\n        if source not in current_connections:\n            logging.info(f\"\ud83d\udd17  handle_connection_message | Trigger | Connecting to {source}\")\n            await self.cm.connect(source, direct=True)\n\n    async def _connection_disconnect_callback(self, source, message):\n        logging.info(f\"\ud83d\udd17  handle_connection_message | Trigger | Received disconnection message from {source}\")\n        await self.cm.disconnect(source, mutual_disconnection=False)\n\n    async def _federation_federation_ready_callback(self, source, message):\n        logging.info(f\"\ud83d\udcdd  handle_federation_message | Trigger | Received ready federation message from {source}\")\n        if self.config.participant[\"device_args\"][\"start\"]:\n            logging.info(f\"\ud83d\udcdd  handle_federation_message | Trigger | Adding ready connection {source}\")\n            await self.cm.add_ready_connection(source)\n\n    async def _federation_federation_start_callback(self, source, message):\n        logging.info(f\"\ud83d\udcdd  handle_federation_message | Trigger | Received start federation message from {source}\")\n        await self.create_trainer_module()\n\n    async def _federation_federation_models_included_callback(self, source, message):\n        logging.info(f\"\ud83d\udcdd  handle_federation_message | Trigger | Received aggregation finished message from {source}\")\n        current_round = await self.get_round()\n        try:\n            await self.cm.get_connections_lock().acquire_async()\n            if current_round is not None and source in self.cm.connections:\n                try:\n                    if message is not None and len(message.arguments) &gt; 0:\n                        self.cm.connections[source].update_round(int(message.arguments[0])) if message.round in [\n                            current_round - 1,\n                            current_round,\n                        ] else None\n                except Exception as e:\n                    logging.exception(f\"Error updating round in connection: {e}\")\n            else:\n                logging.error(f\"Connection not found for {source}\")\n        except Exception as e:\n            logging.exception(f\"Error updating round in connection: {e}\")\n        finally:\n            await self.cm.get_connections_lock().release_async()\n\n    async def _reputation_share_callback(self, source, message):\n        try:\n            logging.info(f\"handle_reputation_message | Trigger | Received reputation message from {source} | Node: {message.node_id} | Score: {message.score} | Round: {message.round}\")\n\n            current_node = self.addr\n            nei = message.node_id\n\n            if hasattr(self, '_reputation') and self._reputation is not None:\n                if current_node != nei:\n                    key = (current_node, nei, message.round)\n                    if key not in self._reputation.reputation_with_all_feedback:\n                        self._reputation.reputation_with_all_feedback[key] = []\n                    self._reputation.reputation_with_all_feedback[key].append(message.score)\n        except Exception as e:\n            logging.exception(f\"Error handling reputation message: {e}\")\n\n    \"\"\"                                                     ##############################\n                                                            #    REGISTERING CALLBACKS   #\n                                                            ##############################\n    \"\"\"\n\n    async def register_events_callbacks(self):\n        await self.init_message_callbacks()\n        await EventManager.get_instance().subscribe_node_event(AggregationEvent, self.broadcast_models_include)\n\n    async def init_message_callbacks(self):\n        logging.info(\"Registering callbacks for MessageEvents...\")\n        await self.register_message_events_callbacks()\n        # Additional callbacks not registered automatically\n        await self.register_message_callback((\"model\", \"initialization\"), \"model_initialization_callback\")\n        await self.register_message_callback((\"model\", \"update\"), \"model_update_callback\")\n\n    async def register_message_events_callbacks(self):\n        me_dict = self.cm.get_messages_events()\n        message_events = [\n            (message_name, message_action)\n            for (message_name, message_actions) in me_dict.items()\n            for message_action in message_actions\n        ]\n        for event_type, action in message_events:\n            callback_name = f\"_{event_type}_{action}_callback\"\n            method = getattr(self, callback_name, None)\n\n            if callable(method):\n                await EventManager.get_instance().subscribe((event_type, action), method)\n\n    async def register_message_callback(self, message_event: tuple[str, str], callback: str):\n        event_type, action = message_event\n        method = getattr(self, callback, None)\n        if callable(method):\n            await EventManager.get_instance().subscribe((event_type, action), method)\n\n    \"\"\"                                                     ##############################\n                                                            #    ENGINE FUNCTIONALITY    #\n                                                            ##############################\n    \"\"\"\n\n    async def _aditional_node_start(self):\n        \"\"\"\n        Starts the initialization process for an additional node joining the federation.\n\n        This method triggers the situational awareness module to initiate a late connection\n        process to discover and join the federation. Once connected, it starts the learning\n        process asynchronously.\n        \"\"\"\n        logging.info(f\"Aditional node | {self.addr} | going to stablish connection with federation\")\n        await self.sa.start_late_connection_process()\n        # continue ..\n        logging.info(\"Creating trainer service to start the federation process..\")\n        asyncio.create_task(self._start_learning_late())\n\n    async def update_neighbors(self, removed_neighbor_addr, neighbors, remove=False):\n        \"\"\"\n        Updates the internal list of federation neighbors and publishes a neighbor update event.\n\n        Args:\n            removed_neighbor_addr (str): Address of the neighbor that was removed (or affected).\n            neighbors (set): The updated set of current federation neighbors.\n            remove (bool): Flag indicating whether the specified neighbor was removed (True)\n                        or added (False).\n\n        Publishes:\n            UpdateNeighborEvent: An event describing the neighbor update, for use by listeners.\n        \"\"\"\n        await self.update_federation_nodes(neighbors)\n        updt_nei_event = UpdateNeighborEvent(removed_neighbor_addr, remove)\n        asyncio.create_task(EventManager.get_instance().publish_node_event(updt_nei_event))\n\n    async def broadcast_models_include(self, age: AggregationEvent):\n        \"\"\"\n        Broadcasts a message to federation neighbors indicating that aggregation is ready.\n\n        Args:\n            age (AggregationEvent): The event containing information about the completed aggregation.\n\n        Sends:\n            federation_models_included: A message containing the round number of the aggregation.\n        \"\"\"\n        logging.info(f\"\ud83d\udd04  Broadcasting MODELS_INCLUDED for round {await self.get_round()}\")\n        current_round = await self.get_round()\n        message = self.cm.create_message(\n            \"federation\", \"federation_models_included\", [str(arg) for arg in [current_round]]\n        )\n        asyncio.create_task(self.cm.send_message_to_neighbors(message))\n\n    async def update_model_learning_rate(self, new_lr):\n        \"\"\"\n        Updates the learning rate of the current training model.\n\n        Args:\n            new_lr (float): The new learning rate to apply to the trainer model.\n\n        This method ensures that the operation is protected by a lock to avoid\n        conflicts with ongoing training operations.\n        \"\"\"\n        await self.trainning_in_progress_lock.acquire_async()\n        logging.info(\"Update | learning rate modified...\")\n        self.trainer.update_model_learning_rate(new_lr)\n        await self.trainning_in_progress_lock.release_async()\n\n    async def _start_learning_late(self):\n        \"\"\"\n        Initializes the training process for a node joining the federation after it has already started.\n\n        This method retrieves the training configuration from the situational awareness module,\n        including the model parameters, total number of training rounds, current round, and number\n        of epochs. It initializes the model and the trainer accordingly, and starts the learning cycle.\n\n        Locks:\n            - Acquires and releases `learning_cycle_lock` to ensure exclusive access during setup.\n            - Acquires and updates `round` via `round_lock`.\n            - Releases `federation_ready_lock` to indicate that the node is ready to begin learning.\n\n        Handles:\n            - Late start by setting model parameters and synchronization state.\n            - Runtime exceptions gracefully in case of double lock releases or other race conditions.\n\n        Logs important initialization information and direct connection state before training begins.\n        \"\"\"\n        await self.learning_cycle_lock.acquire_async()\n        try:\n            model_serialized, rounds, round, _epochs = await self.sa.get_trainning_info()\n            self.total_rounds = rounds\n            epochs = _epochs\n            await self.get_round_lock().acquire_async()\n            self.round = round\n            await self.get_round_lock().release_async()\n            await self.learning_cycle_lock.release_async()\n            print_msg_box(\n                msg=\"Starting Federated Learning process...\",\n                indent=2,\n                title=\"Start of the experiment late\",\n            )\n            logging.info(f\"Trainning setup | total rounds: {rounds} | current round: {round} | epochs: {epochs}\")\n            direct_connections = await self.cm.get_addrs_current_connections(only_direct=True)\n            logging.info(f\"Initial DIRECT connections: {direct_connections}\")\n            await asyncio.sleep(1)\n            try:\n                logging.info(\"\ud83e\udd16  Initializing model...\")\n                await asyncio.sleep(1)\n                model = self.trainer.deserialize_model(model_serialized)\n                self.trainer.set_model_parameters(model, initialize=True)\n                logging.info(\"Model Parameters Initialized\")\n                self.set_initialization_status(True)\n                await (\n                    self.get_federation_ready_lock().release_async()\n                )  # Enable learning cycle once the initialization is done\n                try:\n                    await (\n                        self.get_federation_ready_lock().release_async()\n                    )  # Release the lock acquired at the beginning of the engine\n                except RuntimeError:\n                    pass\n            except RuntimeError:\n                pass\n\n            self.trainer.set_epochs(epochs)\n            self.trainer.set_current_round(round)\n            self.trainer.create_trainer()\n            await self._learning_cycle()\n\n        finally:\n            if await self.learning_cycle_lock.locked_async():\n                await self.learning_cycle_lock.release_async()\n\n    async def create_trainer_module(self):\n        asyncio.create_task(self._start_learning())\n        logging.info(\"Started trainer module...\")\n\n    async def start_communications(self):\n        \"\"\"\n        Initializes communication with neighboring nodes and registers internal event callbacks.\n\n        This method performs the following steps:\n        1. Registers all event callbacks used by the node.\n        2. Parses the list of initial neighbors from the configuration and initiates communications with them.\n        3. Waits for half of the configured grace time to allow initial network stabilization.\n\n        This grace period provides time for initial peer discovery and message exchange\n        before other services or training processes begin.\n        \"\"\"\n        await self.register_events_callbacks()\n        initial_neighbors = self.config.participant[\"network_args\"][\"neighbors\"].split()\n        await self.cm.start_communications(initial_neighbors)\n        await asyncio.sleep(self.config.participant[\"misc_args\"][\"grace_time_connection\"] // 2)\n\n    async def deploy_components(self):\n        \"\"\"\n        Initializes and deploys the core components required for node operation in the federation.\n\n        This method performs the following actions:\n        1. Initializes the aggregator, which handles the model aggregation process.\n        2. Optionally initializes the situational awareness module if enabled in the configuration.\n        3. Sets up the reputation system if enabled.\n        4. Starts the reporting service for logging and monitoring purposes.\n        5. Deploys any additional add-ons registered via the addon manager.\n\n        This method ensures all critical and optional components are ready before\n        the federated learning process starts.\n        \"\"\"\n        await self.aggregator.init()\n        if \"situational_awareness\" in self.config.participant:\n            await self.sa.init()\n        if self.config.participant[\"defense_args\"][\"reputation\"][\"enabled\"]:\n            await self._reputation.setup()\n        await self._reporter.start()\n        await self._addon_manager.deploy_additional_services()\n\n    async def deploy_federation(self):\n        \"\"\"\n        Manages the startup logic for the federated learning process.\n\n        The behavior is determined by the configuration:\n        - If the device is responsible for starting the federation:\n        1. Waits for a configured grace period to allow peers to initialize.\n        2. Waits until the network is ready (all nodes are prepared).\n        3. Sends a 'FEDERATION_START' message to notify neighbors.\n        4. Initializes the trainer module and marks the node as ready.\n\n        - If the device is not the starter:\n        1. Sends a 'FEDERATION_READY' message to neighbors.\n        2. Waits passively for a start signal from the initiating node.\n\n        This function ensures proper synchronization and coordination before the federated rounds begin.\n        \"\"\"\n        await self.federation_ready_lock.acquire_async()\n        if self.config.participant[\"device_args\"][\"start\"]:\n            logging.info(\n                f\"\ud83d\udca4  Waiting for {self.config.participant['misc_args']['grace_time_start_federation']} seconds to start the federation\"\n            )\n            await asyncio.sleep(self.config.participant[\"misc_args\"][\"grace_time_start_federation\"])\n            if self.round is None:\n                while not await self.cm.check_federation_ready():\n                    await asyncio.sleep(1)\n                logging.info(\"Sending FEDERATION_START to neighbors...\")\n                message = self.cm.create_message(\"federation\", \"federation_start\")\n                await self.cm.send_message_to_neighbors(message)\n                await self.get_federation_ready_lock().release_async()\n                await self.create_trainer_module()\n                self.set_initialization_status(True)\n            else:\n                logging.info(\"Federation already started\")\n\n        else:\n            logging.info(\"Sending FEDERATION_READY to neighbors...\")\n            message = self.cm.create_message(\"federation\", \"federation_ready\")\n            await self.cm.send_message_to_neighbors(message)\n            logging.info(\"\ud83d\udca4  Waiting until receiving the start signal from the start node\")\n\n    async def _start_learning(self):\n        \"\"\"\n        Starts the federated learning process from the beginning if no prior round exists.\n\n        This method performs the following sequence:\n        1. Acquires the learning cycle lock to ensure exclusive execution.\n        2. If no round has been initialized:\n        - Reads total rounds and epochs from the configuration.\n        - Sets the initial round to 0 and releases the round lock.\n        - Waits for the federation to be ready if the device is not the starter.\n        - If the device is the starter, it propagates the initial model to neighbors.\n        - Sets the number of epochs and creates the trainer instance.\n        - Initiates the federated learning cycle.\n        3. If a round already exists and the lock is still held, it is released to avoid deadlock.\n\n        This method ensures that the learning process is initialized safely and only once,\n        synchronizing startup across nodes and managing dependencies on federation readiness.\n        \"\"\"\n        await self.learning_cycle_lock.acquire_async()\n        try:\n            if self.round is None:\n                self.total_rounds = self.config.participant[\"scenario_args\"][\"rounds\"]\n                epochs = self.config.participant[\"training_args\"][\"epochs\"]\n                await self.get_round_lock().acquire_async()\n                self.round = 0\n                await self.get_round_lock().release_async()\n                await self.learning_cycle_lock.release_async()\n                print_msg_box(\n                    msg=\"Starting Federated Learning process...\",\n                    indent=2,\n                    title=\"Start of the experiment\",\n                )\n                direct_connections = await self.cm.get_addrs_current_connections(only_direct=True)\n                undirected_connections = await self.cm.get_addrs_current_connections(only_undirected=True)\n                logging.info(\n                    f\"Initial DIRECT connections: {direct_connections} | Initial UNDIRECT participants: {undirected_connections}\"\n                )\n                logging.info(\"\ud83d\udca4  Waiting initialization of the federation...\")\n                # Lock to wait for the federation to be ready (only affects the first round, when the learning starts)\n                # Only applies to non-start nodes --&gt; start node does not wait for the federation to be ready\n                await self.get_federation_ready_lock().acquire_async()\n                if self.config.participant[\"device_args\"][\"start\"]:\n                    logging.info(\"Propagate initial model updates.\")\n\n                    mpe = ModelPropagationEvent(await self.cm.get_addrs_current_connections(only_direct=True, myself=False), \"initialization\")\n                    await EventManager.get_instance().publish_node_event(mpe)\n\n                    await self.get_federation_ready_lock().release_async()\n\n                self.trainer.set_epochs(epochs)\n                self.trainer.create_trainer()\n\n                await self._learning_cycle()\n            else:\n                if await self.learning_cycle_lock.locked_async():\n                    await self.learning_cycle_lock.release_async()\n        finally:\n            if await self.learning_cycle_lock.locked_async():\n                await self.learning_cycle_lock.release_async()\n\n    async def _waiting_model_updates(self):\n        \"\"\"\n        Waits for the model aggregation results and updates the local model accordingly.\n\n        This method:\n        1. Awaits the result of the aggregation from the aggregator component.\n        2. If aggregation parameters are successfully received:\n        - Updates the local model with the aggregated parameters.\n        3. If no parameters are returned:\n        - Logs an error indicating aggregation failure.\n\n        This method is called after local training and before proceeding to the next round,\n        ensuring the model is synchronized with the federation's latest aggregated state.\n        \"\"\"\n        logging.info(f\"\ud83d\udca4  Waiting convergence in round {self.round}.\")\n        params = await self.aggregator.get_aggregation()\n        if params is not None:\n            logging.info(\n                f\"_waiting_model_updates | Aggregation done for round {self.round}, including parameters in local model.\"\n            )\n            self.trainer.set_model_parameters(params)\n        else:\n            logging.error(\"Aggregation finished with no parameters\")\n\n    def print_round_information(self):\n        print_msg_box(\n            msg=f\"Round {self.round} of {self.total_rounds} started.\",\n            indent=2,\n            title=\"Round information\",\n        )\n\n    async def learning_cycle_finished(self):\n        current_round = await self.get_round()\n        if not current_round or not self.total_rounds:\n            return False\n        else:\n            return current_round &gt;= self.total_rounds\n\n    async def resolve_missing_updates(self):\n        \"\"\"\n        Delegates the resolution strategy for missing updates to the current role behavior.\n\n        This function is called when the node receives no model updates from neighbors\n        and needs to apply a fallback strategy depending on its role (e.g., using default weights\n        if aggregator, or local model if trainer).\n\n        Returns:\n            The result of the role-specific resolution strategy.\n        \"\"\"\n        logging.info(f\"Using Role behavior: {self.rb.get_role_name()} conflict resolve strategy\")\n        return await self.rb.resolve_missing_updates()\n\n    async def update_self_role(self):\n        \"\"\"\n        Checks whether a role update is required and performs the transition if necessary.\n\n        If a new role has been assigned (i.e., self.rb.update_role_needed() is True),\n        this function updates the role behavior accordingly and notifies the source\n        that initiated the role transfer, if applicable.\n\n        It logs the role change and spawns an async task to send a control message\n        acknowledging the update to the initiating node.\n\n        Raises:\n            Any exceptions from change_role_behavior or communication logic.\n        \"\"\"\n        if await self.rb.update_role_needed():\n            logging.info(\"Starting Role Behavior modification...\")\n            from_role = self.rb.get_role_name()\n            next_role = await self.rb.get_next_role()\n            source_to_notificate = await self.rb.get_source_to_notificate()\n            self._role_behavior: RoleBehavior = change_role_behavior(self.rb, next_role, self, self.config)\n            to_role = self.rb.get_role_name()\n            logging.info(f\"Role behavior changing from: {from_role} to {to_role}\")\n            self.config.participant[\"device_args\"][\"role\"] = to_role\n            if source_to_notificate:\n                logging.info(f\"Sending role modification ACK to transferer: {source_to_notificate}\")\n                message = self.cm.create_message(\"control\", \"leadership_transfer_ack\")\n                asyncio.create_task(self.cm.send_message(source_to_notificate, message))\n\n    async def _learning_cycle(self):\n        \"\"\"\n        Main asynchronous loop for executing the Federated Learning process across multiple rounds.\n\n        This method orchestrates the entire lifecycle of each federated learning round, including:\n        1. Starting each round:\n        - Updating the list of federation nodes.\n        - Publishing a `RoundStartEvent` for local and global monitoring.\n        - Preparing the trainer and aggregator components.\n        2. Running the core learning logic via `_extended_learning_cycle`.\n        3. Ending each round:\n        - Publishing a `RoundEndEvent`.\n        - Releasing and updating the current round state in the configuration.\n        - Invoking callbacks for the trainer to handle end-of-round logic.\n\n        After completing all rounds:\n        - Finalizes the trainer by calling `on_learning_cycle_end()` and optionally performs testing.\n        - Reports the scenario status to the controller if required.\n        - Optionally stops the Docker container if deployed in a containerized environment.\n\n        This function blocks (awaits) until the full FL process concludes.\n        \"\"\"\n        while self.round is not None and self.round &lt; self.total_rounds:\n            async with self._round_in_process_lock:\n                current_time = time.time()\n                print_msg_box(\n                    msg=f\"Round {self.round} of {self.total_rounds - 1} started (max. {self.total_rounds} rounds)\",\n                    indent=2,\n                    title=\"Round information\",\n                )\n\n                await self.update_self_role()\n\n                logging.info(f\"Federation nodes: {self.federation_nodes}\")\n                await self.update_federation_nodes(\n                    await self.cm.get_addrs_current_connections(only_direct=True, myself=True)\n                )\n                expected_nodes = await self.rb.select_nodes_to_wait()\n                rse = RoundStartEvent(self.round, current_time, expected_nodes)\n                await EventManager.get_instance().publish_node_event(rse)\n                self.trainer.on_round_start()\n                logging.info(f\"Expected nodes: {expected_nodes}\")\n                direct_connections = await self.cm.get_addrs_current_connections(only_direct=True)\n                undirected_connections = await self.cm.get_addrs_current_connections(only_undirected=True)\n\n                logging.info(f\"Direct connections: {direct_connections} | Undirected connections: {undirected_connections}\")\n                logging.info(f\"[Role {self.rb.get_role_name()}] Starting learning cycle...\")\n\n                await self.aggregator.update_federation_nodes(expected_nodes)\n                async with self._role_behavior_performance_lock:\n                    await self.rb.extended_learning_cycle()\n\n                current_time = time.time()\n                ree = RoundEndEvent(self.round, current_time)\n                await EventManager.get_instance().publish_node_event(ree)\n\n                await self.get_round_lock().acquire_async()\n\n                print_msg_box(\n                    msg=f\"Round {self.round} of {self.total_rounds - 1} finished (max. {self.total_rounds} rounds)\",\n                    indent=2,\n                    title=\"Round information\",\n                )\n                # await self.aggregator.reset()\n                self.trainer.on_round_end()\n                self.round += 1\n                self.config.participant[\"federation_args\"][\"round\"] = (\n                    self.round\n                )  # Set current round in config (send to the controller)\n                await self.get_round_lock().release_async()\n\n        # End of the learning cycle\n        self.trainer.on_learning_cycle_end()\n\n        await self.trainer.test()\n\n        # Shutdown protocol\n        await self._shutdown_protocol()\n\n    async def _shutdown_protocol(self):\n        logging.info(\"Starting graceful shutdown process...\")\n\n        # 1.- Publish Experiment Finish Event to the last update on modules\n        logging.info(\"Publishing Experiment Finish Event...\")\n        efe = ExperimentFinishEvent()\n        await EventManager.get_instance().publish_node_event(efe)\n\n        # 2.- Log finish message\n        print_msg_box(\n            msg=f\"FL process has been completed successfully (max. {self.total_rounds} rounds reached)\",\n            indent=2,\n            title=\"End of the experiment\",\n        )\n        # Report\n        if self.config.participant[\"scenario_args\"][\"controller\"] != \"nebula-test\":\n            try:\n                result = await self.reporter.report_scenario_finished()\n                if result:\n                    logging.info(\"\ud83d\udcdd  Scenario finished reported successfully\")\n                    await self.reporter.stop()\n                else:\n                    logging.error(\"\ud83d\udcdd  Error reporting scenario finished\")\n            except Exception as e:\n                logging.exception(f\"\ud83d\udcdd  Error during scenario finish report: {e}\")\n\n        # Call centralized shutdown\n        await self.shutdown()\n        return\n\n    async def shutdown(self):\n        logging.info(\"\ud83d\udea6 Engine shutdown initiated\")\n\n        # Stop addon services first\n        try:\n            await self._addon_manager.stop_additional_services()\n        except Exception as e:\n            logging.exception(\"Error stopping add-ons: %s\", e)\n\n        # Stop reporter\n        try:\n            await self._reporter.stop()\n        except Exception as e:\n            logging.exception(\"Error stopping reporter: %s\", e)\n\n        # Stop communications manager (includes forwarder, discoverer, propagator, ECS)\n        try:\n            await self.cm.stop()\n        except Exception as e:\n            logging.exception(\"Error stopping communications manager: %s\", e)\n\n        # Stop situational awareness\n        try:\n            if self.sa:\n                await self.sa.stop()\n        except Exception as e:\n            logging.exception(\"Error stopping situational awareness: %s\", e)\n\n        # Task cleanup with improved handling\n        logging.info(\"Starting graceful task cleanup...\")\n        tasks = [t for t in asyncio.all_tasks() if t is not asyncio.current_task()]\n\n        if tasks:\n            logging.info(f\"Found {len(tasks)} remaining tasks to clean up\")\n            for task in tasks:\n                logging.info(f\"  \u2022 Task: {task.get_name()} - {task}\")\n                logging.info(f\"  \u2022 State: {task._state} - Done: {task.done()} - Cancelled: {task.cancelled()}\")\n\n            # Wait for tasks to complete naturally with shorter timeout\n            try:\n                await asyncio.wait_for(asyncio.gather(*tasks, return_exceptions=True), timeout=3)\n            except asyncio.CancelledError:\n                logging.warning(\n                    \"Timeout reached during task cleanup (CancelledError); proceeding with shutdown anyway.\"\n                )\n                # Do not re-raise, just continue\n            except TimeoutError:\n                logging.warning(\"Some tasks did not complete in time, forcing cancellation...\")\n                for task in tasks:\n                    if not task.done():\n                        task.cancel()\n                # Wait a bit more for cancellations to take effect\n                try:\n                    await asyncio.wait_for(asyncio.gather(*tasks, return_exceptions=True), timeout=2)\n                except asyncio.CancelledError:\n                    logging.warning(\n                        \"Timeout reached during forced cancellation (CancelledError); proceeding with shutdown anyway.\"\n                    )\n                    # Do not re-raise, just continue\n                except TimeoutError:\n                    logging.warning(\"Some tasks still not responding to cancellation\")\n                    # Final aggressive cleanup - cancel all remaining tasks\n                    remaining_tasks = [\n                        t for t in asyncio.all_tasks() if t is not asyncio.current_task() and not t.done()\n                    ]\n                    if remaining_tasks:\n                        logging.warning(f\"Forcing cancellation of {len(remaining_tasks)} remaining tasks\")\n                        for task in remaining_tasks:\n                            task.cancel()\n                        try:\n                            await asyncio.wait_for(asyncio.gather(*remaining_tasks, return_exceptions=True), timeout=1)\n                        except asyncio.CancelledError:\n                            logging.warning(\n                                \"Timeout reached during final forced cancellation (CancelledError); proceeding with shutdown anyway.\"\n                            )\n                            # Do not re-raise, just continue\n                        except TimeoutError:\n                            logging.exception(\"Some tasks still not responding to forced cancellation\")\n            # Proceed anyway after all cancellation attempts\n            logging.warning(\"Proceeding with shutdown even if some tasks are still pending/cancelled.\")\n        else:\n            logging.info(\"No remaining tasks to clean up.\")\n\n        logging.info(\"\u2705 Engine shutdown complete\")\n\n        # Kill Docker container if running in Docker\n        if self.config.participant[\"scenario_args\"][\"deployment\"] == \"docker\":\n            try:\n                docker_id = socket.gethostname()\n                logging.info(f\"\ud83d\udce6  Removing docker container with ID {docker_id}\")\n                container = self.client.containers.get(docker_id)\n                container.remove(force=True)\n                logging.info(f\"\ud83d\udce6  Successfully removed docker container {docker_id}\")\n            except Exception as e:\n                logging.exception(f\"\ud83d\udce6  Error removing Docker container {docker_id}: {e}\")\n                # Try to force kill the container as last resort\n                try:\n                    import subprocess\n\n                    subprocess.run([\"docker\", \"rm\", \"-f\", docker_id], check=False)\n                    logging.info(f\"\ud83d\udce6  Forced removal of container {docker_id} via subprocess\")\n                except Exception as sub_e:\n                    logging.exception(f\"\ud83d\udce6  Failed to force remove container {docker_id}: {sub_e}\")\n</code></pre>"},{"location":"api/core/engine/#nebula.core.engine.Engine.aggregator","title":"<code>aggregator</code>  <code>property</code>","text":"<p>Aggregator</p>"},{"location":"api/core/engine/#nebula.core.engine.Engine.cm","title":"<code>cm</code>  <code>property</code>","text":"<p>Communication Manager</p>"},{"location":"api/core/engine/#nebula.core.engine.Engine.rb","title":"<code>rb</code>  <code>property</code>","text":"<p>Role Behavior</p>"},{"location":"api/core/engine/#nebula.core.engine.Engine.reporter","title":"<code>reporter</code>  <code>property</code>","text":"<p>Reporter</p>"},{"location":"api/core/engine/#nebula.core.engine.Engine.sa","title":"<code>sa</code>  <code>property</code>","text":"<p>Situational Awareness Module</p>"},{"location":"api/core/engine/#nebula.core.engine.Engine.trainer","title":"<code>trainer</code>  <code>property</code>","text":"<p>Trainer</p>"},{"location":"api/core/engine/#nebula.core.engine.Engine.broadcast_models_include","title":"<code>broadcast_models_include(age)</code>  <code>async</code>","text":"<p>Broadcasts a message to federation neighbors indicating that aggregation is ready.</p> <p>Parameters:</p> Name Type Description Default <code>age</code> <code>AggregationEvent</code> <p>The event containing information about the completed aggregation.</p> required Sends <p>federation_models_included: A message containing the round number of the aggregation.</p> Source code in <code>nebula/core/engine.py</code> <pre><code>async def broadcast_models_include(self, age: AggregationEvent):\n    \"\"\"\n    Broadcasts a message to federation neighbors indicating that aggregation is ready.\n\n    Args:\n        age (AggregationEvent): The event containing information about the completed aggregation.\n\n    Sends:\n        federation_models_included: A message containing the round number of the aggregation.\n    \"\"\"\n    logging.info(f\"\ud83d\udd04  Broadcasting MODELS_INCLUDED for round {await self.get_round()}\")\n    current_round = await self.get_round()\n    message = self.cm.create_message(\n        \"federation\", \"federation_models_included\", [str(arg) for arg in [current_round]]\n    )\n    asyncio.create_task(self.cm.send_message_to_neighbors(message))\n</code></pre>"},{"location":"api/core/engine/#nebula.core.engine.Engine.deploy_components","title":"<code>deploy_components()</code>  <code>async</code>","text":"<p>Initializes and deploys the core components required for node operation in the federation.</p> <p>This method performs the following actions: 1. Initializes the aggregator, which handles the model aggregation process. 2. Optionally initializes the situational awareness module if enabled in the configuration. 3. Sets up the reputation system if enabled. 4. Starts the reporting service for logging and monitoring purposes. 5. Deploys any additional add-ons registered via the addon manager.</p> <p>This method ensures all critical and optional components are ready before the federated learning process starts.</p> Source code in <code>nebula/core/engine.py</code> <pre><code>async def deploy_components(self):\n    \"\"\"\n    Initializes and deploys the core components required for node operation in the federation.\n\n    This method performs the following actions:\n    1. Initializes the aggregator, which handles the model aggregation process.\n    2. Optionally initializes the situational awareness module if enabled in the configuration.\n    3. Sets up the reputation system if enabled.\n    4. Starts the reporting service for logging and monitoring purposes.\n    5. Deploys any additional add-ons registered via the addon manager.\n\n    This method ensures all critical and optional components are ready before\n    the federated learning process starts.\n    \"\"\"\n    await self.aggregator.init()\n    if \"situational_awareness\" in self.config.participant:\n        await self.sa.init()\n    if self.config.participant[\"defense_args\"][\"reputation\"][\"enabled\"]:\n        await self._reputation.setup()\n    await self._reporter.start()\n    await self._addon_manager.deploy_additional_services()\n</code></pre>"},{"location":"api/core/engine/#nebula.core.engine.Engine.deploy_federation","title":"<code>deploy_federation()</code>  <code>async</code>","text":"<p>Manages the startup logic for the federated learning process.</p> <p>The behavior is determined by the configuration: - If the device is responsible for starting the federation: 1. Waits for a configured grace period to allow peers to initialize. 2. Waits until the network is ready (all nodes are prepared). 3. Sends a 'FEDERATION_START' message to notify neighbors. 4. Initializes the trainer module and marks the node as ready.</p> <ul> <li>If the device is not the starter:</li> <li>Sends a 'FEDERATION_READY' message to neighbors.</li> <li>Waits passively for a start signal from the initiating node.</li> </ul> <p>This function ensures proper synchronization and coordination before the federated rounds begin.</p> Source code in <code>nebula/core/engine.py</code> <pre><code>async def deploy_federation(self):\n    \"\"\"\n    Manages the startup logic for the federated learning process.\n\n    The behavior is determined by the configuration:\n    - If the device is responsible for starting the federation:\n    1. Waits for a configured grace period to allow peers to initialize.\n    2. Waits until the network is ready (all nodes are prepared).\n    3. Sends a 'FEDERATION_START' message to notify neighbors.\n    4. Initializes the trainer module and marks the node as ready.\n\n    - If the device is not the starter:\n    1. Sends a 'FEDERATION_READY' message to neighbors.\n    2. Waits passively for a start signal from the initiating node.\n\n    This function ensures proper synchronization and coordination before the federated rounds begin.\n    \"\"\"\n    await self.federation_ready_lock.acquire_async()\n    if self.config.participant[\"device_args\"][\"start\"]:\n        logging.info(\n            f\"\ud83d\udca4  Waiting for {self.config.participant['misc_args']['grace_time_start_federation']} seconds to start the federation\"\n        )\n        await asyncio.sleep(self.config.participant[\"misc_args\"][\"grace_time_start_federation\"])\n        if self.round is None:\n            while not await self.cm.check_federation_ready():\n                await asyncio.sleep(1)\n            logging.info(\"Sending FEDERATION_START to neighbors...\")\n            message = self.cm.create_message(\"federation\", \"federation_start\")\n            await self.cm.send_message_to_neighbors(message)\n            await self.get_federation_ready_lock().release_async()\n            await self.create_trainer_module()\n            self.set_initialization_status(True)\n        else:\n            logging.info(\"Federation already started\")\n\n    else:\n        logging.info(\"Sending FEDERATION_READY to neighbors...\")\n        message = self.cm.create_message(\"federation\", \"federation_ready\")\n        await self.cm.send_message_to_neighbors(message)\n        logging.info(\"\ud83d\udca4  Waiting until receiving the start signal from the start node\")\n</code></pre>"},{"location":"api/core/engine/#nebula.core.engine.Engine.resolve_missing_updates","title":"<code>resolve_missing_updates()</code>  <code>async</code>","text":"<p>Delegates the resolution strategy for missing updates to the current role behavior.</p> <p>This function is called when the node receives no model updates from neighbors and needs to apply a fallback strategy depending on its role (e.g., using default weights if aggregator, or local model if trainer).</p> <p>Returns:</p> Type Description <p>The result of the role-specific resolution strategy.</p> Source code in <code>nebula/core/engine.py</code> <pre><code>async def resolve_missing_updates(self):\n    \"\"\"\n    Delegates the resolution strategy for missing updates to the current role behavior.\n\n    This function is called when the node receives no model updates from neighbors\n    and needs to apply a fallback strategy depending on its role (e.g., using default weights\n    if aggregator, or local model if trainer).\n\n    Returns:\n        The result of the role-specific resolution strategy.\n    \"\"\"\n    logging.info(f\"Using Role behavior: {self.rb.get_role_name()} conflict resolve strategy\")\n    return await self.rb.resolve_missing_updates()\n</code></pre>"},{"location":"api/core/engine/#nebula.core.engine.Engine.start_communications","title":"<code>start_communications()</code>  <code>async</code>","text":"<p>Initializes communication with neighboring nodes and registers internal event callbacks.</p> <p>This method performs the following steps: 1. Registers all event callbacks used by the node. 2. Parses the list of initial neighbors from the configuration and initiates communications with them. 3. Waits for half of the configured grace time to allow initial network stabilization.</p> <p>This grace period provides time for initial peer discovery and message exchange before other services or training processes begin.</p> Source code in <code>nebula/core/engine.py</code> <pre><code>async def start_communications(self):\n    \"\"\"\n    Initializes communication with neighboring nodes and registers internal event callbacks.\n\n    This method performs the following steps:\n    1. Registers all event callbacks used by the node.\n    2. Parses the list of initial neighbors from the configuration and initiates communications with them.\n    3. Waits for half of the configured grace time to allow initial network stabilization.\n\n    This grace period provides time for initial peer discovery and message exchange\n    before other services or training processes begin.\n    \"\"\"\n    await self.register_events_callbacks()\n    initial_neighbors = self.config.participant[\"network_args\"][\"neighbors\"].split()\n    await self.cm.start_communications(initial_neighbors)\n    await asyncio.sleep(self.config.participant[\"misc_args\"][\"grace_time_connection\"] // 2)\n</code></pre>"},{"location":"api/core/engine/#nebula.core.engine.Engine.update_model_learning_rate","title":"<code>update_model_learning_rate(new_lr)</code>  <code>async</code>","text":"<p>Updates the learning rate of the current training model.</p> <p>Parameters:</p> Name Type Description Default <code>new_lr</code> <code>float</code> <p>The new learning rate to apply to the trainer model.</p> required <p>This method ensures that the operation is protected by a lock to avoid conflicts with ongoing training operations.</p> Source code in <code>nebula/core/engine.py</code> <pre><code>async def update_model_learning_rate(self, new_lr):\n    \"\"\"\n    Updates the learning rate of the current training model.\n\n    Args:\n        new_lr (float): The new learning rate to apply to the trainer model.\n\n    This method ensures that the operation is protected by a lock to avoid\n    conflicts with ongoing training operations.\n    \"\"\"\n    await self.trainning_in_progress_lock.acquire_async()\n    logging.info(\"Update | learning rate modified...\")\n    self.trainer.update_model_learning_rate(new_lr)\n    await self.trainning_in_progress_lock.release_async()\n</code></pre>"},{"location":"api/core/engine/#nebula.core.engine.Engine.update_neighbors","title":"<code>update_neighbors(removed_neighbor_addr, neighbors, remove=False)</code>  <code>async</code>","text":"<p>Updates the internal list of federation neighbors and publishes a neighbor update event.</p> <p>Parameters:</p> Name Type Description Default <code>removed_neighbor_addr</code> <code>str</code> <p>Address of the neighbor that was removed (or affected).</p> required <code>neighbors</code> <code>set</code> <p>The updated set of current federation neighbors.</p> required <code>remove</code> <code>bool</code> <p>Flag indicating whether the specified neighbor was removed (True)         or added (False).</p> <code>False</code> Publishes <p>UpdateNeighborEvent: An event describing the neighbor update, for use by listeners.</p> Source code in <code>nebula/core/engine.py</code> <pre><code>async def update_neighbors(self, removed_neighbor_addr, neighbors, remove=False):\n    \"\"\"\n    Updates the internal list of federation neighbors and publishes a neighbor update event.\n\n    Args:\n        removed_neighbor_addr (str): Address of the neighbor that was removed (or affected).\n        neighbors (set): The updated set of current federation neighbors.\n        remove (bool): Flag indicating whether the specified neighbor was removed (True)\n                    or added (False).\n\n    Publishes:\n        UpdateNeighborEvent: An event describing the neighbor update, for use by listeners.\n    \"\"\"\n    await self.update_federation_nodes(neighbors)\n    updt_nei_event = UpdateNeighborEvent(removed_neighbor_addr, remove)\n    asyncio.create_task(EventManager.get_instance().publish_node_event(updt_nei_event))\n</code></pre>"},{"location":"api/core/engine/#nebula.core.engine.Engine.update_self_role","title":"<code>update_self_role()</code>  <code>async</code>","text":"<p>Checks whether a role update is required and performs the transition if necessary.</p> <p>If a new role has been assigned (i.e., self.rb.update_role_needed() is True), this function updates the role behavior accordingly and notifies the source that initiated the role transfer, if applicable.</p> <p>It logs the role change and spawns an async task to send a control message acknowledging the update to the initiating node.</p> Source code in <code>nebula/core/engine.py</code> <pre><code>async def update_self_role(self):\n    \"\"\"\n    Checks whether a role update is required and performs the transition if necessary.\n\n    If a new role has been assigned (i.e., self.rb.update_role_needed() is True),\n    this function updates the role behavior accordingly and notifies the source\n    that initiated the role transfer, if applicable.\n\n    It logs the role change and spawns an async task to send a control message\n    acknowledging the update to the initiating node.\n\n    Raises:\n        Any exceptions from change_role_behavior or communication logic.\n    \"\"\"\n    if await self.rb.update_role_needed():\n        logging.info(\"Starting Role Behavior modification...\")\n        from_role = self.rb.get_role_name()\n        next_role = await self.rb.get_next_role()\n        source_to_notificate = await self.rb.get_source_to_notificate()\n        self._role_behavior: RoleBehavior = change_role_behavior(self.rb, next_role, self, self.config)\n        to_role = self.rb.get_role_name()\n        logging.info(f\"Role behavior changing from: {from_role} to {to_role}\")\n        self.config.participant[\"device_args\"][\"role\"] = to_role\n        if source_to_notificate:\n            logging.info(f\"Sending role modification ACK to transferer: {source_to_notificate}\")\n            message = self.cm.create_message(\"control\", \"leadership_transfer_ack\")\n            asyncio.create_task(self.cm.send_message(source_to_notificate, message))\n</code></pre>"},{"location":"api/core/eventmanager/","title":"Documentation for Eventmanager Module","text":""},{"location":"api/core/eventmanager/#nebula.core.eventmanager.EventManager","title":"<code>EventManager</code>","text":"Source code in <code>nebula/core/eventmanager.py</code> <pre><code>class EventManager:\n    _instance = None\n    _lock = Locker(\"event_manager\")\n\n    def __new__(cls, *args, **kwargs):\n        \"\"\"Singleton implementation\"\"\"\n        with cls._lock:\n            if cls._instance is None:\n                cls._instance = super().__new__(cls)\n                cls._instance._initialize(*args, **kwargs)\n        return cls._instance\n\n    def _initialize(self, verbose=False):\n        \"\"\"Single initialization\"\"\"\n        if hasattr(self, \"_initialized\"):\n            return\n        self._subscribers: dict[tuple[str, str], list] = {}\n        self._message_events_lock = Locker(\"message_events_lock\", async_lock=True)\n        self._addons_events_subs: dict[type, list] = {}\n        self._addons_event_lock = Locker(\"addons_event_lock\", async_lock=True)\n        self._node_events_subs: dict[type, list] = {}\n        self._node_events_lock = Locker(\"node_events_lock\", async_lock=True)\n        self._global_message_subscribers: list[Callable] = []\n        self._global_message_subscribers_lock = Locker(\"global_message_subscribers_lock\", async_lock=True)\n        self._verbose = False\n        self._initialized = True\n\n    @staticmethod\n    def get_instance(verbose=False):\n        \"\"\"Static method to obtain EventManager instance\"\"\"\n        if EventManager._instance is None:\n            EventManager(verbose=verbose)\n        return EventManager._instance\n\n    async def subscribe(self, event_type: tuple[str, str] | None, callback: Callable):\n        \"\"\"Register a callback for a message event.\"\"\"\n        if not event_type:\n            async with self._global_message_subscribers_lock:\n                self._global_message_subscribers.append(callback)\n                logging.info(f\"EventManager | Subscribed callback for all message events: {event_type}\")\n                return\n\n        async with self._message_events_lock:\n            if event_type not in self._subscribers:\n                self._subscribers[event_type] = []\n            self._subscribers[event_type].append(callback)\n        logging.info(f\"EventManager | Subscribed callback for event: {event_type}\")\n\n    async def publish(self, message_event: MessageEvent):\n        \"\"\"Trigger all callbacks registered for a specific event type.\"\"\"\n        if self._verbose:\n            logging.info(f\"Publishing MessageEvent: {message_event.message_type}\")\n        async with self._message_events_lock:\n            event_type = message_event.message_type\n            callbacks = self._subscribers.get(event_type, [])\n        if not callbacks:\n            logging.error(f\"EventManager | No subscribers for event: {event_type}\")\n            return\n\n        for callback in self._subscribers[event_type]:\n            try:\n                if self._verbose:\n                    logging.info(\n                        f\"EventManager | Triggering callback for event: {event_type}, from source: {message_event.source}\"\n                    )\n                if asyncio.iscoroutinefunction(callback) or inspect.iscoroutine(callback):\n                    await callback(message_event.source, message_event.message)\n                else:\n                    callback(message_event.source, message_event.message)\n            except Exception as e:\n                logging.exception(f\"EventManager | Error in callback for event {event_type}: {e}\")\n\n        # Global callbacks (callbacks for all message events)\n        async with self._global_message_subscribers_lock:\n            global_callbacks = self._global_message_subscribers.copy()\n\n        for global_cb in global_callbacks:\n            try:\n                if self._verbose:\n                    logging.info(\n                        f\"EventManager | Triggering callback for event: {event_type}, from source: {message_event.source}\"\n                    )\n                if asyncio.iscoroutinefunction(callback) or inspect.iscoroutine(callback):\n                    await global_cb(message_event.source, message_event.message)\n                else:\n                    global_cb(message_event.source, message_event.message)\n            except Exception as e:\n                logging.exception(f\"EventManager | Error in callback for event {event_type}: {e}\")\n\n    async def subscribe_addonevent(self, addonEventType: type[AddonEvent], callback: Callable):\n        \"\"\"Register a callback for a specific type of AddonEvent.\"\"\"\n        async with self._addons_event_lock:\n            if addonEventType not in self._addons_events_subs:\n                self._addons_events_subs[addonEventType] = []\n            self._addons_events_subs[addonEventType].append(callback)\n        logging.info(f\"EventManager | Subscribed callback for AddonEvent type: {addonEventType.__name__}\")\n\n    async def publish_addonevent(self, addonevent: AddonEvent):\n        \"\"\"Trigger all callbacks registered for a specific type of AddonEvent.\"\"\"\n        if self._verbose:\n            logging.info(f\"Publishing AddonEvent: {addonevent}\")\n        async with self._addons_event_lock:\n            event_type = type(addonevent)\n            callbacks = self._addons_events_subs.get(event_type, [])\n\n        if not callbacks:\n            if self._verbose:\n                logging.error(f\"EventManager | No subscribers for AddonEvent type: {event_type.__name__}\")\n            return\n\n        for callback in self._addons_events_subs[event_type]:\n            try:\n                if self._verbose:\n                    logging.info(f\"EventManager | Triggering callback for event type: {event_type.__name__}\")\n                if asyncio.iscoroutinefunction(callback) or inspect.iscoroutine(callback):\n                    await callback(addonevent)\n                else:\n                    callback(addonevent)\n            except Exception as e:\n                logging.exception(f\"EventManager | Error in callback for AddonEvent {event_type.__name__}: {e}\")\n\n    async def subscribe_node_event(self, nodeEventType: type[NodeEvent], callback: Callable):\n        \"\"\"Register a callback for a specific type of AddonEvent.\"\"\"\n        async with self._node_events_lock:\n            if nodeEventType not in self._node_events_subs:\n                self._node_events_subs[nodeEventType] = []\n            self._node_events_subs[nodeEventType].append(callback)\n        logging.info(f\"EventManager | Subscribed callback for NodeEvent type: {nodeEventType.__name__}\")\n\n    async def publish_node_event(self, nodeevent: NodeEvent):\n        \"\"\"Trigger all callbacks registered for a specific type of AddonEvent.\"\"\"\n        if self._verbose:\n            logging.info(f\"Publishing NodeEvent: {nodeevent}\")\n        async with self._node_events_lock:\n            event_type = type(nodeevent)\n            callbacks = self._node_events_subs.get(event_type, [])\n\n        if not callbacks:\n            if self._verbose:\n                logging.error(f\"EventManager | No subscribers for NodeEvent type: {event_type.__name__}\")\n            return\n\n        for callback in self._node_events_subs[event_type]:\n            try:\n                if self._verbose:\n                    logging.info(f\"EventManager | Triggering callback for event type: {event_type.__name__}\")\n                if asyncio.iscoroutinefunction(callback) or inspect.iscoroutine(callback):\n                    if await nodeevent.is_concurrent():\n                        asyncio.create_task(callback(nodeevent))\n                    else:\n                        await callback(nodeevent)\n                else:\n                    callback(nodeevent)\n            except Exception as e:\n                logging.exception(f\"EventManager | Error in callback for NodeEvent {event_type.__name__}: {e}\")\n\n    async def unsubscribe_event(self, event_type, callback):\n        \"\"\"Unsubscribe a callback from a given event type (MessageEvent, AddonEvent, or NodeEvent).\"\"\"\n        if isinstance(event_type, tuple):  # MessageEvent\n            async with self._message_events_lock:\n                if event_type in self._subscribers and callback in self._subscribers[event_type]:\n                    self._subscribers[event_type].remove(callback)\n                    logging.info(f\"EventManager | Unsubscribed callback for MessageEvent: {event_type}\")\n        elif issubclass(event_type, AddonEvent):  # AddonEvent\n            async with self._addons_event_lock:\n                if event_type in self._addons_events_subs and callback in self._addons_events_subs[event_type]:\n                    self._addons_events_subs[event_type].remove(callback)\n                    logging.info(f\"EventManager | Unsubscribed callback for AddonEvent: {event_type.__name__}\")\n        elif issubclass(event_type, NodeEvent):  # NodeEvent\n            async with self._node_events_lock:\n                if event_type in self._node_events_subs and callback in self._node_events_subs[event_type]:\n                    self._node_events_subs[event_type].remove(callback)\n                    logging.info(f\"EventManager | Unsubscribed callback for NodeEvent: {event_type.__name__}\")\n</code></pre>"},{"location":"api/core/eventmanager/#nebula.core.eventmanager.EventManager.__new__","title":"<code>__new__(*args, **kwargs)</code>","text":"<p>Singleton implementation</p> Source code in <code>nebula/core/eventmanager.py</code> <pre><code>def __new__(cls, *args, **kwargs):\n    \"\"\"Singleton implementation\"\"\"\n    with cls._lock:\n        if cls._instance is None:\n            cls._instance = super().__new__(cls)\n            cls._instance._initialize(*args, **kwargs)\n    return cls._instance\n</code></pre>"},{"location":"api/core/eventmanager/#nebula.core.eventmanager.EventManager.get_instance","title":"<code>get_instance(verbose=False)</code>  <code>staticmethod</code>","text":"<p>Static method to obtain EventManager instance</p> Source code in <code>nebula/core/eventmanager.py</code> <pre><code>@staticmethod\ndef get_instance(verbose=False):\n    \"\"\"Static method to obtain EventManager instance\"\"\"\n    if EventManager._instance is None:\n        EventManager(verbose=verbose)\n    return EventManager._instance\n</code></pre>"},{"location":"api/core/eventmanager/#nebula.core.eventmanager.EventManager.publish","title":"<code>publish(message_event)</code>  <code>async</code>","text":"<p>Trigger all callbacks registered for a specific event type.</p> Source code in <code>nebula/core/eventmanager.py</code> <pre><code>async def publish(self, message_event: MessageEvent):\n    \"\"\"Trigger all callbacks registered for a specific event type.\"\"\"\n    if self._verbose:\n        logging.info(f\"Publishing MessageEvent: {message_event.message_type}\")\n    async with self._message_events_lock:\n        event_type = message_event.message_type\n        callbacks = self._subscribers.get(event_type, [])\n    if not callbacks:\n        logging.error(f\"EventManager | No subscribers for event: {event_type}\")\n        return\n\n    for callback in self._subscribers[event_type]:\n        try:\n            if self._verbose:\n                logging.info(\n                    f\"EventManager | Triggering callback for event: {event_type}, from source: {message_event.source}\"\n                )\n            if asyncio.iscoroutinefunction(callback) or inspect.iscoroutine(callback):\n                await callback(message_event.source, message_event.message)\n            else:\n                callback(message_event.source, message_event.message)\n        except Exception as e:\n            logging.exception(f\"EventManager | Error in callback for event {event_type}: {e}\")\n\n    # Global callbacks (callbacks for all message events)\n    async with self._global_message_subscribers_lock:\n        global_callbacks = self._global_message_subscribers.copy()\n\n    for global_cb in global_callbacks:\n        try:\n            if self._verbose:\n                logging.info(\n                    f\"EventManager | Triggering callback for event: {event_type}, from source: {message_event.source}\"\n                )\n            if asyncio.iscoroutinefunction(callback) or inspect.iscoroutine(callback):\n                await global_cb(message_event.source, message_event.message)\n            else:\n                global_cb(message_event.source, message_event.message)\n        except Exception as e:\n            logging.exception(f\"EventManager | Error in callback for event {event_type}: {e}\")\n</code></pre>"},{"location":"api/core/eventmanager/#nebula.core.eventmanager.EventManager.publish_addonevent","title":"<code>publish_addonevent(addonevent)</code>  <code>async</code>","text":"<p>Trigger all callbacks registered for a specific type of AddonEvent.</p> Source code in <code>nebula/core/eventmanager.py</code> <pre><code>async def publish_addonevent(self, addonevent: AddonEvent):\n    \"\"\"Trigger all callbacks registered for a specific type of AddonEvent.\"\"\"\n    if self._verbose:\n        logging.info(f\"Publishing AddonEvent: {addonevent}\")\n    async with self._addons_event_lock:\n        event_type = type(addonevent)\n        callbacks = self._addons_events_subs.get(event_type, [])\n\n    if not callbacks:\n        if self._verbose:\n            logging.error(f\"EventManager | No subscribers for AddonEvent type: {event_type.__name__}\")\n        return\n\n    for callback in self._addons_events_subs[event_type]:\n        try:\n            if self._verbose:\n                logging.info(f\"EventManager | Triggering callback for event type: {event_type.__name__}\")\n            if asyncio.iscoroutinefunction(callback) or inspect.iscoroutine(callback):\n                await callback(addonevent)\n            else:\n                callback(addonevent)\n        except Exception as e:\n            logging.exception(f\"EventManager | Error in callback for AddonEvent {event_type.__name__}: {e}\")\n</code></pre>"},{"location":"api/core/eventmanager/#nebula.core.eventmanager.EventManager.publish_node_event","title":"<code>publish_node_event(nodeevent)</code>  <code>async</code>","text":"<p>Trigger all callbacks registered for a specific type of AddonEvent.</p> Source code in <code>nebula/core/eventmanager.py</code> <pre><code>async def publish_node_event(self, nodeevent: NodeEvent):\n    \"\"\"Trigger all callbacks registered for a specific type of AddonEvent.\"\"\"\n    if self._verbose:\n        logging.info(f\"Publishing NodeEvent: {nodeevent}\")\n    async with self._node_events_lock:\n        event_type = type(nodeevent)\n        callbacks = self._node_events_subs.get(event_type, [])\n\n    if not callbacks:\n        if self._verbose:\n            logging.error(f\"EventManager | No subscribers for NodeEvent type: {event_type.__name__}\")\n        return\n\n    for callback in self._node_events_subs[event_type]:\n        try:\n            if self._verbose:\n                logging.info(f\"EventManager | Triggering callback for event type: {event_type.__name__}\")\n            if asyncio.iscoroutinefunction(callback) or inspect.iscoroutine(callback):\n                if await nodeevent.is_concurrent():\n                    asyncio.create_task(callback(nodeevent))\n                else:\n                    await callback(nodeevent)\n            else:\n                callback(nodeevent)\n        except Exception as e:\n            logging.exception(f\"EventManager | Error in callback for NodeEvent {event_type.__name__}: {e}\")\n</code></pre>"},{"location":"api/core/eventmanager/#nebula.core.eventmanager.EventManager.subscribe","title":"<code>subscribe(event_type, callback)</code>  <code>async</code>","text":"<p>Register a callback for a message event.</p> Source code in <code>nebula/core/eventmanager.py</code> <pre><code>async def subscribe(self, event_type: tuple[str, str] | None, callback: Callable):\n    \"\"\"Register a callback for a message event.\"\"\"\n    if not event_type:\n        async with self._global_message_subscribers_lock:\n            self._global_message_subscribers.append(callback)\n            logging.info(f\"EventManager | Subscribed callback for all message events: {event_type}\")\n            return\n\n    async with self._message_events_lock:\n        if event_type not in self._subscribers:\n            self._subscribers[event_type] = []\n        self._subscribers[event_type].append(callback)\n    logging.info(f\"EventManager | Subscribed callback for event: {event_type}\")\n</code></pre>"},{"location":"api/core/eventmanager/#nebula.core.eventmanager.EventManager.subscribe_addonevent","title":"<code>subscribe_addonevent(addonEventType, callback)</code>  <code>async</code>","text":"<p>Register a callback for a specific type of AddonEvent.</p> Source code in <code>nebula/core/eventmanager.py</code> <pre><code>async def subscribe_addonevent(self, addonEventType: type[AddonEvent], callback: Callable):\n    \"\"\"Register a callback for a specific type of AddonEvent.\"\"\"\n    async with self._addons_event_lock:\n        if addonEventType not in self._addons_events_subs:\n            self._addons_events_subs[addonEventType] = []\n        self._addons_events_subs[addonEventType].append(callback)\n    logging.info(f\"EventManager | Subscribed callback for AddonEvent type: {addonEventType.__name__}\")\n</code></pre>"},{"location":"api/core/eventmanager/#nebula.core.eventmanager.EventManager.subscribe_node_event","title":"<code>subscribe_node_event(nodeEventType, callback)</code>  <code>async</code>","text":"<p>Register a callback for a specific type of AddonEvent.</p> Source code in <code>nebula/core/eventmanager.py</code> <pre><code>async def subscribe_node_event(self, nodeEventType: type[NodeEvent], callback: Callable):\n    \"\"\"Register a callback for a specific type of AddonEvent.\"\"\"\n    async with self._node_events_lock:\n        if nodeEventType not in self._node_events_subs:\n            self._node_events_subs[nodeEventType] = []\n        self._node_events_subs[nodeEventType].append(callback)\n    logging.info(f\"EventManager | Subscribed callback for NodeEvent type: {nodeEventType.__name__}\")\n</code></pre>"},{"location":"api/core/eventmanager/#nebula.core.eventmanager.EventManager.unsubscribe_event","title":"<code>unsubscribe_event(event_type, callback)</code>  <code>async</code>","text":"<p>Unsubscribe a callback from a given event type (MessageEvent, AddonEvent, or NodeEvent).</p> Source code in <code>nebula/core/eventmanager.py</code> <pre><code>async def unsubscribe_event(self, event_type, callback):\n    \"\"\"Unsubscribe a callback from a given event type (MessageEvent, AddonEvent, or NodeEvent).\"\"\"\n    if isinstance(event_type, tuple):  # MessageEvent\n        async with self._message_events_lock:\n            if event_type in self._subscribers and callback in self._subscribers[event_type]:\n                self._subscribers[event_type].remove(callback)\n                logging.info(f\"EventManager | Unsubscribed callback for MessageEvent: {event_type}\")\n    elif issubclass(event_type, AddonEvent):  # AddonEvent\n        async with self._addons_event_lock:\n            if event_type in self._addons_events_subs and callback in self._addons_events_subs[event_type]:\n                self._addons_events_subs[event_type].remove(callback)\n                logging.info(f\"EventManager | Unsubscribed callback for AddonEvent: {event_type.__name__}\")\n    elif issubclass(event_type, NodeEvent):  # NodeEvent\n        async with self._node_events_lock:\n            if event_type in self._node_events_subs and callback in self._node_events_subs[event_type]:\n                self._node_events_subs[event_type].remove(callback)\n                logging.info(f\"EventManager | Unsubscribed callback for NodeEvent: {event_type.__name__}\")\n</code></pre>"},{"location":"api/core/nebulaevents/","title":"Documentation for Nebulaevents Module","text":""},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.AddonEvent","title":"<code>AddonEvent</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all addon-related events in the system.</p> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>class AddonEvent(ABC):\n    \"\"\"\n    Abstract base class for all addon-related events in the system.\n    \"\"\"\n\n    @abstractmethod\n    async def get_event_data(self):\n        \"\"\"\n        Retrieve the data associated with the event.\n\n        Returns:\n            Any: The event-specific data payload.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.AddonEvent.get_event_data","title":"<code>get_event_data()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Retrieve the data associated with the event.</p> <p>Returns:</p> Name Type Description <code>Any</code> <p>The event-specific data payload.</p> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>@abstractmethod\nasync def get_event_data(self):\n    \"\"\"\n    Retrieve the data associated with the event.\n\n    Returns:\n        Any: The event-specific data payload.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.AggregationEvent","title":"<code>AggregationEvent</code>","text":"<p>               Bases: <code>NodeEvent</code></p> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>class AggregationEvent(NodeEvent):\n    def __init__(self, updates: dict, expected_nodes: set, missing_nodes: set):\n        \"\"\"Event triggered when model aggregation is ready.\n\n        Args:\n            updates (dict): Dictionary containing model updates.\n            expected_nodes (set): Set of nodes expected to participate in aggregation.\n            missing_nodes (set): Set of nodes that did not send their update.\n        \"\"\"\n        self._updates = updates\n        self._expected_nodes = expected_nodes\n        self._missing_nodes = missing_nodes\n\n    def __str__(self):\n        return \"Aggregation Ready\"\n\n    def update_updates(self, new_updates: dict):\n        \"\"\"Allows an external module to update the updates dictionary.\"\"\"\n        self._updates = new_updates\n\n    async def get_event_data(self) -&gt; tuple[dict, set, set]:\n        \"\"\"Retrieves the aggregation event data.\n\n        Returns:\n            tuple[dict, set, set]:\n                - updates (dict): Model updates.\n                - expected_nodes (set): Expected nodes.\n                - missing_nodes (set): Missing nodes.\n        \"\"\"\n        return (self._updates, self._expected_nodes, self._missing_nodes)\n\n    async def is_concurrent(self) -&gt; bool:\n        return False\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.AggregationEvent.__init__","title":"<code>__init__(updates, expected_nodes, missing_nodes)</code>","text":"<p>Event triggered when model aggregation is ready.</p> <p>Parameters:</p> Name Type Description Default <code>updates</code> <code>dict</code> <p>Dictionary containing model updates.</p> required <code>expected_nodes</code> <code>set</code> <p>Set of nodes expected to participate in aggregation.</p> required <code>missing_nodes</code> <code>set</code> <p>Set of nodes that did not send their update.</p> required Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>def __init__(self, updates: dict, expected_nodes: set, missing_nodes: set):\n    \"\"\"Event triggered when model aggregation is ready.\n\n    Args:\n        updates (dict): Dictionary containing model updates.\n        expected_nodes (set): Set of nodes expected to participate in aggregation.\n        missing_nodes (set): Set of nodes that did not send their update.\n    \"\"\"\n    self._updates = updates\n    self._expected_nodes = expected_nodes\n    self._missing_nodes = missing_nodes\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.AggregationEvent.get_event_data","title":"<code>get_event_data()</code>  <code>async</code>","text":"<p>Retrieves the aggregation event data.</p> <p>Returns:</p> Type Description <code>tuple[dict, set, set]</code> <p>tuple[dict, set, set]: - updates (dict): Model updates. - expected_nodes (set): Expected nodes. - missing_nodes (set): Missing nodes.</p> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>async def get_event_data(self) -&gt; tuple[dict, set, set]:\n    \"\"\"Retrieves the aggregation event data.\n\n    Returns:\n        tuple[dict, set, set]:\n            - updates (dict): Model updates.\n            - expected_nodes (set): Expected nodes.\n            - missing_nodes (set): Missing nodes.\n    \"\"\"\n    return (self._updates, self._expected_nodes, self._missing_nodes)\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.AggregationEvent.update_updates","title":"<code>update_updates(new_updates)</code>","text":"<p>Allows an external module to update the updates dictionary.</p> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>def update_updates(self, new_updates: dict):\n    \"\"\"Allows an external module to update the updates dictionary.\"\"\"\n    self._updates = new_updates\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.BeaconRecievedEvent","title":"<code>BeaconRecievedEvent</code>","text":"<p>               Bases: <code>NodeEvent</code></p> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>class BeaconRecievedEvent(NodeEvent):\n    def __init__(self, source, geoloc):\n        \"\"\"\n        Initializes an BeaconRecievedEvent.\n\n        Args:\n            source (str): The received beacon source.\n            geoloc (tuple): The geolocalzition associated with the received beacon source.\n        \"\"\"\n        self._source = source\n        self._geoloc = geoloc\n\n    def __str__(self):\n        return \"Beacon recieved\"\n\n    async def get_event_data(self) -&gt; tuple[str, tuple[float, float]]:\n        \"\"\"\n        Retrieves the event data.\n\n        Returns:\n            tuple[str, tuple[float, float]]: A tuple containing:\n                - The beacon's source.\n                - the device geolocalization (latitude, longitude).\n        \"\"\"\n        return (self._source, self._geoloc)\n\n    async def is_concurrent(self) -&gt; bool:\n        return True\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.BeaconRecievedEvent.__init__","title":"<code>__init__(source, geoloc)</code>","text":"<p>Initializes an BeaconRecievedEvent.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The received beacon source.</p> required <code>geoloc</code> <code>tuple</code> <p>The geolocalzition associated with the received beacon source.</p> required Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>def __init__(self, source, geoloc):\n    \"\"\"\n    Initializes an BeaconRecievedEvent.\n\n    Args:\n        source (str): The received beacon source.\n        geoloc (tuple): The geolocalzition associated with the received beacon source.\n    \"\"\"\n    self._source = source\n    self._geoloc = geoloc\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.BeaconRecievedEvent.get_event_data","title":"<code>get_event_data()</code>  <code>async</code>","text":"<p>Retrieves the event data.</p> <p>Returns:</p> Type Description <code>tuple[str, tuple[float, float]]</code> <p>tuple[str, tuple[float, float]]: A tuple containing: - The beacon's source. - the device geolocalization (latitude, longitude).</p> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>async def get_event_data(self) -&gt; tuple[str, tuple[float, float]]:\n    \"\"\"\n    Retrieves the event data.\n\n    Returns:\n        tuple[str, tuple[float, float]]: A tuple containing:\n            - The beacon's source.\n            - the device geolocalization (latitude, longitude).\n    \"\"\"\n    return (self._source, self._geoloc)\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.ChangeLocationEvent","title":"<code>ChangeLocationEvent</code>","text":"<p>               Bases: <code>AddonEvent</code></p> <p>Event used to signal a change in the node's geographical location.</p> <p>Attributes:</p> Name Type Description <code>latitude</code> <code>float</code> <p>New latitude of the node.</p> <code>longitude</code> <code>float</code> <p>New longitude of the node.</p> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>class ChangeLocationEvent(AddonEvent):\n    \"\"\"\n    Event used to signal a change in the node's geographical location.\n\n    Attributes:\n        latitude (float): New latitude of the node.\n        longitude (float): New longitude of the node.\n    \"\"\"\n\n    def __init__(self, latitude, longitude):\n        \"\"\"\n        Initializes a ChangeLocationEvent.\n\n        Args:\n            latitude (float): The new latitude value.\n            longitude (float): The new longitude value.\n        \"\"\"\n        self.latitude = latitude\n        self.longitude = longitude\n\n    def __str__(self):\n        return \"ChangeLocationEvent\"\n\n    async def get_event_data(self):\n        \"\"\"\n        Returns the new location coordinates associated with this event.\n\n        Returns:\n            tuple: A tuple containing latitude and longitude.\n        \"\"\"\n        return (self.latitude, self.longitude)\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.ChangeLocationEvent.__init__","title":"<code>__init__(latitude, longitude)</code>","text":"<p>Initializes a ChangeLocationEvent.</p> <p>Parameters:</p> Name Type Description Default <code>latitude</code> <code>float</code> <p>The new latitude value.</p> required <code>longitude</code> <code>float</code> <p>The new longitude value.</p> required Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>def __init__(self, latitude, longitude):\n    \"\"\"\n    Initializes a ChangeLocationEvent.\n\n    Args:\n        latitude (float): The new latitude value.\n        longitude (float): The new longitude value.\n    \"\"\"\n    self.latitude = latitude\n    self.longitude = longitude\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.ChangeLocationEvent.get_event_data","title":"<code>get_event_data()</code>  <code>async</code>","text":"<p>Returns the new location coordinates associated with this event.</p> <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing latitude and longitude.</p> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>async def get_event_data(self):\n    \"\"\"\n    Returns the new location coordinates associated with this event.\n\n    Returns:\n        tuple: A tuple containing latitude and longitude.\n    \"\"\"\n    return (self.latitude, self.longitude)\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.DuplicatedMessageEvent","title":"<code>DuplicatedMessageEvent</code>","text":"<p>               Bases: <code>NodeEvent</code></p> <p>Event triggered when a message is received that has already been processed.</p> <p>Attributes:</p> Name Type Description <code>source</code> <code>str</code> <p>The address of the node that sent the duplicated message.</p> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>class DuplicatedMessageEvent(NodeEvent):\n    \"\"\"\n    Event triggered when a message is received that has already been processed.\n\n    Attributes:\n        source (str): The address of the node that sent the duplicated message.\n    \"\"\"\n\n    def __init__(self, source: str, message_type: str):\n        self.source = source\n\n    def __str__(self):\n        return f\"DuplicatedMessageEvent from {self.source}\"\n\n    async def get_event_data(self) -&gt; tuple[str]:\n        return (self.source)\n\n    async def is_concurrent(self) -&gt; bool:\n        return True\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.ExperimentFinishEvent","title":"<code>ExperimentFinishEvent</code>","text":"<p>               Bases: <code>NodeEvent</code></p> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>class ExperimentFinishEvent(NodeEvent):\n    def __init__(self):\n        \"\"\"Event triggered when experiment is going to finish.\"\"\"\n\n    def __str__(self):\n        return \"Experiment finished\"\n\n    async def get_event_data(self):\n        pass\n\n    async def is_concurrent(self):\n        return False\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.ExperimentFinishEvent.__init__","title":"<code>__init__()</code>","text":"<p>Event triggered when experiment is going to finish.</p> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>def __init__(self):\n    \"\"\"Event triggered when experiment is going to finish.\"\"\"\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.GPSEvent","title":"<code>GPSEvent</code>","text":"<p>               Bases: <code>AddonEvent</code></p> <p>Event triggered by a GPS module providing distance data between nodes.</p> <p>Attributes:</p> Name Type Description <code>distances</code> <code>dict</code> <p>A dictionary mapping node addresses to their respective distances.</p> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>class GPSEvent(AddonEvent):\n    \"\"\"\n    Event triggered by a GPS module providing distance data between nodes.\n\n    Attributes:\n        distances (dict): A dictionary mapping node addresses to their respective distances.\n    \"\"\"\n\n    def __init__(self, distances: dict):\n        \"\"\"\n        Initializes a GPSEvent.\n\n        Args:\n            distances (dict): Dictionary of distances from the current node to others.\n        \"\"\"\n        self.distances = distances\n\n    def __str__(self):\n        return \"GPSEvent\"\n\n    async def get_event_data(self) -&gt; dict:\n        \"\"\"\n        Returns the distance data associated with this event.\n\n        Returns:\n            dict: A copy of the distances dictionary.\n        \"\"\"\n        return self.distances.copy()\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.GPSEvent.__init__","title":"<code>__init__(distances)</code>","text":"<p>Initializes a GPSEvent.</p> <p>Parameters:</p> Name Type Description Default <code>distances</code> <code>dict</code> <p>Dictionary of distances from the current node to others.</p> required Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>def __init__(self, distances: dict):\n    \"\"\"\n    Initializes a GPSEvent.\n\n    Args:\n        distances (dict): Dictionary of distances from the current node to others.\n    \"\"\"\n    self.distances = distances\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.GPSEvent.get_event_data","title":"<code>get_event_data()</code>  <code>async</code>","text":"<p>Returns the distance data associated with this event.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A copy of the distances dictionary.</p> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>async def get_event_data(self) -&gt; dict:\n    \"\"\"\n    Returns the distance data associated with this event.\n\n    Returns:\n        dict: A copy of the distances dictionary.\n    \"\"\"\n    return self.distances.copy()\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.MessageEvent","title":"<code>MessageEvent</code>","text":"<p>Event class for wrapping received messages in the system.</p> <p>Attributes:</p> Name Type Description <code>message_type</code> <code>str</code> <p>Type/category of the message.</p> <code>source</code> <code>str</code> <p>Address or identifier of the message sender.</p> <code>message</code> <code>Any</code> <p>The actual message payload.</p> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>class MessageEvent:\n    \"\"\"\n    Event class for wrapping received messages in the system.\n\n    Attributes:\n        message_type (str): Type/category of the message.\n        source (str): Address or identifier of the message sender.\n        message (Any): The actual message payload.\n    \"\"\"\n\n    def __init__(self, message_type, source, message):\n        \"\"\"\n        Initializes a MessageEvent instance.\n\n        Args:\n            message_type (str): Type/category of the message.\n            source (str): Address or identifier of the message sender.\n            message (Any): The actual message payload.\n        \"\"\"\n        self.source = source\n        self.message_type = message_type\n        self.message = message\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.MessageEvent.__init__","title":"<code>__init__(message_type, source, message)</code>","text":"<p>Initializes a MessageEvent instance.</p> <p>Parameters:</p> Name Type Description Default <code>message_type</code> <code>str</code> <p>Type/category of the message.</p> required <code>source</code> <code>str</code> <p>Address or identifier of the message sender.</p> required <code>message</code> <code>Any</code> <p>The actual message payload.</p> required Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>def __init__(self, message_type, source, message):\n    \"\"\"\n    Initializes a MessageEvent instance.\n\n    Args:\n        message_type (str): Type/category of the message.\n        source (str): Address or identifier of the message sender.\n        message (Any): The actual message payload.\n    \"\"\"\n    self.source = source\n    self.message_type = message_type\n    self.message = message\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.ModelPropagationEvent","title":"<code>ModelPropagationEvent</code>","text":"<p>               Bases: <code>NodeEvent</code></p> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>class ModelPropagationEvent(NodeEvent):\n    def __init__(self, eligible_neighbors, strategy):\n        \"\"\"Event triggered when model propagation is ready.\n\n        Args:\n            eligible_neighbors (set): The elegible neighbors to propagate model.\n            strategy (str): Strategy to propagete the model\n        \"\"\"\n        self.eligible_neighbors = eligible_neighbors\n        self._strategy = strategy\n\n    def __str__(self):\n        return f\"Model propagation event, strategy: {self._strategy}\"\n\n    async def get_event_data(self) -&gt; tuple[set, str]:\n        \"\"\"\n        Retrieves the event data.\n\n        Returns:\n            tuple[set, str]: A tuple containing:\n                - The elegible neighbors to propagate model.\n                - The propagation strategy.\n        \"\"\"\n        return (self.eligible_neighbors, self._strategy)\n\n    async def is_concurrent(self) -&gt; bool:\n        return False    \n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.ModelPropagationEvent.__init__","title":"<code>__init__(eligible_neighbors, strategy)</code>","text":"<p>Event triggered when model propagation is ready.</p> <p>Parameters:</p> Name Type Description Default <code>eligible_neighbors</code> <code>set</code> <p>The elegible neighbors to propagate model.</p> required <code>strategy</code> <code>str</code> <p>Strategy to propagete the model</p> required Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>def __init__(self, eligible_neighbors, strategy):\n    \"\"\"Event triggered when model propagation is ready.\n\n    Args:\n        eligible_neighbors (set): The elegible neighbors to propagate model.\n        strategy (str): Strategy to propagete the model\n    \"\"\"\n    self.eligible_neighbors = eligible_neighbors\n    self._strategy = strategy\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.ModelPropagationEvent.get_event_data","title":"<code>get_event_data()</code>  <code>async</code>","text":"<p>Retrieves the event data.</p> <p>Returns:</p> Type Description <code>tuple[set, str]</code> <p>tuple[set, str]: A tuple containing: - The elegible neighbors to propagate model. - The propagation strategy.</p> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>async def get_event_data(self) -&gt; tuple[set, str]:\n    \"\"\"\n    Retrieves the event data.\n\n    Returns:\n        tuple[set, str]: A tuple containing:\n            - The elegible neighbors to propagate model.\n            - The propagation strategy.\n    \"\"\"\n    return (self.eligible_neighbors, self._strategy)\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.NodeBlacklistedEvent","title":"<code>NodeBlacklistedEvent</code>","text":"<p>               Bases: <code>NodeEvent</code></p> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>class NodeBlacklistedEvent(NodeEvent):\n    def __init__(self, node_addr, blacklisted: bool = False):\n        \"\"\"\n        Initializes a NodeBlacklistedEvent.\n\n        Args:\n            node_addr (str): The address of the node.\n            blacklisted (bool, optional): True if the node is blacklisted,\n                                          False if it's just marked as recently disconnected.\n        \"\"\"\n        self._node_addr = node_addr\n        self._blacklisted = blacklisted\n\n    def __str__(self):\n        return f\"Node addr: {self._node_addr} | Blacklisted: {self._blacklisted} | Recently disconnected: {not self._blacklisted}\"\n\n    async def get_event_data(self) -&gt; tuple[str, bool]:\n        \"\"\"\n        Retrieves the address of the node and its blacklist status.\n\n        Returns:\n            tuple[str, bool]: A tuple containing the node address and blacklist flag.\n        \"\"\"\n        return (self._node_addr, self._blacklisted)\n\n    async def is_concurrent(self):\n        return True\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.NodeBlacklistedEvent.__init__","title":"<code>__init__(node_addr, blacklisted=False)</code>","text":"<p>Initializes a NodeBlacklistedEvent.</p> <p>Parameters:</p> Name Type Description Default <code>node_addr</code> <code>str</code> <p>The address of the node.</p> required <code>blacklisted</code> <code>bool</code> <p>True if the node is blacklisted,                           False if it's just marked as recently disconnected.</p> <code>False</code> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>def __init__(self, node_addr, blacklisted: bool = False):\n    \"\"\"\n    Initializes a NodeBlacklistedEvent.\n\n    Args:\n        node_addr (str): The address of the node.\n        blacklisted (bool, optional): True if the node is blacklisted,\n                                      False if it's just marked as recently disconnected.\n    \"\"\"\n    self._node_addr = node_addr\n    self._blacklisted = blacklisted\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.NodeBlacklistedEvent.get_event_data","title":"<code>get_event_data()</code>  <code>async</code>","text":"<p>Retrieves the address of the node and its blacklist status.</p> <p>Returns:</p> Type Description <code>tuple[str, bool]</code> <p>tuple[str, bool]: A tuple containing the node address and blacklist flag.</p> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>async def get_event_data(self) -&gt; tuple[str, bool]:\n    \"\"\"\n    Retrieves the address of the node and its blacklist status.\n\n    Returns:\n        tuple[str, bool]: A tuple containing the node address and blacklist flag.\n    \"\"\"\n    return (self._node_addr, self._blacklisted)\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.NodeEvent","title":"<code>NodeEvent</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all node-related events in the system.</p> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>class NodeEvent(ABC):\n    \"\"\"\n    Abstract base class for all node-related events in the system.\n    \"\"\"\n\n    @abstractmethod\n    async def get_event_data(self):\n        \"\"\"\n        Retrieve the data associated with the event.\n\n        Returns:\n            Any: The event-specific data payload.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def is_concurrent(self):\n        \"\"\"\n        Indicates whether the event can be handled concurrently.\n\n        Returns:\n            bool: True if concurrent handling is allowed, False otherwise.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.NodeEvent.get_event_data","title":"<code>get_event_data()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Retrieve the data associated with the event.</p> <p>Returns:</p> Name Type Description <code>Any</code> <p>The event-specific data payload.</p> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>@abstractmethod\nasync def get_event_data(self):\n    \"\"\"\n    Retrieve the data associated with the event.\n\n    Returns:\n        Any: The event-specific data payload.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.NodeEvent.is_concurrent","title":"<code>is_concurrent()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Indicates whether the event can be handled concurrently.</p> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if concurrent handling is allowed, False otherwise.</p> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>@abstractmethod\nasync def is_concurrent(self):\n    \"\"\"\n    Indicates whether the event can be handled concurrently.\n\n    Returns:\n        bool: True if concurrent handling is allowed, False otherwise.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.NodeFoundEvent","title":"<code>NodeFoundEvent</code>","text":"<p>               Bases: <code>NodeEvent</code></p> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>class NodeFoundEvent(NodeEvent):\n    def __init__(self, node_addr):\n        \"\"\"Event triggered when a new node is found.\n\n        Args:\n            node_addr (str): Address of the neighboring node.\n        \"\"\"\n        self._node_addr = node_addr\n\n    def __str__(self):\n        return f\"Node addr: {self._node_addr} found\"\n\n    async def get_event_data(self) -&gt; tuple[str, bool]:\n        \"\"\"Retrieves the node found event data.\n\n        Returns:\n            tuple[str, bool]:\n                - node_addr (str): Address of the node found.\n        \"\"\"\n        return self._node_addr\n\n    async def is_concurrent(self) -&gt; bool:\n        return True\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.NodeFoundEvent.__init__","title":"<code>__init__(node_addr)</code>","text":"<p>Event triggered when a new node is found.</p> <p>Parameters:</p> Name Type Description Default <code>node_addr</code> <code>str</code> <p>Address of the neighboring node.</p> required Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>def __init__(self, node_addr):\n    \"\"\"Event triggered when a new node is found.\n\n    Args:\n        node_addr (str): Address of the neighboring node.\n    \"\"\"\n    self._node_addr = node_addr\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.NodeFoundEvent.get_event_data","title":"<code>get_event_data()</code>  <code>async</code>","text":"<p>Retrieves the node found event data.</p> <p>Returns:</p> Type Description <code>tuple[str, bool]</code> <p>tuple[str, bool]: - node_addr (str): Address of the node found.</p> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>async def get_event_data(self) -&gt; tuple[str, bool]:\n    \"\"\"Retrieves the node found event data.\n\n    Returns:\n        tuple[str, bool]:\n            - node_addr (str): Address of the node found.\n    \"\"\"\n    return self._node_addr\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.RoundEndEvent","title":"<code>RoundEndEvent</code>","text":"<p>               Bases: <code>NodeEvent</code></p> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>class RoundEndEvent(NodeEvent):\n    def __init__(self, round, end_time):\n        \"\"\"Event triggered when round is going to start.\n\n        Args:\n            round (int): Round number.\n            end_time (time): Current time when round has ended.\n        \"\"\"\n        self._round_end_time = end_time\n        self._round = round\n\n    def __str__(self):\n        return \"Round ending\"\n\n    async def get_event_data(self):\n        \"\"\"Retrieves the round start event data.\n\n        Returns:\n            tuple[int, float]:\n                -round (int): Round number.\n                -end_time (time): Current time when round has ended.\n        \"\"\"\n        return (self._round, self._round_end_time)\n\n    async def is_concurrent(self):\n        return False\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.RoundEndEvent.__init__","title":"<code>__init__(round, end_time)</code>","text":"<p>Event triggered when round is going to start.</p> <p>Parameters:</p> Name Type Description Default <code>round</code> <code>int</code> <p>Round number.</p> required <code>end_time</code> <code>time</code> <p>Current time when round has ended.</p> required Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>def __init__(self, round, end_time):\n    \"\"\"Event triggered when round is going to start.\n\n    Args:\n        round (int): Round number.\n        end_time (time): Current time when round has ended.\n    \"\"\"\n    self._round_end_time = end_time\n    self._round = round\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.RoundEndEvent.get_event_data","title":"<code>get_event_data()</code>  <code>async</code>","text":"<p>Retrieves the round start event data.</p> <p>Returns:</p> Type Description <p>tuple[int, float]: -round (int): Round number. -end_time (time): Current time when round has ended.</p> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>async def get_event_data(self):\n    \"\"\"Retrieves the round start event data.\n\n    Returns:\n        tuple[int, float]:\n            -round (int): Round number.\n            -end_time (time): Current time when round has ended.\n    \"\"\"\n    return (self._round, self._round_end_time)\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.RoundStartEvent","title":"<code>RoundStartEvent</code>","text":"<p>               Bases: <code>NodeEvent</code></p> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>class RoundStartEvent(NodeEvent):\n    def __init__(self, round, start_time, expected_nodes):\n        \"\"\"Event triggered when round is going to start.\n\n        Args:\n            round (int): Round number.\n            start_time (time): Current time when round is going to start.\n        \"\"\"\n        self._round_start_time = start_time\n        self._round = round\n        self._expected_nodes = expected_nodes\n\n    def __str__(self):\n        return \"Round starting\"\n\n    async def get_event_data(self):\n        \"\"\"Retrieves the round start event data.\n\n        Returns:\n            tuple[int, float]:\n                -round (int): Round number.\n                -start_time (time): Current time when round is going to start.\n        \"\"\"\n        return (self._round, self._round_start_time, self._expected_nodes)\n\n    async def is_concurrent(self):\n        return False\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.RoundStartEvent.__init__","title":"<code>__init__(round, start_time, expected_nodes)</code>","text":"<p>Event triggered when round is going to start.</p> <p>Parameters:</p> Name Type Description Default <code>round</code> <code>int</code> <p>Round number.</p> required <code>start_time</code> <code>time</code> <p>Current time when round is going to start.</p> required Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>def __init__(self, round, start_time, expected_nodes):\n    \"\"\"Event triggered when round is going to start.\n\n    Args:\n        round (int): Round number.\n        start_time (time): Current time when round is going to start.\n    \"\"\"\n    self._round_start_time = start_time\n    self._round = round\n    self._expected_nodes = expected_nodes\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.RoundStartEvent.get_event_data","title":"<code>get_event_data()</code>  <code>async</code>","text":"<p>Retrieves the round start event data.</p> <p>Returns:</p> Type Description <p>tuple[int, float]: -round (int): Round number. -start_time (time): Current time when round is going to start.</p> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>async def get_event_data(self):\n    \"\"\"Retrieves the round start event data.\n\n    Returns:\n        tuple[int, float]:\n            -round (int): Round number.\n            -start_time (time): Current time when round is going to start.\n    \"\"\"\n    return (self._round, self._round_start_time, self._expected_nodes)\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.UpdateNeighborEvent","title":"<code>UpdateNeighborEvent</code>","text":"<p>               Bases: <code>NodeEvent</code></p> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>class UpdateNeighborEvent(NodeEvent):\n    def __init__(self, node_addr, removed=False, joining=False):\n        \"\"\"Event triggered when a neighboring node is updated.\n\n        Args:\n            node_addr (str): Address of the neighboring node.\n            removed (bool, optional): Indicates whether the node was removed.\n                                      Defaults to False.\n        \"\"\"\n        self._node_addr = node_addr\n        self._removed = removed\n        self._joining_federation = joining\n\n    def __str__(self):\n        return f\"Node addr: {self._node_addr}, removed: {self._removed}\"\n\n    async def get_event_data(self) -&gt; tuple[str, bool]:\n        \"\"\"Retrieves the neighbor update event data.\n\n        Returns:\n            tuple[str, bool]:\n                - node_addr (str): Address of the neighboring node.\n                - removed (bool): Whether the node was removed.\n        \"\"\"\n        return (self._node_addr, self._removed)\n\n    async def is_concurrent(self) -&gt; bool:\n        return False\n\n    def is_joining_federation(self):\n        return self._joining_federation\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.UpdateNeighborEvent.__init__","title":"<code>__init__(node_addr, removed=False, joining=False)</code>","text":"<p>Event triggered when a neighboring node is updated.</p> <p>Parameters:</p> Name Type Description Default <code>node_addr</code> <code>str</code> <p>Address of the neighboring node.</p> required <code>removed</code> <code>bool</code> <p>Indicates whether the node was removed.                       Defaults to False.</p> <code>False</code> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>def __init__(self, node_addr, removed=False, joining=False):\n    \"\"\"Event triggered when a neighboring node is updated.\n\n    Args:\n        node_addr (str): Address of the neighboring node.\n        removed (bool, optional): Indicates whether the node was removed.\n                                  Defaults to False.\n    \"\"\"\n    self._node_addr = node_addr\n    self._removed = removed\n    self._joining_federation = joining\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.UpdateNeighborEvent.get_event_data","title":"<code>get_event_data()</code>  <code>async</code>","text":"<p>Retrieves the neighbor update event data.</p> <p>Returns:</p> Type Description <code>tuple[str, bool]</code> <p>tuple[str, bool]: - node_addr (str): Address of the neighboring node. - removed (bool): Whether the node was removed.</p> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>async def get_event_data(self) -&gt; tuple[str, bool]:\n    \"\"\"Retrieves the neighbor update event data.\n\n    Returns:\n        tuple[str, bool]:\n            - node_addr (str): Address of the neighboring node.\n            - removed (bool): Whether the node was removed.\n    \"\"\"\n    return (self._node_addr, self._removed)\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.UpdateReceivedEvent","title":"<code>UpdateReceivedEvent</code>","text":"<p>               Bases: <code>NodeEvent</code></p> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>class UpdateReceivedEvent(NodeEvent):\n    def __init__(self, decoded_model, weight, source, round, local=False):\n        \"\"\"\n        Initializes an UpdateReceivedEvent.\n\n        Args:\n            decoded_model (Any): The received model update.\n            weight (float): The weight associated with the received update.\n            source (str): The identifier or address of the node that sent the update.\n            round (int): The round number in which the update was received.\n            local (bool): Local update\n        \"\"\"\n        self._source = source\n        self._round = round\n        self._model = decoded_model\n        self._weight = weight\n        self._local = local\n\n    def __str__(self):\n        return f\"Update received from source: {self._source}, round: {self._round}\"\n\n    async def get_event_data(self) -&gt; tuple[object, int, str, int, bool]:\n        \"\"\"\n        Retrieves the event data.\n\n        Returns:\n            tuple[Any, float, str, int, bool]: A tuple containing:\n                - The received model update.\n                - The weight associated with the update.\n                - The source node identifier.\n                - The round number of the update.\n                - If the update is local\n        \"\"\"\n        return (self._model, self._weight, self._source, self._round, self._local)\n\n    async def is_concurrent(self) -&gt; bool:\n        return False\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.UpdateReceivedEvent.__init__","title":"<code>__init__(decoded_model, weight, source, round, local=False)</code>","text":"<p>Initializes an UpdateReceivedEvent.</p> <p>Parameters:</p> Name Type Description Default <code>decoded_model</code> <code>Any</code> <p>The received model update.</p> required <code>weight</code> <code>float</code> <p>The weight associated with the received update.</p> required <code>source</code> <code>str</code> <p>The identifier or address of the node that sent the update.</p> required <code>round</code> <code>int</code> <p>The round number in which the update was received.</p> required <code>local</code> <code>bool</code> <p>Local update</p> <code>False</code> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>def __init__(self, decoded_model, weight, source, round, local=False):\n    \"\"\"\n    Initializes an UpdateReceivedEvent.\n\n    Args:\n        decoded_model (Any): The received model update.\n        weight (float): The weight associated with the received update.\n        source (str): The identifier or address of the node that sent the update.\n        round (int): The round number in which the update was received.\n        local (bool): Local update\n    \"\"\"\n    self._source = source\n    self._round = round\n    self._model = decoded_model\n    self._weight = weight\n    self._local = local\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.UpdateReceivedEvent.get_event_data","title":"<code>get_event_data()</code>  <code>async</code>","text":"<p>Retrieves the event data.</p> <p>Returns:</p> Type Description <code>tuple[object, int, str, int, bool]</code> <p>tuple[Any, float, str, int, bool]: A tuple containing: - The received model update. - The weight associated with the update. - The source node identifier. - The round number of the update. - If the update is local</p> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>async def get_event_data(self) -&gt; tuple[object, int, str, int, bool]:\n    \"\"\"\n    Retrieves the event data.\n\n    Returns:\n        tuple[Any, float, str, int, bool]: A tuple containing:\n            - The received model update.\n            - The weight associated with the update.\n            - The source node identifier.\n            - The round number of the update.\n            - If the update is local\n    \"\"\"\n    return (self._model, self._weight, self._source, self._round, self._local)\n</code></pre>"},{"location":"api/core/node/","title":"Documentation for Node Module","text":""},{"location":"api/core/node/#nebula.core.node.main","title":"<code>main(config)</code>  <code>async</code>","text":"<p>Main function to start the NEBULA node.</p> <p>This function initiates the NEBULA core component deployed on each federation participant. It configures the node using the provided configuration object, setting up dataset partitions, selecting and initializing the appropriate model and data handler, and establishing training mechanisms. Additionally, it adjusts specific node parameters (such as indices and timing intervals) based on the participant's configuration, and deploys the node's network communications for federated learning.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>Configuration object containing settings for: - scenario (including federation and deployment parameters), - model selection and its corresponding hyperparameters, - dataset and data partitioning, - training strategy and related arguments, - device roles and security flags.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported model, dataset, or device role is specified.</p> <code>NotImplementedError</code> <p>If an unsupported training strategy (e.g., \"scikit\") is requested.</p> <p>Returns:</p> Type Description <p>Coroutine that initializes and starts the NEBULA node.</p> Source code in <code>nebula/core/node.py</code> <pre><code>async def main(config: Config):\n    \"\"\"\n    Main function to start the NEBULA node.\n\n    This function initiates the NEBULA core component deployed on each federation participant.\n    It configures the node using the provided configuration object, setting up dataset partitions,\n    selecting and initializing the appropriate model and data handler, and establishing training\n    mechanisms. Additionally, it adjusts specific node parameters (such as indices and timing intervals)\n    based on the participant's configuration, and deploys the node's network communications for\n    federated learning.\n\n    Parameters:\n        config (Config): Configuration object containing settings for:\n            - scenario (including federation and deployment parameters),\n            - model selection and its corresponding hyperparameters,\n            - dataset and data partitioning,\n            - training strategy and related arguments,\n            - device roles and security flags.\n\n    Raises:\n        ValueError: If an unsupported model, dataset, or device role is specified.\n        NotImplementedError: If an unsupported training strategy (e.g., \"scikit\") is requested.\n\n    Returns:\n        Coroutine that initializes and starts the NEBULA node.\n    \"\"\"\n    n_nodes = config.participant[\"scenario_args\"][\"n_nodes\"]\n    model_name = config.participant[\"model_args\"][\"model\"]\n    idx = config.participant[\"device_args\"][\"idx\"]\n\n    additional_node_status = config.participant[\"mobility_args\"][\"additional_node\"][\"status\"]\n\n    # Adjust the total number of nodes and the index of the current node for CFL, as it doesn't require a specific partition for the server (not used for training)\n    if config.participant[\"scenario_args\"][\"federation\"] == \"CFL\":\n        n_nodes -= 1\n        if idx &gt; 0:\n            idx -= 1\n\n    dataset = None\n    dataset_name = config.participant[\"data_args\"][\"dataset\"]\n    handler = None\n    batch_size = None\n    num_workers = config.participant[\"data_args\"][\"num_workers\"]\n    model = None\n\n    if dataset_name == \"MNIST\":\n        batch_size = 32\n        handler = MNISTPartitionHandler\n        if model_name == \"MLP\":\n            model = MNISTModelMLP()\n        elif model_name == \"CNN\":\n            model = MNISTModelCNN()\n        else:\n            raise ValueError(f\"Model {model} not supported for dataset {dataset_name}\")\n    elif dataset_name == \"FashionMNIST\":\n        batch_size = 32\n        handler = FashionMNISTPartitionHandler\n        if model_name == \"MLP\":\n            model = FashionMNISTModelMLP()\n        elif model_name == \"CNN\":\n            model = FashionMNISTModelCNN()\n        else:\n            raise ValueError(f\"Model {model} not supported for dataset {dataset_name}\")\n    elif dataset_name == \"EMNIST\":\n        batch_size = 32\n        handler = EMNISTPartitionHandler\n        if model_name == \"MLP\":\n            model = EMNISTModelMLP()\n        elif model_name == \"CNN\":\n            model = EMNISTModelCNN()\n        else:\n            raise ValueError(f\"Model {model} not supported for dataset {dataset_name}\")\n    elif dataset_name == \"CIFAR10\":\n        batch_size = 32\n        handler = CIFAR10PartitionHandler\n        if model_name == \"ResNet9\":\n            model = CIFAR10ModelResNet(classifier=\"resnet9\")\n        elif model_name == \"fastermobilenet\":\n            model = FasterMobileNet()\n        elif model_name == \"simplemobilenet\":\n            model = SimpleMobileNetV1()\n        elif model_name == \"CNN\":\n            model = CIFAR10ModelCNN()\n        elif model_name == \"CNNv2\":\n            model = CIFAR10ModelCNN_V2()\n        elif model_name == \"CNNv3\":\n            model = CIFAR10ModelCNN_V3()\n        else:\n            raise ValueError(f\"Model {model} not supported for dataset {dataset_name}\")\n    elif dataset_name == \"CIFAR100\":\n        batch_size = 128\n        handler = CIFAR100PartitionHandler\n        if model_name == \"CNN\":\n            model = CIFAR100ModelCNN()\n        else:\n            raise ValueError(f\"Model {model} not supported for dataset {dataset_name}\")\n    else:\n        raise ValueError(f\"Dataset {dataset_name} not supported\")\n\n    dataset = NebulaPartition(handler=handler, config=config)\n    dataset.load_partition()\n    dataset.log_partition()\n    samples_per_label = Counter(dataset.get_train_labels())\n\n    datamodule = DataModule(\n        train_set=dataset.train_set,\n        train_set_indices=dataset.train_indices,\n        test_set=dataset.test_set,\n        test_set_indices=dataset.test_indices,\n        local_test_set=dataset.local_test_set,\n        local_test_set_indices=dataset.local_test_indices,\n        num_workers=num_workers,\n        batch_size=batch_size,\n        samples_per_label=samples_per_label,\n    )\n\n    trainer = None\n    trainer_str = config.participant[\"training_args\"][\"trainer\"]\n    if trainer_str == \"lightning\":\n        trainer = Lightning\n    elif trainer_str == \"scikit\":\n        raise NotImplementedError\n    elif trainer_str == \"siamese\":\n        trainer = Siamese\n    else:\n        raise ValueError(f\"Trainer {trainer_str} not supported\")\n\n    VARIABILITY = 0.5\n\n    def randomize_value(value, variability):\n        min_value = max(0, value - variability)\n        max_value = value + variability\n        return random.uniform(min_value, max_value)\n\n    config_keys = [\n        [\"reporter_args\", \"report_frequency\"],\n        [\"discoverer_args\", \"discovery_frequency\"],\n        [\"health_args\", \"health_interval\"],\n        [\"health_args\", \"grace_time_health\"],\n        [\"health_args\", \"check_alive_interval\"],\n        [\"health_args\", \"send_alive_interval\"],\n        [\"forwarder_args\", \"forwarder_interval\"],\n        [\"forwarder_args\", \"forward_messages_interval\"],\n    ]\n\n    for keys in config_keys:\n        value = config.participant\n        for key in keys[:-1]:\n            value = value[key]\n        value[keys[-1]] = randomize_value(value[keys[-1]], VARIABILITY)\n\n    role = config.participant[\"device_args\"][\"role\"]\n    logging.info(f\"Starting node {idx} with model {model_name}, trainer {trainer.__name__}, and as {role}\")\n\n    node = Engine(\n        model=model,\n        datamodule=datamodule,\n        config=config,\n        trainer=trainer,\n        security=False,\n    )\n    await node.start_communications()\n    await node.deploy_components()\n    await node.deploy_federation()\n\n    if additional_node_status:\n        time = config.participant[\"mobility_args\"][\"additional_node\"][\"time_start\"]\n        logging.info(f\"Waiting time to start finding federation: {time}\")\n        await asyncio.sleep(int(config.participant[\"mobility_args\"][\"additional_node\"][\"time_start\"]))\n        await node._aditional_node_start()\n\n    if node.cm is not None:\n        await node.cm.network_wait()\n\n    # Ensure shutdown is always called and awaited before main() returns\n    if hasattr(node, \"shutdown\") and callable(node.shutdown):\n        logging.info(\"Calling node.shutdown() for final cleanup and Docker removal...\")\n        await node.shutdown()\n    else:\n        logging.warning(\"Node does not have a shutdown() method; skipping explicit shutdown.\")\n</code></pre>"},{"location":"api/core/noderole/","title":"Documentation for Noderole Module","text":""},{"location":"api/core/noderole/#nebula.core.noderole.Role","title":"<code>Role</code>","text":"<p>               Bases: <code>Enum</code></p> <p>This class defines the participant roles of the platform.</p> Source code in <code>nebula/core/noderole.py</code> <pre><code>class Role(Enum):\n    \"\"\"\n    This class defines the participant roles of the platform.\n    \"\"\"\n    TRAINER = \"trainer\"\n    AGGREGATOR = \"aggregator\"\n    TRAINER_AGGREGATOR = \"trainer_aggregator\"\n    PROXY = \"proxy\"\n    IDLE = \"idle\"\n    SERVER = \"server\"\n    MALICIOUS = \"malicious\"\n</code></pre>"},{"location":"api/core/noderole/#nebula.core.noderole.RoleBehavior","title":"<code>RoleBehavior</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for defining the role-specific behavior of a node in CFL, DFL, or SDFL systems.</p> <p>Each subclass encapsulates the logic needed for a particular node role (e.g., trainer, aggregator), providing custom implementations for role-related operations such as training cycles, update aggregation, and recovery strategies.</p> <p>Attributes:</p> Name Type Description <code>_next_role</code> <code>Role</code> <p>The role to which the node is expected to transition.</p> <code>_next_role_locker</code> <code>Locker</code> <p>An asynchronous lock to protect access to _next_role.</p> <code>_source_to_notificate</code> <code>Optional[Any]</code> <p>The source node to notify once a role change is applied.</p> Source code in <code>nebula/core/noderole.py</code> <pre><code>class RoleBehavior(ABC):\n    \"\"\"\n    Abstract base class for defining the role-specific behavior of a node in CFL, DFL, or SDFL systems.\n\n    Each subclass encapsulates the logic needed for a particular node role (e.g., trainer, aggregator),\n    providing custom implementations for role-related operations such as training cycles,\n    update aggregation, and recovery strategies.\n\n    Attributes:\n        _next_role (Role): The role to which the node is expected to transition.\n        _next_role_locker (Locker): An asynchronous lock to protect access to _next_role.\n        _source_to_notificate (Optional[Any]): The source node to notify once a role change is applied.\n    \"\"\"\n    def __init__(self):\n        self._next_role: Role = None\n        self._next_role_locker = Locker(\"next_role_locker\", async_lock=True)\n        self._source_to_notificate = None\n\n    @abstractmethod\n    def get_role(self):\n        \"\"\"\n        Returns the Role enum value representing the current role of the node.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def get_role_name(self, effective=False):\n        \"\"\"\n        Returns a string representation of the current role.\n\n        Args:\n            effective (bool): Whether to return the name of the current effective role when going as malicious.\n\n        Returns:\n            str: Name of the role.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    async def extended_learning_cycle(self):\n        \"\"\"\n        Performs the main learning or aggregation cycle associated with the current role.\n\n        This method encapsulates all the logic tied to the behavior of the node in its current role,\n        including training, aggregating updates, and coordinating with neighbors.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    async def select_nodes_to_wait(self):\n        \"\"\"\n        Determines which neighbors the node should wait for during the current cycle.\n\n        This logic varies depending on whether the node is an aggregator, trainer, or other role.\n\n        Returns:\n            Set[Any]: A set of neighbor node identifiers to wait for.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    async def resolve_missing_updates(self):\n        \"\"\"\n        Defines the fallback strategy when expected model updates are not received.\n\n        For example, an aggregator might default to a fresh model, while a trainer might proceed\n        with its own local model.\n\n        Returns:\n            Any: The resolution outcome depending on the role's specific logic.\n        \"\"\"\n        raise NotImplementedError\n\n    async def set_next_role(self, role: Role, source_to_notificate = None):\n        \"\"\"\n        Schedules a role change and optionally stores the source to notify upon completion.\n\n        Args:\n            role (Role): The new role to transition to.\n            source_to_notificate (Optional[Any]): Identifier of the node that triggered the change.\n        \"\"\"\n        async with self._next_role_locker:\n            self._next_role = role\n            self._source_to_notificate = source_to_notificate\n\n    async def get_next_role(self) -&gt; Role:\n        \"\"\"\n        Retrieves and clears the next role value.\n\n        Returns:\n            Role: The next role to transition into.\n        \"\"\"\n        async with self._next_role_locker:\n            next_role = self._next_role\n            self._next_role = None\n        return next_role\n\n    async def get_source_to_notificate(self):\n        \"\"\"\n        Retrieves and clears the stored source to notify after a role change.\n\n        Returns:\n            Any: The source node identifier, or None if not set.\n        \"\"\"\n        async with self._next_role_locker:\n            source_to_notificate = self._source_to_notificate\n            self._source_to_notificate = None\n        return source_to_notificate\n\n    async def update_role_needed(self):\n        \"\"\"\n        Checks whether a role update is scheduled.\n\n        Returns:\n            bool: True if a role update is pending, False otherwise.\n        \"\"\"\n        async with self._next_role_locker:\n            updt_needed = self._next_role != None\n        return updt_needed\n</code></pre>"},{"location":"api/core/noderole/#nebula.core.noderole.RoleBehavior.extended_learning_cycle","title":"<code>extended_learning_cycle()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Performs the main learning or aggregation cycle associated with the current role.</p> <p>This method encapsulates all the logic tied to the behavior of the node in its current role, including training, aggregating updates, and coordinating with neighbors.</p> Source code in <code>nebula/core/noderole.py</code> <pre><code>@abstractmethod\nasync def extended_learning_cycle(self):\n    \"\"\"\n    Performs the main learning or aggregation cycle associated with the current role.\n\n    This method encapsulates all the logic tied to the behavior of the node in its current role,\n    including training, aggregating updates, and coordinating with neighbors.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/core/noderole/#nebula.core.noderole.RoleBehavior.get_next_role","title":"<code>get_next_role()</code>  <code>async</code>","text":"<p>Retrieves and clears the next role value.</p> <p>Returns:</p> Name Type Description <code>Role</code> <code>Role</code> <p>The next role to transition into.</p> Source code in <code>nebula/core/noderole.py</code> <pre><code>async def get_next_role(self) -&gt; Role:\n    \"\"\"\n    Retrieves and clears the next role value.\n\n    Returns:\n        Role: The next role to transition into.\n    \"\"\"\n    async with self._next_role_locker:\n        next_role = self._next_role\n        self._next_role = None\n    return next_role\n</code></pre>"},{"location":"api/core/noderole/#nebula.core.noderole.RoleBehavior.get_role","title":"<code>get_role()</code>  <code>abstractmethod</code>","text":"<p>Returns the Role enum value representing the current role of the node.</p> Source code in <code>nebula/core/noderole.py</code> <pre><code>@abstractmethod\ndef get_role(self):\n    \"\"\"\n    Returns the Role enum value representing the current role of the node.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/core/noderole/#nebula.core.noderole.RoleBehavior.get_role_name","title":"<code>get_role_name(effective=False)</code>  <code>abstractmethod</code>","text":"<p>Returns a string representation of the current role.</p> <p>Parameters:</p> Name Type Description Default <code>effective</code> <code>bool</code> <p>Whether to return the name of the current effective role when going as malicious.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <p>Name of the role.</p> Source code in <code>nebula/core/noderole.py</code> <pre><code>@abstractmethod\ndef get_role_name(self, effective=False):\n    \"\"\"\n    Returns a string representation of the current role.\n\n    Args:\n        effective (bool): Whether to return the name of the current effective role when going as malicious.\n\n    Returns:\n        str: Name of the role.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/core/noderole/#nebula.core.noderole.RoleBehavior.get_source_to_notificate","title":"<code>get_source_to_notificate()</code>  <code>async</code>","text":"<p>Retrieves and clears the stored source to notify after a role change.</p> <p>Returns:</p> Name Type Description <code>Any</code> <p>The source node identifier, or None if not set.</p> Source code in <code>nebula/core/noderole.py</code> <pre><code>async def get_source_to_notificate(self):\n    \"\"\"\n    Retrieves and clears the stored source to notify after a role change.\n\n    Returns:\n        Any: The source node identifier, or None if not set.\n    \"\"\"\n    async with self._next_role_locker:\n        source_to_notificate = self._source_to_notificate\n        self._source_to_notificate = None\n    return source_to_notificate\n</code></pre>"},{"location":"api/core/noderole/#nebula.core.noderole.RoleBehavior.resolve_missing_updates","title":"<code>resolve_missing_updates()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Defines the fallback strategy when expected model updates are not received.</p> <p>For example, an aggregator might default to a fresh model, while a trainer might proceed with its own local model.</p> <p>Returns:</p> Name Type Description <code>Any</code> <p>The resolution outcome depending on the role's specific logic.</p> Source code in <code>nebula/core/noderole.py</code> <pre><code>@abstractmethod\nasync def resolve_missing_updates(self):\n    \"\"\"\n    Defines the fallback strategy when expected model updates are not received.\n\n    For example, an aggregator might default to a fresh model, while a trainer might proceed\n    with its own local model.\n\n    Returns:\n        Any: The resolution outcome depending on the role's specific logic.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/core/noderole/#nebula.core.noderole.RoleBehavior.select_nodes_to_wait","title":"<code>select_nodes_to_wait()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Determines which neighbors the node should wait for during the current cycle.</p> <p>This logic varies depending on whether the node is an aggregator, trainer, or other role.</p> <p>Returns:</p> Type Description <p>Set[Any]: A set of neighbor node identifiers to wait for.</p> Source code in <code>nebula/core/noderole.py</code> <pre><code>@abstractmethod\nasync def select_nodes_to_wait(self):\n    \"\"\"\n    Determines which neighbors the node should wait for during the current cycle.\n\n    This logic varies depending on whether the node is an aggregator, trainer, or other role.\n\n    Returns:\n        Set[Any]: A set of neighbor node identifiers to wait for.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/core/noderole/#nebula.core.noderole.RoleBehavior.set_next_role","title":"<code>set_next_role(role, source_to_notificate=None)</code>  <code>async</code>","text":"<p>Schedules a role change and optionally stores the source to notify upon completion.</p> <p>Parameters:</p> Name Type Description Default <code>role</code> <code>Role</code> <p>The new role to transition to.</p> required <code>source_to_notificate</code> <code>Optional[Any]</code> <p>Identifier of the node that triggered the change.</p> <code>None</code> Source code in <code>nebula/core/noderole.py</code> <pre><code>async def set_next_role(self, role: Role, source_to_notificate = None):\n    \"\"\"\n    Schedules a role change and optionally stores the source to notify upon completion.\n\n    Args:\n        role (Role): The new role to transition to.\n        source_to_notificate (Optional[Any]): Identifier of the node that triggered the change.\n    \"\"\"\n    async with self._next_role_locker:\n        self._next_role = role\n        self._source_to_notificate = source_to_notificate\n</code></pre>"},{"location":"api/core/noderole/#nebula.core.noderole.RoleBehavior.update_role_needed","title":"<code>update_role_needed()</code>  <code>async</code>","text":"<p>Checks whether a role update is scheduled.</p> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if a role update is pending, False otherwise.</p> Source code in <code>nebula/core/noderole.py</code> <pre><code>async def update_role_needed(self):\n    \"\"\"\n    Checks whether a role update is scheduled.\n\n    Returns:\n        bool: True if a role update is pending, False otherwise.\n    \"\"\"\n    async with self._next_role_locker:\n        updt_needed = self._next_role != None\n    return updt_needed\n</code></pre>"},{"location":"api/core/role/","title":"Documentation for Role Module","text":""},{"location":"api/core/role/#nebula.core.role.Role","title":"<code>Role</code>","text":"<p>               Bases: <code>Enum</code></p> <p>This class defines the participant roles of the platform.</p> Source code in <code>nebula/core/role.py</code> <pre><code>class Role(Enum):\n    \"\"\"\n    This class defines the participant roles of the platform.\n    \"\"\"\n\n    TRAINER = \"trainer\"\n    AGGREGATOR = \"aggregator\"\n    PROXY = \"proxy\"\n    IDLE = \"idle\"\n    SERVER = \"server\"\n</code></pre>"},{"location":"api/core/aggregation/","title":"Documentation for Aggregation Module","text":""},{"location":"api/core/aggregation/aggregator/","title":"Documentation for Aggregator Module","text":""},{"location":"api/core/aggregation/aggregator/#nebula.core.aggregation.aggregator.Aggregator","title":"<code>Aggregator</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>nebula/core/aggregation/aggregator.py</code> <pre><code>class Aggregator(ABC):\n    def __init__(self, config=None, engine=None):\n        self.config = config\n        self.engine: Engine = engine\n        self._addr = config.participant[\"network_args\"][\"addr\"]\n        logging.info(f\"[{self.__class__.__name__}] Starting Aggregator\")\n        self._federation_nodes = set()\n        self._pending_models_to_aggregate = {}\n        self._pending_models_to_aggregate_lock = Locker(name=\"pending_models_to_aggregate_lock\", async_lock=True)\n        self._aggregation_done_lock = Locker(name=\"aggregation_done_lock\", async_lock=True)\n        self._aggregation_waiting_skip = asyncio.Event()\n\n        scenario = self.config.participant[\"scenario_args\"][\"federation\"]\n        self._update_storage = factory_update_handler(scenario, self, self._addr)\n\n    def __str__(self):\n        return self.__class__.__name__\n\n    def __repr__(self):\n        return self.__str__()\n\n    @property\n    def us(self):\n        \"\"\"Federation type UpdateHandler (e.g. DFL-UpdateHandler, CFL-UpdateHandler...)\"\"\"\n        return self._update_storage\n\n    @abstractmethod\n    def run_aggregation(self, models):\n        if len(models) == 0:\n            logging.error(\"Trying to aggregate models when there are no models\")\n            return None\n\n    async def init(self):\n        await self.us.init(self.engine.rb.get_role_name(True))\n\n    async def update_federation_nodes(self, federation_nodes: set):\n        \"\"\"\n        Updates the current set of nodes expected to participate in the upcoming aggregation round.\n\n        This method informs the update handler (`us`) about the new set of federation nodes, \n        clears any pending models, and attempts to acquire the aggregation lock to prepare \n        for model aggregation. If the aggregation process is already running, it releases the lock\n        and tries again to ensure proper cleanup between rounds.\n\n        Args:\n            federation_nodes (set): A set of addresses representing the nodes expected to contribute \n                                    updates for the next aggregation round.\n\n        Raises:\n            Exception: If the aggregation process is already running and the lock cannot be released.\n        \"\"\"\n        await self.us.round_expected_updates(federation_nodes=federation_nodes)\n\n        # If the aggregation lock is held, release it to prepare for the new round\n        if self._aggregation_done_lock.locked():\n            logging.info(\"\ud83d\udd04  update_federation_nodes | Aggregation lock is held, releasing for new round\")\n            try:\n                await self._aggregation_done_lock.release_async()\n            except Exception as e:\n                logging.warning(f\"\ud83d\udd04  update_federation_nodes | Error releasing aggregation lock: {e}\")\n                # If we can't release the lock, we might be in the middle of aggregation\n                # In this case, we should wait a bit and try again\n                await asyncio.sleep(0.1)\n                if self._aggregation_done_lock.locked():\n                    raise Exception(\"It is not possible to set nodes to aggregate when the aggregation is running.\")\n\n        # Now acquire the lock for the new round\n        self._federation_nodes = federation_nodes\n        self._pending_models_to_aggregate.clear()\n        await self._aggregation_done_lock.acquire_async(\n            timeout=self.config.participant[\"aggregator_args\"][\"aggregation_timeout\"]\n        )\n\n    def get_nodes_pending_models_to_aggregate(self):\n        return self._federation_nodes\n\n    async def get_aggregation(self):\n        \"\"\"\n        Handles the aggregation process for a training round.\n\n        This method waits for all expected model updates from federation nodes or until a timeout occurs.\n        It uses an asynchronous lock to coordinate access and includes an early exit mechanism if all\n        updates are received before the timeout. Once the condition is satisfied, it releases the lock,\n        collects the updates, identifies any missing nodes, and publishes an `AggregationEvent`.\n        Finally, it runs the aggregation algorithm and returns the result.\n\n        Returns:\n            Any: The result of the aggregation process, as returned by `run_aggregation`.\n\n        Raises:\n            TimeoutError: If the aggregation lock is not acquired within the defined timeout.\n            asyncio.CancelledError: If the aggregation lock acquisition is cancelled.\n            Exception: For any other unexpected errors during the aggregation process.\n        \"\"\"            \n        try:\n            timeout = self.config.participant[\"aggregator_args\"][\"aggregation_timeout\"]\n            logging.info(f\"Aggregation timeout: {timeout} starts...\")\n            await self.us.notify_if_all_updates_received()\n            lock_task = asyncio.create_task(self._aggregation_done_lock.acquire_async(timeout=timeout))\n            skip_task = asyncio.create_task(self._aggregation_waiting_skip.wait())\n            done, pending = await asyncio.wait(\n                [lock_task, skip_task],\n                return_when=asyncio.FIRST_COMPLETED,\n            )\n            lock_acquired = lock_task in done\n            if skip_task in done:\n                logging.info(\"Skipping aggregation timeout, updates received before grace time\")\n                self._aggregation_waiting_skip.clear()\n                if not lock_acquired:\n                    lock_task.cancel()\n                try:\n                    await lock_task  # Clean cancel\n                except asyncio.CancelledError:\n                    pass\n\n        except TimeoutError:\n            logging.exception(\"\ud83d\udd04  get_aggregation | Timeout reached for aggregation\")\n        except asyncio.CancelledError:\n            logging.exception(\"\ud83d\udd04  get_aggregation | Lock acquisition was cancelled\")\n        except Exception as e:\n            logging.exception(f\"\ud83d\udd04  get_aggregation | Error acquiring lock: {e}\")\n        finally:\n            if lock_acquired or self._aggregation_done_lock.locked():\n                await self._aggregation_done_lock.release_async()\n\n        await self.us.stop_notifying_updates()\n        updates = await self.us.get_round_updates()\n        if not updates:\n            logging.info(f\"\ud83d\udd04  get_aggregation | No updates has been received..resolving conflict to continue...\")\n            updates = {self._addr: await self.engine.resolve_missing_updates()}\n\n        missing_nodes = await self.us.get_round_missing_nodes()\n        if missing_nodes:\n            logging.info(f\"\ud83d\udd04  get_aggregation | Aggregation incomplete, missing models from: {missing_nodes}\")\n        else:\n            logging.info(\"\ud83d\udd04  get_aggregation | All models accounted for, proceeding with aggregation.\")\n\n        agg_event = AggregationEvent(updates, self._federation_nodes, missing_nodes)\n        await EventManager.get_instance().publish_node_event(agg_event)\n        aggregated_result = self.run_aggregation(updates)\n        return aggregated_result\n\n    def print_model_size(self, model):\n        total_memory = 0\n\n        for _, param in model.items():\n            num_params = param.numel()\n            memory_usage = param.element_size() * num_params\n            total_memory += memory_usage\n\n        total_memory_in_mb = total_memory / (1024**2)\n        logging.info(f\"print_model_size | Model size: {total_memory_in_mb} MB\")\n\n    async def notify_all_updates_received(self):\n        self._aggregation_waiting_skip.set()\n</code></pre>"},{"location":"api/core/aggregation/aggregator/#nebula.core.aggregation.aggregator.Aggregator.us","title":"<code>us</code>  <code>property</code>","text":"<p>Federation type UpdateHandler (e.g. DFL-UpdateHandler, CFL-UpdateHandler...)</p>"},{"location":"api/core/aggregation/aggregator/#nebula.core.aggregation.aggregator.Aggregator.get_aggregation","title":"<code>get_aggregation()</code>  <code>async</code>","text":"<p>Handles the aggregation process for a training round.</p> <p>This method waits for all expected model updates from federation nodes or until a timeout occurs. It uses an asynchronous lock to coordinate access and includes an early exit mechanism if all updates are received before the timeout. Once the condition is satisfied, it releases the lock, collects the updates, identifies any missing nodes, and publishes an <code>AggregationEvent</code>. Finally, it runs the aggregation algorithm and returns the result.</p> <p>Returns:</p> Name Type Description <code>Any</code> <p>The result of the aggregation process, as returned by <code>run_aggregation</code>.</p> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If the aggregation lock is not acquired within the defined timeout.</p> <code>CancelledError</code> <p>If the aggregation lock acquisition is cancelled.</p> <code>Exception</code> <p>For any other unexpected errors during the aggregation process.</p> Source code in <code>nebula/core/aggregation/aggregator.py</code> <pre><code>async def get_aggregation(self):\n    \"\"\"\n    Handles the aggregation process for a training round.\n\n    This method waits for all expected model updates from federation nodes or until a timeout occurs.\n    It uses an asynchronous lock to coordinate access and includes an early exit mechanism if all\n    updates are received before the timeout. Once the condition is satisfied, it releases the lock,\n    collects the updates, identifies any missing nodes, and publishes an `AggregationEvent`.\n    Finally, it runs the aggregation algorithm and returns the result.\n\n    Returns:\n        Any: The result of the aggregation process, as returned by `run_aggregation`.\n\n    Raises:\n        TimeoutError: If the aggregation lock is not acquired within the defined timeout.\n        asyncio.CancelledError: If the aggregation lock acquisition is cancelled.\n        Exception: For any other unexpected errors during the aggregation process.\n    \"\"\"            \n    try:\n        timeout = self.config.participant[\"aggregator_args\"][\"aggregation_timeout\"]\n        logging.info(f\"Aggregation timeout: {timeout} starts...\")\n        await self.us.notify_if_all_updates_received()\n        lock_task = asyncio.create_task(self._aggregation_done_lock.acquire_async(timeout=timeout))\n        skip_task = asyncio.create_task(self._aggregation_waiting_skip.wait())\n        done, pending = await asyncio.wait(\n            [lock_task, skip_task],\n            return_when=asyncio.FIRST_COMPLETED,\n        )\n        lock_acquired = lock_task in done\n        if skip_task in done:\n            logging.info(\"Skipping aggregation timeout, updates received before grace time\")\n            self._aggregation_waiting_skip.clear()\n            if not lock_acquired:\n                lock_task.cancel()\n            try:\n                await lock_task  # Clean cancel\n            except asyncio.CancelledError:\n                pass\n\n    except TimeoutError:\n        logging.exception(\"\ud83d\udd04  get_aggregation | Timeout reached for aggregation\")\n    except asyncio.CancelledError:\n        logging.exception(\"\ud83d\udd04  get_aggregation | Lock acquisition was cancelled\")\n    except Exception as e:\n        logging.exception(f\"\ud83d\udd04  get_aggregation | Error acquiring lock: {e}\")\n    finally:\n        if lock_acquired or self._aggregation_done_lock.locked():\n            await self._aggregation_done_lock.release_async()\n\n    await self.us.stop_notifying_updates()\n    updates = await self.us.get_round_updates()\n    if not updates:\n        logging.info(f\"\ud83d\udd04  get_aggregation | No updates has been received..resolving conflict to continue...\")\n        updates = {self._addr: await self.engine.resolve_missing_updates()}\n\n    missing_nodes = await self.us.get_round_missing_nodes()\n    if missing_nodes:\n        logging.info(f\"\ud83d\udd04  get_aggregation | Aggregation incomplete, missing models from: {missing_nodes}\")\n    else:\n        logging.info(\"\ud83d\udd04  get_aggregation | All models accounted for, proceeding with aggregation.\")\n\n    agg_event = AggregationEvent(updates, self._federation_nodes, missing_nodes)\n    await EventManager.get_instance().publish_node_event(agg_event)\n    aggregated_result = self.run_aggregation(updates)\n    return aggregated_result\n</code></pre>"},{"location":"api/core/aggregation/aggregator/#nebula.core.aggregation.aggregator.Aggregator.update_federation_nodes","title":"<code>update_federation_nodes(federation_nodes)</code>  <code>async</code>","text":"<p>Updates the current set of nodes expected to participate in the upcoming aggregation round.</p> <p>This method informs the update handler (<code>us</code>) about the new set of federation nodes,  clears any pending models, and attempts to acquire the aggregation lock to prepare  for model aggregation. If the aggregation process is already running, it releases the lock and tries again to ensure proper cleanup between rounds.</p> <p>Parameters:</p> Name Type Description Default <code>federation_nodes</code> <code>set</code> <p>A set of addresses representing the nodes expected to contribute                      updates for the next aggregation round.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If the aggregation process is already running and the lock cannot be released.</p> Source code in <code>nebula/core/aggregation/aggregator.py</code> <pre><code>async def update_federation_nodes(self, federation_nodes: set):\n    \"\"\"\n    Updates the current set of nodes expected to participate in the upcoming aggregation round.\n\n    This method informs the update handler (`us`) about the new set of federation nodes, \n    clears any pending models, and attempts to acquire the aggregation lock to prepare \n    for model aggregation. If the aggregation process is already running, it releases the lock\n    and tries again to ensure proper cleanup between rounds.\n\n    Args:\n        federation_nodes (set): A set of addresses representing the nodes expected to contribute \n                                updates for the next aggregation round.\n\n    Raises:\n        Exception: If the aggregation process is already running and the lock cannot be released.\n    \"\"\"\n    await self.us.round_expected_updates(federation_nodes=federation_nodes)\n\n    # If the aggregation lock is held, release it to prepare for the new round\n    if self._aggregation_done_lock.locked():\n        logging.info(\"\ud83d\udd04  update_federation_nodes | Aggregation lock is held, releasing for new round\")\n        try:\n            await self._aggregation_done_lock.release_async()\n        except Exception as e:\n            logging.warning(f\"\ud83d\udd04  update_federation_nodes | Error releasing aggregation lock: {e}\")\n            # If we can't release the lock, we might be in the middle of aggregation\n            # In this case, we should wait a bit and try again\n            await asyncio.sleep(0.1)\n            if self._aggregation_done_lock.locked():\n                raise Exception(\"It is not possible to set nodes to aggregate when the aggregation is running.\")\n\n    # Now acquire the lock for the new round\n    self._federation_nodes = federation_nodes\n    self._pending_models_to_aggregate.clear()\n    await self._aggregation_done_lock.acquire_async(\n        timeout=self.config.participant[\"aggregator_args\"][\"aggregation_timeout\"]\n    )\n</code></pre>"},{"location":"api/core/aggregation/fedavg/","title":"Documentation for Fedavg Module","text":""},{"location":"api/core/aggregation/fedavg/#nebula.core.aggregation.fedavg.FedAvg","title":"<code>FedAvg</code>","text":"<p>               Bases: <code>Aggregator</code></p> <p>Aggregator: Federated Averaging (FedAvg) Authors: McMahan et al. Year: 2016</p> Source code in <code>nebula/core/aggregation/fedavg.py</code> <pre><code>class FedAvg(Aggregator):\n    \"\"\"\n    Aggregator: Federated Averaging (FedAvg)\n    Authors: McMahan et al.\n    Year: 2016\n    \"\"\"\n\n    def __init__(self, config=None, **kwargs):\n        super().__init__(config, **kwargs)\n\n    def run_aggregation(self, models):\n        super().run_aggregation(models)\n\n        models = list(models.values())\n\n        total_samples = float(sum(weight for _, weight in models))\n\n        if total_samples == 0:\n            raise ValueError(\"Total number of samples must be greater than zero.\")\n\n        last_model_params = models[-1][0]\n        accum = {layer: torch.zeros_like(param, dtype=torch.float32) for layer, param in last_model_params.items()}\n\n        with torch.no_grad():\n            for model_parameters, weight in models:\n                normalized_weight = weight / total_samples\n                for layer in accum:\n                    accum[layer].add_(\n                        model_parameters[layer].to(accum[layer].dtype),\n                        alpha=normalized_weight,\n                    )\n\n        del models\n        gc.collect()\n\n        # self.print_model_size(accum)\n        return accum\n</code></pre>"},{"location":"api/core/aggregation/krum/","title":"Documentation for Krum Module","text":""},{"location":"api/core/aggregation/krum/#nebula.core.aggregation.krum.Krum","title":"<code>Krum</code>","text":"<p>               Bases: <code>Aggregator</code></p> <p>Aggregator: Krum Authors: Peva Blanchard et al. Year: 2017 Note: https://papers.nips.cc/paper/2017/hash/f4b9ec30ad9f68f89b29639786cb62ef-Abstract.html</p> Source code in <code>nebula/core/aggregation/krum.py</code> <pre><code>class Krum(Aggregator):\n    \"\"\"\n    Aggregator: Krum\n    Authors: Peva Blanchard et al.\n    Year: 2017\n    Note: https://papers.nips.cc/paper/2017/hash/f4b9ec30ad9f68f89b29639786cb62ef-Abstract.html\n    \"\"\"\n\n    def __init__(self, config=None, **kwargs):\n        super().__init__(config, **kwargs)\n\n    def run_aggregation(self, models):\n        super().run_aggregation(models)\n\n        models = list(models.values())\n\n        accum = {layer: torch.zeros_like(param).float() for layer, param in models[-1][0].items()}\n        total_models = len(models)\n        distance_list = [0 for i in range(0, total_models)]\n        min_index = 0\n        min_distance_sum = float(\"inf\")\n\n        for i in range(0, total_models):\n            m1, _ = models[i]\n            for j in range(0, total_models):\n                m2, _ = models[j]\n                distance = 0\n                if i == j:\n                    distance = 0\n                else:\n                    for layer in m1:\n                        l1 = m1[layer]\n\n                        l2 = m2[layer]\n                        distance += numpy.linalg.norm(l1 - l2)\n                distance_list[i] += distance\n\n            if min_distance_sum &gt; distance_list[i]:\n                min_distance_sum = distance_list[i]\n                min_index = i\n        m, _ = models[min_index]\n        for layer in m:\n            accum[layer] = accum[layer] + m[layer]\n\n        return accum\n</code></pre>"},{"location":"api/core/aggregation/median/","title":"Documentation for Median Module","text":""},{"location":"api/core/aggregation/median/#nebula.core.aggregation.median.Median","title":"<code>Median</code>","text":"<p>               Bases: <code>Aggregator</code></p> <p>Aggregator: Median Authors: Dong Yin et al et al. Year: 2021 Note: https://arxiv.org/pdf/1803.01498.pdf</p> Source code in <code>nebula/core/aggregation/median.py</code> <pre><code>class Median(Aggregator):\n    \"\"\"\n    Aggregator: Median\n    Authors: Dong Yin et al et al.\n    Year: 2021\n    Note: https://arxiv.org/pdf/1803.01498.pdf\n    \"\"\"\n\n    def __init__(self, config=None, **kwargs):\n        super().__init__(config, **kwargs)\n\n    def get_median(self, weights):\n        # check if the weight tensor has enough space\n        weight_len = len(weights)\n\n        median = 0\n        if weight_len % 2 == 1:\n            # odd number, return the median\n            median, _ = torch.median(weights, 0)\n        else:\n            # even number, return the mean of median two numbers\n            # sort the tensor\n            arr_weights = np.asarray(weights)\n            nobs = arr_weights.shape[0]\n            start = int(nobs / 2) - 1\n            end = int(nobs / 2) + 1\n            atmp = np.partition(arr_weights, (start, end - 1), 0)\n            sl = [slice(None)] * atmp.ndim\n            sl[0] = slice(start, end)\n            arr_median = np.mean(atmp[tuple(sl)], axis=0)\n            median = torch.tensor(arr_median)\n        return median\n\n    def run_aggregation(self, models):\n        super().run_aggregation(models)\n\n        models = list(models.values())\n        models_params = [m for m, _ in models]\n\n        total_models = len(models)\n\n        accum = {layer: torch.zeros_like(param).float() for layer, param in models[-1][0].items()}\n\n        # Calculate the trimmedmean for each parameter\n        for layer in accum:\n            weight_layer = accum[layer]\n            # get the shape of layer tensor\n            l_shape = list(weight_layer.shape)\n\n            # get the number of elements of layer tensor\n            number_layer_weights = torch.numel(weight_layer)\n            # if its 0-d tensor\n            if l_shape == []:\n                weights = torch.tensor([models_params[j][layer] for j in range(0, total_models)])\n                weights = weights.double()\n                w = self.get_median(weights)\n                accum[layer] = w\n\n            else:\n                # flatten the tensor\n                weight_layer_flatten = weight_layer.view(number_layer_weights)\n\n                # flatten the tensor of each model\n                models_layer_weight_flatten = torch.stack(\n                    [models_params[j][layer].view(number_layer_weights) for j in range(0, total_models)],\n                    0,\n                )\n\n                # get the weight list [w1j,w2j,\u00b7\u00b7\u00b7 ,wmj], where wij is the jth parameter of the ith local model\n                median = self.get_median(models_layer_weight_flatten)\n                accum[layer] = median.view(l_shape)\n        return accum\n</code></pre>"},{"location":"api/core/aggregation/trimmedmean/","title":"Documentation for Trimmedmean Module","text":""},{"location":"api/core/aggregation/trimmedmean/#nebula.core.aggregation.trimmedmean.TrimmedMean","title":"<code>TrimmedMean</code>","text":"<p>               Bases: <code>Aggregator</code></p> <p>Aggregator: TrimmedMean Authors: Dong Yin et al et al. Year: 2021 Note: https://arxiv.org/pdf/1803.01498.pdf</p> Source code in <code>nebula/core/aggregation/trimmedmean.py</code> <pre><code>class TrimmedMean(Aggregator):\n    \"\"\"\n    Aggregator: TrimmedMean\n    Authors: Dong Yin et al et al.\n    Year: 2021\n    Note: https://arxiv.org/pdf/1803.01498.pdf\n    \"\"\"\n\n    def __init__(self, config=None, beta=0, **kwargs):\n        super().__init__(config, **kwargs)\n        self.beta = beta\n\n    def get_trimmedmean(self, weights):\n        # check if the weight tensor has enough space\n        weight_len = len(weights)\n\n        if weight_len &lt;= 2 * self.beta:\n            remaining_wrights = weights\n            res = torch.mean(remaining_wrights, 0)\n\n        else:\n            # remove the largest and smallest \u03b2 items\n            arr_weights = np.asarray(weights)\n            nobs = arr_weights.shape[0]\n            start = self.beta\n            end = nobs - self.beta\n            atmp = np.partition(arr_weights, (start, end - 1), 0)\n            sl = [slice(None)] * atmp.ndim\n            sl[0] = slice(start, end)\n            print(atmp[tuple(sl)])\n            arr_median = np.mean(atmp[tuple(sl)], axis=0)\n            res = torch.tensor(arr_median)\n\n        # get the mean of the remaining weights\n\n        return res\n\n    def run_aggregation(self, models):\n        super().run_aggregation(models)\n\n        models = list(models.values())\n        models_params = [m for m, _ in models]\n\n        total_models = len(models)\n\n        accum = {layer: torch.zeros_like(param).float() for layer, param in models[-1][0].items()}\n\n        for layer in accum:\n            weight_layer = accum[layer]\n            # get the shape of layer tensor\n            l_shape = list(weight_layer.shape)\n\n            # get the number of elements of layer tensor\n            number_layer_weights = torch.numel(weight_layer)\n            # if its 0-d tensor\n            if l_shape == []:\n                weights = torch.tensor([models_params[j][layer] for j in range(0, total_models)])\n                weights = weights.double()\n                w = self.get_trimmedmean(weights)\n                accum[layer] = w\n\n            else:\n                # flatten the tensor\n                weight_layer_flatten = weight_layer.view(number_layer_weights)\n\n                # flatten the tensor of each model\n                models_layer_weight_flatten = torch.stack(\n                    [models_params[j][layer].view(number_layer_weights) for j in range(0, total_models)],\n                    0,\n                )\n\n                # get the weight list [w1j,w2j,\u00b7\u00b7\u00b7 ,wmj], where wij is the jth parameter of the ith local model\n                trimmedmean = self.get_trimmedmean(models_layer_weight_flatten)\n                accum[layer] = trimmedmean.view(l_shape)\n\n        return accum\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/","title":"Documentation for Updatehandlers Module","text":""},{"location":"api/core/aggregation/updatehandlers/cflupdatehandler/","title":"Documentation for Cflupdatehandler Module","text":""},{"location":"api/core/aggregation/updatehandlers/cflupdatehandler/#nebula.core.aggregation.updatehandlers.cflupdatehandler.CFLUpdateHandler","title":"<code>CFLUpdateHandler</code>","text":"<p>               Bases: <code>UpdateHandler</code></p> <p>Handles updates received in a cross-silo/federated learning setup, managing synchronization and aggregation across distributed nodes.</p> <p>Attributes:</p> Name Type Description <code>_aggregator</code> <code>Aggregator</code> <p>Reference to the aggregator managing the global model.</p> <code>_addr</code> <code>str</code> <p>Local address of the node.</p> <code>_buffersize</code> <code>int</code> <p>Max number of updates to store per node.</p> <code>_updates_storage</code> <code>dict</code> <p>Stores received updates per source node.</p> <code>_sources_expected</code> <code>set</code> <p>Set of nodes expected to send updates this round.</p> <code>_sources_received</code> <code>set</code> <p>Set of nodes that have sent updates this round.</p> <code>_missing_ones</code> <code>set</code> <p>Tracks nodes whose updates are missing.</p> <code>_role</code> <code>str</code> <p>Role of this node (e.g., trainer or server).</p> Source code in <code>nebula/core/aggregation/updatehandlers/cflupdatehandler.py</code> <pre><code>class CFLUpdateHandler(UpdateHandler):\n    \"\"\"\n    Handles updates received in a cross-silo/federated learning setup,\n    managing synchronization and aggregation across distributed nodes.\n\n    Attributes:\n        _aggregator (Aggregator): Reference to the aggregator managing the global model.\n        _addr (str): Local address of the node.\n        _buffersize (int): Max number of updates to store per node.\n        _updates_storage (dict): Stores received updates per source node.\n        _sources_expected (set): Set of nodes expected to send updates this round.\n        _sources_received (set): Set of nodes that have sent updates this round.\n        _missing_ones (set): Tracks nodes whose updates are missing.\n        _role (str): Role of this node (e.g., trainer or server).\n    \"\"\"\n\n    def __init__(self, aggregator, addr, buffersize=MAX_UPDATE_BUFFER_SIZE):\n        self._addr = addr\n        self._aggregator: Aggregator = aggregator\n        self._buffersize = buffersize\n        self._updates_storage: dict[str, deque[Update]] = {}\n        self._updates_storage_lock = Locker(name=\"updates_storage_lock\", async_lock=True)\n        self._sources_expected = set()\n        self._sources_received = set()\n        self._round_updates_lock = Locker(\n            name=\"round_updates_lock\", async_lock=True\n        )  # se coge cuando se empieza a comprobar si estan todas las updates\n        self._update_federation_lock = Locker(name=\"update_federation_lock\", async_lock=True)\n        self._notification_sent_lock = Locker(name=\"notification_sent_lock\", async_lock=True)\n        self._notification = False\n        self._missing_ones = set()\n        self._role = \"\"\n\n    @property\n    def us(self):\n        \"\"\"Returns the internal updates storage dictionary.\"\"\"\n        return self._updates_storage\n\n    @property\n    def agg(self):\n        \"\"\"Returns the aggregator instance.\"\"\"\n        return self._aggregator\n\n    async def init(self, config):\n        \"\"\"\n        Initializes the handler with the participant configuration,\n        and subscribes to relevant node events.\n        \"\"\"\n        self._role = config\n        await EventManager.get_instance().subscribe_node_event(UpdateNeighborEvent, self.notify_federation_update)\n        await EventManager.get_instance().subscribe_node_event(UpdateReceivedEvent, self.storage_update)\n\n    async def round_expected_updates(self, federation_nodes: set):\n        \"\"\"\n        Sets the expected nodes for the current training round and updates storage.\n\n        Args:\n            federation_nodes (set): Nodes expected to send updates this round.\n        \"\"\"\n        await self._update_federation_lock.acquire_async()\n        await self._updates_storage_lock.acquire_async()\n        self._sources_expected = federation_nodes.copy()\n        self._sources_received.clear()\n\n        # Initialize new nodes\n        for fn in federation_nodes:\n            if fn not in self.us:\n                self.us[fn] = deque(maxlen=self._buffersize)\n\n        # Clear removed nodes\n        removed_nodes = [node for node in self._updates_storage.keys() if node not in federation_nodes]\n        for rn in removed_nodes:\n            del self._updates_storage[rn]\n\n        await self._updates_storage_lock.release_async()\n        await self._update_federation_lock.release_async()\n\n        # Lock to check if all updates received\n        if self._round_updates_lock.locked():\n            self._round_updates_lock.release_async()\n\n        self._notification = False\n\n    async def storage_update(self, updt_received_event: UpdateReceivedEvent):\n        \"\"\"\n        Stores a received update if it comes from an expected source.\n\n        Args:\n            updt_received_event (UpdateReceivedEvent): The event containing the update.\n        \"\"\"\n        time_received = time.time()\n        (model, weight, source, round, _) = await updt_received_event.get_event_data()\n\n        if source in self._sources_expected:\n            updt = Update(model, weight, source, round, time_received)\n            await self._updates_storage_lock.acquire_async()\n            if updt in self.us[source]:\n                logging.info(f\"Discard | Alerady received update from source: {source} for round: {round}\")\n            else:\n                self.us[source].append(updt)\n                logging.info(\n                    f\"Storage Update | source={source} | round={round} | weight={weight} | federation nodes: {self._sources_expected}\"\n                )\n\n                self._sources_received.add(source)\n                updates_left = self._sources_expected.difference(self._sources_received)\n                logging.info(\n                    f\"Updates received ({len(self._sources_received)}/{len(self._sources_expected)}) | Missing nodes: {updates_left}\"\n                )\n                if self._round_updates_lock.locked() and not updates_left:\n                    all_rec = await self._all_updates_received()\n                    if all_rec:\n                        await self._notify()\n            await self._updates_storage_lock.release_async()\n        else:\n            if source not in self._sources_received:\n                logging.info(f\"Discard update | source: {source} not in expected updates for this Round\")\n\n    async def get_round_updates(self) -&gt; dict[str, tuple[object, float]]:\n        \"\"\"\n        Retrieves the latest updates received this round.\n\n        Returns:\n            dict: Mapping of source to (model, weight) tuples.\n        \"\"\"\n        await self._updates_storage_lock.acquire_async()\n        updates_missing = self._sources_expected.difference(self._sources_received)\n        if updates_missing:\n            self._missing_ones = updates_missing\n            logging.info(f\"Missing updates from sources: {updates_missing}\")\n        updates = {}\n        for sr in self._sources_received:\n            if (\n                self._role == \"trainer\" and len(self._sources_received) &gt; 1\n            ):  # if trainer node ignore self updt if has received udpate from server\n                if sr == self._addr:\n                    continue\n            source_historic = self.us[sr]\n            updt: Update = None\n            updt = source_historic[-1]  # Get last update received\n            updates[sr] = (updt.model, updt.weight)\n        await self._updates_storage_lock.release_async()\n        return updates\n\n    async def notify_federation_update(self, updt_nei_event: UpdateNeighborEvent):\n        \"\"\"\n        Reacts to neighbor updates (e.g., join or leave).\n\n        Args:\n            updt_nei_event (UpdateNeighborEvent): The neighbor update event.\n        \"\"\"\n        source, remove = await updt_nei_event.get_event_data()\n        if not remove:\n            if self._round_updates_lock.locked():\n                logging.info(f\"Source: {source} will be count next round\")\n            else:\n                await self._update_source(source, remove)\n        else:\n            if source not in self._sources_received:  # Not received update from this source yet\n                await self._update_source(source, remove=True)\n                await self._all_updates_received()  # Verify if discarding node aggregation could be done\n            else:\n                logging.info(f\"Already received update from: {source}, it will be discarded next round\")\n\n    async def _update_source(self, source, remove=False):\n        \"\"\"\n        Updates internal tracking for a specific source node.\n\n        Args:\n            source (str): Source node ID.\n            remove (bool): Whether the source should be removed.\n        \"\"\"\n        logging.info(f\"\ud83d\udd04 Update | remove: {remove} | source: {source}\")\n        await self._updates_storage_lock.acquire_async()\n        if remove:\n            self._sources_expected.discard(source)\n        else:\n            self.us[source] = deque(maxlen=self._buffersize)\n            self._sources_expected.add(source)\n        logging.info(f\"federation nodes expected this round: {self._sources_expected}\")\n        await self._updates_storage_lock.release_async()\n\n    async def get_round_missing_nodes(self):\n        \"\"\"\n        Returns nodes whose updates were expected but not received.\n        \"\"\"\n        return self._missing_ones\n\n    async def notify_if_all_updates_received(self):\n        \"\"\"\n        Acquires a lock to notify the aggregator if all updates have been received.\n        \"\"\"\n        logging.info(\"Set notification when all expected updates received\")\n        await self._round_updates_lock.acquire_async()\n        await self._updates_storage_lock.acquire_async()\n        all_received = await self._all_updates_received()\n        await self._updates_storage_lock.release_async()\n        if all_received:\n            await self._notify()\n\n    async def stop_notifying_updates(self):\n        \"\"\"\n        Stops waiting for updates and releases the notification lock if held.\n        \"\"\"\n        if self._round_updates_lock.locked():\n            logging.info(\"Stop notification updates\")\n            await self._round_updates_lock.release_async()\n\n    async def _notify(self):\n        \"\"\"\n        Notifies the aggregator that all updates have been received.\n        \"\"\"\n        await self._notification_sent_lock.acquire_async()\n        if self._notification:\n            await self._notification_sent_lock.release_async()\n            return\n        self._notification = True\n        await self.stop_notifying_updates()\n        await self._notification_sent_lock.release_async()\n        logging.info(\"\ud83d\udd04 Notifying aggregator to release aggregation\")\n        await self.agg.notify_all_updates_received()\n\n    async def _all_updates_received(self):\n        \"\"\"\n        Checks if updates from all expected nodes have been received.\n\n        Returns:\n            bool: True if all updates are received, False otherwise.\n        \"\"\"\n        updates_left = self._sources_expected.difference(self._sources_received)\n        all_received = False\n        if len(updates_left) == 0:\n            logging.info(\"All updates have been received this round\")\n            await self._round_updates_lock.release_async()\n            all_received = True\n        return all_received\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/cflupdatehandler/#nebula.core.aggregation.updatehandlers.cflupdatehandler.CFLUpdateHandler.agg","title":"<code>agg</code>  <code>property</code>","text":"<p>Returns the aggregator instance.</p>"},{"location":"api/core/aggregation/updatehandlers/cflupdatehandler/#nebula.core.aggregation.updatehandlers.cflupdatehandler.CFLUpdateHandler.us","title":"<code>us</code>  <code>property</code>","text":"<p>Returns the internal updates storage dictionary.</p>"},{"location":"api/core/aggregation/updatehandlers/cflupdatehandler/#nebula.core.aggregation.updatehandlers.cflupdatehandler.CFLUpdateHandler.get_round_missing_nodes","title":"<code>get_round_missing_nodes()</code>  <code>async</code>","text":"<p>Returns nodes whose updates were expected but not received.</p> Source code in <code>nebula/core/aggregation/updatehandlers/cflupdatehandler.py</code> <pre><code>async def get_round_missing_nodes(self):\n    \"\"\"\n    Returns nodes whose updates were expected but not received.\n    \"\"\"\n    return self._missing_ones\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/cflupdatehandler/#nebula.core.aggregation.updatehandlers.cflupdatehandler.CFLUpdateHandler.get_round_updates","title":"<code>get_round_updates()</code>  <code>async</code>","text":"<p>Retrieves the latest updates received this round.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, tuple[object, float]]</code> <p>Mapping of source to (model, weight) tuples.</p> Source code in <code>nebula/core/aggregation/updatehandlers/cflupdatehandler.py</code> <pre><code>async def get_round_updates(self) -&gt; dict[str, tuple[object, float]]:\n    \"\"\"\n    Retrieves the latest updates received this round.\n\n    Returns:\n        dict: Mapping of source to (model, weight) tuples.\n    \"\"\"\n    await self._updates_storage_lock.acquire_async()\n    updates_missing = self._sources_expected.difference(self._sources_received)\n    if updates_missing:\n        self._missing_ones = updates_missing\n        logging.info(f\"Missing updates from sources: {updates_missing}\")\n    updates = {}\n    for sr in self._sources_received:\n        if (\n            self._role == \"trainer\" and len(self._sources_received) &gt; 1\n        ):  # if trainer node ignore self updt if has received udpate from server\n            if sr == self._addr:\n                continue\n        source_historic = self.us[sr]\n        updt: Update = None\n        updt = source_historic[-1]  # Get last update received\n        updates[sr] = (updt.model, updt.weight)\n    await self._updates_storage_lock.release_async()\n    return updates\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/cflupdatehandler/#nebula.core.aggregation.updatehandlers.cflupdatehandler.CFLUpdateHandler.init","title":"<code>init(config)</code>  <code>async</code>","text":"<p>Initializes the handler with the participant configuration, and subscribes to relevant node events.</p> Source code in <code>nebula/core/aggregation/updatehandlers/cflupdatehandler.py</code> <pre><code>async def init(self, config):\n    \"\"\"\n    Initializes the handler with the participant configuration,\n    and subscribes to relevant node events.\n    \"\"\"\n    self._role = config\n    await EventManager.get_instance().subscribe_node_event(UpdateNeighborEvent, self.notify_federation_update)\n    await EventManager.get_instance().subscribe_node_event(UpdateReceivedEvent, self.storage_update)\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/cflupdatehandler/#nebula.core.aggregation.updatehandlers.cflupdatehandler.CFLUpdateHandler.notify_federation_update","title":"<code>notify_federation_update(updt_nei_event)</code>  <code>async</code>","text":"<p>Reacts to neighbor updates (e.g., join or leave).</p> <p>Parameters:</p> Name Type Description Default <code>updt_nei_event</code> <code>UpdateNeighborEvent</code> <p>The neighbor update event.</p> required Source code in <code>nebula/core/aggregation/updatehandlers/cflupdatehandler.py</code> <pre><code>async def notify_federation_update(self, updt_nei_event: UpdateNeighborEvent):\n    \"\"\"\n    Reacts to neighbor updates (e.g., join or leave).\n\n    Args:\n        updt_nei_event (UpdateNeighborEvent): The neighbor update event.\n    \"\"\"\n    source, remove = await updt_nei_event.get_event_data()\n    if not remove:\n        if self._round_updates_lock.locked():\n            logging.info(f\"Source: {source} will be count next round\")\n        else:\n            await self._update_source(source, remove)\n    else:\n        if source not in self._sources_received:  # Not received update from this source yet\n            await self._update_source(source, remove=True)\n            await self._all_updates_received()  # Verify if discarding node aggregation could be done\n        else:\n            logging.info(f\"Already received update from: {source}, it will be discarded next round\")\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/cflupdatehandler/#nebula.core.aggregation.updatehandlers.cflupdatehandler.CFLUpdateHandler.notify_if_all_updates_received","title":"<code>notify_if_all_updates_received()</code>  <code>async</code>","text":"<p>Acquires a lock to notify the aggregator if all updates have been received.</p> Source code in <code>nebula/core/aggregation/updatehandlers/cflupdatehandler.py</code> <pre><code>async def notify_if_all_updates_received(self):\n    \"\"\"\n    Acquires a lock to notify the aggregator if all updates have been received.\n    \"\"\"\n    logging.info(\"Set notification when all expected updates received\")\n    await self._round_updates_lock.acquire_async()\n    await self._updates_storage_lock.acquire_async()\n    all_received = await self._all_updates_received()\n    await self._updates_storage_lock.release_async()\n    if all_received:\n        await self._notify()\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/cflupdatehandler/#nebula.core.aggregation.updatehandlers.cflupdatehandler.CFLUpdateHandler.round_expected_updates","title":"<code>round_expected_updates(federation_nodes)</code>  <code>async</code>","text":"<p>Sets the expected nodes for the current training round and updates storage.</p> <p>Parameters:</p> Name Type Description Default <code>federation_nodes</code> <code>set</code> <p>Nodes expected to send updates this round.</p> required Source code in <code>nebula/core/aggregation/updatehandlers/cflupdatehandler.py</code> <pre><code>async def round_expected_updates(self, federation_nodes: set):\n    \"\"\"\n    Sets the expected nodes for the current training round and updates storage.\n\n    Args:\n        federation_nodes (set): Nodes expected to send updates this round.\n    \"\"\"\n    await self._update_federation_lock.acquire_async()\n    await self._updates_storage_lock.acquire_async()\n    self._sources_expected = federation_nodes.copy()\n    self._sources_received.clear()\n\n    # Initialize new nodes\n    for fn in federation_nodes:\n        if fn not in self.us:\n            self.us[fn] = deque(maxlen=self._buffersize)\n\n    # Clear removed nodes\n    removed_nodes = [node for node in self._updates_storage.keys() if node not in federation_nodes]\n    for rn in removed_nodes:\n        del self._updates_storage[rn]\n\n    await self._updates_storage_lock.release_async()\n    await self._update_federation_lock.release_async()\n\n    # Lock to check if all updates received\n    if self._round_updates_lock.locked():\n        self._round_updates_lock.release_async()\n\n    self._notification = False\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/cflupdatehandler/#nebula.core.aggregation.updatehandlers.cflupdatehandler.CFLUpdateHandler.stop_notifying_updates","title":"<code>stop_notifying_updates()</code>  <code>async</code>","text":"<p>Stops waiting for updates and releases the notification lock if held.</p> Source code in <code>nebula/core/aggregation/updatehandlers/cflupdatehandler.py</code> <pre><code>async def stop_notifying_updates(self):\n    \"\"\"\n    Stops waiting for updates and releases the notification lock if held.\n    \"\"\"\n    if self._round_updates_lock.locked():\n        logging.info(\"Stop notification updates\")\n        await self._round_updates_lock.release_async()\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/cflupdatehandler/#nebula.core.aggregation.updatehandlers.cflupdatehandler.CFLUpdateHandler.storage_update","title":"<code>storage_update(updt_received_event)</code>  <code>async</code>","text":"<p>Stores a received update if it comes from an expected source.</p> <p>Parameters:</p> Name Type Description Default <code>updt_received_event</code> <code>UpdateReceivedEvent</code> <p>The event containing the update.</p> required Source code in <code>nebula/core/aggregation/updatehandlers/cflupdatehandler.py</code> <pre><code>async def storage_update(self, updt_received_event: UpdateReceivedEvent):\n    \"\"\"\n    Stores a received update if it comes from an expected source.\n\n    Args:\n        updt_received_event (UpdateReceivedEvent): The event containing the update.\n    \"\"\"\n    time_received = time.time()\n    (model, weight, source, round, _) = await updt_received_event.get_event_data()\n\n    if source in self._sources_expected:\n        updt = Update(model, weight, source, round, time_received)\n        await self._updates_storage_lock.acquire_async()\n        if updt in self.us[source]:\n            logging.info(f\"Discard | Alerady received update from source: {source} for round: {round}\")\n        else:\n            self.us[source].append(updt)\n            logging.info(\n                f\"Storage Update | source={source} | round={round} | weight={weight} | federation nodes: {self._sources_expected}\"\n            )\n\n            self._sources_received.add(source)\n            updates_left = self._sources_expected.difference(self._sources_received)\n            logging.info(\n                f\"Updates received ({len(self._sources_received)}/{len(self._sources_expected)}) | Missing nodes: {updates_left}\"\n            )\n            if self._round_updates_lock.locked() and not updates_left:\n                all_rec = await self._all_updates_received()\n                if all_rec:\n                    await self._notify()\n        await self._updates_storage_lock.release_async()\n    else:\n        if source not in self._sources_received:\n            logging.info(f\"Discard update | source: {source} not in expected updates for this Round\")\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/cflupdatehandler/#nebula.core.aggregation.updatehandlers.cflupdatehandler.Update","title":"<code>Update</code>","text":"<p>Represents a model update received from a node in a specific training round.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>object</code> <p>The model object or weights received.</p> <code>weight</code> <code>float</code> <p>The weight or importance of the update.</p> <code>source</code> <code>str</code> <p>Identifier of the node that sent the update.</p> <code>round</code> <code>int</code> <p>Training round this update belongs to.</p> <code>time_received</code> <code>float</code> <p>Timestamp when the update was received.</p> Source code in <code>nebula/core/aggregation/updatehandlers/cflupdatehandler.py</code> <pre><code>class Update:\n    \"\"\"\n    Represents a model update received from a node in a specific training round.\n\n    Attributes:\n        model (object): The model object or weights received.\n        weight (float): The weight or importance of the update.\n        source (str): Identifier of the node that sent the update.\n        round (int): Training round this update belongs to.\n        time_received (float): Timestamp when the update was received.\n    \"\"\"\n    def __init__(self, model, weight, source, round, time_received):\n        self.model = model\n        self.weight = weight\n        self.source = source\n        self.round = round\n        self.time_received = time_received\n\n    def __eq__(self, other):\n        \"\"\"\n        Checks if two updates belong to the same round.\n        \"\"\"\n        return self.round == other.round\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/cflupdatehandler/#nebula.core.aggregation.updatehandlers.cflupdatehandler.Update.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Checks if two updates belong to the same round.</p> Source code in <code>nebula/core/aggregation/updatehandlers/cflupdatehandler.py</code> <pre><code>def __eq__(self, other):\n    \"\"\"\n    Checks if two updates belong to the same round.\n    \"\"\"\n    return self.round == other.round\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/dflupdatehandler/","title":"Documentation for Dflupdatehandler Module","text":""},{"location":"api/core/aggregation/updatehandlers/dflupdatehandler/#nebula.core.aggregation.updatehandlers.dflupdatehandler.DFLUpdateHandler","title":"<code>DFLUpdateHandler</code>","text":"<p>               Bases: <code>UpdateHandler</code></p> <p>Distributed Federated Learning (DFL) Update Handler.</p> <p>This handler manages the reception, storage, and tracking of model updates from federation nodes during asynchronous rounds. It supports partial updates, late arrivals, and maintains update history.</p> Source code in <code>nebula/core/aggregation/updatehandlers/dflupdatehandler.py</code> <pre><code>class DFLUpdateHandler(UpdateHandler):\n    \"\"\"\n    Distributed Federated Learning (DFL) Update Handler.\n\n    This handler manages the reception, storage, and tracking of model updates from federation nodes\n    during asynchronous rounds. It supports partial updates, late arrivals, and maintains update history.\n    \"\"\"\n\n    def __init__(self, aggregator, addr, buffersize=MAX_UPDATE_BUFFER_SIZE):\n        \"\"\"\n        Initialize the update handler with required locks and storage.\n\n        Args:\n            aggregator (Aggregator): Aggregator instance for the federation.\n            addr (str): Address of the local node.\n            buffersize (int): Maximum number of historical updates to keep per node.\n        \"\"\"\n        self._addr = addr\n        self._aggregator: Aggregator = aggregator\n        self._buffersize = buffersize\n        self._updates_storage: dict[str, tuple[Update, deque[Update]]] = {}\n        self._updates_storage_lock = Locker(name=\"updates_storage_lock\", async_lock=True)\n        self._sources_expected = set()\n        self._sources_received = set()\n        self._round_updates_lock = Locker(name=\"round_updates_lock\", async_lock=True)\n        self._update_federation_lock = Locker(name=\"update_federation_lock\", async_lock=True)\n        self._notification_sent_lock = Locker(name=\"notification_sent_lock\", async_lock=True)\n        self._notification = False\n        self._missing_ones = set()\n        self._nodes_using_historic = set()\n\n    @property\n    def us(self):\n        \"\"\"Returns the internal updates storage dictionary.\"\"\"\n        return self._updates_storage\n\n    @property\n    def agg(self):\n        \"\"\"Returns the aggregator instance.\"\"\"\n        return self._aggregator\n\n    async def init(self, config=None):\n        \"\"\"\n        Subscribe to update-related events from the event manager.\n        \"\"\"\n        await EventManager.get_instance().subscribe_node_event(UpdateNeighborEvent, self.notify_federation_update)\n        await EventManager.get_instance().subscribe_node_event(UpdateReceivedEvent, self.storage_update)\n\n    async def round_expected_updates(self, federation_nodes: set):\n        \"\"\"\n        Define which nodes are expected to send updates in this round and reset internal state.\n\n        Args:\n            federation_nodes (set): Set of node IDs expected to participate this round.\n        \"\"\"\n        await self._update_federation_lock.acquire_async()\n        await self._updates_storage_lock.acquire_async()\n        self._sources_expected = federation_nodes.copy()\n        self._sources_received.clear()\n\n        # Initialize new nodes\n        for fn in federation_nodes:\n            if fn not in self.us:\n                self.us[fn] = (None, deque(maxlen=self._buffersize))\n\n        # Clear removed nodes\n        removed_nodes = [node for node in self._updates_storage.keys() if node not in federation_nodes]\n        for rn in removed_nodes:\n            del self._updates_storage[rn]\n\n        # Check already received updates\n        await self._check_updates_already_received()\n\n        await self._updates_storage_lock.release_async()\n        await self._update_federation_lock.release_async()\n\n        # Lock to check if all updates received\n        if self._round_updates_lock.locked():\n            self._round_updates_lock.release_async()\n\n        self._notification = False\n\n    async def _check_updates_already_received(self):\n        \"\"\"\n        Scan storage for updates already received in this round.\n        \"\"\"\n        for se in self._sources_expected:\n            (last_updt, node_storage) = self._updates_storage[se]\n            if len(node_storage):\n                try:\n                    if (last_updt and node_storage[-1] and last_updt != node_storage[-1]) or (\n                        node_storage[-1] and not last_updt\n                    ):\n                        self._sources_received.add(se)\n                        logging.info(\n                            f\"Update already received from source: {se} | ({len(self._sources_received)}/{len(self._sources_expected)}) Updates received\"\n                        )\n                except:\n                    logging.exception(\n                        f\"ERROR: source expected: {se} | last_update None: {(True if not last_updt else False)}, last update storaged None: {(True if not node_storage[-1] else False)}\"\n                    )\n\n    async def storage_update(self, updt_received_event: UpdateReceivedEvent):\n        \"\"\"\n        Store an incoming update and trigger aggregation if all updates are received.\n\n        Args:\n            updt_received_event (UpdateReceivedEvent): Event with model update data.\n        \"\"\"\n        time_received = time.time()\n        (model, weight, source, round, _) = await updt_received_event.get_event_data()\n        if source in self._sources_expected:\n            updt = Update(model, weight, source, round, time_received)\n            await self._updates_storage_lock.acquire_async()\n            if updt in self.us[source][1]:\n                logging.info(f\"Discard | Alerady received update from source: {source} for round: {round}\")\n            else:\n                last_update_used = self.us[source][0]\n                self.us[source][1].append(updt)\n                self.us[source] = (last_update_used, self.us[source][1])\n                logging.info(\n                    f\"Storage Update | source={source} | round={round} | weight={weight} | federation nodes: {self._sources_expected}\"\n                )\n\n                self._sources_received.add(source)\n                updates_left = self._sources_expected.difference(self._sources_received)\n                logging.info(\n                    f\"Updates received ({len(self._sources_received)}/{len(self._sources_expected)}) | Missing nodes: {updates_left}\"\n                )\n                if self._round_updates_lock.locked() and not updates_left:\n                    all_rec = await self._all_updates_received()\n                    if all_rec:\n                        await self._notify()\n            await self._updates_storage_lock.release_async()\n        else:\n            if source not in self._sources_received:\n                logging.info(f\"Discard update | source: {source} not in expected updates for this Round\")\n\n    async def get_round_updates(self):\n        \"\"\"\n        Retrieve the most recent valid updates for this round, filling gaps if needed.\n\n        Returns:\n            dict: A dictionary mapping node ID to (model, weight) tuples.\n        \"\"\"\n        await self._updates_storage_lock.acquire_async()\n        updates_missing = self._sources_expected.difference(self._sources_received)\n        if updates_missing:\n            self._missing_ones = updates_missing\n            logging.info(f\"Missing updates from sources: {updates_missing}\")\n        else:\n            self._missing_ones.clear()\n\n        self._nodes_using_historic.clear()\n        updates = {}\n        for sr in self._sources_received:\n            source_historic = self.us[sr][1]\n            last_updt_received = self.us[sr][0]\n            updt: Update = None\n            updt = source_historic[-1]  # Get last update received\n            if last_updt_received and last_updt_received == updt:\n                logging.info(f\"Missing update from source: {sr}, using last update received..\")\n                self._nodes_using_historic.add(sr)\n            else:\n                last_updt_received = updt\n                self.us[sr] = (last_updt_received, source_historic)  # Update storage with new last update used\n            updates[sr] = (updt.model, updt.weight)\n\n        await self._updates_storage_lock.release_async()\n        return updates\n\n    async def notify_federation_update(self, updt_nei_event: UpdateNeighborEvent):\n        \"\"\"\n        Handle federation node join/leave events.\n\n        Args:\n            updt_nei_event (UpdateNeighborEvent): Event with neighbor update data.\n        \"\"\"\n        source, remove = await updt_nei_event.get_event_data()\n        if not remove:\n            if self._round_updates_lock.locked():\n                logging.info(f\"Source: {source} will be count next round\")\n            else:\n                await self._update_source(source, remove)\n        else:\n            if source not in self._sources_received:  # Not received update from this source yet\n                await self._update_source(source, remove=True)\n                all_rec = await self._all_updates_received()  # Verify if discarding node aggregation could be done\n                if all_rec:\n                    await self._notify()\n            else:\n                logging.info(f\"Already received update from: {source}, it will be discarded next round\")\n\n    async def _update_source(self, source, remove=False):\n        \"\"\"\n        Add or remove a node from the expected sources.\n\n        Args:\n            source (str): Node ID.\n            remove (bool): Whether to remove the node from the expected list.\n        \"\"\"\n        logging.info(f\"\ud83d\udd04 Update | remove: {remove} | source: {source}\")\n        await self._updates_storage_lock.acquire_async()\n        if remove:\n            self._sources_expected.discard(source)\n        else:\n            self.us[source] = (None, deque(maxlen=self._buffersize))\n            self._sources_expected.add(source)\n        logging.info(f\"federation nodes expected this round: {self._sources_expected}\")\n        await self._updates_storage_lock.release_async()\n\n    async def get_round_missing_nodes(self):\n        \"\"\"\n        Return the set of nodes whose updates were not received this round.\n\n        Returns:\n            set: Missing node IDs.\n        \"\"\"\n        return self._missing_ones\n\n    async def notify_if_all_updates_received(self):\n        \"\"\"\n        Set a notification trigger and notify aggregator if all updates are already received.\n        \"\"\"\n        logging.info(\"Set notification when all expected updates received\")\n        await self._round_updates_lock.acquire_async()\n        await self._updates_storage_lock.acquire_async()\n        all_received = await self._all_updates_received()\n        await self._updates_storage_lock.release_async()\n        if all_received:\n            await self._notify()\n\n    async def stop_notifying_updates(self):\n        \"\"\"\n        Cancel any notification triggers for update reception.\n        \"\"\"\n        if self._round_updates_lock.locked():\n            logging.info(\"Stop notification updates\")\n            await self._round_updates_lock.release_async()\n\n    async def _notify(self):\n        \"\"\"\n        Notify the aggregator that all expected updates have been received.\n        \"\"\"\n        await self._notification_sent_lock.acquire_async()\n        if self._notification:\n            await self._notification_sent_lock.release_async()\n            return\n        self._notification = True\n        await self.stop_notifying_updates()\n        await self._notification_sent_lock.release_async()\n        logging.info(\"\ud83d\udd04 Notifying aggregator to release aggregation\")\n        await self.agg.notify_all_updates_received()\n\n    async def _all_updates_received(self):\n        \"\"\"\n        Check if all expected updates have been received.\n\n        Returns:\n            bool: True if no updates are missing.\n        \"\"\"\n        updates_left = self._sources_expected.difference(self._sources_received)\n        all_received = False\n        if len(updates_left) == 0:\n            logging.info(\"All updates have been received this round\")\n            if await self._round_updates_lock.locked_async():\n                await self._round_updates_lock.release_async()\n            all_received = True\n        return all_received\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/dflupdatehandler/#nebula.core.aggregation.updatehandlers.dflupdatehandler.DFLUpdateHandler.agg","title":"<code>agg</code>  <code>property</code>","text":"<p>Returns the aggregator instance.</p>"},{"location":"api/core/aggregation/updatehandlers/dflupdatehandler/#nebula.core.aggregation.updatehandlers.dflupdatehandler.DFLUpdateHandler.us","title":"<code>us</code>  <code>property</code>","text":"<p>Returns the internal updates storage dictionary.</p>"},{"location":"api/core/aggregation/updatehandlers/dflupdatehandler/#nebula.core.aggregation.updatehandlers.dflupdatehandler.DFLUpdateHandler.__init__","title":"<code>__init__(aggregator, addr, buffersize=MAX_UPDATE_BUFFER_SIZE)</code>","text":"<p>Initialize the update handler with required locks and storage.</p> <p>Parameters:</p> Name Type Description Default <code>aggregator</code> <code>Aggregator</code> <p>Aggregator instance for the federation.</p> required <code>addr</code> <code>str</code> <p>Address of the local node.</p> required <code>buffersize</code> <code>int</code> <p>Maximum number of historical updates to keep per node.</p> <code>MAX_UPDATE_BUFFER_SIZE</code> Source code in <code>nebula/core/aggregation/updatehandlers/dflupdatehandler.py</code> <pre><code>def __init__(self, aggregator, addr, buffersize=MAX_UPDATE_BUFFER_SIZE):\n    \"\"\"\n    Initialize the update handler with required locks and storage.\n\n    Args:\n        aggregator (Aggregator): Aggregator instance for the federation.\n        addr (str): Address of the local node.\n        buffersize (int): Maximum number of historical updates to keep per node.\n    \"\"\"\n    self._addr = addr\n    self._aggregator: Aggregator = aggregator\n    self._buffersize = buffersize\n    self._updates_storage: dict[str, tuple[Update, deque[Update]]] = {}\n    self._updates_storage_lock = Locker(name=\"updates_storage_lock\", async_lock=True)\n    self._sources_expected = set()\n    self._sources_received = set()\n    self._round_updates_lock = Locker(name=\"round_updates_lock\", async_lock=True)\n    self._update_federation_lock = Locker(name=\"update_federation_lock\", async_lock=True)\n    self._notification_sent_lock = Locker(name=\"notification_sent_lock\", async_lock=True)\n    self._notification = False\n    self._missing_ones = set()\n    self._nodes_using_historic = set()\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/dflupdatehandler/#nebula.core.aggregation.updatehandlers.dflupdatehandler.DFLUpdateHandler.get_round_missing_nodes","title":"<code>get_round_missing_nodes()</code>  <code>async</code>","text":"<p>Return the set of nodes whose updates were not received this round.</p> <p>Returns:</p> Name Type Description <code>set</code> <p>Missing node IDs.</p> Source code in <code>nebula/core/aggregation/updatehandlers/dflupdatehandler.py</code> <pre><code>async def get_round_missing_nodes(self):\n    \"\"\"\n    Return the set of nodes whose updates were not received this round.\n\n    Returns:\n        set: Missing node IDs.\n    \"\"\"\n    return self._missing_ones\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/dflupdatehandler/#nebula.core.aggregation.updatehandlers.dflupdatehandler.DFLUpdateHandler.get_round_updates","title":"<code>get_round_updates()</code>  <code>async</code>","text":"<p>Retrieve the most recent valid updates for this round, filling gaps if needed.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary mapping node ID to (model, weight) tuples.</p> Source code in <code>nebula/core/aggregation/updatehandlers/dflupdatehandler.py</code> <pre><code>async def get_round_updates(self):\n    \"\"\"\n    Retrieve the most recent valid updates for this round, filling gaps if needed.\n\n    Returns:\n        dict: A dictionary mapping node ID to (model, weight) tuples.\n    \"\"\"\n    await self._updates_storage_lock.acquire_async()\n    updates_missing = self._sources_expected.difference(self._sources_received)\n    if updates_missing:\n        self._missing_ones = updates_missing\n        logging.info(f\"Missing updates from sources: {updates_missing}\")\n    else:\n        self._missing_ones.clear()\n\n    self._nodes_using_historic.clear()\n    updates = {}\n    for sr in self._sources_received:\n        source_historic = self.us[sr][1]\n        last_updt_received = self.us[sr][0]\n        updt: Update = None\n        updt = source_historic[-1]  # Get last update received\n        if last_updt_received and last_updt_received == updt:\n            logging.info(f\"Missing update from source: {sr}, using last update received..\")\n            self._nodes_using_historic.add(sr)\n        else:\n            last_updt_received = updt\n            self.us[sr] = (last_updt_received, source_historic)  # Update storage with new last update used\n        updates[sr] = (updt.model, updt.weight)\n\n    await self._updates_storage_lock.release_async()\n    return updates\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/dflupdatehandler/#nebula.core.aggregation.updatehandlers.dflupdatehandler.DFLUpdateHandler.init","title":"<code>init(config=None)</code>  <code>async</code>","text":"<p>Subscribe to update-related events from the event manager.</p> Source code in <code>nebula/core/aggregation/updatehandlers/dflupdatehandler.py</code> <pre><code>async def init(self, config=None):\n    \"\"\"\n    Subscribe to update-related events from the event manager.\n    \"\"\"\n    await EventManager.get_instance().subscribe_node_event(UpdateNeighborEvent, self.notify_federation_update)\n    await EventManager.get_instance().subscribe_node_event(UpdateReceivedEvent, self.storage_update)\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/dflupdatehandler/#nebula.core.aggregation.updatehandlers.dflupdatehandler.DFLUpdateHandler.notify_federation_update","title":"<code>notify_federation_update(updt_nei_event)</code>  <code>async</code>","text":"<p>Handle federation node join/leave events.</p> <p>Parameters:</p> Name Type Description Default <code>updt_nei_event</code> <code>UpdateNeighborEvent</code> <p>Event with neighbor update data.</p> required Source code in <code>nebula/core/aggregation/updatehandlers/dflupdatehandler.py</code> <pre><code>async def notify_federation_update(self, updt_nei_event: UpdateNeighborEvent):\n    \"\"\"\n    Handle federation node join/leave events.\n\n    Args:\n        updt_nei_event (UpdateNeighborEvent): Event with neighbor update data.\n    \"\"\"\n    source, remove = await updt_nei_event.get_event_data()\n    if not remove:\n        if self._round_updates_lock.locked():\n            logging.info(f\"Source: {source} will be count next round\")\n        else:\n            await self._update_source(source, remove)\n    else:\n        if source not in self._sources_received:  # Not received update from this source yet\n            await self._update_source(source, remove=True)\n            all_rec = await self._all_updates_received()  # Verify if discarding node aggregation could be done\n            if all_rec:\n                await self._notify()\n        else:\n            logging.info(f\"Already received update from: {source}, it will be discarded next round\")\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/dflupdatehandler/#nebula.core.aggregation.updatehandlers.dflupdatehandler.DFLUpdateHandler.notify_if_all_updates_received","title":"<code>notify_if_all_updates_received()</code>  <code>async</code>","text":"<p>Set a notification trigger and notify aggregator if all updates are already received.</p> Source code in <code>nebula/core/aggregation/updatehandlers/dflupdatehandler.py</code> <pre><code>async def notify_if_all_updates_received(self):\n    \"\"\"\n    Set a notification trigger and notify aggregator if all updates are already received.\n    \"\"\"\n    logging.info(\"Set notification when all expected updates received\")\n    await self._round_updates_lock.acquire_async()\n    await self._updates_storage_lock.acquire_async()\n    all_received = await self._all_updates_received()\n    await self._updates_storage_lock.release_async()\n    if all_received:\n        await self._notify()\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/dflupdatehandler/#nebula.core.aggregation.updatehandlers.dflupdatehandler.DFLUpdateHandler.round_expected_updates","title":"<code>round_expected_updates(federation_nodes)</code>  <code>async</code>","text":"<p>Define which nodes are expected to send updates in this round and reset internal state.</p> <p>Parameters:</p> Name Type Description Default <code>federation_nodes</code> <code>set</code> <p>Set of node IDs expected to participate this round.</p> required Source code in <code>nebula/core/aggregation/updatehandlers/dflupdatehandler.py</code> <pre><code>async def round_expected_updates(self, federation_nodes: set):\n    \"\"\"\n    Define which nodes are expected to send updates in this round and reset internal state.\n\n    Args:\n        federation_nodes (set): Set of node IDs expected to participate this round.\n    \"\"\"\n    await self._update_federation_lock.acquire_async()\n    await self._updates_storage_lock.acquire_async()\n    self._sources_expected = federation_nodes.copy()\n    self._sources_received.clear()\n\n    # Initialize new nodes\n    for fn in federation_nodes:\n        if fn not in self.us:\n            self.us[fn] = (None, deque(maxlen=self._buffersize))\n\n    # Clear removed nodes\n    removed_nodes = [node for node in self._updates_storage.keys() if node not in federation_nodes]\n    for rn in removed_nodes:\n        del self._updates_storage[rn]\n\n    # Check already received updates\n    await self._check_updates_already_received()\n\n    await self._updates_storage_lock.release_async()\n    await self._update_federation_lock.release_async()\n\n    # Lock to check if all updates received\n    if self._round_updates_lock.locked():\n        self._round_updates_lock.release_async()\n\n    self._notification = False\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/dflupdatehandler/#nebula.core.aggregation.updatehandlers.dflupdatehandler.DFLUpdateHandler.stop_notifying_updates","title":"<code>stop_notifying_updates()</code>  <code>async</code>","text":"<p>Cancel any notification triggers for update reception.</p> Source code in <code>nebula/core/aggregation/updatehandlers/dflupdatehandler.py</code> <pre><code>async def stop_notifying_updates(self):\n    \"\"\"\n    Cancel any notification triggers for update reception.\n    \"\"\"\n    if self._round_updates_lock.locked():\n        logging.info(\"Stop notification updates\")\n        await self._round_updates_lock.release_async()\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/dflupdatehandler/#nebula.core.aggregation.updatehandlers.dflupdatehandler.DFLUpdateHandler.storage_update","title":"<code>storage_update(updt_received_event)</code>  <code>async</code>","text":"<p>Store an incoming update and trigger aggregation if all updates are received.</p> <p>Parameters:</p> Name Type Description Default <code>updt_received_event</code> <code>UpdateReceivedEvent</code> <p>Event with model update data.</p> required Source code in <code>nebula/core/aggregation/updatehandlers/dflupdatehandler.py</code> <pre><code>async def storage_update(self, updt_received_event: UpdateReceivedEvent):\n    \"\"\"\n    Store an incoming update and trigger aggregation if all updates are received.\n\n    Args:\n        updt_received_event (UpdateReceivedEvent): Event with model update data.\n    \"\"\"\n    time_received = time.time()\n    (model, weight, source, round, _) = await updt_received_event.get_event_data()\n    if source in self._sources_expected:\n        updt = Update(model, weight, source, round, time_received)\n        await self._updates_storage_lock.acquire_async()\n        if updt in self.us[source][1]:\n            logging.info(f\"Discard | Alerady received update from source: {source} for round: {round}\")\n        else:\n            last_update_used = self.us[source][0]\n            self.us[source][1].append(updt)\n            self.us[source] = (last_update_used, self.us[source][1])\n            logging.info(\n                f\"Storage Update | source={source} | round={round} | weight={weight} | federation nodes: {self._sources_expected}\"\n            )\n\n            self._sources_received.add(source)\n            updates_left = self._sources_expected.difference(self._sources_received)\n            logging.info(\n                f\"Updates received ({len(self._sources_received)}/{len(self._sources_expected)}) | Missing nodes: {updates_left}\"\n            )\n            if self._round_updates_lock.locked() and not updates_left:\n                all_rec = await self._all_updates_received()\n                if all_rec:\n                    await self._notify()\n        await self._updates_storage_lock.release_async()\n    else:\n        if source not in self._sources_received:\n            logging.info(f\"Discard update | source: {source} not in expected updates for this Round\")\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/dflupdatehandler/#nebula.core.aggregation.updatehandlers.dflupdatehandler.Update","title":"<code>Update</code>","text":"<p>Represents a model update received from a node in a specific training round.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>object</code> <p>The model object or weights received.</p> <code>weight</code> <code>float</code> <p>The weight or importance of the update.</p> <code>source</code> <code>str</code> <p>Identifier of the node that sent the update.</p> <code>round</code> <code>int</code> <p>Training round this update belongs to.</p> <code>time_received</code> <code>float</code> <p>Timestamp when the update was received.</p> Source code in <code>nebula/core/aggregation/updatehandlers/dflupdatehandler.py</code> <pre><code>class Update:\n    \"\"\"\n    Represents a model update received from a node in a specific training round.\n\n    Attributes:\n        model (object): The model object or weights received.\n        weight (float): The weight or importance of the update.\n        source (str): Identifier of the node that sent the update.\n        round (int): Training round this update belongs to.\n        time_received (float): Timestamp when the update was received.\n    \"\"\"\n    def __init__(self, model, weight, source, round, time_received):\n        self.model = model\n        self.weight = weight\n        self.source = source\n        self.round = round\n        self.time_received = time_received\n\n    def __eq__(self, other):\n        \"\"\"\n        Checks if two updates belong to the same round.\n        \"\"\"\n        return self.round == other.round\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/dflupdatehandler/#nebula.core.aggregation.updatehandlers.dflupdatehandler.Update.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Checks if two updates belong to the same round.</p> Source code in <code>nebula/core/aggregation/updatehandlers/dflupdatehandler.py</code> <pre><code>def __eq__(self, other):\n    \"\"\"\n    Checks if two updates belong to the same round.\n    \"\"\"\n    return self.round == other.round\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/sdflupdatehandler/","title":"Documentation for Sdflupdatehandler Module","text":""},{"location":"api/core/aggregation/updatehandlers/sdflupdatehandler/#nebula.core.aggregation.updatehandlers.sdflupdatehandler.SDFLUpdateHandler","title":"<code>SDFLUpdateHandler</code>","text":"<p>               Bases: <code>UpdateHandler</code></p> Source code in <code>nebula/core/aggregation/updatehandlers/sdflupdatehandler.py</code> <pre><code>class SDFLUpdateHandler(UpdateHandler):\n    def __init__(self, aggregator, addr, buffersize=MAX_UPDATE_BUFFER_SIZE):\n        \"\"\"\n        Initialize the update handler with required locks and storage.\n\n        Args:\n            aggregator (Aggregator): Aggregator instance for the federation.\n            addr (str): Address of the local node.\n            buffersize (int): Maximum number of historical updates to keep per node.\n        \"\"\"\n        self._addr = addr\n        self._aggregator: Aggregator = aggregator\n        self._buffersize = buffersize\n        self._updates_storage: dict[str, tuple[Update, deque[Update]]] = {}\n        self._updates_storage_lock = Locker(name=\"updates_storage_lock\", async_lock=True)\n        self._sources_expected = set()\n        self._sources_received = set()\n        self._round_updates_lock = Locker(name=\"round_updates_lock\", async_lock=True)\n        self._update_federation_lock = Locker(name=\"update_federation_lock\", async_lock=True)\n        self._notification_sent_lock = Locker(name=\"notification_sent_lock\", async_lock=True)\n        self._notification = False\n        self._missing_ones = set()\n        self._nodes_using_historic = set()\n\n    @property\n    def us(self):\n        \"\"\"Returns the internal updates storage dictionary.\"\"\"\n        return self._updates_storage\n\n    @property\n    def agg(self):\n        \"\"\"Returns the aggregator instance.\"\"\"\n        return self._aggregator\n\n    async def init(self, config=None):\n        \"\"\"\n        Subscribe to update-related events from the event manager.\n        \"\"\"\n        await EventManager.get_instance().subscribe_node_event(UpdateNeighborEvent, self.notify_federation_update)\n        await EventManager.get_instance().subscribe_node_event(UpdateReceivedEvent, self.storage_update)\n\n    async def round_expected_updates(self, federation_nodes: set):\n        \"\"\"\n        Define which nodes are expected to send updates in this round and reset internal state.\n\n        Args:\n            federation_nodes (set): Set of node IDs expected to participate this round.\n        \"\"\"\n        await self._update_federation_lock.acquire_async()\n        await self._updates_storage_lock.acquire_async()\n        self._sources_expected = federation_nodes.copy()\n        self._sources_received.clear()\n\n        # Initialize new nodes\n        for fn in federation_nodes:\n            if fn not in self.us:\n                self.us[fn] = (None, deque(maxlen=self._buffersize))\n\n        # Clear removed nodes\n        removed_nodes = [node for node in self._updates_storage.keys() if node not in federation_nodes]\n        for rn in removed_nodes:\n            del self._updates_storage[rn]\n\n        # Check already received updates\n        await self._check_updates_already_received()\n\n        await self._updates_storage_lock.release_async()\n        await self._update_federation_lock.release_async()\n\n        # Lock to check if all updates received\n        if self._round_updates_lock.locked():\n            self._round_updates_lock.release_async()\n\n        self._notification = False\n\n    async def _check_updates_already_received(self):\n        \"\"\"\n        Scan storage for updates already received in this round.\n        \"\"\"\n        for se in self._sources_expected:\n            (last_updt, node_storage) = self._updates_storage[se]\n            if len(node_storage):\n                try:\n                    if (last_updt and node_storage[-1] and last_updt != node_storage[-1]) or (\n                        node_storage[-1] and not last_updt\n                    ):\n                        self._sources_received.add(se)\n                        logging.info(\n                            f\"Update already received from source: {se} | ({len(self._sources_received)}/{len(self._sources_expected)}) Updates received\"\n                        )\n                except:\n                    logging.exception(\n                        f\"ERROR: source expected: {se} | last_update None: {(True if not last_updt else False)}, last update storaged None: {(True if not node_storage[-1] else False)}\"\n                    )\n\n    async def storage_update(self, updt_received_event: UpdateReceivedEvent):\n        \"\"\"\n        Store an incoming update and trigger aggregation if all updates are received.\n\n        Args:\n            updt_received_event (UpdateReceivedEvent): Event with model update data.\n        \"\"\"\n        time_received = time.time()\n        (model, weight, source, round, _) = await updt_received_event.get_event_data()\n        if source in self._sources_expected:\n            updt = Update(model, weight, source, round, time_received)\n            await self._updates_storage_lock.acquire_async()\n            if updt in self.us[source][1]:\n                logging.info(f\"Discard | Alerady received update from source: {source} for round: {round}\")\n            else:\n                last_update_used = self.us[source][0]\n                self.us[source][1].append(updt)\n                self.us[source] = (last_update_used, self.us[source][1])\n                logging.info(\n                    f\"Storage Update | source={source} | round={round} | weight={weight} | federation nodes: {self._sources_expected}\"\n                )\n\n                self._sources_received.add(source)\n                updates_left = self._sources_expected.difference(self._sources_received)\n                logging.info(\n                    f\"Updates received ({len(self._sources_received)}/{len(self._sources_expected)}) | Missing nodes: {updates_left}\"\n                )\n                if self._round_updates_lock.locked() and not updates_left:\n                    all_rec = await self._all_updates_received()\n                    if all_rec:\n                        await self._notify()\n            await self._updates_storage_lock.release_async()\n        else:\n            if source not in self._sources_received:\n                logging.info(f\"Discard update | source: {source} not in expected updates for this Round\")\n\n    async def get_round_updates(self):\n        \"\"\"\n        Retrieve the most recent valid updates for this round, filling gaps if needed.\n\n        Returns:\n            dict: A dictionary mapping node ID to (model, weight) tuples.\n        \"\"\"\n        await self._updates_storage_lock.acquire_async()\n        updates_missing = self._sources_expected.difference(self._sources_received)\n        if updates_missing:\n            self._missing_ones = updates_missing\n            logging.info(f\"Missing updates from sources: {updates_missing}\")\n        else:\n            self._missing_ones.clear()\n\n        self._nodes_using_historic.clear()\n        updates = {}\n        for sr in self._sources_received:\n            source_historic = self.us[sr][1]\n            last_updt_received = self.us[sr][0]\n            updt: Update = None\n            updt = source_historic[-1]  # Get last update received\n            if last_updt_received and last_updt_received == updt:\n                logging.info(f\"Missing update from source: {sr}, using last update received..\")\n                self._nodes_using_historic.add(sr)\n            else:\n                last_updt_received = updt\n                self.us[sr] = (last_updt_received, source_historic)  # Update storage with new last update used\n            updates[sr] = (updt.model, updt.weight)\n\n        await self._updates_storage_lock.release_async()\n        return updates\n\n    async def notify_federation_update(self, updt_nei_event: UpdateNeighborEvent):\n        \"\"\"\n        Handle federation node join/leave events.\n\n        Args:\n            updt_nei_event (UpdateNeighborEvent): Event with neighbor update data.\n        \"\"\"\n        source, remove = await updt_nei_event.get_event_data()\n        if not remove:\n            if self._round_updates_lock.locked():\n                logging.info(f\"Source: {source} will be count next round\")\n            else:\n                await self._update_source(source, remove)\n        else:\n            if source not in self._sources_received:  # Not received update from this source yet\n                await self._update_source(source, remove=True)\n                await self._all_updates_received()  # Verify if discarding node aggregation could be done\n            else:\n                logging.info(f\"Already received update from: {source}, it will be discarded next round\")\n\n    async def _update_source(self, source, remove=False):\n        \"\"\"\n        Add or remove a node from the expected sources.\n\n        Args:\n            source (str): Node ID.\n            remove (bool): Whether to remove the node from the expected list.\n        \"\"\"\n        logging.info(f\"\ud83d\udd04 Update | remove: {remove} | source: {source}\")\n        await self._updates_storage_lock.acquire_async()\n        if remove:\n            self._sources_expected.discard(source)\n        else:\n            self.us[source] = (None, deque(maxlen=self._buffersize))\n            self._sources_expected.add(source)\n        logging.info(f\"federation nodes expected this round: {self._sources_expected}\")\n        await self._updates_storage_lock.release_async()\n\n    async def get_round_missing_nodes(self):\n        \"\"\"\n        Return the set of nodes whose updates were not received this round.\n\n        Returns:\n            set: Missing node IDs.\n        \"\"\"\n        return self._missing_ones\n\n    async def notify_if_all_updates_received(self):\n        \"\"\"\n        Set a notification trigger and notify aggregator if all updates are already received.\n        \"\"\"\n        logging.info(\"Set notification when all expected updates received\")\n        await self._round_updates_lock.acquire_async()\n        await self._updates_storage_lock.acquire_async()\n        all_received = await self._all_updates_received()\n        await self._updates_storage_lock.release_async()\n        if all_received:\n            await self._notify()\n\n    async def stop_notifying_updates(self):\n        \"\"\"\n        Cancel any notification triggers for update reception.\n        \"\"\"\n        if self._round_updates_lock.locked():\n            logging.info(\"Stop notification updates\")\n            await self._round_updates_lock.release_async()\n\n    async def _notify(self):\n        \"\"\"\n        Notify the aggregator that all expected updates have been received.\n        \"\"\"\n        await self._notification_sent_lock.acquire_async()\n        if self._notification:\n            await self._notification_sent_lock.release_async()\n            return\n        self._notification = True\n        await self.stop_notifying_updates()\n        await self._notification_sent_lock.release_async()\n        logging.info(\"\ud83d\udd04 Notifying aggregator to release aggregation\")\n        await self.agg.notify_all_updates_received()\n\n    async def _all_updates_received(self):\n        \"\"\"\n        Check if all expected updates have been received.\n\n        Returns:\n            bool: True if no updates are missing.\n        \"\"\"\n        updates_left = self._sources_expected.difference(self._sources_received)\n        all_received = False\n        if len(updates_left) == 0:\n            logging.info(\"All updates have been received this round\")\n            if await self._round_updates_lock.locked_async():\n                await self._round_updates_lock.release_async()\n            all_received = True\n        return all_received\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/sdflupdatehandler/#nebula.core.aggregation.updatehandlers.sdflupdatehandler.SDFLUpdateHandler.agg","title":"<code>agg</code>  <code>property</code>","text":"<p>Returns the aggregator instance.</p>"},{"location":"api/core/aggregation/updatehandlers/sdflupdatehandler/#nebula.core.aggregation.updatehandlers.sdflupdatehandler.SDFLUpdateHandler.us","title":"<code>us</code>  <code>property</code>","text":"<p>Returns the internal updates storage dictionary.</p>"},{"location":"api/core/aggregation/updatehandlers/sdflupdatehandler/#nebula.core.aggregation.updatehandlers.sdflupdatehandler.SDFLUpdateHandler.__init__","title":"<code>__init__(aggregator, addr, buffersize=MAX_UPDATE_BUFFER_SIZE)</code>","text":"<p>Initialize the update handler with required locks and storage.</p> <p>Parameters:</p> Name Type Description Default <code>aggregator</code> <code>Aggregator</code> <p>Aggregator instance for the federation.</p> required <code>addr</code> <code>str</code> <p>Address of the local node.</p> required <code>buffersize</code> <code>int</code> <p>Maximum number of historical updates to keep per node.</p> <code>MAX_UPDATE_BUFFER_SIZE</code> Source code in <code>nebula/core/aggregation/updatehandlers/sdflupdatehandler.py</code> <pre><code>def __init__(self, aggregator, addr, buffersize=MAX_UPDATE_BUFFER_SIZE):\n    \"\"\"\n    Initialize the update handler with required locks and storage.\n\n    Args:\n        aggregator (Aggregator): Aggregator instance for the federation.\n        addr (str): Address of the local node.\n        buffersize (int): Maximum number of historical updates to keep per node.\n    \"\"\"\n    self._addr = addr\n    self._aggregator: Aggregator = aggregator\n    self._buffersize = buffersize\n    self._updates_storage: dict[str, tuple[Update, deque[Update]]] = {}\n    self._updates_storage_lock = Locker(name=\"updates_storage_lock\", async_lock=True)\n    self._sources_expected = set()\n    self._sources_received = set()\n    self._round_updates_lock = Locker(name=\"round_updates_lock\", async_lock=True)\n    self._update_federation_lock = Locker(name=\"update_federation_lock\", async_lock=True)\n    self._notification_sent_lock = Locker(name=\"notification_sent_lock\", async_lock=True)\n    self._notification = False\n    self._missing_ones = set()\n    self._nodes_using_historic = set()\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/sdflupdatehandler/#nebula.core.aggregation.updatehandlers.sdflupdatehandler.SDFLUpdateHandler.get_round_missing_nodes","title":"<code>get_round_missing_nodes()</code>  <code>async</code>","text":"<p>Return the set of nodes whose updates were not received this round.</p> <p>Returns:</p> Name Type Description <code>set</code> <p>Missing node IDs.</p> Source code in <code>nebula/core/aggregation/updatehandlers/sdflupdatehandler.py</code> <pre><code>async def get_round_missing_nodes(self):\n    \"\"\"\n    Return the set of nodes whose updates were not received this round.\n\n    Returns:\n        set: Missing node IDs.\n    \"\"\"\n    return self._missing_ones\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/sdflupdatehandler/#nebula.core.aggregation.updatehandlers.sdflupdatehandler.SDFLUpdateHandler.get_round_updates","title":"<code>get_round_updates()</code>  <code>async</code>","text":"<p>Retrieve the most recent valid updates for this round, filling gaps if needed.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary mapping node ID to (model, weight) tuples.</p> Source code in <code>nebula/core/aggregation/updatehandlers/sdflupdatehandler.py</code> <pre><code>async def get_round_updates(self):\n    \"\"\"\n    Retrieve the most recent valid updates for this round, filling gaps if needed.\n\n    Returns:\n        dict: A dictionary mapping node ID to (model, weight) tuples.\n    \"\"\"\n    await self._updates_storage_lock.acquire_async()\n    updates_missing = self._sources_expected.difference(self._sources_received)\n    if updates_missing:\n        self._missing_ones = updates_missing\n        logging.info(f\"Missing updates from sources: {updates_missing}\")\n    else:\n        self._missing_ones.clear()\n\n    self._nodes_using_historic.clear()\n    updates = {}\n    for sr in self._sources_received:\n        source_historic = self.us[sr][1]\n        last_updt_received = self.us[sr][0]\n        updt: Update = None\n        updt = source_historic[-1]  # Get last update received\n        if last_updt_received and last_updt_received == updt:\n            logging.info(f\"Missing update from source: {sr}, using last update received..\")\n            self._nodes_using_historic.add(sr)\n        else:\n            last_updt_received = updt\n            self.us[sr] = (last_updt_received, source_historic)  # Update storage with new last update used\n        updates[sr] = (updt.model, updt.weight)\n\n    await self._updates_storage_lock.release_async()\n    return updates\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/sdflupdatehandler/#nebula.core.aggregation.updatehandlers.sdflupdatehandler.SDFLUpdateHandler.init","title":"<code>init(config=None)</code>  <code>async</code>","text":"<p>Subscribe to update-related events from the event manager.</p> Source code in <code>nebula/core/aggregation/updatehandlers/sdflupdatehandler.py</code> <pre><code>async def init(self, config=None):\n    \"\"\"\n    Subscribe to update-related events from the event manager.\n    \"\"\"\n    await EventManager.get_instance().subscribe_node_event(UpdateNeighborEvent, self.notify_federation_update)\n    await EventManager.get_instance().subscribe_node_event(UpdateReceivedEvent, self.storage_update)\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/sdflupdatehandler/#nebula.core.aggregation.updatehandlers.sdflupdatehandler.SDFLUpdateHandler.notify_federation_update","title":"<code>notify_federation_update(updt_nei_event)</code>  <code>async</code>","text":"<p>Handle federation node join/leave events.</p> <p>Parameters:</p> Name Type Description Default <code>updt_nei_event</code> <code>UpdateNeighborEvent</code> <p>Event with neighbor update data.</p> required Source code in <code>nebula/core/aggregation/updatehandlers/sdflupdatehandler.py</code> <pre><code>async def notify_federation_update(self, updt_nei_event: UpdateNeighborEvent):\n    \"\"\"\n    Handle federation node join/leave events.\n\n    Args:\n        updt_nei_event (UpdateNeighborEvent): Event with neighbor update data.\n    \"\"\"\n    source, remove = await updt_nei_event.get_event_data()\n    if not remove:\n        if self._round_updates_lock.locked():\n            logging.info(f\"Source: {source} will be count next round\")\n        else:\n            await self._update_source(source, remove)\n    else:\n        if source not in self._sources_received:  # Not received update from this source yet\n            await self._update_source(source, remove=True)\n            await self._all_updates_received()  # Verify if discarding node aggregation could be done\n        else:\n            logging.info(f\"Already received update from: {source}, it will be discarded next round\")\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/sdflupdatehandler/#nebula.core.aggregation.updatehandlers.sdflupdatehandler.SDFLUpdateHandler.notify_if_all_updates_received","title":"<code>notify_if_all_updates_received()</code>  <code>async</code>","text":"<p>Set a notification trigger and notify aggregator if all updates are already received.</p> Source code in <code>nebula/core/aggregation/updatehandlers/sdflupdatehandler.py</code> <pre><code>async def notify_if_all_updates_received(self):\n    \"\"\"\n    Set a notification trigger and notify aggregator if all updates are already received.\n    \"\"\"\n    logging.info(\"Set notification when all expected updates received\")\n    await self._round_updates_lock.acquire_async()\n    await self._updates_storage_lock.acquire_async()\n    all_received = await self._all_updates_received()\n    await self._updates_storage_lock.release_async()\n    if all_received:\n        await self._notify()\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/sdflupdatehandler/#nebula.core.aggregation.updatehandlers.sdflupdatehandler.SDFLUpdateHandler.round_expected_updates","title":"<code>round_expected_updates(federation_nodes)</code>  <code>async</code>","text":"<p>Define which nodes are expected to send updates in this round and reset internal state.</p> <p>Parameters:</p> Name Type Description Default <code>federation_nodes</code> <code>set</code> <p>Set of node IDs expected to participate this round.</p> required Source code in <code>nebula/core/aggregation/updatehandlers/sdflupdatehandler.py</code> <pre><code>async def round_expected_updates(self, federation_nodes: set):\n    \"\"\"\n    Define which nodes are expected to send updates in this round and reset internal state.\n\n    Args:\n        federation_nodes (set): Set of node IDs expected to participate this round.\n    \"\"\"\n    await self._update_federation_lock.acquire_async()\n    await self._updates_storage_lock.acquire_async()\n    self._sources_expected = federation_nodes.copy()\n    self._sources_received.clear()\n\n    # Initialize new nodes\n    for fn in federation_nodes:\n        if fn not in self.us:\n            self.us[fn] = (None, deque(maxlen=self._buffersize))\n\n    # Clear removed nodes\n    removed_nodes = [node for node in self._updates_storage.keys() if node not in federation_nodes]\n    for rn in removed_nodes:\n        del self._updates_storage[rn]\n\n    # Check already received updates\n    await self._check_updates_already_received()\n\n    await self._updates_storage_lock.release_async()\n    await self._update_federation_lock.release_async()\n\n    # Lock to check if all updates received\n    if self._round_updates_lock.locked():\n        self._round_updates_lock.release_async()\n\n    self._notification = False\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/sdflupdatehandler/#nebula.core.aggregation.updatehandlers.sdflupdatehandler.SDFLUpdateHandler.stop_notifying_updates","title":"<code>stop_notifying_updates()</code>  <code>async</code>","text":"<p>Cancel any notification triggers for update reception.</p> Source code in <code>nebula/core/aggregation/updatehandlers/sdflupdatehandler.py</code> <pre><code>async def stop_notifying_updates(self):\n    \"\"\"\n    Cancel any notification triggers for update reception.\n    \"\"\"\n    if self._round_updates_lock.locked():\n        logging.info(\"Stop notification updates\")\n        await self._round_updates_lock.release_async()\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/sdflupdatehandler/#nebula.core.aggregation.updatehandlers.sdflupdatehandler.SDFLUpdateHandler.storage_update","title":"<code>storage_update(updt_received_event)</code>  <code>async</code>","text":"<p>Store an incoming update and trigger aggregation if all updates are received.</p> <p>Parameters:</p> Name Type Description Default <code>updt_received_event</code> <code>UpdateReceivedEvent</code> <p>Event with model update data.</p> required Source code in <code>nebula/core/aggregation/updatehandlers/sdflupdatehandler.py</code> <pre><code>async def storage_update(self, updt_received_event: UpdateReceivedEvent):\n    \"\"\"\n    Store an incoming update and trigger aggregation if all updates are received.\n\n    Args:\n        updt_received_event (UpdateReceivedEvent): Event with model update data.\n    \"\"\"\n    time_received = time.time()\n    (model, weight, source, round, _) = await updt_received_event.get_event_data()\n    if source in self._sources_expected:\n        updt = Update(model, weight, source, round, time_received)\n        await self._updates_storage_lock.acquire_async()\n        if updt in self.us[source][1]:\n            logging.info(f\"Discard | Alerady received update from source: {source} for round: {round}\")\n        else:\n            last_update_used = self.us[source][0]\n            self.us[source][1].append(updt)\n            self.us[source] = (last_update_used, self.us[source][1])\n            logging.info(\n                f\"Storage Update | source={source} | round={round} | weight={weight} | federation nodes: {self._sources_expected}\"\n            )\n\n            self._sources_received.add(source)\n            updates_left = self._sources_expected.difference(self._sources_received)\n            logging.info(\n                f\"Updates received ({len(self._sources_received)}/{len(self._sources_expected)}) | Missing nodes: {updates_left}\"\n            )\n            if self._round_updates_lock.locked() and not updates_left:\n                all_rec = await self._all_updates_received()\n                if all_rec:\n                    await self._notify()\n        await self._updates_storage_lock.release_async()\n    else:\n        if source not in self._sources_received:\n            logging.info(f\"Discard update | source: {source} not in expected updates for this Round\")\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/sdflupdatehandler/#nebula.core.aggregation.updatehandlers.sdflupdatehandler.Update","title":"<code>Update</code>","text":"<p>Represents a model update received from a node in a specific training round.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>object</code> <p>The model object or weights received.</p> <code>weight</code> <code>float</code> <p>The weight or importance of the update.</p> <code>source</code> <code>str</code> <p>Identifier of the node that sent the update.</p> <code>round</code> <code>int</code> <p>Training round this update belongs to.</p> <code>time_received</code> <code>float</code> <p>Timestamp when the update was received.</p> Source code in <code>nebula/core/aggregation/updatehandlers/sdflupdatehandler.py</code> <pre><code>class Update:\n    \"\"\"\n    Represents a model update received from a node in a specific training round.\n\n    Attributes:\n        model (object): The model object or weights received.\n        weight (float): The weight or importance of the update.\n        source (str): Identifier of the node that sent the update.\n        round (int): Training round this update belongs to.\n        time_received (float): Timestamp when the update was received.\n    \"\"\"\n\n    def __init__(self, model, weight, source, round, time_received):\n        self.model = model\n        self.weight = weight\n        self.source = source\n        self.round = round\n        self.time_received = time_received\n\n    def __eq__(self, other):\n        \"\"\"\n        Checks if two updates belong to the same round.\n        \"\"\"\n        return self.round == other.round\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/sdflupdatehandler/#nebula.core.aggregation.updatehandlers.sdflupdatehandler.Update.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Checks if two updates belong to the same round.</p> Source code in <code>nebula/core/aggregation/updatehandlers/sdflupdatehandler.py</code> <pre><code>def __eq__(self, other):\n    \"\"\"\n    Checks if two updates belong to the same round.\n    \"\"\"\n    return self.round == other.round\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/updatehandler/","title":"Documentation for Updatehandler Module","text":""},{"location":"api/core/aggregation/updatehandlers/updatehandler/#nebula.core.aggregation.updatehandlers.updatehandler.UpdateHandler","title":"<code>UpdateHandler</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for managing update storage and retrieval in a federated learning setting.</p> <p>This class defines the required methods for handling updates from multiple sources, ensuring they are properly stored, retrieved, and processed during the aggregation process.</p> Source code in <code>nebula/core/aggregation/updatehandlers/updatehandler.py</code> <pre><code>class UpdateHandler(ABC):\n    \"\"\"\n    Abstract base class for managing update storage and retrieval in a federated learning setting.\n\n    This class defines the required methods for handling updates from multiple sources,\n    ensuring they are properly stored, retrieved, and processed during the aggregation process.\n    \"\"\"\n\n    @abstractmethod\n    async def init(self, config: dict):\n        raise NotImplementedError\n\n    @abstractmethod\n    async def round_expected_updates(self, federation_nodes: set):\n        \"\"\"\n        Initializes the expected updates for the current round.\n\n        This method sets up the expected sources (`federation_nodes`) that should provide updates\n        in the current training round. It ensures that each source has an entry in the storage\n        and resets any previous tracking of received updates.\n\n        Args:\n            federation_nodes (set): A set of node identifiers expected to provide updates.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    async def storage_update(self, updt_received_event: UpdateReceivedEvent):\n        \"\"\"\n        Stores an update from a source in the update storage.\n\n        This method ensures that an update received from a source is properly stored in the buffer,\n        avoiding duplicates and managing update history if necessary.\n\n        Args:\n            model: The model associated with the update.\n            weight: The weight assigned to the update (e.g., based on the amount of data used in training).\n            source (str): The identifier of the node sending the update.\n            round (int): The current device local training round when the update was done.\n            local (boolean): Local update\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    async def get_round_updates(self) -&gt; dict[str, tuple[object, float]]:\n        \"\"\"\n        Retrieves the latest updates from all received sources in the current round.\n\n        This method collects updates from all sources that have sent updates,\n        prioritizing the most recent update available in the buffer.\n\n        Returns:\n            dict: A dictionary where keys are source identifiers and values are tuples `(model, weight)`,\n                  representing the latest updates received from each source.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    async def notify_federation_update(self, updt_nei_event: UpdateNeighborEvent):\n        \"\"\"\n        Notifies the system of a change in the federation regarding a specific source.\n\n        If a source leaves the federation, it is removed from the list of expected updates.\n        If a source is newly added, it is registered for future updates.\n\n        Args:\n            source (str): The identifier of the source node.\n            remove (bool, optional): Whether to remove the source from the federation. Defaults to `False`.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    async def get_round_missing_nodes(self) -&gt; set[str]:\n        \"\"\"\n        Identifies sources that have not yet provided updates in the current round.\n\n        Returns:\n            set: A set of source identifiers that are expected to send updates but have not yet been received.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    async def notify_if_all_updates_received(self):\n        \"\"\"\n        Notifies the system when all expected updates for the current round have been received.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    async def stop_notifying_updates(self):\n        \"\"\"\n        Stops notifications related to update reception.\n\n        This method can be used to reset any notification mechanisms or stop tracking updates\n        if the aggregation process is halted.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/updatehandler/#nebula.core.aggregation.updatehandlers.updatehandler.UpdateHandler.get_round_missing_nodes","title":"<code>get_round_missing_nodes()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Identifies sources that have not yet provided updates in the current round.</p> <p>Returns:</p> Name Type Description <code>set</code> <code>set[str]</code> <p>A set of source identifiers that are expected to send updates but have not yet been received.</p> Source code in <code>nebula/core/aggregation/updatehandlers/updatehandler.py</code> <pre><code>@abstractmethod\nasync def get_round_missing_nodes(self) -&gt; set[str]:\n    \"\"\"\n    Identifies sources that have not yet provided updates in the current round.\n\n    Returns:\n        set: A set of source identifiers that are expected to send updates but have not yet been received.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/updatehandler/#nebula.core.aggregation.updatehandlers.updatehandler.UpdateHandler.get_round_updates","title":"<code>get_round_updates()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Retrieves the latest updates from all received sources in the current round.</p> <p>This method collects updates from all sources that have sent updates, prioritizing the most recent update available in the buffer.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, tuple[object, float]]</code> <p>A dictionary where keys are source identifiers and values are tuples <code>(model, weight)</code>,   representing the latest updates received from each source.</p> Source code in <code>nebula/core/aggregation/updatehandlers/updatehandler.py</code> <pre><code>@abstractmethod\nasync def get_round_updates(self) -&gt; dict[str, tuple[object, float]]:\n    \"\"\"\n    Retrieves the latest updates from all received sources in the current round.\n\n    This method collects updates from all sources that have sent updates,\n    prioritizing the most recent update available in the buffer.\n\n    Returns:\n        dict: A dictionary where keys are source identifiers and values are tuples `(model, weight)`,\n              representing the latest updates received from each source.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/updatehandler/#nebula.core.aggregation.updatehandlers.updatehandler.UpdateHandler.notify_federation_update","title":"<code>notify_federation_update(updt_nei_event)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Notifies the system of a change in the federation regarding a specific source.</p> <p>If a source leaves the federation, it is removed from the list of expected updates. If a source is newly added, it is registered for future updates.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The identifier of the source node.</p> required <code>remove</code> <code>bool</code> <p>Whether to remove the source from the federation. Defaults to <code>False</code>.</p> required Source code in <code>nebula/core/aggregation/updatehandlers/updatehandler.py</code> <pre><code>@abstractmethod\nasync def notify_federation_update(self, updt_nei_event: UpdateNeighborEvent):\n    \"\"\"\n    Notifies the system of a change in the federation regarding a specific source.\n\n    If a source leaves the federation, it is removed from the list of expected updates.\n    If a source is newly added, it is registered for future updates.\n\n    Args:\n        source (str): The identifier of the source node.\n        remove (bool, optional): Whether to remove the source from the federation. Defaults to `False`.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/updatehandler/#nebula.core.aggregation.updatehandlers.updatehandler.UpdateHandler.notify_if_all_updates_received","title":"<code>notify_if_all_updates_received()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Notifies the system when all expected updates for the current round have been received.</p> Source code in <code>nebula/core/aggregation/updatehandlers/updatehandler.py</code> <pre><code>@abstractmethod\nasync def notify_if_all_updates_received(self):\n    \"\"\"\n    Notifies the system when all expected updates for the current round have been received.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/updatehandler/#nebula.core.aggregation.updatehandlers.updatehandler.UpdateHandler.round_expected_updates","title":"<code>round_expected_updates(federation_nodes)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Initializes the expected updates for the current round.</p> <p>This method sets up the expected sources (<code>federation_nodes</code>) that should provide updates in the current training round. It ensures that each source has an entry in the storage and resets any previous tracking of received updates.</p> <p>Parameters:</p> Name Type Description Default <code>federation_nodes</code> <code>set</code> <p>A set of node identifiers expected to provide updates.</p> required Source code in <code>nebula/core/aggregation/updatehandlers/updatehandler.py</code> <pre><code>@abstractmethod\nasync def round_expected_updates(self, federation_nodes: set):\n    \"\"\"\n    Initializes the expected updates for the current round.\n\n    This method sets up the expected sources (`federation_nodes`) that should provide updates\n    in the current training round. It ensures that each source has an entry in the storage\n    and resets any previous tracking of received updates.\n\n    Args:\n        federation_nodes (set): A set of node identifiers expected to provide updates.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/updatehandler/#nebula.core.aggregation.updatehandlers.updatehandler.UpdateHandler.stop_notifying_updates","title":"<code>stop_notifying_updates()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Stops notifications related to update reception.</p> <p>This method can be used to reset any notification mechanisms or stop tracking updates if the aggregation process is halted.</p> Source code in <code>nebula/core/aggregation/updatehandlers/updatehandler.py</code> <pre><code>@abstractmethod\nasync def stop_notifying_updates(self):\n    \"\"\"\n    Stops notifications related to update reception.\n\n    This method can be used to reset any notification mechanisms or stop tracking updates\n    if the aggregation process is halted.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/updatehandler/#nebula.core.aggregation.updatehandlers.updatehandler.UpdateHandler.storage_update","title":"<code>storage_update(updt_received_event)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Stores an update from a source in the update storage.</p> <p>This method ensures that an update received from a source is properly stored in the buffer, avoiding duplicates and managing update history if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The model associated with the update.</p> required <code>weight</code> <p>The weight assigned to the update (e.g., based on the amount of data used in training).</p> required <code>source</code> <code>str</code> <p>The identifier of the node sending the update.</p> required <code>round</code> <code>int</code> <p>The current device local training round when the update was done.</p> required <code>local</code> <code>boolean</code> <p>Local update</p> required Source code in <code>nebula/core/aggregation/updatehandlers/updatehandler.py</code> <pre><code>@abstractmethod\nasync def storage_update(self, updt_received_event: UpdateReceivedEvent):\n    \"\"\"\n    Stores an update from a source in the update storage.\n\n    This method ensures that an update received from a source is properly stored in the buffer,\n    avoiding duplicates and managing update history if necessary.\n\n    Args:\n        model: The model associated with the update.\n        weight: The weight assigned to the update (e.g., based on the amount of data used in training).\n        source (str): The identifier of the node sending the update.\n        round (int): The current device local training round when the update was done.\n        local (boolean): Local update\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/core/datasets/","title":"Documentation for Datasets Module","text":""},{"location":"api/core/datasets/changeablesubset/","title":"Documentation for Changeablesubset Module","text":""},{"location":"api/core/datasets/datamodule/","title":"Documentation for Datamodule Module","text":""},{"location":"api/core/datasets/nebuladataset/","title":"Documentation for Nebuladataset Module","text":""},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset","title":"<code>NebulaDataset</code>","text":"Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>class NebulaDataset:\n    def __init__(\n        self,\n        num_classes=10,\n        partitions_number=1,\n        batch_size=32,\n        num_workers=4,\n        iid=False,\n        partition=\"dirichlet\",\n        partition_parameter=0.5,\n        nsplits_percentages=[1.0],\n        nsplits_iid=[\"Non-IID\"],\n        npartitions=[\"dirichlet\"],\n        npartitions_parameter=[0.1],\n        seed=42,\n        config_dir=None,\n    ):\n        self.num_classes = num_classes\n        self.partitions_number = partitions_number\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.iid = iid\n        self.partition = partition\n        self.partition_parameter = partition_parameter\n        self.seed = seed\n        self.config_dir = config_dir\n        self._nsplits_percentages = nsplits_percentages\n        self._nsplits_iid = nsplits_iid\n        self._npartitions = npartitions\n        self._npartitions_parameter = npartitions_parameter\n        self._targets_reales = None\n\n        logging.info(\n            f\"Dataset {self.__class__.__name__} initialized | Partitions: {self.partitions_number} | IID: {self.iid} | Partition: {self.partition} | Partition parameter: {self.partition_parameter}\"\n        )\n\n        # Dataset\n        self.train_set = None\n        self.train_indices_map = None\n        self.test_set = None\n        self.test_indices_map = None\n        self.local_test_indices_map = None\n\n        enable_deterministic(self.seed)\n\n    @abstractmethod\n    def initialize_dataset(self):\n        \"\"\"\n        Initialize the dataset. This should load or create the dataset.\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement this method.\")\n\n    def clear(self):\n        \"\"\"\n        Clear the dataset. This should remove or reset the dataset.\n        \"\"\"\n        if self.train_set is not None and hasattr(self.train_set, \"close\"):\n            self.train_set.close()\n        if self.test_set is not None and hasattr(self.test_set, \"close\"):\n            self.test_set.close()\n\n        self.train_set = None\n        self.train_indices_map = None\n        self.test_set = None\n        self.test_indices_map = None\n        self.local_test_indices_map = None\n\n    def data_partitioning(self, plot=False):\n        \"\"\"\n        Perform the data partitioning.\n        \"\"\"\n\n        logging.info(\n            f\"Partitioning data for {self.__class__.__name__} | Partitions: {self.partitions_number} | IID: {self.iid} | Partition: {self.partition} | Partition parameter: {self.partition_parameter}\"\n        )\n\n        logging.info(f\"Scenario with data distribution IID: {self.iid}\")\n        if self.iid:\n            self.train_indices_map = self.generate_iid_map(self.train_set)\n        else:\n            self.train_indices_map = self.generate_non_iid_map(\n                self.train_set, partition=self.partition, partition_parameter=self.partition_parameter\n            )\n        # else:\n        #     self.train_indices_map = self.generate_hybrid_map()\n\n        self.test_indices_map = self.get_test_indices_map()\n        self.local_test_indices_map = self.get_local_test_indices_map()\n\n        if plot:\n            self.plot_data_distribution(\"train\", self.train_set, self.train_indices_map)\n            self.plot_all_data_distribution(\"train\", self.train_set, self.train_indices_map)\n            self.plot_data_distribution(\"local_test\", self.test_set, self.local_test_indices_map)\n            self.plot_all_data_distribution(\"local_test\", self.test_set, self.local_test_indices_map)\n\n        self.save_partitions()\n\n    def get_test_indices_map(self):\n        \"\"\"\n        Get the indices of the test set for each participant.\n\n        Returns:\n            A dictionary mapping participant_id to a list of indices.\n        \"\"\"\n        try:\n            test_indices_map = {}\n            for participant_id in range(self.partitions_number):\n                test_indices_map[participant_id] = list(range(len(self.test_set)))\n            return test_indices_map\n        except Exception as e:\n            logging.exception(f\"Error in get_test_indices_map: {e}\")\n\n    def get_local_test_indices_map(self):\n        \"\"\"\n        Get the indices of the local test set for each participant.\n        Indices whose labels are the same as the training set are selected.\n\n        Returns:\n            A dictionary mapping participant_id to a list of indices.\n        \"\"\"\n        try:\n            local_test_indices_map = {}\n            test_targets = np.array(self.test_set.targets)\n            for participant_id in range(self.partitions_number):\n                train_labels = np.array([self.train_set.targets[idx] for idx in self.train_indices_map[participant_id]])\n                indices = np.where(np.isin(test_targets, train_labels))[0].tolist()\n                local_test_indices_map[participant_id] = indices\n            return local_test_indices_map\n        except Exception as e:\n            logging.exception(f\"Error in get_local_test_indices_map: {e}\")\n            raise\n\n    def save_partition(self, obj, file, name):\n        try:\n            logging.info(f\"Saving pickled object of type {type(obj)}\")\n            pickled = pickle.dumps(obj)\n\n            size_in_mb = len(pickled) / (1024 * 1024)\n            logging.info(f\"Pickled object size: {size_in_mb:.2f} MB\")\n\n            if size_in_mb &gt; 10:\n                logging.info(f\"Large object detected ({size_in_mb:.2f} MB). Using chunked storage with compression.\")\n                data = np.frombuffer(pickled, dtype=np.uint8)\n                chunk_size = min(4 * 1024 * 1024, len(data) // 10)\n                chunk_length = max(1, chunk_size // data.itemsize)\n                ds = file.create_dataset(\n                    name,\n                    data=data,\n                    chunks=(chunk_length,),\n                    compression=\"lzf\",\n                    shuffle=True,\n                )\n                ds.attrs[\"__type__\"] = \"pickle_bytes\"\n            else:\n                ds = file.create_dataset(name, data=np.void(pickled))\n                ds.attrs[\"__type__\"] = \"pickle\"\n            logging.info(f\"Saved pickled object of type {type(obj)} to {name}\")\n        except Exception as e:\n            logging.exception(f\"Error saving object to HDF5: {e}\")\n            raise\n\n    def save_partitions(self):\n        \"\"\"\n        Save each partition data (train, test, and local test) to separate pickle files.\n        The controller saves one file per partition for each data split.\n        \"\"\"\n        try:\n            logging.info(f\"Saving partitions data for ALL participants ({self.partitions_number}) in {self.config_dir}\")\n            path = self.config_dir\n            if not os.path.exists(path):\n                raise FileNotFoundError(f\"Path {path} does not exist\")\n            # Check that the partition maps have the expected number of partitions\n            if not (\n                len(self.train_indices_map)\n                == len(self.test_indices_map)\n                == len(self.local_test_indices_map)\n                == self.partitions_number\n            ):\n                raise ValueError(\"One of the partition maps has an unexpected length.\")\n\n            # Save global test data\n            file_name = os.path.join(path, \"global_test.h5\")\n            with h5py.File(file_name, \"w\") as f:\n                indices = list(range(len(self.test_set)))\n                test_data = [self.test_set[i] for i in indices]\n                self.save_partition(test_data, f, \"test_data\")\n                f[\"test_data\"].attrs[\"num_classes\"] = self.num_classes\n                test_targets = np.array(self.test_set.targets)\n                f.create_dataset(\"test_targets\", data=test_targets, compression=\"gzip\")\n\n            for participant in range(self.partitions_number):\n                file_name = os.path.join(path, f\"participant_{participant}_train.h5\")\n                with h5py.File(file_name, \"w\") as f:\n                    logging.info(f\"Saving training data for participant {participant} in {file_name}\")\n                    indices = self.train_indices_map[participant]\n                    train_data = [self.train_set[i] for i in indices]\n                    self.save_partition(train_data, f, \"train_data\")\n                    f[\"train_data\"].attrs[\"num_classes\"] = self.num_classes\n                    train_targets = np.array([self.train_set.targets[i] for i in indices])\n                    f.create_dataset(\"train_targets\", data=train_targets, compression=\"gzip\")\n                    logging.info(f\"Partition saved for participant {participant}.\")\n\n            logging.info(\"Successfully saved all partition files.\")\n\n        except Exception as e:\n            logging.exception(f\"Error in save_partitions: {e}\")\n\n        finally:\n            self.clear()\n            logging.info(\"Cleared dataset after saving partitions.\")\n\n    @abstractmethod\n    def generate_non_iid_map(self, dataset, partition=\"dirichlet\", plot=False):\n        \"\"\"\n        Create a non-iid map of the dataset.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def generate_iid_map(self, dataset, plot=False, num_clients=None):\n        \"\"\"\n        Create an iid map of the dataset.\n        \"\"\"\n        pass\n\n    def generate_hybrid_map(self):\n        index = 0\n        data = []\n        targets = []\n        sample, target = self.train_set.__getitem__(index)\n        while sample != None and target != None:\n            data.append(sample)\n            targets.append(target)\n            index += 1\n            try:\n                sample, target = self.train_set.__getitem__(index)\n            except Exception:\n                break\n        data = np.array(data)\n        targets = np.array(targets)\n        self._targets_reales = targets.copy()  # TODO remove\n        logging.info(f\"number of samples on dataset: {len(data)}, targets: {targets}\")\n\n        remaining_size = 1.0\n        subsets = []\n        subset_to_split, targets_to_split = copy.deepcopy(data), copy.deepcopy(targets)\n\n        participants = [i for i in range(self.partitions_number)]\n        num_participants = len(participants)\n        grouped_participants = []\n        start_idx = 0\n\n        or_indices = np.arange(len(data))\n\n        # Inicializar las estructuras que se dividir\u00e1n en cada iteraci\u00f3n\n        subset_to_split, targets_to_split, indices_to_split = (\n            copy.deepcopy(data),\n            copy.deepcopy(targets),\n            copy.deepcopy(or_indices),\n        )\n\n        for i, size in enumerate(self._nsplits_percentages[:-1]):  # Last one doesn't require split\n            relative_size = size / remaining_size  # Tama\u00f1o relativo respecto al conjunto restante\n            logging.info(f\"size: {size}, relative size: {relative_size}, remaining size: {remaining_size}\")\n\n            # Dividir manteniendo referencias originales\n            x_s1, x_s2, y_s1, y_s2, idx_s1, idx_s2 = train_test_split(\n                subset_to_split,\n                targets_to_split,\n                indices_to_split,\n                test_size=(1 - relative_size),\n                stratify=targets_to_split,\n                random_state=42,\n            )\n\n            # Guardar los datos y etiquetas originales asociados a los \u00edndices seleccionados\n            original_X_s1, original_y_s1 = data[idx_s1], targets[idx_s1]\n\n            logging.info(f\"Subset {i + 1}: {len(original_X_s1)} samples\")\n\n            # Guardar subset con referencia a los datos originales\n            subsets.append((original_X_s1, original_y_s1, idx_s1))\n\n            num_in_group = round(size * num_participants)\n            grouped_participants.append(participants[start_idx : start_idx + num_in_group])\n\n            # Actualizar para la siguiente iteraci\u00f3n\n            subset_to_split, targets_to_split, indices_to_split = data[idx_s2], targets[idx_s2], idx_s2\n            remaining_size -= size\n            start_idx += num_in_group\n\n        # Guardar el \u00faltimo subset con sus \u00edndices originales\n        original_X_s2, original_y_s2 = data[indices_to_split], targets[indices_to_split]\n        subsets.append((original_X_s2, original_y_s2, indices_to_split))\n        grouped_participants.append(participants[start_idx:])\n\n        for i, (_, ysubset, _) in enumerate(subsets):\n            logging.info(f\"Subset {i + 1} - {np.bincount(ysubset)}\")\n\n        general_map = {}\n        for i, subset in enumerate(subsets):\n            data_mapped = dict()\n            real_indexes = subset[2]\n            subset_real_data = data[real_indexes]\n            subset_real_targets = targets[real_indexes]\n\n            dataset_wrapped = SimpleNamespace(\n                data=subset_real_data, targets=subset_real_targets, real_indexes=real_indexes\n            )\n\n            if self._nsplits_iid[i] == \"IID\":\n                logging.info(\n                    f\"Generating dataset subset IID for participants: {grouped_participants[i]}, num_clients: {len(grouped_participants[i])}\"\n                )\n                subset_map = self.generate_iid_map(\n                    dataset_wrapped,\n                    self._npartitions[i],\n                    self._npartitions_parameter[i],\n                    num_clients=len(grouped_participants[i]),\n                )\n                for j, real_id in enumerate(\n                    grouped_participants[i]\n                ):  # Mapping subset map generated to real clients IDs\n                    data_mapped[real_id] = subset_map[j]\n\n            else:\n                logging.info(\n                    f\"Generating dataset subset Non-IID for participants: {grouped_participants[i]}, num_clients: {len(grouped_participants[i])}\"\n                )\n                subset_map = self.generate_non_iid_map(\n                    dataset_wrapped,\n                    self._npartitions[i],\n                    self._npartitions_parameter[i],\n                    num_clients=len(grouped_participants[i]),\n                )\n                for j, real_id in enumerate(\n                    grouped_participants[i]\n                ):  # Mapping subset map generated to real clients IDs\n                    data_mapped[real_id] = subset_map[j]\n\n            general_map.update(data_mapped)\n        for id, indexes in general_map.items():\n            logging.info(\n                f\" Participant id: {id}, num samples: {len(indexes)}, targets: {np.bincount(targets[indexes])}\"\n            )\n        return general_map\n\n    def plot_data_distribution(self, phase, dataset, partitions_map):\n        \"\"\"\n        Plot the data distribution of the dataset.\n\n        Plot the data distribution of the dataset according to the partitions map provided.\n\n        Args:\n            phase: The phase of the dataset (train, test, local_test).\n            dataset: The dataset to plot (torch.utils.data.Dataset).\n            partitions_map: The map of the dataset partitions.\n        \"\"\"\n        logging_training.info(f\"Plotting data distribution for {phase} dataset\")\n        # Plot the data distribution of the dataset, one graph per partition\n        sns.set()\n        sns.set_style(\"whitegrid\", {\"axes.grid\": False})\n        sns.set_context(\"paper\", font_scale=1.5)\n        sns.set_palette(\"Set2\")\n\n        # Plot bar charts for each partition\n        partition_index = 0\n        for partition_index in range(self.partitions_number):\n            indices = partitions_map[partition_index]\n            class_counts = [0] * self.num_classes\n            for idx in indices:\n                label = dataset.targets[idx]\n                class_counts[label] += 1\n\n            logging_training.info(f\"[{phase}] Participant {partition_index} total samples: {len(indices)}\")\n            logging_training.info(f\"[{phase}] Participant {partition_index} data distribution: {class_counts}\")\n\n            plt.figure(figsize=(12, 8))\n            plt.bar(range(self.num_classes), class_counts)\n            for j, count in enumerate(class_counts):\n                plt.text(j, count, str(count), ha=\"center\", va=\"bottom\", fontsize=12)\n            plt.xlabel(\"Class\")\n            plt.ylabel(\"Number of samples\")\n            plt.xticks(range(self.num_classes))\n            plt.title(\n                f\"[{phase}] Participant {partition_index} data distribution ({'IID' if self.iid else f'Non-IID - {self.partition}'} - {self.partition_parameter})\"\n            )\n            plt.tight_layout()\n            path_to_save = f\"{self.config_dir}/participant_{partition_index}_data_distribution_{'iid' if self.iid else 'non_iid'}{'_' + self.partition if not self.iid else ''}_{phase}.pdf\"\n            logging_training.info(f\"Saving data distribution for participant {partition_index} to {path_to_save}\")\n            plt.savefig(path_to_save, dpi=300, bbox_inches=\"tight\")\n            plt.close()\n\n        if hasattr(self, \"tsne\") and self.tsne:\n            self.visualize_tsne(dataset)\n\n    def visualize_tsne(self, dataset):\n        X = []  # List for storing the characteristics of the samples\n        y = []  # Ready to store the labels of the samples\n        for idx in range(len(dataset)):  # Assuming that 'dataset' is a list or array of your samples\n            sample, label = dataset[idx]\n            X.append(sample.flatten())\n            y.append(label)\n\n        X = np.array(X)\n        y = np.array(y)\n\n        tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n        tsne_results = tsne.fit_transform(X)\n\n        plt.figure(figsize=(16, 10))\n        sns.scatterplot(\n            x=tsne_results[:, 0],\n            y=tsne_results[:, 1],\n            hue=y,\n            palette=sns.color_palette(\"hsv\", self.num_classes),\n            legend=\"full\",\n            alpha=0.7,\n        )\n\n        plt.title(\"t-SNE visualization of the dataset\")\n        plt.xlabel(\"t-SNE axis 1\")\n        plt.ylabel(\"t-SNE axis 2\")\n        plt.legend(title=\"Class\")\n        plt.tight_layout()\n\n        path_to_save_tsne = f\"{self.config_dir}/tsne_visualization.png\"\n        plt.savefig(path_to_save_tsne, dpi=300, bbox_inches=\"tight\")\n        plt.close()\n\n    def dirichlet_partition(\n        self,\n        dataset: Any,\n        alpha: float = 0.2,\n        n_clients=None,\n        min_samples_size: int = 50,\n        balanced: bool = False,\n        max_iter: int = 100,\n        verbose: bool = False,\n    ) -&gt; dict[int, list[int]]:\n        \"\"\"\n        Partition the dataset among clients using a Dirichlet distribution.\n        This function ensures each client gets at least min_samples_size samples.\n\n        Parameters\n        ----------\n        dataset : Dataset\n            The dataset to partition. Must have a 'targets' attribute.\n        alpha : float, default=0.5\n            Concentration parameter for the Dirichlet distribution.\n        min_samples_size : int, default=50\n            Minimum number of samples required in each partition.\n        balanced : bool, default=False\n            If True, distribute each class evenly among clients.\n            Otherwise, allocate according to a Dirichlet distribution.\n        max_iter : int, default=100\n            Maximum number of iterations to try finding a valid partition.\n        verbose : bool, default=True\n            If True, print debug information per iteration.\n\n        Returns\n        -------\n        partitions : dict[int, list[int]]\n            Dictionary mapping each client index to a list of sample indices.\n        \"\"\"\n        num_clients = self.partitions_number if not n_clients else n_clients\n        logging.info(f\"Generating Dirichlet Partitioning, alpha: {alpha}, num_clients: {num_clients}\")\n\n        # Extract targets and unique labels.\n        if not n_clients:\n            y_data = self._get_targets(dataset)\n            unique_labels = np.unique(y_data)\n        else:\n            if verbose:\n                logging.info(\"Extracting dataset partition targets...\")\n            # For hybrid dataset scenarios\n            y_data = dataset.targets\n            unique_labels = np.unique(y_data)\n        if verbose:\n            logging.info(f\"Unique labels in dataset: {unique_labels}\")\n\n        # For each class, get a shuffled list of indices.\n        class_indices = {}\n        base_rng = np.random.default_rng(self.seed)\n        for label in unique_labels:\n            if not n_clients:\n                idx = np.where(y_data == label)[0]\n            else:\n                ri = dataset.real_indexes\n                idx = np.where(self._targets_reales[ri] == label)[0]\n                idx = ri[idx]\n                # logging.info(f\"attempting: {self._targets_reales[idx]}\")\n\n            base_rng.shuffle(idx)\n            class_indices[label] = idx\n\n        # Prepare container for client indices.\n        indices_per_partition = [[] for _ in range(num_clients)]\n\n        def allocate_for_label(label_idx: np.ndarray, rng: np.random.Generator, n_clients) -&gt; np.ndarray:\n            num_label_samples = len(label_idx)\n            if verbose:\n                logging.info(f\"number of samples allocating {num_label_samples}\")\n            if balanced:\n                proportions = np.full(n_clients, 1.0 / n_clients)\n            else:\n                proportions = rng.dirichlet([alpha] * n_clients)\n            sample_counts = (proportions * num_label_samples).astype(int)\n            remainder = num_label_samples - sample_counts.sum()\n            if remainder &gt; 0:\n                extra_indices = rng.choice(n_clients, size=remainder, replace=False)\n                for idx in extra_indices:\n                    sample_counts[idx] += 1\n            if verbose:\n                logging.info(f\"Samples allocated per client: {sample_counts}\")\n            return sample_counts\n\n        for iteration in range(1, max_iter + 1):\n            rng = np.random.default_rng(self.seed + iteration)\n            temp_indices_per_partition = [[] for _ in range(num_clients)]\n            for label in unique_labels:\n                label_idx = class_indices[label]\n                if verbose:\n                    logging.info(f\"Calculating samples distribution for label: {label}\")\n                counts = allocate_for_label(label_idx, rng, num_clients)\n                start = 0\n                for client_idx, count in enumerate(counts):\n                    end = start + count\n                    temp_indices_per_partition[client_idx].extend(label_idx[start:end])\n                    if verbose:\n                        logging.info(\n                            f\"Counting check: {np.bincount(self._targets_reales[temp_indices_per_partition[client_idx]])}\"\n                        )\n                    start = end\n\n            client_sizes = [len(indices) for indices in temp_indices_per_partition]\n            if min(client_sizes) &gt;= min_samples_size:\n                indices_per_partition = temp_indices_per_partition\n                if verbose:\n                    logging.info(f\"Partition successful at iteration {iteration}. Client sizes: {client_sizes}\")\n                break\n            if verbose:\n                logging.info(f\"Iteration {iteration}: client sizes {client_sizes}\")\n\n        else:\n            raise ValueError(\n                f\"Could not create partitions with at least {min_samples_size} samples per client after {max_iter} iterations.\"\n            )\n\n        initial_partition = {i: indices for i, indices in enumerate(indices_per_partition)}\n        final_partition = initial_partition  # self.postprocess_partition(initial_partition, y_data)\n        return final_partition\n\n    @staticmethod\n    def _get_targets(dataset) -&gt; np.ndarray:\n        if isinstance(dataset.targets, np.ndarray):\n            return dataset.targets\n        elif hasattr(dataset.targets, \"numpy\"):\n            return dataset.targets.numpy()\n        else:\n            return np.asarray(dataset.targets)\n\n    def postprocess_partition(\n        self, partition: dict[int, list[int]], y_data: np.ndarray, min_samples_per_class: int = 10\n    ) -&gt; dict[int, list[int]]:\n        \"\"\"\n        Post-process a partition to remove (and reassign) classes with too few samples per client.\n\n        For each class:\n        - For clients with a count &gt; 0 but below min_samples_per_class, remove those samples.\n        - Reassign the removed samples to the client that already has the maximum count for that class.\n\n        Parameters\n        ----------\n        partition : dict[int, list[int]]\n            The initial partition mapping client indices to sample indices.\n        y_data : np.ndarray\n            The array of labels corresponding to the dataset samples.\n        min_samples_per_class : int, default=10\n            The minimum acceptable number of samples per class for each client.\n\n        Returns\n        -------\n        new_partition : dict[int, list[int]]\n            The updated partition.\n        \"\"\"\n        # Copy partition so we can modify it.\n        new_partition = {client: list(indices) for client, indices in partition.items()}\n\n        # Iterate over each class.\n        for label in np.unique(y_data):\n            # For each client, count how many samples of this label exist.\n            client_counts = {}\n            for client, indices in new_partition.items():\n                client_counts[client] = np.sum(np.array(y_data)[indices] == label)\n\n            # Identify clients with fewer than min_samples_per_class but nonzero counts.\n            donors = [client for client, count in client_counts.items() if 0 &lt; count &lt; min_samples_per_class]\n            # Identify potential recipients: those with at least min_samples_per_class.\n            recipients = [client for client, count in client_counts.items() if count &gt;= min_samples_per_class]\n            # If no client meets the threshold, choose the one with the highest count.\n            if not recipients:\n                best_recipient = max(client_counts, key=client_counts.get)\n                recipients = [best_recipient]\n            # Choose the recipient with the maximum count.\n            best_recipient = max(recipients, key=lambda c: client_counts[c])\n\n            # For each donor, remove samples of this label and reassign them.\n            for donor in donors:\n                donor_indices = new_partition[donor]\n                # Identify indices corresponding to this label.\n                donor_label_indices = [idx for idx in donor_indices if y_data[idx] == label]\n                # Remove these from the donor.\n                new_partition[donor] = [idx for idx in donor_indices if y_data[idx] != label]\n                # Add these to the best recipient.\n                new_partition[best_recipient].extend(donor_label_indices)\n\n        return new_partition\n\n    def homo_partition(self, dataset):\n        \"\"\"\n        Homogeneously partition the dataset into multiple subsets.\n\n        This function divides a dataset into a specified number of subsets, where each subset\n        is intended to have a roughly equal number of samples. This method aims to ensure a\n        homogeneous distribution of data across all subsets. It's particularly useful in\n        scenarios where a uniform distribution of data is desired among all federated learning\n        clients.\n\n        Args:\n            dataset (torch.utils.data.Dataset): The dataset to partition. It should have\n                                                'data' and 'targets' attributes.\n\n        Returns:\n            dict: A dictionary where keys are subset indices (ranging from 0 to partitions_number-1)\n                and values are lists of indices corresponding to the samples in each subset.\n\n        The function randomly shuffles the entire dataset and then splits it into the number\n        of subsets specified by `partitions_number`. It ensures that each subset has a similar number\n        of samples. The function also prints the class distribution in each subset for reference.\n\n        Example usage:\n            federated_data = homo_partition(my_dataset)\n            # This creates federated data subsets with homogeneous distribution.\n        \"\"\"\n        n_nets = self.partitions_number\n\n        n_train = len(dataset.targets)\n        np.random.seed(self.seed)\n        idxs = np.random.permutation(n_train)\n        batch_idxs = np.array_split(idxs, n_nets)\n        net_dataidx_map = {i: batch_idxs[i] for i in range(n_nets)}\n\n        # partitioned_datasets = []\n        for i in range(self.partitions_number):\n            # subset = torch.utils.data.Subset(dataset, net_dataidx_map[i])\n            # partitioned_datasets.append(subset)\n\n            # Print class distribution in the current partition\n            class_counts = [0] * self.num_classes\n            for idx in net_dataidx_map[i]:\n                label = dataset.targets[idx]\n                class_counts[label] += 1\n            logging.info(f\"Partition {i + 1} class distribution: {class_counts}\")\n\n        return net_dataidx_map\n\n    def balanced_iid_partition(self, dataset, n_clients=None):\n        \"\"\"\n        Partition the dataset into balanced and IID (Independent and Identically Distributed)\n        subsets for each client.\n\n        This function divides a dataset into a specified number of subsets (federated clients),\n        where each subset has an equal class distribution. This makes the partition suitable for\n        simulating IID data scenarios in federated learning.\n\n        Args:\n            dataset (list): The dataset to partition. It should be a list of tuples where each\n                            tuple represents a data sample and its corresponding label.\n\n        Returns:\n            dict: A dictionary where keys are client IDs (ranging from 0 to partitions_number-1) and\n                    values are lists of indices corresponding to the samples assigned to each client.\n\n        The function ensures that each class is represented equally in each subset. The\n        partitioning process involves iterating over each class, shuffling the indices of that class,\n        and then splitting them equally among the clients. The function does not print the class\n        distribution in each subset.\n\n        Example usage:\n            federated_data = balanced_iid_partition(my_dataset)\n            # This creates federated data subsets with equal class distributions.\n        \"\"\"\n        logging.info(\"Generating balanced IID partition\")\n        num_clients = self.partitions_number if not n_clients else n_clients\n        clients_data = {i: [] for i in range(num_clients)}\n\n        # Get the labels from the dataset\n        if isinstance(dataset.targets, np.ndarray):\n            labels = dataset.targets\n        elif hasattr(dataset.targets, \"numpy\"):  # Check if it's a tensor with .numpy() method\n            labels = dataset.targets.numpy()\n        else:  # If it's a list\n            labels = np.asarray(dataset.targets)\n\n        label_counts = np.bincount(labels)\n        min_label = label_counts.argmin()\n        min_count = label_counts[min_label]\n\n        for label in range(self.num_classes):\n            if not n_clients:\n                label_indices = np.where(labels == label)[0]\n            else:  # For hybrid dataset scenarios\n                ri = dataset.real_indexes\n                label_indices = np.where(self._targets_reales[ri] == label)[0]\n                label_indices = ri[label_indices]\n\n            np.random.seed(self.seed)\n            np.random.shuffle(label_indices)\n\n            # Split the data based on their labels\n            samples_per_client = min_count // num_clients\n\n            for i in range(num_clients):\n                start_idx = i * samples_per_client\n                end_idx = (i + 1) * samples_per_client\n                clients_data[i].extend(label_indices[start_idx:end_idx])\n\n        return clients_data\n\n    def unbalanced_iid_partition(self, dataset, imbalance_factor=2, n_clients=None):\n        \"\"\"\n        Partition the dataset into multiple IID (Independent and Identically Distributed)\n        subsets with different size.\n\n        This function divides a dataset into a specified number of IID subsets (federated\n        clients), where each subset has a different number of samples. The number of samples\n        in each subset is determined by an imbalance factor, making the partition suitable\n        for simulating imbalanced data scenarios in federated learning.\n\n        Args:\n            dataset (list): The dataset to partition. It should be a list of tuples where\n                            each tuple represents a data sample and its corresponding label.\n            imbalance_factor (float): The factor to determine the degree of imbalance\n                                    among the subsets. A lower imbalance factor leads to more\n                                    imbalanced partitions.\n\n        Returns:\n            dict: A dictionary where keys are client IDs (ranging from 0 to partitions_number-1) and\n                    values are lists of indices corresponding to the samples assigned to each client.\n\n        The function ensures that each class is represented in each subset but with varying\n        proportions. The partitioning process involves iterating over each class, shuffling\n        the indices of that class, and then splitting them according to the calculated subset\n        sizes. The function does not print the class distribution in each subset.\n\n        Example usage:\n            federated_data = unbalanced_iid_partition(my_dataset, imbalance_factor=2)\n            # This creates federated data subsets with varying number of samples based on\n            # an imbalance factor of 2.\n        \"\"\"\n        logging.info(\"Generating unbalanced IID partition\")\n        num_clients = self.partitions_number if not n_clients else n_clients\n        clients_data = {i: [] for i in range(num_clients)}\n\n        # Get the labels from the dataset\n        if not n_clients:\n            labels = np.array([dataset.targets[idx] for idx in range(len(dataset))])\n        else:\n            labels = np.array(self._targets_reales[dataset.real_indexes])\n\n        label_counts = np.bincount(labels)\n        logging.info(f\"label_counts: {label_counts}\")\n\n        min_label = label_counts.argmin()\n        min_count = label_counts[min_label]\n\n        # Set the initial_subset_size\n        initial_subset_size = min_count // num_clients\n\n        # Calculate the number of samples for each subset based on the imbalance factor\n        subset_sizes = [initial_subset_size]\n        for i in range(1, num_clients):\n            subset_sizes.append(int(subset_sizes[i - 1] * ((imbalance_factor - 1) / imbalance_factor)))\n\n        for label in range(self.num_classes):\n            # Get the indices of the same label samples\n            if not n_clients:\n                label_indices = np.where(labels == label)[0]\n            else:  # For hybrid dataset scenarios\n                ri = dataset.real_indexes\n                label_indices = np.where(self._targets_reales[ri] == label)[0]\n                label_indices = ri[label_indices]\n\n            np.random.seed(self.seed)\n            np.random.shuffle(label_indices)\n\n            # Split the data based on their labels\n            start = 0\n            for i in range(num_clients):\n                end = start + subset_sizes[i]\n                clients_data[i].extend(label_indices[start:end])\n                start = end\n\n        return clients_data\n\n    def percentage_partition(self, dataset, percentage=20, n_clients=None):\n        \"\"\"\n        Partition a dataset into multiple subsets with a specified level of non-IID-ness.\n\n        Args:\n            dataset (torch.utils.data.Dataset): The dataset to partition. It should have\n                                                'data' and 'targets' attributes.\n            percentage (int): A value between 0 and 100 that specifies the desired\n                                level of non-IID-ness for the labels of the federated data.\n                                This percentage controls the imbalance in the class distribution\n                                across different subsets.\n\n        Returns:\n            dict: A dictionary where keys are subset indices (ranging from 0 to partitions_number-1)\n                and values are lists of indices corresponding to the samples in each subset.\n\n        Example usage:\n            federated_data = percentage_partition(my_dataset, percentage=20)\n            # This creates federated data subsets with varying class distributions based on\n            # a percentage of 20.\n        \"\"\"\n        if isinstance(dataset.targets, np.ndarray):\n            y_train = dataset.targets\n        elif hasattr(dataset.targets, \"numpy\"):  # Check if it's a tensor with .numpy() method\n            y_train = dataset.targets.numpy()\n        else:  # If it's a list\n            y_train = np.asarray(dataset.targets)\n\n        num_classes = self.num_classes\n        num_subsets = self.partitions_number if not n_clients else n_clients\n\n        if not n_clients:\n            class_indices = {i: np.where(y_train == i)[0] for i in range(num_classes)}\n        else:\n            # TODO adapt, bad right now\n            ri = dataset.real_indexes\n            class_indices = {i: \"\" for i in range(num_classes)}\n\n        # Get the labels from the dataset\n        if not n_clients:\n            labels = np.array([dataset.targets[idx] for idx in range(len(dataset))])\n        else:\n            labels = np.array(self._targets_reales[dataset.real_indexes])\n        label_counts = np.bincount(labels)\n\n        min_label = label_counts.argmin()\n        min_count = label_counts[min_label]\n\n        classes_per_subset = int(num_classes * percentage / 100)\n        if classes_per_subset &lt; 1:\n            raise ValueError(\"The percentage is too low to assign at least one class to each subset.\")\n\n        subset_indices = [[] for _ in range(num_subsets)]\n        class_list = list(range(num_classes))\n        np.random.seed(self.seed)\n        np.random.shuffle(class_list)\n\n        for i in range(num_subsets):\n            for j in range(classes_per_subset):\n                # Use modulo operation to cycle through the class_list\n                class_idx = class_list[(i * classes_per_subset + j) % num_classes]\n                indices = class_indices[class_idx]\n                np.random.seed(self.seed)\n                np.random.shuffle(indices)\n                # Select approximately 50% of the indices\n                subset_indices[i].extend(indices[: min_count // 2])\n\n            class_counts = np.bincount(np.array([dataset.targets[idx] for idx in subset_indices[i]]))\n            logging.info(f\"Partition {i + 1} class distribution: {class_counts.tolist()}\")\n\n        partitioned_datasets = {i: subset_indices[i] for i in range(num_subsets)}\n\n        return partitioned_datasets\n\n    def plot_all_data_distribution(self, phase, dataset, partitions_map):\n        \"\"\"\n\n        Plot all of the data distribution of the dataset according to the partitions map provided.\n\n        Args:\n            dataset: The dataset to plot (torch.utils.data.Dataset).\n            partitions_map: The map of the dataset partitions.\n        \"\"\"\n        sns.set()\n        sns.set_style(\"whitegrid\", {\"axes.grid\": False})\n        sns.set_context(\"paper\", font_scale=1.5)\n        sns.set_palette(\"Set2\")\n\n        num_clients = len(partitions_map)\n        num_classes = self.num_classes\n\n        # Plot number of samples per class in the dataset\n        plt.figure(figsize=(12, 8))\n\n        class_counts = [0] * num_classes\n        for target in dataset.targets:\n            class_counts[target] += 1\n\n        plt.bar(range(num_classes), class_counts, tick_label=dataset.classes)\n        for j, count in enumerate(class_counts):\n            plt.text(j, count, str(count), ha=\"center\", va=\"bottom\", fontsize=12)\n        plt.title(f\"[{phase}] Number of samples per class in the dataset\")\n        plt.xlabel(\"Class\")\n        plt.ylabel(\"Number of samples\")\n        plt.tight_layout()\n\n        path_to_save_class_distribution = f\"{self.config_dir}/full_data_distribution_{'iid' if self.iid else 'non_iid'}{'_' + self.partition if not self.iid else ''}_{phase}.pdf\"\n        plt.savefig(path_to_save_class_distribution, dpi=300, bbox_inches=\"tight\")\n        plt.close()\n\n        # Plot distribution of samples across participants\n        plt.figure(figsize=(12, 8))\n\n        label_distribution = [[] for _ in range(num_classes)]\n        for c_id, idc in partitions_map.items():\n            for idx in idc:\n                label_distribution[dataset.targets[idx]].append(c_id)\n\n        plt.hist(\n            label_distribution,\n            stacked=True,\n            bins=np.arange(-0.5, num_clients + 1.5, 1),\n            label=dataset.classes,\n            rwidth=0.5,\n        )\n        plt.xticks(\n            np.arange(num_clients),\n            [\"Participant %d\" % (c_id + 1) for c_id in range(num_clients)],\n        )\n        plt.title(f\"[{phase}] Distribution of splited datasets\")\n        plt.xlabel(\"Participant\")\n        plt.ylabel(\"Number of samples\")\n        plt.xticks(range(num_clients), [f\" {i}\" for i in range(num_clients)])\n        plt.legend(loc=\"upper right\")\n        plt.tight_layout()\n\n        path_to_save = f\"{self.config_dir}/all_data_distribution_HIST_{'iid' if self.iid else 'non_iid'}{'_' + self.partition if not self.iid else ''}_{phase}.pdf\"\n        plt.savefig(path_to_save, dpi=300, bbox_inches=\"tight\")\n        plt.close()\n\n        plt.figure(figsize=(12, 8))\n        max_point_size = 1200\n        min_point_size = 0\n\n        for i in range(self.partitions_number):\n            class_counts = [0] * self.num_classes\n            indices = partitions_map[i]\n            for idx in indices:\n                label = dataset.targets[idx]\n                class_counts[label] += 1\n\n            # Normalize the point sizes for this partition, handling the case where max_samples_partition is 0\n            max_samples_partition = max(class_counts)\n            if max_samples_partition == 0:\n                logging.warning(f\"[{phase}] Participant {i} has no samples. Skipping size normalization.\")\n                sizes = [min_point_size] * self.num_classes\n            else:\n                sizes = [\n                    (size / max_samples_partition) * (max_point_size - min_point_size) + min_point_size\n                    for size in class_counts\n                ]\n            plt.scatter([i] * self.num_classes, range(self.num_classes), s=sizes, alpha=0.5)\n\n        plt.xlabel(\"Participant\")\n        plt.ylabel(\"Class\")\n        plt.xticks(range(self.partitions_number))\n        plt.yticks(range(self.num_classes))\n        plt.title(\n            f\"[{phase}] Data distribution across participants ({'IID' if self.iid else f'Non-IID - {self.partition}'} - {self.partition_parameter})\"\n        )\n        plt.tight_layout()\n\n        path_to_save = f\"{self.config_dir}/all_data_distribution_CIRCLES_{'iid' if self.iid else 'non_iid'}{'_' + self.partition if not self.iid else ''}_{phase}.pdf\"\n        plt.savefig(path_to_save, dpi=300, bbox_inches=\"tight\")\n        plt.close()\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.balanced_iid_partition","title":"<code>balanced_iid_partition(dataset, n_clients=None)</code>","text":"<p>Partition the dataset into balanced and IID (Independent and Identically Distributed) subsets for each client.</p> <p>This function divides a dataset into a specified number of subsets (federated clients), where each subset has an equal class distribution. This makes the partition suitable for simulating IID data scenarios in federated learning.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>list</code> <p>The dataset to partition. It should be a list of tuples where each             tuple represents a data sample and its corresponding label.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary where keys are client IDs (ranging from 0 to partitions_number-1) and     values are lists of indices corresponding to the samples assigned to each client.</p> <p>The function ensures that each class is represented equally in each subset. The partitioning process involves iterating over each class, shuffling the indices of that class, and then splitting them equally among the clients. The function does not print the class distribution in each subset.</p> Example usage <p>federated_data = balanced_iid_partition(my_dataset)</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def balanced_iid_partition(self, dataset, n_clients=None):\n    \"\"\"\n    Partition the dataset into balanced and IID (Independent and Identically Distributed)\n    subsets for each client.\n\n    This function divides a dataset into a specified number of subsets (federated clients),\n    where each subset has an equal class distribution. This makes the partition suitable for\n    simulating IID data scenarios in federated learning.\n\n    Args:\n        dataset (list): The dataset to partition. It should be a list of tuples where each\n                        tuple represents a data sample and its corresponding label.\n\n    Returns:\n        dict: A dictionary where keys are client IDs (ranging from 0 to partitions_number-1) and\n                values are lists of indices corresponding to the samples assigned to each client.\n\n    The function ensures that each class is represented equally in each subset. The\n    partitioning process involves iterating over each class, shuffling the indices of that class,\n    and then splitting them equally among the clients. The function does not print the class\n    distribution in each subset.\n\n    Example usage:\n        federated_data = balanced_iid_partition(my_dataset)\n        # This creates federated data subsets with equal class distributions.\n    \"\"\"\n    logging.info(\"Generating balanced IID partition\")\n    num_clients = self.partitions_number if not n_clients else n_clients\n    clients_data = {i: [] for i in range(num_clients)}\n\n    # Get the labels from the dataset\n    if isinstance(dataset.targets, np.ndarray):\n        labels = dataset.targets\n    elif hasattr(dataset.targets, \"numpy\"):  # Check if it's a tensor with .numpy() method\n        labels = dataset.targets.numpy()\n    else:  # If it's a list\n        labels = np.asarray(dataset.targets)\n\n    label_counts = np.bincount(labels)\n    min_label = label_counts.argmin()\n    min_count = label_counts[min_label]\n\n    for label in range(self.num_classes):\n        if not n_clients:\n            label_indices = np.where(labels == label)[0]\n        else:  # For hybrid dataset scenarios\n            ri = dataset.real_indexes\n            label_indices = np.where(self._targets_reales[ri] == label)[0]\n            label_indices = ri[label_indices]\n\n        np.random.seed(self.seed)\n        np.random.shuffle(label_indices)\n\n        # Split the data based on their labels\n        samples_per_client = min_count // num_clients\n\n        for i in range(num_clients):\n            start_idx = i * samples_per_client\n            end_idx = (i + 1) * samples_per_client\n            clients_data[i].extend(label_indices[start_idx:end_idx])\n\n    return clients_data\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.balanced_iid_partition--this-creates-federated-data-subsets-with-equal-class-distributions","title":"This creates federated data subsets with equal class distributions.","text":""},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.clear","title":"<code>clear()</code>","text":"<p>Clear the dataset. This should remove or reset the dataset.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def clear(self):\n    \"\"\"\n    Clear the dataset. This should remove or reset the dataset.\n    \"\"\"\n    if self.train_set is not None and hasattr(self.train_set, \"close\"):\n        self.train_set.close()\n    if self.test_set is not None and hasattr(self.test_set, \"close\"):\n        self.test_set.close()\n\n    self.train_set = None\n    self.train_indices_map = None\n    self.test_set = None\n    self.test_indices_map = None\n    self.local_test_indices_map = None\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.data_partitioning","title":"<code>data_partitioning(plot=False)</code>","text":"<p>Perform the data partitioning.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def data_partitioning(self, plot=False):\n    \"\"\"\n    Perform the data partitioning.\n    \"\"\"\n\n    logging.info(\n        f\"Partitioning data for {self.__class__.__name__} | Partitions: {self.partitions_number} | IID: {self.iid} | Partition: {self.partition} | Partition parameter: {self.partition_parameter}\"\n    )\n\n    logging.info(f\"Scenario with data distribution IID: {self.iid}\")\n    if self.iid:\n        self.train_indices_map = self.generate_iid_map(self.train_set)\n    else:\n        self.train_indices_map = self.generate_non_iid_map(\n            self.train_set, partition=self.partition, partition_parameter=self.partition_parameter\n        )\n    # else:\n    #     self.train_indices_map = self.generate_hybrid_map()\n\n    self.test_indices_map = self.get_test_indices_map()\n    self.local_test_indices_map = self.get_local_test_indices_map()\n\n    if plot:\n        self.plot_data_distribution(\"train\", self.train_set, self.train_indices_map)\n        self.plot_all_data_distribution(\"train\", self.train_set, self.train_indices_map)\n        self.plot_data_distribution(\"local_test\", self.test_set, self.local_test_indices_map)\n        self.plot_all_data_distribution(\"local_test\", self.test_set, self.local_test_indices_map)\n\n    self.save_partitions()\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.dirichlet_partition","title":"<code>dirichlet_partition(dataset, alpha=0.2, n_clients=None, min_samples_size=50, balanced=False, max_iter=100, verbose=False)</code>","text":"<p>Partition the dataset among clients using a Dirichlet distribution. This function ensures each client gets at least min_samples_size samples.</p>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.dirichlet_partition--parameters","title":"Parameters","text":"<p>dataset : Dataset     The dataset to partition. Must have a 'targets' attribute. alpha : float, default=0.5     Concentration parameter for the Dirichlet distribution. min_samples_size : int, default=50     Minimum number of samples required in each partition. balanced : bool, default=False     If True, distribute each class evenly among clients.     Otherwise, allocate according to a Dirichlet distribution. max_iter : int, default=100     Maximum number of iterations to try finding a valid partition. verbose : bool, default=True     If True, print debug information per iteration.</p>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.dirichlet_partition--returns","title":"Returns","text":"<p>partitions : dict[int, list[int]]     Dictionary mapping each client index to a list of sample indices.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def dirichlet_partition(\n    self,\n    dataset: Any,\n    alpha: float = 0.2,\n    n_clients=None,\n    min_samples_size: int = 50,\n    balanced: bool = False,\n    max_iter: int = 100,\n    verbose: bool = False,\n) -&gt; dict[int, list[int]]:\n    \"\"\"\n    Partition the dataset among clients using a Dirichlet distribution.\n    This function ensures each client gets at least min_samples_size samples.\n\n    Parameters\n    ----------\n    dataset : Dataset\n        The dataset to partition. Must have a 'targets' attribute.\n    alpha : float, default=0.5\n        Concentration parameter for the Dirichlet distribution.\n    min_samples_size : int, default=50\n        Minimum number of samples required in each partition.\n    balanced : bool, default=False\n        If True, distribute each class evenly among clients.\n        Otherwise, allocate according to a Dirichlet distribution.\n    max_iter : int, default=100\n        Maximum number of iterations to try finding a valid partition.\n    verbose : bool, default=True\n        If True, print debug information per iteration.\n\n    Returns\n    -------\n    partitions : dict[int, list[int]]\n        Dictionary mapping each client index to a list of sample indices.\n    \"\"\"\n    num_clients = self.partitions_number if not n_clients else n_clients\n    logging.info(f\"Generating Dirichlet Partitioning, alpha: {alpha}, num_clients: {num_clients}\")\n\n    # Extract targets and unique labels.\n    if not n_clients:\n        y_data = self._get_targets(dataset)\n        unique_labels = np.unique(y_data)\n    else:\n        if verbose:\n            logging.info(\"Extracting dataset partition targets...\")\n        # For hybrid dataset scenarios\n        y_data = dataset.targets\n        unique_labels = np.unique(y_data)\n    if verbose:\n        logging.info(f\"Unique labels in dataset: {unique_labels}\")\n\n    # For each class, get a shuffled list of indices.\n    class_indices = {}\n    base_rng = np.random.default_rng(self.seed)\n    for label in unique_labels:\n        if not n_clients:\n            idx = np.where(y_data == label)[0]\n        else:\n            ri = dataset.real_indexes\n            idx = np.where(self._targets_reales[ri] == label)[0]\n            idx = ri[idx]\n            # logging.info(f\"attempting: {self._targets_reales[idx]}\")\n\n        base_rng.shuffle(idx)\n        class_indices[label] = idx\n\n    # Prepare container for client indices.\n    indices_per_partition = [[] for _ in range(num_clients)]\n\n    def allocate_for_label(label_idx: np.ndarray, rng: np.random.Generator, n_clients) -&gt; np.ndarray:\n        num_label_samples = len(label_idx)\n        if verbose:\n            logging.info(f\"number of samples allocating {num_label_samples}\")\n        if balanced:\n            proportions = np.full(n_clients, 1.0 / n_clients)\n        else:\n            proportions = rng.dirichlet([alpha] * n_clients)\n        sample_counts = (proportions * num_label_samples).astype(int)\n        remainder = num_label_samples - sample_counts.sum()\n        if remainder &gt; 0:\n            extra_indices = rng.choice(n_clients, size=remainder, replace=False)\n            for idx in extra_indices:\n                sample_counts[idx] += 1\n        if verbose:\n            logging.info(f\"Samples allocated per client: {sample_counts}\")\n        return sample_counts\n\n    for iteration in range(1, max_iter + 1):\n        rng = np.random.default_rng(self.seed + iteration)\n        temp_indices_per_partition = [[] for _ in range(num_clients)]\n        for label in unique_labels:\n            label_idx = class_indices[label]\n            if verbose:\n                logging.info(f\"Calculating samples distribution for label: {label}\")\n            counts = allocate_for_label(label_idx, rng, num_clients)\n            start = 0\n            for client_idx, count in enumerate(counts):\n                end = start + count\n                temp_indices_per_partition[client_idx].extend(label_idx[start:end])\n                if verbose:\n                    logging.info(\n                        f\"Counting check: {np.bincount(self._targets_reales[temp_indices_per_partition[client_idx]])}\"\n                    )\n                start = end\n\n        client_sizes = [len(indices) for indices in temp_indices_per_partition]\n        if min(client_sizes) &gt;= min_samples_size:\n            indices_per_partition = temp_indices_per_partition\n            if verbose:\n                logging.info(f\"Partition successful at iteration {iteration}. Client sizes: {client_sizes}\")\n            break\n        if verbose:\n            logging.info(f\"Iteration {iteration}: client sizes {client_sizes}\")\n\n    else:\n        raise ValueError(\n            f\"Could not create partitions with at least {min_samples_size} samples per client after {max_iter} iterations.\"\n        )\n\n    initial_partition = {i: indices for i, indices in enumerate(indices_per_partition)}\n    final_partition = initial_partition  # self.postprocess_partition(initial_partition, y_data)\n    return final_partition\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.generate_iid_map","title":"<code>generate_iid_map(dataset, plot=False, num_clients=None)</code>  <code>abstractmethod</code>","text":"<p>Create an iid map of the dataset.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>@abstractmethod\ndef generate_iid_map(self, dataset, plot=False, num_clients=None):\n    \"\"\"\n    Create an iid map of the dataset.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.generate_non_iid_map","title":"<code>generate_non_iid_map(dataset, partition='dirichlet', plot=False)</code>  <code>abstractmethod</code>","text":"<p>Create a non-iid map of the dataset.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>@abstractmethod\ndef generate_non_iid_map(self, dataset, partition=\"dirichlet\", plot=False):\n    \"\"\"\n    Create a non-iid map of the dataset.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.get_local_test_indices_map","title":"<code>get_local_test_indices_map()</code>","text":"<p>Get the indices of the local test set for each participant. Indices whose labels are the same as the training set are selected.</p> <p>Returns:</p> Type Description <p>A dictionary mapping participant_id to a list of indices.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def get_local_test_indices_map(self):\n    \"\"\"\n    Get the indices of the local test set for each participant.\n    Indices whose labels are the same as the training set are selected.\n\n    Returns:\n        A dictionary mapping participant_id to a list of indices.\n    \"\"\"\n    try:\n        local_test_indices_map = {}\n        test_targets = np.array(self.test_set.targets)\n        for participant_id in range(self.partitions_number):\n            train_labels = np.array([self.train_set.targets[idx] for idx in self.train_indices_map[participant_id]])\n            indices = np.where(np.isin(test_targets, train_labels))[0].tolist()\n            local_test_indices_map[participant_id] = indices\n        return local_test_indices_map\n    except Exception as e:\n        logging.exception(f\"Error in get_local_test_indices_map: {e}\")\n        raise\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.get_test_indices_map","title":"<code>get_test_indices_map()</code>","text":"<p>Get the indices of the test set for each participant.</p> <p>Returns:</p> Type Description <p>A dictionary mapping participant_id to a list of indices.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def get_test_indices_map(self):\n    \"\"\"\n    Get the indices of the test set for each participant.\n\n    Returns:\n        A dictionary mapping participant_id to a list of indices.\n    \"\"\"\n    try:\n        test_indices_map = {}\n        for participant_id in range(self.partitions_number):\n            test_indices_map[participant_id] = list(range(len(self.test_set)))\n        return test_indices_map\n    except Exception as e:\n        logging.exception(f\"Error in get_test_indices_map: {e}\")\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.homo_partition","title":"<code>homo_partition(dataset)</code>","text":"<p>Homogeneously partition the dataset into multiple subsets.</p> <p>This function divides a dataset into a specified number of subsets, where each subset is intended to have a roughly equal number of samples. This method aims to ensure a homogeneous distribution of data across all subsets. It's particularly useful in scenarios where a uniform distribution of data is desired among all federated learning clients.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>The dataset to partition. It should have                                 'data' and 'targets' attributes.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary where keys are subset indices (ranging from 0 to partitions_number-1) and values are lists of indices corresponding to the samples in each subset.</p> <p>The function randomly shuffles the entire dataset and then splits it into the number of subsets specified by <code>partitions_number</code>. It ensures that each subset has a similar number of samples. The function also prints the class distribution in each subset for reference.</p> Example usage <p>federated_data = homo_partition(my_dataset)</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def homo_partition(self, dataset):\n    \"\"\"\n    Homogeneously partition the dataset into multiple subsets.\n\n    This function divides a dataset into a specified number of subsets, where each subset\n    is intended to have a roughly equal number of samples. This method aims to ensure a\n    homogeneous distribution of data across all subsets. It's particularly useful in\n    scenarios where a uniform distribution of data is desired among all federated learning\n    clients.\n\n    Args:\n        dataset (torch.utils.data.Dataset): The dataset to partition. It should have\n                                            'data' and 'targets' attributes.\n\n    Returns:\n        dict: A dictionary where keys are subset indices (ranging from 0 to partitions_number-1)\n            and values are lists of indices corresponding to the samples in each subset.\n\n    The function randomly shuffles the entire dataset and then splits it into the number\n    of subsets specified by `partitions_number`. It ensures that each subset has a similar number\n    of samples. The function also prints the class distribution in each subset for reference.\n\n    Example usage:\n        federated_data = homo_partition(my_dataset)\n        # This creates federated data subsets with homogeneous distribution.\n    \"\"\"\n    n_nets = self.partitions_number\n\n    n_train = len(dataset.targets)\n    np.random.seed(self.seed)\n    idxs = np.random.permutation(n_train)\n    batch_idxs = np.array_split(idxs, n_nets)\n    net_dataidx_map = {i: batch_idxs[i] for i in range(n_nets)}\n\n    # partitioned_datasets = []\n    for i in range(self.partitions_number):\n        # subset = torch.utils.data.Subset(dataset, net_dataidx_map[i])\n        # partitioned_datasets.append(subset)\n\n        # Print class distribution in the current partition\n        class_counts = [0] * self.num_classes\n        for idx in net_dataidx_map[i]:\n            label = dataset.targets[idx]\n            class_counts[label] += 1\n        logging.info(f\"Partition {i + 1} class distribution: {class_counts}\")\n\n    return net_dataidx_map\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.homo_partition--this-creates-federated-data-subsets-with-homogeneous-distribution","title":"This creates federated data subsets with homogeneous distribution.","text":""},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.initialize_dataset","title":"<code>initialize_dataset()</code>  <code>abstractmethod</code>","text":"<p>Initialize the dataset. This should load or create the dataset.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>@abstractmethod\ndef initialize_dataset(self):\n    \"\"\"\n    Initialize the dataset. This should load or create the dataset.\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement this method.\")\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.percentage_partition","title":"<code>percentage_partition(dataset, percentage=20, n_clients=None)</code>","text":"<p>Partition a dataset into multiple subsets with a specified level of non-IID-ness.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>The dataset to partition. It should have                                 'data' and 'targets' attributes.</p> required <code>percentage</code> <code>int</code> <p>A value between 0 and 100 that specifies the desired                 level of non-IID-ness for the labels of the federated data.                 This percentage controls the imbalance in the class distribution                 across different subsets.</p> <code>20</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary where keys are subset indices (ranging from 0 to partitions_number-1) and values are lists of indices corresponding to the samples in each subset.</p> Example usage <p>federated_data = percentage_partition(my_dataset, percentage=20)</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def percentage_partition(self, dataset, percentage=20, n_clients=None):\n    \"\"\"\n    Partition a dataset into multiple subsets with a specified level of non-IID-ness.\n\n    Args:\n        dataset (torch.utils.data.Dataset): The dataset to partition. It should have\n                                            'data' and 'targets' attributes.\n        percentage (int): A value between 0 and 100 that specifies the desired\n                            level of non-IID-ness for the labels of the federated data.\n                            This percentage controls the imbalance in the class distribution\n                            across different subsets.\n\n    Returns:\n        dict: A dictionary where keys are subset indices (ranging from 0 to partitions_number-1)\n            and values are lists of indices corresponding to the samples in each subset.\n\n    Example usage:\n        federated_data = percentage_partition(my_dataset, percentage=20)\n        # This creates federated data subsets with varying class distributions based on\n        # a percentage of 20.\n    \"\"\"\n    if isinstance(dataset.targets, np.ndarray):\n        y_train = dataset.targets\n    elif hasattr(dataset.targets, \"numpy\"):  # Check if it's a tensor with .numpy() method\n        y_train = dataset.targets.numpy()\n    else:  # If it's a list\n        y_train = np.asarray(dataset.targets)\n\n    num_classes = self.num_classes\n    num_subsets = self.partitions_number if not n_clients else n_clients\n\n    if not n_clients:\n        class_indices = {i: np.where(y_train == i)[0] for i in range(num_classes)}\n    else:\n        # TODO adapt, bad right now\n        ri = dataset.real_indexes\n        class_indices = {i: \"\" for i in range(num_classes)}\n\n    # Get the labels from the dataset\n    if not n_clients:\n        labels = np.array([dataset.targets[idx] for idx in range(len(dataset))])\n    else:\n        labels = np.array(self._targets_reales[dataset.real_indexes])\n    label_counts = np.bincount(labels)\n\n    min_label = label_counts.argmin()\n    min_count = label_counts[min_label]\n\n    classes_per_subset = int(num_classes * percentage / 100)\n    if classes_per_subset &lt; 1:\n        raise ValueError(\"The percentage is too low to assign at least one class to each subset.\")\n\n    subset_indices = [[] for _ in range(num_subsets)]\n    class_list = list(range(num_classes))\n    np.random.seed(self.seed)\n    np.random.shuffle(class_list)\n\n    for i in range(num_subsets):\n        for j in range(classes_per_subset):\n            # Use modulo operation to cycle through the class_list\n            class_idx = class_list[(i * classes_per_subset + j) % num_classes]\n            indices = class_indices[class_idx]\n            np.random.seed(self.seed)\n            np.random.shuffle(indices)\n            # Select approximately 50% of the indices\n            subset_indices[i].extend(indices[: min_count // 2])\n\n        class_counts = np.bincount(np.array([dataset.targets[idx] for idx in subset_indices[i]]))\n        logging.info(f\"Partition {i + 1} class distribution: {class_counts.tolist()}\")\n\n    partitioned_datasets = {i: subset_indices[i] for i in range(num_subsets)}\n\n    return partitioned_datasets\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.percentage_partition--this-creates-federated-data-subsets-with-varying-class-distributions-based-on","title":"This creates federated data subsets with varying class distributions based on","text":""},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.percentage_partition--a-percentage-of-20","title":"a percentage of 20.","text":""},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.plot_all_data_distribution","title":"<code>plot_all_data_distribution(phase, dataset, partitions_map)</code>","text":"<p>Plot all of the data distribution of the dataset according to the partitions map provided.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <p>The dataset to plot (torch.utils.data.Dataset).</p> required <code>partitions_map</code> <p>The map of the dataset partitions.</p> required Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def plot_all_data_distribution(self, phase, dataset, partitions_map):\n    \"\"\"\n\n    Plot all of the data distribution of the dataset according to the partitions map provided.\n\n    Args:\n        dataset: The dataset to plot (torch.utils.data.Dataset).\n        partitions_map: The map of the dataset partitions.\n    \"\"\"\n    sns.set()\n    sns.set_style(\"whitegrid\", {\"axes.grid\": False})\n    sns.set_context(\"paper\", font_scale=1.5)\n    sns.set_palette(\"Set2\")\n\n    num_clients = len(partitions_map)\n    num_classes = self.num_classes\n\n    # Plot number of samples per class in the dataset\n    plt.figure(figsize=(12, 8))\n\n    class_counts = [0] * num_classes\n    for target in dataset.targets:\n        class_counts[target] += 1\n\n    plt.bar(range(num_classes), class_counts, tick_label=dataset.classes)\n    for j, count in enumerate(class_counts):\n        plt.text(j, count, str(count), ha=\"center\", va=\"bottom\", fontsize=12)\n    plt.title(f\"[{phase}] Number of samples per class in the dataset\")\n    plt.xlabel(\"Class\")\n    plt.ylabel(\"Number of samples\")\n    plt.tight_layout()\n\n    path_to_save_class_distribution = f\"{self.config_dir}/full_data_distribution_{'iid' if self.iid else 'non_iid'}{'_' + self.partition if not self.iid else ''}_{phase}.pdf\"\n    plt.savefig(path_to_save_class_distribution, dpi=300, bbox_inches=\"tight\")\n    plt.close()\n\n    # Plot distribution of samples across participants\n    plt.figure(figsize=(12, 8))\n\n    label_distribution = [[] for _ in range(num_classes)]\n    for c_id, idc in partitions_map.items():\n        for idx in idc:\n            label_distribution[dataset.targets[idx]].append(c_id)\n\n    plt.hist(\n        label_distribution,\n        stacked=True,\n        bins=np.arange(-0.5, num_clients + 1.5, 1),\n        label=dataset.classes,\n        rwidth=0.5,\n    )\n    plt.xticks(\n        np.arange(num_clients),\n        [\"Participant %d\" % (c_id + 1) for c_id in range(num_clients)],\n    )\n    plt.title(f\"[{phase}] Distribution of splited datasets\")\n    plt.xlabel(\"Participant\")\n    plt.ylabel(\"Number of samples\")\n    plt.xticks(range(num_clients), [f\" {i}\" for i in range(num_clients)])\n    plt.legend(loc=\"upper right\")\n    plt.tight_layout()\n\n    path_to_save = f\"{self.config_dir}/all_data_distribution_HIST_{'iid' if self.iid else 'non_iid'}{'_' + self.partition if not self.iid else ''}_{phase}.pdf\"\n    plt.savefig(path_to_save, dpi=300, bbox_inches=\"tight\")\n    plt.close()\n\n    plt.figure(figsize=(12, 8))\n    max_point_size = 1200\n    min_point_size = 0\n\n    for i in range(self.partitions_number):\n        class_counts = [0] * self.num_classes\n        indices = partitions_map[i]\n        for idx in indices:\n            label = dataset.targets[idx]\n            class_counts[label] += 1\n\n        # Normalize the point sizes for this partition, handling the case where max_samples_partition is 0\n        max_samples_partition = max(class_counts)\n        if max_samples_partition == 0:\n            logging.warning(f\"[{phase}] Participant {i} has no samples. Skipping size normalization.\")\n            sizes = [min_point_size] * self.num_classes\n        else:\n            sizes = [\n                (size / max_samples_partition) * (max_point_size - min_point_size) + min_point_size\n                for size in class_counts\n            ]\n        plt.scatter([i] * self.num_classes, range(self.num_classes), s=sizes, alpha=0.5)\n\n    plt.xlabel(\"Participant\")\n    plt.ylabel(\"Class\")\n    plt.xticks(range(self.partitions_number))\n    plt.yticks(range(self.num_classes))\n    plt.title(\n        f\"[{phase}] Data distribution across participants ({'IID' if self.iid else f'Non-IID - {self.partition}'} - {self.partition_parameter})\"\n    )\n    plt.tight_layout()\n\n    path_to_save = f\"{self.config_dir}/all_data_distribution_CIRCLES_{'iid' if self.iid else 'non_iid'}{'_' + self.partition if not self.iid else ''}_{phase}.pdf\"\n    plt.savefig(path_to_save, dpi=300, bbox_inches=\"tight\")\n    plt.close()\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.plot_data_distribution","title":"<code>plot_data_distribution(phase, dataset, partitions_map)</code>","text":"<p>Plot the data distribution of the dataset.</p> <p>Plot the data distribution of the dataset according to the partitions map provided.</p> <p>Parameters:</p> Name Type Description Default <code>phase</code> <p>The phase of the dataset (train, test, local_test).</p> required <code>dataset</code> <p>The dataset to plot (torch.utils.data.Dataset).</p> required <code>partitions_map</code> <p>The map of the dataset partitions.</p> required Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def plot_data_distribution(self, phase, dataset, partitions_map):\n    \"\"\"\n    Plot the data distribution of the dataset.\n\n    Plot the data distribution of the dataset according to the partitions map provided.\n\n    Args:\n        phase: The phase of the dataset (train, test, local_test).\n        dataset: The dataset to plot (torch.utils.data.Dataset).\n        partitions_map: The map of the dataset partitions.\n    \"\"\"\n    logging_training.info(f\"Plotting data distribution for {phase} dataset\")\n    # Plot the data distribution of the dataset, one graph per partition\n    sns.set()\n    sns.set_style(\"whitegrid\", {\"axes.grid\": False})\n    sns.set_context(\"paper\", font_scale=1.5)\n    sns.set_palette(\"Set2\")\n\n    # Plot bar charts for each partition\n    partition_index = 0\n    for partition_index in range(self.partitions_number):\n        indices = partitions_map[partition_index]\n        class_counts = [0] * self.num_classes\n        for idx in indices:\n            label = dataset.targets[idx]\n            class_counts[label] += 1\n\n        logging_training.info(f\"[{phase}] Participant {partition_index} total samples: {len(indices)}\")\n        logging_training.info(f\"[{phase}] Participant {partition_index} data distribution: {class_counts}\")\n\n        plt.figure(figsize=(12, 8))\n        plt.bar(range(self.num_classes), class_counts)\n        for j, count in enumerate(class_counts):\n            plt.text(j, count, str(count), ha=\"center\", va=\"bottom\", fontsize=12)\n        plt.xlabel(\"Class\")\n        plt.ylabel(\"Number of samples\")\n        plt.xticks(range(self.num_classes))\n        plt.title(\n            f\"[{phase}] Participant {partition_index} data distribution ({'IID' if self.iid else f'Non-IID - {self.partition}'} - {self.partition_parameter})\"\n        )\n        plt.tight_layout()\n        path_to_save = f\"{self.config_dir}/participant_{partition_index}_data_distribution_{'iid' if self.iid else 'non_iid'}{'_' + self.partition if not self.iid else ''}_{phase}.pdf\"\n        logging_training.info(f\"Saving data distribution for participant {partition_index} to {path_to_save}\")\n        plt.savefig(path_to_save, dpi=300, bbox_inches=\"tight\")\n        plt.close()\n\n    if hasattr(self, \"tsne\") and self.tsne:\n        self.visualize_tsne(dataset)\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.postprocess_partition","title":"<code>postprocess_partition(partition, y_data, min_samples_per_class=10)</code>","text":"<p>Post-process a partition to remove (and reassign) classes with too few samples per client.</p> <p>For each class: - For clients with a count &gt; 0 but below min_samples_per_class, remove those samples. - Reassign the removed samples to the client that already has the maximum count for that class.</p>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.postprocess_partition--parameters","title":"Parameters","text":"<p>partition : dict[int, list[int]]     The initial partition mapping client indices to sample indices. y_data : np.ndarray     The array of labels corresponding to the dataset samples. min_samples_per_class : int, default=10     The minimum acceptable number of samples per class for each client.</p>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.postprocess_partition--returns","title":"Returns","text":"<p>new_partition : dict[int, list[int]]     The updated partition.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def postprocess_partition(\n    self, partition: dict[int, list[int]], y_data: np.ndarray, min_samples_per_class: int = 10\n) -&gt; dict[int, list[int]]:\n    \"\"\"\n    Post-process a partition to remove (and reassign) classes with too few samples per client.\n\n    For each class:\n    - For clients with a count &gt; 0 but below min_samples_per_class, remove those samples.\n    - Reassign the removed samples to the client that already has the maximum count for that class.\n\n    Parameters\n    ----------\n    partition : dict[int, list[int]]\n        The initial partition mapping client indices to sample indices.\n    y_data : np.ndarray\n        The array of labels corresponding to the dataset samples.\n    min_samples_per_class : int, default=10\n        The minimum acceptable number of samples per class for each client.\n\n    Returns\n    -------\n    new_partition : dict[int, list[int]]\n        The updated partition.\n    \"\"\"\n    # Copy partition so we can modify it.\n    new_partition = {client: list(indices) for client, indices in partition.items()}\n\n    # Iterate over each class.\n    for label in np.unique(y_data):\n        # For each client, count how many samples of this label exist.\n        client_counts = {}\n        for client, indices in new_partition.items():\n            client_counts[client] = np.sum(np.array(y_data)[indices] == label)\n\n        # Identify clients with fewer than min_samples_per_class but nonzero counts.\n        donors = [client for client, count in client_counts.items() if 0 &lt; count &lt; min_samples_per_class]\n        # Identify potential recipients: those with at least min_samples_per_class.\n        recipients = [client for client, count in client_counts.items() if count &gt;= min_samples_per_class]\n        # If no client meets the threshold, choose the one with the highest count.\n        if not recipients:\n            best_recipient = max(client_counts, key=client_counts.get)\n            recipients = [best_recipient]\n        # Choose the recipient with the maximum count.\n        best_recipient = max(recipients, key=lambda c: client_counts[c])\n\n        # For each donor, remove samples of this label and reassign them.\n        for donor in donors:\n            donor_indices = new_partition[donor]\n            # Identify indices corresponding to this label.\n            donor_label_indices = [idx for idx in donor_indices if y_data[idx] == label]\n            # Remove these from the donor.\n            new_partition[donor] = [idx for idx in donor_indices if y_data[idx] != label]\n            # Add these to the best recipient.\n            new_partition[best_recipient].extend(donor_label_indices)\n\n    return new_partition\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.save_partitions","title":"<code>save_partitions()</code>","text":"<p>Save each partition data (train, test, and local test) to separate pickle files. The controller saves one file per partition for each data split.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def save_partitions(self):\n    \"\"\"\n    Save each partition data (train, test, and local test) to separate pickle files.\n    The controller saves one file per partition for each data split.\n    \"\"\"\n    try:\n        logging.info(f\"Saving partitions data for ALL participants ({self.partitions_number}) in {self.config_dir}\")\n        path = self.config_dir\n        if not os.path.exists(path):\n            raise FileNotFoundError(f\"Path {path} does not exist\")\n        # Check that the partition maps have the expected number of partitions\n        if not (\n            len(self.train_indices_map)\n            == len(self.test_indices_map)\n            == len(self.local_test_indices_map)\n            == self.partitions_number\n        ):\n            raise ValueError(\"One of the partition maps has an unexpected length.\")\n\n        # Save global test data\n        file_name = os.path.join(path, \"global_test.h5\")\n        with h5py.File(file_name, \"w\") as f:\n            indices = list(range(len(self.test_set)))\n            test_data = [self.test_set[i] for i in indices]\n            self.save_partition(test_data, f, \"test_data\")\n            f[\"test_data\"].attrs[\"num_classes\"] = self.num_classes\n            test_targets = np.array(self.test_set.targets)\n            f.create_dataset(\"test_targets\", data=test_targets, compression=\"gzip\")\n\n        for participant in range(self.partitions_number):\n            file_name = os.path.join(path, f\"participant_{participant}_train.h5\")\n            with h5py.File(file_name, \"w\") as f:\n                logging.info(f\"Saving training data for participant {participant} in {file_name}\")\n                indices = self.train_indices_map[participant]\n                train_data = [self.train_set[i] for i in indices]\n                self.save_partition(train_data, f, \"train_data\")\n                f[\"train_data\"].attrs[\"num_classes\"] = self.num_classes\n                train_targets = np.array([self.train_set.targets[i] for i in indices])\n                f.create_dataset(\"train_targets\", data=train_targets, compression=\"gzip\")\n                logging.info(f\"Partition saved for participant {participant}.\")\n\n        logging.info(\"Successfully saved all partition files.\")\n\n    except Exception as e:\n        logging.exception(f\"Error in save_partitions: {e}\")\n\n    finally:\n        self.clear()\n        logging.info(\"Cleared dataset after saving partitions.\")\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.unbalanced_iid_partition","title":"<code>unbalanced_iid_partition(dataset, imbalance_factor=2, n_clients=None)</code>","text":"<p>Partition the dataset into multiple IID (Independent and Identically Distributed) subsets with different size.</p> <p>This function divides a dataset into a specified number of IID subsets (federated clients), where each subset has a different number of samples. The number of samples in each subset is determined by an imbalance factor, making the partition suitable for simulating imbalanced data scenarios in federated learning.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>list</code> <p>The dataset to partition. It should be a list of tuples where             each tuple represents a data sample and its corresponding label.</p> required <code>imbalance_factor</code> <code>float</code> <p>The factor to determine the degree of imbalance                     among the subsets. A lower imbalance factor leads to more                     imbalanced partitions.</p> <code>2</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary where keys are client IDs (ranging from 0 to partitions_number-1) and     values are lists of indices corresponding to the samples assigned to each client.</p> <p>The function ensures that each class is represented in each subset but with varying proportions. The partitioning process involves iterating over each class, shuffling the indices of that class, and then splitting them according to the calculated subset sizes. The function does not print the class distribution in each subset.</p> Example usage <p>federated_data = unbalanced_iid_partition(my_dataset, imbalance_factor=2)</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def unbalanced_iid_partition(self, dataset, imbalance_factor=2, n_clients=None):\n    \"\"\"\n    Partition the dataset into multiple IID (Independent and Identically Distributed)\n    subsets with different size.\n\n    This function divides a dataset into a specified number of IID subsets (federated\n    clients), where each subset has a different number of samples. The number of samples\n    in each subset is determined by an imbalance factor, making the partition suitable\n    for simulating imbalanced data scenarios in federated learning.\n\n    Args:\n        dataset (list): The dataset to partition. It should be a list of tuples where\n                        each tuple represents a data sample and its corresponding label.\n        imbalance_factor (float): The factor to determine the degree of imbalance\n                                among the subsets. A lower imbalance factor leads to more\n                                imbalanced partitions.\n\n    Returns:\n        dict: A dictionary where keys are client IDs (ranging from 0 to partitions_number-1) and\n                values are lists of indices corresponding to the samples assigned to each client.\n\n    The function ensures that each class is represented in each subset but with varying\n    proportions. The partitioning process involves iterating over each class, shuffling\n    the indices of that class, and then splitting them according to the calculated subset\n    sizes. The function does not print the class distribution in each subset.\n\n    Example usage:\n        federated_data = unbalanced_iid_partition(my_dataset, imbalance_factor=2)\n        # This creates federated data subsets with varying number of samples based on\n        # an imbalance factor of 2.\n    \"\"\"\n    logging.info(\"Generating unbalanced IID partition\")\n    num_clients = self.partitions_number if not n_clients else n_clients\n    clients_data = {i: [] for i in range(num_clients)}\n\n    # Get the labels from the dataset\n    if not n_clients:\n        labels = np.array([dataset.targets[idx] for idx in range(len(dataset))])\n    else:\n        labels = np.array(self._targets_reales[dataset.real_indexes])\n\n    label_counts = np.bincount(labels)\n    logging.info(f\"label_counts: {label_counts}\")\n\n    min_label = label_counts.argmin()\n    min_count = label_counts[min_label]\n\n    # Set the initial_subset_size\n    initial_subset_size = min_count // num_clients\n\n    # Calculate the number of samples for each subset based on the imbalance factor\n    subset_sizes = [initial_subset_size]\n    for i in range(1, num_clients):\n        subset_sizes.append(int(subset_sizes[i - 1] * ((imbalance_factor - 1) / imbalance_factor)))\n\n    for label in range(self.num_classes):\n        # Get the indices of the same label samples\n        if not n_clients:\n            label_indices = np.where(labels == label)[0]\n        else:  # For hybrid dataset scenarios\n            ri = dataset.real_indexes\n            label_indices = np.where(self._targets_reales[ri] == label)[0]\n            label_indices = ri[label_indices]\n\n        np.random.seed(self.seed)\n        np.random.shuffle(label_indices)\n\n        # Split the data based on their labels\n        start = 0\n        for i in range(num_clients):\n            end = start + subset_sizes[i]\n            clients_data[i].extend(label_indices[start:end])\n            start = end\n\n    return clients_data\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.unbalanced_iid_partition--this-creates-federated-data-subsets-with-varying-number-of-samples-based-on","title":"This creates federated data subsets with varying number of samples based on","text":""},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.unbalanced_iid_partition--an-imbalance-factor-of-2","title":"an imbalance factor of 2.","text":""},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaPartition","title":"<code>NebulaPartition</code>","text":"<p>A class to handle the partitioning of datasets for federated learning.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>class NebulaPartition:\n    \"\"\"\n    A class to handle the partitioning of datasets for federated learning.\n    \"\"\"\n\n    def __init__(self, handler: NebulaPartitionHandler, config: dict[str, Any] | None = None):\n        self.handler = handler\n        self.config = config if config is not None else {}\n\n        self.train_set = None\n        self.train_indices = None\n\n        self.test_set = None\n        self.test_indices = None\n\n        self.local_test_set = None\n        self.local_test_indices = None\n\n        enable_deterministic(seed=self.config.participant[\"scenario_args\"][\"random_seed\"])\n\n    def get_train_indices(self):\n        \"\"\"\n        Get the indices of the training set based on the indices map.\n        \"\"\"\n        if self.train_indices is None:\n            return None\n        return self.train_indices\n\n    def get_test_indices(self):\n        \"\"\"\n        Get the indices of the test set based on the indices map.\n        \"\"\"\n        if self.test_indices is None:\n            return None\n        return self.test_indices\n\n    def get_local_test_indices(self):\n        \"\"\"\n        Get the indices of the local test set based on the indices map.\n        \"\"\"\n        if self.local_test_indices is None:\n            return None\n        return self.local_test_indices\n\n    def get_train_labels(self):\n        \"\"\"\n        Get the labels of the training set based on the indices map.\n        \"\"\"\n        if self.train_indices is None:\n            return None\n        return [self.train_set.targets[idx] for idx in self.train_indices]\n\n    def get_test_labels(self):\n        \"\"\"\n        Get the labels of the test set based on the indices map.\n        \"\"\"\n        if self.test_indices is None:\n            return None\n        return [self.test_set.targets[idx] for idx in self.test_indices]\n\n    def get_local_test_labels(self):\n        \"\"\"\n        Get the labels of the test set based on the indices map.\n        \"\"\"\n        if self.local_test_indices is None:\n            return None\n        return [self.test_set.targets[idx] for idx in self.local_test_indices]\n\n    def set_local_test_indices(self):\n        \"\"\"\n        Set the local test indices for the current node.\n        \"\"\"\n        test_labels = self.get_test_labels()\n        train_labels = self.get_train_labels()\n\n        if test_labels is None or train_labels is None:\n            logging_training.warning(\"Either test_labels or train_labels is None in set_local_test_indices\")\n            return []\n\n        if self.test_set is None:\n            logging_training.warning(\"test_set is None in set_local_test_indices\")\n            return []\n\n        return [idx for idx in range(len(self.test_set)) if test_labels[idx] in train_labels]\n\n    def log_partition(self):\n        logging_training.info(f\"{'=' * 10}\")\n        logging_training.info(\n            f\"LOG NEBULA PARTITION DATASET [Participant {self.config.participant['device_args']['idx']}]\"\n        )\n        logging_training.info(f\"{'=' * 10}\")\n        logging_training.info(f\"TRAIN - Train labels unique: {set(self.get_train_labels())}\")\n        logging_training.info(f\"TRAIN - Length of train indices map: {len(self.get_train_indices())}\")\n        logging_training.info(f\"{'=' * 10}\")\n        logging_training.info(f\"LOCAL - Test labels unique: {set(self.get_local_test_labels())}\")\n        logging_training.info(f\"LOCAL - Length of test indices map: {len(self.get_local_test_indices())}\")\n        logging_training.info(f\"{'=' * 10}\")\n        logging_training.info(f\"GLOBAL - Test labels unique: {set(self.get_test_labels())}\")\n        logging_training.info(f\"GLOBAL - Length of test indices map: {len(self.get_test_indices())}\")\n        logging_training.info(f\"{'=' * 10}\")\n\n    def load_partition(self):\n        \"\"\"\n        Load only the partition data corresponding to the current node.\n        The node loads its train, test, and local test partition data from HDF5 files.\n        \"\"\"\n        try:\n            p = self.config.participant[\"device_args\"][\"idx\"]\n            logging_training.info(f\"Loading partition data for participant {p}\")\n            path = self.config.participant[\"tracking_args\"][\"config_dir\"]\n\n            train_partition_file = os.path.join(path, f\"participant_{p}_train.h5\")\n            wait_for_file(train_partition_file)\n            logging_training.info(f\"Loading train data from {train_partition_file}\")\n            self.train_set = self.handler(train_partition_file, \"train\", config=self.config)\n            self.train_indices = list(range(len(self.train_set)))\n\n            test_partition_file = os.path.join(path, \"global_test.h5\")\n            wait_for_file(test_partition_file)\n            logging_training.info(f\"Loading test data from {test_partition_file}\")\n            self.test_set = self.handler(test_partition_file, \"test\", config=self.config)\n            self.test_indices = list(range(len(self.test_set)))\n\n            self.local_test_set = self.handler(test_partition_file, \"local_test\", config=self.config, empty=True)\n            self.local_test_set.set_data(self.test_set.data, self.test_set.targets)\n            self.local_test_indices = self.set_local_test_indices()\n\n            logging_training.info(f\"Successfully loaded partition data for participant {p}.\")\n        except Exception as e:\n            logging_training.error(f\"Error loading partition: {e}\")\n            raise\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaPartition.get_local_test_indices","title":"<code>get_local_test_indices()</code>","text":"<p>Get the indices of the local test set based on the indices map.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def get_local_test_indices(self):\n    \"\"\"\n    Get the indices of the local test set based on the indices map.\n    \"\"\"\n    if self.local_test_indices is None:\n        return None\n    return self.local_test_indices\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaPartition.get_local_test_labels","title":"<code>get_local_test_labels()</code>","text":"<p>Get the labels of the test set based on the indices map.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def get_local_test_labels(self):\n    \"\"\"\n    Get the labels of the test set based on the indices map.\n    \"\"\"\n    if self.local_test_indices is None:\n        return None\n    return [self.test_set.targets[idx] for idx in self.local_test_indices]\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaPartition.get_test_indices","title":"<code>get_test_indices()</code>","text":"<p>Get the indices of the test set based on the indices map.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def get_test_indices(self):\n    \"\"\"\n    Get the indices of the test set based on the indices map.\n    \"\"\"\n    if self.test_indices is None:\n        return None\n    return self.test_indices\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaPartition.get_test_labels","title":"<code>get_test_labels()</code>","text":"<p>Get the labels of the test set based on the indices map.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def get_test_labels(self):\n    \"\"\"\n    Get the labels of the test set based on the indices map.\n    \"\"\"\n    if self.test_indices is None:\n        return None\n    return [self.test_set.targets[idx] for idx in self.test_indices]\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaPartition.get_train_indices","title":"<code>get_train_indices()</code>","text":"<p>Get the indices of the training set based on the indices map.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def get_train_indices(self):\n    \"\"\"\n    Get the indices of the training set based on the indices map.\n    \"\"\"\n    if self.train_indices is None:\n        return None\n    return self.train_indices\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaPartition.get_train_labels","title":"<code>get_train_labels()</code>","text":"<p>Get the labels of the training set based on the indices map.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def get_train_labels(self):\n    \"\"\"\n    Get the labels of the training set based on the indices map.\n    \"\"\"\n    if self.train_indices is None:\n        return None\n    return [self.train_set.targets[idx] for idx in self.train_indices]\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaPartition.load_partition","title":"<code>load_partition()</code>","text":"<p>Load only the partition data corresponding to the current node. The node loads its train, test, and local test partition data from HDF5 files.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def load_partition(self):\n    \"\"\"\n    Load only the partition data corresponding to the current node.\n    The node loads its train, test, and local test partition data from HDF5 files.\n    \"\"\"\n    try:\n        p = self.config.participant[\"device_args\"][\"idx\"]\n        logging_training.info(f\"Loading partition data for participant {p}\")\n        path = self.config.participant[\"tracking_args\"][\"config_dir\"]\n\n        train_partition_file = os.path.join(path, f\"participant_{p}_train.h5\")\n        wait_for_file(train_partition_file)\n        logging_training.info(f\"Loading train data from {train_partition_file}\")\n        self.train_set = self.handler(train_partition_file, \"train\", config=self.config)\n        self.train_indices = list(range(len(self.train_set)))\n\n        test_partition_file = os.path.join(path, \"global_test.h5\")\n        wait_for_file(test_partition_file)\n        logging_training.info(f\"Loading test data from {test_partition_file}\")\n        self.test_set = self.handler(test_partition_file, \"test\", config=self.config)\n        self.test_indices = list(range(len(self.test_set)))\n\n        self.local_test_set = self.handler(test_partition_file, \"local_test\", config=self.config, empty=True)\n        self.local_test_set.set_data(self.test_set.data, self.test_set.targets)\n        self.local_test_indices = self.set_local_test_indices()\n\n        logging_training.info(f\"Successfully loaded partition data for participant {p}.\")\n    except Exception as e:\n        logging_training.error(f\"Error loading partition: {e}\")\n        raise\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaPartition.set_local_test_indices","title":"<code>set_local_test_indices()</code>","text":"<p>Set the local test indices for the current node.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def set_local_test_indices(self):\n    \"\"\"\n    Set the local test indices for the current node.\n    \"\"\"\n    test_labels = self.get_test_labels()\n    train_labels = self.get_train_labels()\n\n    if test_labels is None or train_labels is None:\n        logging_training.warning(\"Either test_labels or train_labels is None in set_local_test_indices\")\n        return []\n\n    if self.test_set is None:\n        logging_training.warning(\"test_set is None in set_local_test_indices\")\n        return []\n\n    return [idx for idx in range(len(self.test_set)) if test_labels[idx] in train_labels]\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaPartitionHandler","title":"<code>NebulaPartitionHandler</code>","text":"<p>               Bases: <code>Dataset</code>, <code>ABC</code></p> <p>A class to handle the loading of datasets from HDF5 files.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>class NebulaPartitionHandler(Dataset, ABC):\n    \"\"\"\n    A class to handle the loading of datasets from HDF5 files.\n    \"\"\"\n\n    def __init__(\n        self,\n        file_path: str,\n        prefix: str = \"train\",\n        config: dict[str, Any] | None = None,\n        empty: bool = False,\n    ):\n        self.file_path = file_path\n        self.prefix = prefix\n        self.config = config\n        self.empty = empty\n        self.transform = None\n        self.target_transform = None\n        self.file = None\n\n        self.data = None\n        self.targets = None\n        self.num_classes = None\n        self.length = None\n\n        self.load_data()\n\n    def load_data(self):\n        if self.empty:\n            logging_training.info(\n                f\"[NebulaPartitionHandler] No data loaded for {self.prefix} partition. Empty dataset.\"\n            )\n            return\n        with h5py.File(self.file_path, \"r\") as f:\n            prefix = (\n                \"test\" if self.prefix == \"local_test\" else self.prefix\n            )  # Local test uses the test prefix (same data but different split)\n            self.data = self.load_partition(f, f\"{prefix}_data\")\n            self.targets = np.array(f[f\"{prefix}_targets\"])\n            self.num_classes = f[f\"{prefix}_data\"].attrs.get(\"num_classes\", 0)\n            self.length = len(self.data)\n        logging_training.info(\n            f\"[NebulaPartitionHandler] [{self.prefix}] Loaded {self.length} samples from {self.file_path} and {self.num_classes} classes.\"\n        )\n\n    def close(self):\n        if self.file is not None:\n            self.file.close()\n            self.file = None\n            logging_training.info(f\"[NebulaPartitionHandler] Closed file {self.file_path}\")\n\n    def __del__(self):\n        self.close()\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, idx):\n        data = self.data[idx]\n        # Persist the modified targets (if any) during the training process\n        target = self.targets[idx] if hasattr(self, \"targets\") and self.targets is not None else None\n        return data, target\n\n    def set_data(self, data, targets, data_opt=None, targets_opt=None):\n        \"\"\"\n        Set the data and targets for the dataset.\n        \"\"\"\n        try:\n            # Input validation\n            if data is None or targets is None:\n                raise ValueError(\"Primary data and targets cannot be None\")\n\n            if len(data) != len(targets):\n                raise ValueError(f\"Data and targets length mismatch: {len(data)} vs {len(targets)}\")\n\n            if data_opt is None or targets_opt is None:\n                self.data = data\n                self.targets = targets\n                self.length = len(data)\n                logging_training.info(f\"[NebulaPartitionHandler] Set data with {self.length} samples.\")\n                return\n\n            if len(data_opt) != len(targets_opt):\n                raise ValueError(f\"Optional data and targets length mismatch: {len(data_opt)} vs {len(targets_opt)}\")\n\n            main_count = int(len(data) * 0.8)\n            opt_count = min(len(data_opt), int(len(data) * (1 - 0.8)))\n            if isinstance(data, np.ndarray):\n                self.data = np.concatenate((data[:main_count], data_opt[:opt_count]))\n            else:\n                self.data = data[:main_count] + data_opt[:opt_count]\n\n            if isinstance(targets, np.ndarray):\n                self.targets = np.concatenate((targets[:main_count], targets_opt[:opt_count]))\n            else:\n                self.targets = targets[:main_count] + targets_opt[:opt_count]\n            self.length = len(self.data)\n\n            indices = np.arange(self.length)\n            np.random.shuffle(indices)\n            if isinstance(self.data, np.ndarray):\n                self.data = self.data[indices]\n            else:\n                self.data = [self.data[i] for i in indices]\n            if isinstance(self.targets, np.ndarray):\n                self.targets = self.targets[indices]\n            else:\n                self.targets = [self.targets[i] for i in indices]\n\n        except Exception as e:\n            logging_training.exception(f\"Error setting data: {e}\")\n\n    def load_partition(self, file, name):\n        item = file[name]\n        if isinstance(item, h5py.Dataset):\n            typ = item.attrs.get(\"__type__\", None)\n            if typ == \"pickle\":\n                logging_training.info(f\"Loading pickled object from {name}\")\n                return pickle.loads(item[()].tobytes())\n            elif typ == \"pickle_bytes\":\n                logging_training.info(f\"Loading compressed pickled bytes object from {name}\")\n                return pickle.loads(item[()])\n            else:\n                logging_training.warning(f\"[NebulaPartitionHandler] Unknown type encountered: {typ} for item {name}\")\n                return item[()]\n        else:\n            logging_training.warning(f\"[NebulaPartitionHandler] Unknown item encountered: {item} for item {name}\")\n            return item[()]\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaPartitionHandler.set_data","title":"<code>set_data(data, targets, data_opt=None, targets_opt=None)</code>","text":"<p>Set the data and targets for the dataset.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def set_data(self, data, targets, data_opt=None, targets_opt=None):\n    \"\"\"\n    Set the data and targets for the dataset.\n    \"\"\"\n    try:\n        # Input validation\n        if data is None or targets is None:\n            raise ValueError(\"Primary data and targets cannot be None\")\n\n        if len(data) != len(targets):\n            raise ValueError(f\"Data and targets length mismatch: {len(data)} vs {len(targets)}\")\n\n        if data_opt is None or targets_opt is None:\n            self.data = data\n            self.targets = targets\n            self.length = len(data)\n            logging_training.info(f\"[NebulaPartitionHandler] Set data with {self.length} samples.\")\n            return\n\n        if len(data_opt) != len(targets_opt):\n            raise ValueError(f\"Optional data and targets length mismatch: {len(data_opt)} vs {len(targets_opt)}\")\n\n        main_count = int(len(data) * 0.8)\n        opt_count = min(len(data_opt), int(len(data) * (1 - 0.8)))\n        if isinstance(data, np.ndarray):\n            self.data = np.concatenate((data[:main_count], data_opt[:opt_count]))\n        else:\n            self.data = data[:main_count] + data_opt[:opt_count]\n\n        if isinstance(targets, np.ndarray):\n            self.targets = np.concatenate((targets[:main_count], targets_opt[:opt_count]))\n        else:\n            self.targets = targets[:main_count] + targets_opt[:opt_count]\n        self.length = len(self.data)\n\n        indices = np.arange(self.length)\n        np.random.shuffle(indices)\n        if isinstance(self.data, np.ndarray):\n            self.data = self.data[indices]\n        else:\n            self.data = [self.data[i] for i in indices]\n        if isinstance(self.targets, np.ndarray):\n            self.targets = self.targets[indices]\n        else:\n            self.targets = [self.targets[i] for i in indices]\n\n    except Exception as e:\n        logging_training.exception(f\"Error setting data: {e}\")\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.wait_for_file","title":"<code>wait_for_file(file_path)</code>","text":"<p>Wait until the given file exists, polling every 'interval' seconds.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def wait_for_file(file_path):\n    \"\"\"Wait until the given file exists, polling every 'interval' seconds.\"\"\"\n    while not os.path.exists(file_path):\n        logging_training.info(f\"Waiting for file: {file_path}\")\n        time.sleep(1)\n    return\n</code></pre>"},{"location":"api/core/datasets/cifar10/","title":"Documentation for Cifar10 Module","text":""},{"location":"api/core/datasets/cifar10/cifar10/","title":"Documentation for Cifar10 Module","text":""},{"location":"api/core/datasets/cifar100/","title":"Documentation for Cifar100 Module","text":""},{"location":"api/core/datasets/cifar100/cifar100/","title":"Documentation for Cifar100 Module","text":""},{"location":"api/core/datasets/emnist/","title":"Documentation for Emnist Module","text":""},{"location":"api/core/datasets/emnist/emnist/","title":"Documentation for Emnist Module","text":""},{"location":"api/core/datasets/fashionmnist/","title":"Documentation for Fashionmnist Module","text":""},{"location":"api/core/datasets/fashionmnist/fashionmnist/","title":"Documentation for Fashionmnist Module","text":""},{"location":"api/core/datasets/mnist/","title":"Documentation for Mnist Module","text":""},{"location":"api/core/datasets/mnist/mnist/","title":"Documentation for Mnist Module","text":""},{"location":"api/core/models/","title":"Documentation for Models Module","text":""},{"location":"api/core/models/nebulamodel/","title":"Documentation for Nebulamodel Module","text":""},{"location":"api/core/models/nebulamodel/#nebula.core.models.nebulamodel.NebulaModel","title":"<code>NebulaModel</code>","text":"<p>               Bases: <code>LightningModule</code>, <code>ABC</code></p> <p>Abstract class for the NEBULA model.</p> <p>This class is an abstract class that defines the interface for the NEBULA model.</p> Source code in <code>nebula/core/models/nebulamodel.py</code> <pre><code>class NebulaModel(pl.LightningModule, ABC):\n    \"\"\"\n    Abstract class for the NEBULA model.\n\n    This class is an abstract class that defines the interface for the NEBULA model.\n    \"\"\"\n\n    def process_metrics(self, phase, y_pred, y, loss=None):\n        \"\"\"\n        Calculate and log metrics for the given phase.\n        The metrics are calculated in each batch.\n        Args:\n            phase (str): One of 'Train', 'Validation', or 'Test'\n            y_pred (torch.Tensor): Model predictions\n            y (torch.Tensor): Ground truth labels\n            loss (torch.Tensor, optional): Loss value\n        \"\"\"\n\n        y_pred_classes = torch.argmax(y_pred, dim=1).detach()\n        y = y.detach()\n        if phase == \"Train\":\n            self.logger.log_data({f\"{phase}/Loss\": loss.detach()})\n            self.train_metrics.update(y_pred_classes, y)\n        elif phase == \"Validation\":\n            self.val_metrics.update(y_pred_classes, y)\n        elif phase == \"Test (Local)\":\n            self.test_metrics.update(y_pred_classes, y)\n            self.cm.update(y_pred_classes, y) if self.cm is not None else None\n        elif phase == \"Test (Global)\":\n            self.test_metrics_global.update(y_pred_classes, y)\n            self.cm_global.update(y_pred_classes, y) if self.cm_global is not None else None\n        else:\n            raise NotImplementedError\n\n        del y_pred_classes, y\n\n    def log_metrics_end(self, phase):\n        \"\"\"\n        Log metrics for the given phase.\n        Args:\n            phase (str): One of 'Train', 'Validation', 'Test (Local)', or 'Test (Global)'\n            print_cm (bool): Print confusion matrix\n            plot_cm (bool): Plot confusion matrix\n        \"\"\"\n        if phase == \"Train\":\n            output = self.train_metrics.compute()\n        elif phase == \"Validation\":\n            output = self.val_metrics.compute()\n        elif phase == \"Test (Local)\":\n            output = self.test_metrics.compute()\n        elif phase == \"Test (Global)\":\n            output = self.test_metrics_global.compute()\n        else:\n            raise NotImplementedError\n\n        output = {\n            f\"{phase}/{key.replace('Multiclass', '').split('/')[-1]}\": value.detach() for key, value in output.items()\n        }\n\n        self.logger.log_data(output, step=self.global_number[phase])\n\n        metrics_str = \"\"\n        for key, value in output.items():\n            metrics_str += f\"{key}: {value:.4f}\\n\"\n        print_msg_box(\n            metrics_str,\n            indent=2,\n            title=f\"{phase} Metrics | Epoch: {self.global_number[phase]} | Round: {self.round}\",\n            logger_name=TRAINING_LOGGER,\n        )\n\n    def generate_confusion_matrix(self, phase, print_cm=False, plot_cm=False):\n        \"\"\"\n        Generate and plot the confusion matrix for the given phase.\n        Args:\n            phase (str): One of 'Train', 'Validation', 'Test (Local)', or 'Test (Global)'\n        \"\"\"\n        if phase == \"Test (Local)\":\n            if self.cm is None:\n                raise ValueError(f\"Confusion matrix not available for {phase} phase.\")\n            cm = self.cm.compute().cpu()\n        elif phase == \"Test (Global)\":\n            if self.cm_global is None:\n                raise ValueError(f\"Confusion matrix not available for {phase} phase.\")\n            cm = self.cm_global.compute().cpu()\n        else:\n            raise NotImplementedError\n\n        if print_cm:\n            logging_training.info(f\"{phase} / Confusion Matrix:\\n{cm}\")\n\n        if plot_cm:\n            cm_numpy = cm.numpy().astype(int)\n            classes = [i for i in range(self.num_classes)]\n            fig, ax = plt.subplots(figsize=(12, 12))\n            sns.heatmap(\n                cm_numpy,\n                annot=False,\n                fmt=\"\",\n                cmap=\"Blues\",\n                ax=ax,\n                xticklabels=classes,\n                yticklabels=classes,\n                square=True,\n            )\n            ax.set_xlabel(\"Predicted labels\", fontsize=12)\n            ax.set_ylabel(\"True labels\", fontsize=12)\n            ax.set_title(f\"{phase} Confusion Matrix\", fontsize=16)\n            plt.xticks(rotation=90, fontsize=6)\n            plt.yticks(rotation=0, fontsize=6)\n            plt.tight_layout()\n            self.logger.log_figure(fig, step=self.round, name=f\"{phase}/CM\")\n            plt.close()\n\n            del cm_numpy, classes, fig, ax\n\n        # Restablecer la matriz de confusi\u00f3n\n        if phase == \"Test (Local)\":\n            self.cm.reset()\n        else:\n            self.cm_global.reset()\n\n        del cm\n\n    def __init__(\n        self,\n        input_channels=1,\n        num_classes=10,\n        learning_rate=1e-3,\n        metrics=None,\n        confusion_matrix=None,\n        seed=None,\n    ):\n        super().__init__()\n\n        self.input_channels = input_channels\n        self.num_classes = num_classes\n        self.learning_rate = learning_rate\n\n        if metrics is None:\n            metrics = MetricCollection([\n                MulticlassAccuracy(num_classes=num_classes),\n                MulticlassPrecision(num_classes=num_classes),\n                MulticlassRecall(num_classes=num_classes),\n                MulticlassF1Score(num_classes=num_classes),\n            ])\n        self.train_metrics = metrics.clone(prefix=\"Train/\")\n        self.val_metrics = metrics.clone(prefix=\"Validation/\")\n        self.test_metrics = metrics.clone(prefix=\"Test (Local)/\")\n        self.test_metrics_global = metrics.clone(prefix=\"Test (Global)/\")\n        del metrics\n        if confusion_matrix is None:\n            self.cm = MulticlassConfusionMatrix(num_classes=num_classes)\n            self.cm_global = MulticlassConfusionMatrix(num_classes=num_classes)\n        if seed is not None:\n            torch.manual_seed(seed)\n            torch.cuda.manual_seed_all(seed)\n\n        # Round counter (number of training-validation-test rounds)\n        self.round = 0\n\n        # Epochs counter\n        self.global_number = {\n            \"Train\": 0,\n            \"Validation\": 0,\n            \"Test (Local)\": 0,\n            \"Test (Global)\": 0,\n        }\n\n        # Communication manager for sending messages from the model (e.g., prototypes, gradients)\n        # Model parameters are sent by default using network.propagator\n        self.communication_manager = None\n\n        self._current_loss = -1\n        self._optimizer = None\n\n    def set_communication_manager(self, communication_manager):\n        self.communication_manager = communication_manager\n\n    def get_communication_manager(self):\n        if self.communication_manager is None:\n            raise ValueError(\"Communication manager not set.\")\n        return self.communication_manager\n\n    @abstractmethod\n    def forward(self, x):\n        \"\"\"Forward pass of the model.\"\"\"\n        pass\n\n    @abstractmethod\n    def configure_optimizers(self):\n        \"\"\"Optimizer configuration.\"\"\"\n        pass\n\n    def step(self, batch, batch_idx, phase):\n        \"\"\"Training/validation/test step.\"\"\"\n        x, y = batch\n        y_pred = self.forward(x)\n        loss = self.criterion(y_pred, y)\n        self.process_metrics(phase, y_pred, y, loss)\n\n        self._current_loss = loss\n        return loss\n\n    def get_loss(self):\n        return self._current_loss\n\n    def modify_learning_rate(self, new_lr):\n        logging.info(f\"Modifiying | learning rate, new value: {new_lr}\")\n        self.learning_rate = new_lr\n        for param_group in self._optimizer.param_groups:\n            param_group[\"lr\"] = new_lr\n\n    def show_current_learning_rate(self):\n        for param_group in self._optimizer.param_groups:\n            logging.info(f\"Showing | Learning rate current value: {param_group['lr']}\")\n\n    def set_updated_round(self, round):\n        self.round = round\n        self.global_number = {\n            \"Train\": round,\n            \"Validation\": round,\n            \"Test (Local)\": round,\n            \"Test (Global)\": round,\n        }\n\n    def training_step(self, batch, batch_idx):\n        \"\"\"\n        Training step for the model.\n        Args:\n            batch:\n            batch_id:\n\n        Returns:\n        \"\"\"\n        return self.step(batch, batch_idx=batch_idx, phase=\"Train\")\n\n    def on_train_start(self):\n        logging_training.info(f\"{'=' * 10} [Training] Started {'=' * 10}\")\n\n    def on_train_end(self):\n        logging_training.info(f\"{'=' * 10} [Training] Done {'=' * 10}\")\n\n    def on_train_epoch_end(self):\n        self.log_metrics_end(\"Train\")\n        self.train_metrics.reset()\n        self.global_number[\"Train\"] += 1\n\n    def validation_step(self, batch, batch_idx):\n        \"\"\"\n        Validation step for the model.\n        Args:\n            batch:\n            batch_idx:\n\n        Returns:\n        \"\"\"\n        return self.step(batch, batch_idx=batch_idx, phase=\"Validation\")\n\n    def on_validation_end(self):\n        pass\n\n    def on_validation_epoch_end(self):\n        # In general, the validation phase is done in one epoch\n        self.log_metrics_end(\"Validation\")\n        self.val_metrics.reset()\n        self.global_number[\"Validation\"] += 1\n\n    def test_step(self, batch, batch_idx, dataloader_idx=None):\n        \"\"\"\n        Test step for the model.\n        Args:\n            batch:\n            batch_idx:\n\n        Returns:\n        \"\"\"\n        x, y = batch\n        y_pred = self.forward(x)\n        loss = self.criterion(y_pred, y)\n        y_pred_classes = torch.argmax(y_pred, dim=1)\n        accuracy = torch.mean((y_pred_classes == y).float())\n\n        if dataloader_idx == 0:\n            self.log(f\"val_loss\", loss, on_epoch=True, prog_bar=False)\n            self.log(f\"val_accuracy\", accuracy, on_epoch=True, prog_bar=False)\n            return self.step(batch, batch_idx=batch_idx, phase=\"Test (Local)\")\n        else:\n            return self.step(batch, batch_idx=batch_idx, phase=\"Test (Global)\")\n\n    def on_test_start(self):\n        logging_training.info(f\"{'=' * 10} [Testing] Started {'=' * 10}\")\n\n    def on_test_end(self):\n        logging_training.info(f\"{'=' * 10} [Testing] Done {'=' * 10}\")\n\n    def on_test_epoch_end(self):\n        # In general, the test phase is done in one epoch\n        self.log_metrics_end(\"Test (Local)\")\n        self.log_metrics_end(\"Test (Global)\")\n        self.generate_confusion_matrix(\"Test (Local)\", print_cm=True, plot_cm=True)\n        self.generate_confusion_matrix(\"Test (Global)\", print_cm=True, plot_cm=True)\n        self.test_metrics.reset()\n        self.test_metrics_global.reset()\n        self.global_number[\"Test (Local)\"] += 1\n        self.global_number[\"Test (Global)\"] += 1\n\n    def on_round_end(self):\n        self.round += 1\n</code></pre>"},{"location":"api/core/models/nebulamodel/#nebula.core.models.nebulamodel.NebulaModel.configure_optimizers","title":"<code>configure_optimizers()</code>  <code>abstractmethod</code>","text":"<p>Optimizer configuration.</p> Source code in <code>nebula/core/models/nebulamodel.py</code> <pre><code>@abstractmethod\ndef configure_optimizers(self):\n    \"\"\"Optimizer configuration.\"\"\"\n    pass\n</code></pre>"},{"location":"api/core/models/nebulamodel/#nebula.core.models.nebulamodel.NebulaModel.forward","title":"<code>forward(x)</code>  <code>abstractmethod</code>","text":"<p>Forward pass of the model.</p> Source code in <code>nebula/core/models/nebulamodel.py</code> <pre><code>@abstractmethod\ndef forward(self, x):\n    \"\"\"Forward pass of the model.\"\"\"\n    pass\n</code></pre>"},{"location":"api/core/models/nebulamodel/#nebula.core.models.nebulamodel.NebulaModel.generate_confusion_matrix","title":"<code>generate_confusion_matrix(phase, print_cm=False, plot_cm=False)</code>","text":"<p>Generate and plot the confusion matrix for the given phase. Args:     phase (str): One of 'Train', 'Validation', 'Test (Local)', or 'Test (Global)'</p> Source code in <code>nebula/core/models/nebulamodel.py</code> <pre><code>def generate_confusion_matrix(self, phase, print_cm=False, plot_cm=False):\n    \"\"\"\n    Generate and plot the confusion matrix for the given phase.\n    Args:\n        phase (str): One of 'Train', 'Validation', 'Test (Local)', or 'Test (Global)'\n    \"\"\"\n    if phase == \"Test (Local)\":\n        if self.cm is None:\n            raise ValueError(f\"Confusion matrix not available for {phase} phase.\")\n        cm = self.cm.compute().cpu()\n    elif phase == \"Test (Global)\":\n        if self.cm_global is None:\n            raise ValueError(f\"Confusion matrix not available for {phase} phase.\")\n        cm = self.cm_global.compute().cpu()\n    else:\n        raise NotImplementedError\n\n    if print_cm:\n        logging_training.info(f\"{phase} / Confusion Matrix:\\n{cm}\")\n\n    if plot_cm:\n        cm_numpy = cm.numpy().astype(int)\n        classes = [i for i in range(self.num_classes)]\n        fig, ax = plt.subplots(figsize=(12, 12))\n        sns.heatmap(\n            cm_numpy,\n            annot=False,\n            fmt=\"\",\n            cmap=\"Blues\",\n            ax=ax,\n            xticklabels=classes,\n            yticklabels=classes,\n            square=True,\n        )\n        ax.set_xlabel(\"Predicted labels\", fontsize=12)\n        ax.set_ylabel(\"True labels\", fontsize=12)\n        ax.set_title(f\"{phase} Confusion Matrix\", fontsize=16)\n        plt.xticks(rotation=90, fontsize=6)\n        plt.yticks(rotation=0, fontsize=6)\n        plt.tight_layout()\n        self.logger.log_figure(fig, step=self.round, name=f\"{phase}/CM\")\n        plt.close()\n\n        del cm_numpy, classes, fig, ax\n\n    # Restablecer la matriz de confusi\u00f3n\n    if phase == \"Test (Local)\":\n        self.cm.reset()\n    else:\n        self.cm_global.reset()\n\n    del cm\n</code></pre>"},{"location":"api/core/models/nebulamodel/#nebula.core.models.nebulamodel.NebulaModel.log_metrics_end","title":"<code>log_metrics_end(phase)</code>","text":"<p>Log metrics for the given phase. Args:     phase (str): One of 'Train', 'Validation', 'Test (Local)', or 'Test (Global)'     print_cm (bool): Print confusion matrix     plot_cm (bool): Plot confusion matrix</p> Source code in <code>nebula/core/models/nebulamodel.py</code> <pre><code>def log_metrics_end(self, phase):\n    \"\"\"\n    Log metrics for the given phase.\n    Args:\n        phase (str): One of 'Train', 'Validation', 'Test (Local)', or 'Test (Global)'\n        print_cm (bool): Print confusion matrix\n        plot_cm (bool): Plot confusion matrix\n    \"\"\"\n    if phase == \"Train\":\n        output = self.train_metrics.compute()\n    elif phase == \"Validation\":\n        output = self.val_metrics.compute()\n    elif phase == \"Test (Local)\":\n        output = self.test_metrics.compute()\n    elif phase == \"Test (Global)\":\n        output = self.test_metrics_global.compute()\n    else:\n        raise NotImplementedError\n\n    output = {\n        f\"{phase}/{key.replace('Multiclass', '').split('/')[-1]}\": value.detach() for key, value in output.items()\n    }\n\n    self.logger.log_data(output, step=self.global_number[phase])\n\n    metrics_str = \"\"\n    for key, value in output.items():\n        metrics_str += f\"{key}: {value:.4f}\\n\"\n    print_msg_box(\n        metrics_str,\n        indent=2,\n        title=f\"{phase} Metrics | Epoch: {self.global_number[phase]} | Round: {self.round}\",\n        logger_name=TRAINING_LOGGER,\n    )\n</code></pre>"},{"location":"api/core/models/nebulamodel/#nebula.core.models.nebulamodel.NebulaModel.process_metrics","title":"<code>process_metrics(phase, y_pred, y, loss=None)</code>","text":"<p>Calculate and log metrics for the given phase. The metrics are calculated in each batch. Args:     phase (str): One of 'Train', 'Validation', or 'Test'     y_pred (torch.Tensor): Model predictions     y (torch.Tensor): Ground truth labels     loss (torch.Tensor, optional): Loss value</p> Source code in <code>nebula/core/models/nebulamodel.py</code> <pre><code>def process_metrics(self, phase, y_pred, y, loss=None):\n    \"\"\"\n    Calculate and log metrics for the given phase.\n    The metrics are calculated in each batch.\n    Args:\n        phase (str): One of 'Train', 'Validation', or 'Test'\n        y_pred (torch.Tensor): Model predictions\n        y (torch.Tensor): Ground truth labels\n        loss (torch.Tensor, optional): Loss value\n    \"\"\"\n\n    y_pred_classes = torch.argmax(y_pred, dim=1).detach()\n    y = y.detach()\n    if phase == \"Train\":\n        self.logger.log_data({f\"{phase}/Loss\": loss.detach()})\n        self.train_metrics.update(y_pred_classes, y)\n    elif phase == \"Validation\":\n        self.val_metrics.update(y_pred_classes, y)\n    elif phase == \"Test (Local)\":\n        self.test_metrics.update(y_pred_classes, y)\n        self.cm.update(y_pred_classes, y) if self.cm is not None else None\n    elif phase == \"Test (Global)\":\n        self.test_metrics_global.update(y_pred_classes, y)\n        self.cm_global.update(y_pred_classes, y) if self.cm_global is not None else None\n    else:\n        raise NotImplementedError\n\n    del y_pred_classes, y\n</code></pre>"},{"location":"api/core/models/nebulamodel/#nebula.core.models.nebulamodel.NebulaModel.step","title":"<code>step(batch, batch_idx, phase)</code>","text":"<p>Training/validation/test step.</p> Source code in <code>nebula/core/models/nebulamodel.py</code> <pre><code>def step(self, batch, batch_idx, phase):\n    \"\"\"Training/validation/test step.\"\"\"\n    x, y = batch\n    y_pred = self.forward(x)\n    loss = self.criterion(y_pred, y)\n    self.process_metrics(phase, y_pred, y, loss)\n\n    self._current_loss = loss\n    return loss\n</code></pre>"},{"location":"api/core/models/nebulamodel/#nebula.core.models.nebulamodel.NebulaModel.test_step","title":"<code>test_step(batch, batch_idx, dataloader_idx=None)</code>","text":"<p>Test step for the model. Args:     batch:     batch_idx:</p> <p>Returns:</p> Source code in <code>nebula/core/models/nebulamodel.py</code> <pre><code>def test_step(self, batch, batch_idx, dataloader_idx=None):\n    \"\"\"\n    Test step for the model.\n    Args:\n        batch:\n        batch_idx:\n\n    Returns:\n    \"\"\"\n    x, y = batch\n    y_pred = self.forward(x)\n    loss = self.criterion(y_pred, y)\n    y_pred_classes = torch.argmax(y_pred, dim=1)\n    accuracy = torch.mean((y_pred_classes == y).float())\n\n    if dataloader_idx == 0:\n        self.log(f\"val_loss\", loss, on_epoch=True, prog_bar=False)\n        self.log(f\"val_accuracy\", accuracy, on_epoch=True, prog_bar=False)\n        return self.step(batch, batch_idx=batch_idx, phase=\"Test (Local)\")\n    else:\n        return self.step(batch, batch_idx=batch_idx, phase=\"Test (Global)\")\n</code></pre>"},{"location":"api/core/models/nebulamodel/#nebula.core.models.nebulamodel.NebulaModel.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Training step for the model. Args:     batch:     batch_id:</p> <p>Returns:</p> Source code in <code>nebula/core/models/nebulamodel.py</code> <pre><code>def training_step(self, batch, batch_idx):\n    \"\"\"\n    Training step for the model.\n    Args:\n        batch:\n        batch_id:\n\n    Returns:\n    \"\"\"\n    return self.step(batch, batch_idx=batch_idx, phase=\"Train\")\n</code></pre>"},{"location":"api/core/models/nebulamodel/#nebula.core.models.nebulamodel.NebulaModel.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Validation step for the model. Args:     batch:     batch_idx:</p> <p>Returns:</p> Source code in <code>nebula/core/models/nebulamodel.py</code> <pre><code>def validation_step(self, batch, batch_idx):\n    \"\"\"\n    Validation step for the model.\n    Args:\n        batch:\n        batch_idx:\n\n    Returns:\n    \"\"\"\n    return self.step(batch, batch_idx=batch_idx, phase=\"Validation\")\n</code></pre>"},{"location":"api/core/models/cifar10/","title":"Documentation for Cifar10 Module","text":""},{"location":"api/core/models/cifar10/cnn/","title":"Documentation for Cnn Module","text":""},{"location":"api/core/models/cifar10/cnnV2/","title":"Documentation for Cnnv2 Module","text":""},{"location":"api/core/models/cifar10/cnnV3/","title":"Documentation for Cnnv3 Module","text":""},{"location":"api/core/models/cifar10/fastermobilenet/","title":"Documentation for Fastermobilenet Module","text":""},{"location":"api/core/models/cifar10/resnet/","title":"Documentation for Resnet Module","text":""},{"location":"api/core/models/cifar10/simplemobilenet/","title":"Documentation for Simplemobilenet Module","text":""},{"location":"api/core/models/cifar100/","title":"Documentation for Cifar100 Module","text":""},{"location":"api/core/models/cifar100/cnn/","title":"Documentation for Cnn Module","text":""},{"location":"api/core/models/emnist/","title":"Documentation for Emnist Module","text":""},{"location":"api/core/models/emnist/cnn/","title":"Documentation for Cnn Module","text":""},{"location":"api/core/models/emnist/mlp/","title":"Documentation for Mlp Module","text":""},{"location":"api/core/models/fashionmnist/","title":"Documentation for Fashionmnist Module","text":""},{"location":"api/core/models/fashionmnist/cnn/","title":"Documentation for Cnn Module","text":""},{"location":"api/core/models/fashionmnist/mlp/","title":"Documentation for Mlp Module","text":""},{"location":"api/core/models/mnist/","title":"Documentation for Mnist Module","text":""},{"location":"api/core/models/mnist/cnn/","title":"Documentation for Cnn Module","text":""},{"location":"api/core/models/mnist/mlp/","title":"Documentation for Mlp Module","text":""},{"location":"api/core/models/sentiment140/","title":"Documentation for Sentiment140 Module","text":""},{"location":"api/core/models/sentiment140/cnn/","title":"Documentation for Cnn Module","text":""},{"location":"api/core/models/sentiment140/rnn/","title":"Documentation for Rnn Module","text":""},{"location":"api/core/network/","title":"Documentation for Network Module","text":""},{"location":"api/core/network/actions/","title":"Documentation for Actions Module","text":""},{"location":"api/core/network/actions/#nebula.core.network.actions.ConnectionAction","title":"<code>ConnectionAction</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enum for connection-related actions exchanged between nodes in the federation.</p> Source code in <code>nebula/core/network/actions.py</code> <pre><code>class ConnectionAction(Enum):\n    \"\"\"\n    Enum for connection-related actions exchanged between nodes in the federation.\n    \"\"\"\n\n    CONNECT = nebula_pb2.ConnectionMessage.Action.CONNECT\n    DISCONNECT = nebula_pb2.ConnectionMessage.Action.DISCONNECT\n    LATE_CONNECT = nebula_pb2.ConnectionMessage.Action.LATE_CONNECT\n    RESTRUCTURE = nebula_pb2.ConnectionMessage.Action.RESTRUCTURE\n</code></pre>"},{"location":"api/core/network/actions/#nebula.core.network.actions.ControlAction","title":"<code>ControlAction</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enum for control signals used to report system status and health.</p> Source code in <code>nebula/core/network/actions.py</code> <pre><code>class ControlAction(Enum):\n    \"\"\"\n    Enum for control signals used to report system status and health.\n    \"\"\"\n\n    ALIVE = nebula_pb2.ControlMessage.Action.ALIVE\n    OVERHEAD = nebula_pb2.ControlMessage.Action.OVERHEAD\n    MOBILITY = nebula_pb2.ControlMessage.Action.MOBILITY\n    RECOVERY = nebula_pb2.ControlMessage.Action.RECOVERY\n    WEAK_LINK = nebula_pb2.ControlMessage.Action.WEAK_LINK\n    LEADERSHIP_TRANSFER = nebula_pb2.ControlMessage.Action.LEADERSHIP_TRANSFER\n    LEADERSHIP_TRANSFER_ACK = nebula_pb2.ControlMessage.Action.LEADERSHIP_TRANSFER_ACK\n</code></pre>"},{"location":"api/core/network/actions/#nebula.core.network.actions.DiscoverAction","title":"<code>DiscoverAction</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enum for extended discovery behaviors in multi-federation scenarios.</p> Source code in <code>nebula/core/network/actions.py</code> <pre><code>class DiscoverAction(Enum):\n    \"\"\"\n    Enum for extended discovery behaviors in multi-federation scenarios.\n    \"\"\"\n\n    DISCOVER_JOIN = nebula_pb2.DiscoverMessage.Action.DISCOVER_JOIN\n    DISCOVER_NODES = nebula_pb2.DiscoverMessage.Action.DISCOVER_NODES\n</code></pre>"},{"location":"api/core/network/actions/#nebula.core.network.actions.DiscoveryAction","title":"<code>DiscoveryAction</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enum for node discovery and registration events.</p> Source code in <code>nebula/core/network/actions.py</code> <pre><code>class DiscoveryAction(Enum):\n    \"\"\"\n    Enum for node discovery and registration events.\n    \"\"\"\n\n    DISCOVER = nebula_pb2.DiscoveryMessage.Action.DISCOVER\n    REGISTER = nebula_pb2.DiscoveryMessage.Action.REGISTER\n    DEREGISTER = nebula_pb2.DiscoveryMessage.Action.DEREGISTER\n</code></pre>"},{"location":"api/core/network/actions/#nebula.core.network.actions.FederationAction","title":"<code>FederationAction</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enum for actions related to federation lifecycle and state management.</p> Source code in <code>nebula/core/network/actions.py</code> <pre><code>class FederationAction(Enum):\n    \"\"\"\n    Enum for actions related to federation lifecycle and state management.\n    \"\"\"\n\n    FEDERATION_START = nebula_pb2.FederationMessage.Action.FEDERATION_START\n    REPUTATION = nebula_pb2.FederationMessage.Action.REPUTATION\n    FEDERATION_MODELS_INCLUDED = nebula_pb2.FederationMessage.Action.FEDERATION_MODELS_INCLUDED\n    FEDERATION_READY = nebula_pb2.FederationMessage.Action.FEDERATION_READY\n</code></pre>"},{"location":"api/core/network/actions/#nebula.core.network.actions.LinkAction","title":"<code>LinkAction</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enum for explicit link manipulation between nodes.</p> Source code in <code>nebula/core/network/actions.py</code> <pre><code>class LinkAction(Enum):\n    \"\"\"\n    Enum for explicit link manipulation between nodes.\n    \"\"\"\n\n    CONNECT_TO = nebula_pb2.LinkMessage.Action.CONNECT_TO\n    DISCONNECT_FROM = nebula_pb2.LinkMessage.Action.DISCONNECT_FROM\n</code></pre>"},{"location":"api/core/network/actions/#nebula.core.network.actions.OfferAction","title":"<code>OfferAction</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enum for offer-related messages, such as model or metric sharing.</p> Source code in <code>nebula/core/network/actions.py</code> <pre><code>class OfferAction(Enum):\n    \"\"\"\n    Enum for offer-related messages, such as model or metric sharing.\n    \"\"\"\n\n    OFFER_MODEL = nebula_pb2.OfferMessage.Action.OFFER_MODEL\n    OFFER_METRIC = nebula_pb2.OfferMessage.Action.OFFER_METRIC\n</code></pre>"},{"location":"api/core/network/actions/#nebula.core.network.actions.ReputationAction","title":"<code>ReputationAction</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enum for reputation exchange messages in the federation.</p> Source code in <code>nebula/core/network/actions.py</code> <pre><code>class ReputationAction(Enum):\n    \"\"\"\n    Enum for reputation exchange messages in the federation.\n    \"\"\"\n\n    SHARE = nebula_pb2.ReputationMessage.Action.SHARE\n</code></pre>"},{"location":"api/core/network/actions/#nebula.core.network.actions.factory_message_action","title":"<code>factory_message_action(message_type, action)</code>","text":"<p>Convert a string action name to its corresponding Enum value.</p> <p>Parameters:</p> Name Type Description Default <code>message_type</code> <code>str</code> <p>The type of the message (e.g., \"offer\", \"link\").</p> required <code>action</code> <code>str</code> <p>The string name of the action.</p> required <p>Returns:</p> Type Description <p>int or None: The integer value of the Enum action, or None if the type is unknown.</p> Source code in <code>nebula/core/network/actions.py</code> <pre><code>def factory_message_action(message_type: str, action: str):\n    \"\"\"\n    Convert a string action name to its corresponding Enum value.\n\n    Args:\n        message_type (str): The type of the message (e.g., \"offer\", \"link\").\n        action (str): The string name of the action.\n\n    Returns:\n        int or None: The integer value of the Enum action, or None if the type is unknown.\n    \"\"\"\n    message_actions = ACTION_CLASSES.get(message_type)\n\n    if message_actions:\n        normalized_action = action.upper()\n        enum_action = message_actions[normalized_action]\n        # logging.info(f\"Message action: {enum_action}, value: {enum_action.value}\")\n        return enum_action.value\n    else:\n        return None\n</code></pre>"},{"location":"api/core/network/actions/#nebula.core.network.actions.get_action_name_from_value","title":"<code>get_action_name_from_value(message_type, action_value)</code>","text":"<p>Retrieve the string name of an action from its integer value.</p> <p>Parameters:</p> Name Type Description Default <code>message_type</code> <code>str</code> <p>The type of the message (e.g., \"connection\", \"control\").</p> required <code>action_value</code> <code>int</code> <p>The numeric value of the action.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The name of the action in lowercase format.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the message type or action value is not recognized.</p> Source code in <code>nebula/core/network/actions.py</code> <pre><code>def get_action_name_from_value(message_type: str, action_value: int) -&gt; str:\n    \"\"\"\n    Retrieve the string name of an action from its integer value.\n\n    Args:\n        message_type (str): The type of the message (e.g., \"connection\", \"control\").\n        action_value (int): The numeric value of the action.\n\n    Returns:\n        str: The name of the action in lowercase format.\n\n    Raises:\n        ValueError: If the message type or action value is not recognized.\n    \"\"\"\n    # Get the Enum corresponding to the message type\n    enum_class = ACTION_CLASSES.get(message_type)\n    if not enum_class:\n        raise ValueError(f\"Unknown message type: {message_type}\")\n\n    # Find the name of the action from the value\n    for action in enum_class:\n        if action.value == action_value:\n            return action.name.lower()  # Convert to lowercase to maintain the format \"late_connect\"\n\n    raise ValueError(f\"Unknown action value {action_value} for message type {message_type}\")\n</code></pre>"},{"location":"api/core/network/actions/#nebula.core.network.actions.get_actions_names","title":"<code>get_actions_names(message_type)</code>","text":"<p>Get all action names for a given message type.</p> <p>Parameters:</p> Name Type Description Default <code>message_type</code> <code>str</code> <p>The type of the message.</p> required <p>Returns:</p> Type Description <p>List[str]: List of action names in lowercase.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the message type is invalid.</p> Source code in <code>nebula/core/network/actions.py</code> <pre><code>def get_actions_names(message_type: str):\n    \"\"\"\n    Get all action names for a given message type.\n\n    Args:\n        message_type (str): The type of the message.\n\n    Returns:\n        List[str]: List of action names in lowercase.\n\n    Raises:\n        ValueError: If the message type is invalid.\n    \"\"\"\n    message_actions = ACTION_CLASSES.get(message_type)\n    if not message_actions:\n        raise ValueError(f\"Invalid message type: {message_type}\")\n\n    return [action.name.lower() for action in message_actions]\n</code></pre>"},{"location":"api/core/network/blacklist/","title":"Documentation for Blacklist Module","text":""},{"location":"api/core/network/blacklist/#nebula.core.network.blacklist.BlackList","title":"<code>BlackList</code>","text":"<p>Manages a dynamic blacklist and a list of recently disconnected nodes in a distributed system.</p> <p>The blacklist tracks nodes that are temporarily excluded from communication or interaction due to malicious behavior or disconnection events. Nodes remain blacklisted for a fixed period defined by <code>max_time_listed</code>.</p> <p>The recently disconnected list tracks peers that were recently disconnected and may need to be temporarily avoided.</p> <p>Key features: - Asynchronous locks for concurrent safety. - Periodic cleaning of the blacklist via a background coroutine. - Integration with an event manager to publish changes.</p> Source code in <code>nebula/core/network/blacklist.py</code> <pre><code>class BlackList:\n    \"\"\"\n    Manages a dynamic blacklist and a list of recently disconnected nodes in a distributed system.\n\n    The blacklist tracks nodes that are temporarily excluded from communication or interaction due to malicious behavior\n    or disconnection events. Nodes remain blacklisted for a fixed period defined by `max_time_listed`.\n\n    The recently disconnected list tracks peers that were recently disconnected and may need to be temporarily avoided.\n\n    Key features:\n    - Asynchronous locks for concurrent safety.\n    - Periodic cleaning of the blacklist via a background coroutine.\n    - Integration with an event manager to publish changes.\n    \"\"\"\n\n    def __init__(self, max_time_listed=BLACKLIST_EXPIRATION_TIME):\n        \"\"\"\n        Initialize the BlackList with the specified expiration time.\n\n        Args:\n            max_time_listed (int): Maximum time in seconds for nodes to remain blacklisted.\n        \"\"\"\n        self._max_time_listed = max_time_listed\n        self._blacklisted_nodes = {}\n        self._recently_disconnected = set()\n        self._blacklisted_nodes_lock = Locker(\"blacklisted_nodes_lock\", async_lock=True)\n        self._recently_disconnected_lock = Locker(\"recently_disconnected_lock\", async_lock=True)\n        self._running = asyncio.Event()\n        self._background_tasks = []  # Track background tasks\n\n    async def apply_restrictions(self, nodes) -&gt; set | None:\n        \"\"\"\n        Applies both blacklist and recently disconnected restrictions to a given set of nodes.\n\n        Args:\n            nodes (set): Set of peer node addresses.\n\n        Returns:\n            set | None: Filtered set excluding blacklisted and recently disconnected nodes, or None if input is empty.\n        \"\"\"\n        nodes_allowed = await self.verify_allowed_nodes(nodes)\n        # logging.info(f\"nodes allowed after appliying blacklist restricttions: {nodes_allowed}\")\n        if nodes_allowed:\n            nodes_allowed = await self.verify_not_recently_disc(nodes_allowed)\n            # logging.info(f\"nodes allowed after seen recently disconnection restrictions: {nodes_allowed}\")\n        return nodes_allowed\n\n    async def clear_restrictions(self):\n        \"\"\"\n        Clears both the blacklist and the recently disconnected list.\n        \"\"\"\n        await self.clear_blacklist()\n        await self.clear_recently_disconected()\n\n    \"\"\"                                                     ##############################\n                                                            #          BLACKLIST         #\n                                                            ##############################\n    \"\"\"\n\n    async def add_to_blacklist(self, addr):\n        \"\"\"\n        Adds a node to the blacklist and starts the cleaner task if not already running.\n\n        Args:\n            addr (str): Address of the node to blacklist.\n        \"\"\"\n        logging.info(f\"Update blackList | addr listed: {addr}\")\n        await self._blacklisted_nodes_lock.acquire_async()\n        expiration_time = time.time()\n        self._blacklisted_nodes[addr] = expiration_time\n        if not self._running.is_set():\n            self._running.set()\n            asyncio.create_task(self._start_blacklist_cleaner())\n        await self._blacklisted_nodes_lock.release_async()\n        nbe = NodeBlacklistedEvent(addr, blacklisted=True)\n        event_manager = EventManager.get_instance()\n        if event_manager is not None:\n            asyncio.create_task(event_manager.publish_node_event(nbe))\n\n    async def get_blacklist(self) -&gt; set:\n        \"\"\"\n        Adds a node to the blacklist and starts the cleaner task if not already running.\n\n        Args:\n            addr (str): Address of the node to blacklist.\n        \"\"\"\n        bl = None\n        await self._blacklisted_nodes_lock.acquire_async()\n        if self._blacklisted_nodes:\n            bl = set(self._blacklisted_nodes.keys())\n        await self._blacklisted_nodes_lock.release_async()\n        return bl or set()\n\n    async def clear_blacklist(self):\n        \"\"\"\n        Clears the blacklist entirely.\n        \"\"\"\n        await self._blacklisted_nodes_lock.acquire_async()\n        logging.info(\"\ud83e\uddf9 Removing nodes from blacklist\")\n        self._blacklisted_nodes.clear()\n        await self._blacklisted_nodes_lock.release_async()\n\n    async def _start_blacklist_cleaner(self):\n        \"\"\"\n        Background task that periodically removes expired entries from the blacklist.\n        \"\"\"\n        while self._running.is_set():\n            await self._blacklist_clean()\n            await self._blacklist_cleaner_wait()\n\n    async def _blacklist_clean(self):\n        \"\"\"\n        Removes nodes from the blacklist whose expiration time has passed.\n        \"\"\"\n        await self._blacklisted_nodes_lock.acquire_async()\n        logging.info(\"BlackList cleaner has waken up\")\n        now = time.time()\n        new_bl = {}\n\n        for addr, timer in self._blacklisted_nodes.items():\n            if timer + self._max_time_listed &gt;= now:\n                new_bl[addr] = timer\n            else:\n                logging.info(f\"Removing addr{addr} from blacklisted nodes...\")\n\n        self._blacklisted_nodes = new_bl\n        if not new_bl:\n            self._running.clear()\n        await self._blacklisted_nodes_lock.release_async()\n\n    async def _blacklist_cleaner_wait(self):\n        \"\"\"\n        Waits for the blacklist cleaner delay duration.\n        \"\"\"\n        try:\n            await asyncio.sleep(self._max_time_listed)\n        except TimeoutError:\n            pass\n\n    async def node_in_blacklist(self, addr):\n        \"\"\"\n        Checks whether a given address is currently blacklisted.\n\n        Args:\n            addr (str): Node address.\n\n        Returns:\n            bool: True if the node is blacklisted, False otherwise.\n        \"\"\"\n        blacklisted = False\n        await self._blacklisted_nodes_lock.acquire_async()\n        if self._blacklisted_nodes:\n            blacklisted = addr in self._blacklisted_nodes.keys()\n        await self._blacklisted_nodes_lock.release_async()\n        return blacklisted\n\n    async def verify_allowed_nodes(self, nodes: set) -&gt; set | None:\n        \"\"\"\n        Filters out blacklisted nodes from the given set.\n\n        Args:\n            nodes (set): Set of node addresses to check.\n\n        Returns:\n            set | None: Nodes not in the blacklist, or None if input is empty.\n        \"\"\"\n        if not nodes:\n            return None\n        nodes_not_listed = nodes\n        await self._blacklisted_nodes_lock.acquire_async()\n        blacklist = self._blacklisted_nodes\n        if blacklist:\n            nodes_not_listed = nodes.difference(blacklist)\n        await self._blacklisted_nodes_lock.release_async()\n        return nodes_not_listed\n\n    \"\"\"                                                     ##############################\n                                                            #    RECENTLY DISCONNECTED   #\n                                                            ##############################\n    \"\"\"\n\n    async def add_recently_disconnected(self, addr):\n        \"\"\"\n        Marks a node as recently disconnected and schedules its expiration.\n\n        Args:\n            addr (str): Address of the disconnected node.\n        \"\"\"\n        logging.info(f\"Recently disconnected from: {addr}\")\n        await self._recently_disconnected_lock.acquire_async()\n        self._recently_disconnected.add(addr)\n        await self._recently_disconnected_lock.release_async()\n        task = asyncio.create_task(self._remove_recently_disc(addr), name=f\"BlackList_remove_recently_{addr}\")\n        self._background_tasks.append(task)\n        nbe = NodeBlacklistedEvent(addr)\n        event_manager = EventManager.get_instance()\n        if event_manager is not None:\n            asyncio.create_task(event_manager.publish_node_event(nbe))\n\n    async def clear_recently_disconected(self):\n        \"\"\"\n        Clears the list of recently disconnected nodes.\n        \"\"\"\n        await self._recently_disconnected_lock.acquire_async()\n        logging.info(\"\ud83e\uddf9 Removing nodes from Recently Disconencted list\")\n        self._recently_disconnected.clear()\n        await self._recently_disconnected_lock.release_async()\n\n    async def get_recently_disconnected(self):\n        \"\"\"\n        Retrieves a copy of the recently disconnected nodes.\n\n        Returns:\n            set: Addresses of recently disconnected nodes.\n        \"\"\"\n        rd = None\n        await self._recently_disconnected_lock.acquire_async()\n        rd = self._recently_disconnected.copy()\n        await self._recently_disconnected_lock.release_async()\n        return rd\n\n    async def _remove_recently_disc(self, addr):\n        \"\"\"\n        Waits for the expiration time and then removes the node from the recently disconnected list.\n\n        Args:\n            addr (str): Address to remove after expiration.\n        \"\"\"\n        await asyncio.sleep(RECENTLY_DISCONNECTED_EXPIRE_TIME)\n        await self._recently_disconnected_lock.acquire_async()\n        self._recently_disconnected.discard(addr)\n        logging.info(f\"Recently disconnection timeout expired for souce: {addr}\")\n        await self._recently_disconnected_lock.release_async()\n\n    async def verify_not_recently_disc(self, nodes: set) -&gt; set | None:\n        \"\"\"\n        Filters out recently disconnected nodes from the given set.\n\n        Args:\n            nodes (set): Set of node addresses to filter.\n\n        Returns:\n            set | None: Set of nodes not recently disconnected, or None if input is empty.\n        \"\"\"\n        if not nodes:\n            return None\n        nodes_not_listed = nodes\n        await self._recently_disconnected_lock.acquire_async()\n        rec_disc = self._recently_disconnected\n        # logging.info(f\"recently disconencted nodes: {rec_disc}\")\n        if rec_disc:\n            nodes_not_listed = nodes.difference(rec_disc)\n        await self._recently_disconnected_lock.release_async()\n        return nodes_not_listed\n\n    async def stop(self):\n        \"\"\"\n        Stop the BlackList by clearing all data and stopping background tasks.\n        \"\"\"\n        logging.info(\"\ud83d\uded1  Stopping BlackList...\")\n\n        # Stop the background cleaner\n        self._running.clear()\n\n        # Cancel all background tasks\n        if self._background_tasks:\n            logging.info(f\"\ud83d\uded1  Cancelling {len(self._background_tasks)} background tasks...\")\n            for task in self._background_tasks:\n                if not task.done():\n                    task.cancel()\n                    try:\n                        await task\n                    except asyncio.CancelledError:\n                        pass\n            self._background_tasks.clear()\n            logging.info(\"\ud83d\uded1  All background tasks cancelled\")\n\n        # Clear all data\n        try:\n            async with self._blacklisted_nodes_lock:\n                self._blacklisted_nodes.clear()\n        except Exception as e:\n            logging.warning(f\"Error clearing blacklist: {e}\")\n\n        try:\n            async with self._recently_disconnected_lock:\n                self._recently_disconnected.clear()\n        except Exception as e:\n            logging.warning(f\"Error clearing recently disconnected: {e}\")\n\n        logging.info(\"\u2705  BlackList stopped successfully\")\n</code></pre>"},{"location":"api/core/network/blacklist/#nebula.core.network.blacklist.BlackList.__init__","title":"<code>__init__(max_time_listed=BLACKLIST_EXPIRATION_TIME)</code>","text":"<p>Initialize the BlackList with the specified expiration time.</p> <p>Parameters:</p> Name Type Description Default <code>max_time_listed</code> <code>int</code> <p>Maximum time in seconds for nodes to remain blacklisted.</p> <code>BLACKLIST_EXPIRATION_TIME</code> Source code in <code>nebula/core/network/blacklist.py</code> <pre><code>def __init__(self, max_time_listed=BLACKLIST_EXPIRATION_TIME):\n    \"\"\"\n    Initialize the BlackList with the specified expiration time.\n\n    Args:\n        max_time_listed (int): Maximum time in seconds for nodes to remain blacklisted.\n    \"\"\"\n    self._max_time_listed = max_time_listed\n    self._blacklisted_nodes = {}\n    self._recently_disconnected = set()\n    self._blacklisted_nodes_lock = Locker(\"blacklisted_nodes_lock\", async_lock=True)\n    self._recently_disconnected_lock = Locker(\"recently_disconnected_lock\", async_lock=True)\n    self._running = asyncio.Event()\n    self._background_tasks = []  # Track background tasks\n</code></pre>"},{"location":"api/core/network/blacklist/#nebula.core.network.blacklist.BlackList.add_recently_disconnected","title":"<code>add_recently_disconnected(addr)</code>  <code>async</code>","text":"<p>Marks a node as recently disconnected and schedules its expiration.</p> <p>Parameters:</p> Name Type Description Default <code>addr</code> <code>str</code> <p>Address of the disconnected node.</p> required Source code in <code>nebula/core/network/blacklist.py</code> <pre><code>async def add_recently_disconnected(self, addr):\n    \"\"\"\n    Marks a node as recently disconnected and schedules its expiration.\n\n    Args:\n        addr (str): Address of the disconnected node.\n    \"\"\"\n    logging.info(f\"Recently disconnected from: {addr}\")\n    await self._recently_disconnected_lock.acquire_async()\n    self._recently_disconnected.add(addr)\n    await self._recently_disconnected_lock.release_async()\n    task = asyncio.create_task(self._remove_recently_disc(addr), name=f\"BlackList_remove_recently_{addr}\")\n    self._background_tasks.append(task)\n    nbe = NodeBlacklistedEvent(addr)\n    event_manager = EventManager.get_instance()\n    if event_manager is not None:\n        asyncio.create_task(event_manager.publish_node_event(nbe))\n</code></pre>"},{"location":"api/core/network/blacklist/#nebula.core.network.blacklist.BlackList.add_to_blacklist","title":"<code>add_to_blacklist(addr)</code>  <code>async</code>","text":"<p>Adds a node to the blacklist and starts the cleaner task if not already running.</p> <p>Parameters:</p> Name Type Description Default <code>addr</code> <code>str</code> <p>Address of the node to blacklist.</p> required Source code in <code>nebula/core/network/blacklist.py</code> <pre><code>async def add_to_blacklist(self, addr):\n    \"\"\"\n    Adds a node to the blacklist and starts the cleaner task if not already running.\n\n    Args:\n        addr (str): Address of the node to blacklist.\n    \"\"\"\n    logging.info(f\"Update blackList | addr listed: {addr}\")\n    await self._blacklisted_nodes_lock.acquire_async()\n    expiration_time = time.time()\n    self._blacklisted_nodes[addr] = expiration_time\n    if not self._running.is_set():\n        self._running.set()\n        asyncio.create_task(self._start_blacklist_cleaner())\n    await self._blacklisted_nodes_lock.release_async()\n    nbe = NodeBlacklistedEvent(addr, blacklisted=True)\n    event_manager = EventManager.get_instance()\n    if event_manager is not None:\n        asyncio.create_task(event_manager.publish_node_event(nbe))\n</code></pre>"},{"location":"api/core/network/blacklist/#nebula.core.network.blacklist.BlackList.apply_restrictions","title":"<code>apply_restrictions(nodes)</code>  <code>async</code>","text":"<p>Applies both blacklist and recently disconnected restrictions to a given set of nodes.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>set</code> <p>Set of peer node addresses.</p> required <p>Returns:</p> Type Description <code>set | None</code> <p>set | None: Filtered set excluding blacklisted and recently disconnected nodes, or None if input is empty.</p> Source code in <code>nebula/core/network/blacklist.py</code> <pre><code>async def apply_restrictions(self, nodes) -&gt; set | None:\n    \"\"\"\n    Applies both blacklist and recently disconnected restrictions to a given set of nodes.\n\n    Args:\n        nodes (set): Set of peer node addresses.\n\n    Returns:\n        set | None: Filtered set excluding blacklisted and recently disconnected nodes, or None if input is empty.\n    \"\"\"\n    nodes_allowed = await self.verify_allowed_nodes(nodes)\n    # logging.info(f\"nodes allowed after appliying blacklist restricttions: {nodes_allowed}\")\n    if nodes_allowed:\n        nodes_allowed = await self.verify_not_recently_disc(nodes_allowed)\n        # logging.info(f\"nodes allowed after seen recently disconnection restrictions: {nodes_allowed}\")\n    return nodes_allowed\n</code></pre>"},{"location":"api/core/network/blacklist/#nebula.core.network.blacklist.BlackList.clear_blacklist","title":"<code>clear_blacklist()</code>  <code>async</code>","text":"<p>Clears the blacklist entirely.</p> Source code in <code>nebula/core/network/blacklist.py</code> <pre><code>async def clear_blacklist(self):\n    \"\"\"\n    Clears the blacklist entirely.\n    \"\"\"\n    await self._blacklisted_nodes_lock.acquire_async()\n    logging.info(\"\ud83e\uddf9 Removing nodes from blacklist\")\n    self._blacklisted_nodes.clear()\n    await self._blacklisted_nodes_lock.release_async()\n</code></pre>"},{"location":"api/core/network/blacklist/#nebula.core.network.blacklist.BlackList.clear_recently_disconected","title":"<code>clear_recently_disconected()</code>  <code>async</code>","text":"<p>Clears the list of recently disconnected nodes.</p> Source code in <code>nebula/core/network/blacklist.py</code> <pre><code>async def clear_recently_disconected(self):\n    \"\"\"\n    Clears the list of recently disconnected nodes.\n    \"\"\"\n    await self._recently_disconnected_lock.acquire_async()\n    logging.info(\"\ud83e\uddf9 Removing nodes from Recently Disconencted list\")\n    self._recently_disconnected.clear()\n    await self._recently_disconnected_lock.release_async()\n</code></pre>"},{"location":"api/core/network/blacklist/#nebula.core.network.blacklist.BlackList.clear_restrictions","title":"<code>clear_restrictions()</code>  <code>async</code>","text":"<p>Clears both the blacklist and the recently disconnected list.</p> Source code in <code>nebula/core/network/blacklist.py</code> <pre><code>async def clear_restrictions(self):\n    \"\"\"\n    Clears both the blacklist and the recently disconnected list.\n    \"\"\"\n    await self.clear_blacklist()\n    await self.clear_recently_disconected()\n</code></pre>"},{"location":"api/core/network/blacklist/#nebula.core.network.blacklist.BlackList.get_blacklist","title":"<code>get_blacklist()</code>  <code>async</code>","text":"<p>Adds a node to the blacklist and starts the cleaner task if not already running.</p> <p>Parameters:</p> Name Type Description Default <code>addr</code> <code>str</code> <p>Address of the node to blacklist.</p> required Source code in <code>nebula/core/network/blacklist.py</code> <pre><code>async def get_blacklist(self) -&gt; set:\n    \"\"\"\n    Adds a node to the blacklist and starts the cleaner task if not already running.\n\n    Args:\n        addr (str): Address of the node to blacklist.\n    \"\"\"\n    bl = None\n    await self._blacklisted_nodes_lock.acquire_async()\n    if self._blacklisted_nodes:\n        bl = set(self._blacklisted_nodes.keys())\n    await self._blacklisted_nodes_lock.release_async()\n    return bl or set()\n</code></pre>"},{"location":"api/core/network/blacklist/#nebula.core.network.blacklist.BlackList.get_recently_disconnected","title":"<code>get_recently_disconnected()</code>  <code>async</code>","text":"<p>Retrieves a copy of the recently disconnected nodes.</p> <p>Returns:</p> Name Type Description <code>set</code> <p>Addresses of recently disconnected nodes.</p> Source code in <code>nebula/core/network/blacklist.py</code> <pre><code>async def get_recently_disconnected(self):\n    \"\"\"\n    Retrieves a copy of the recently disconnected nodes.\n\n    Returns:\n        set: Addresses of recently disconnected nodes.\n    \"\"\"\n    rd = None\n    await self._recently_disconnected_lock.acquire_async()\n    rd = self._recently_disconnected.copy()\n    await self._recently_disconnected_lock.release_async()\n    return rd\n</code></pre>"},{"location":"api/core/network/blacklist/#nebula.core.network.blacklist.BlackList.node_in_blacklist","title":"<code>node_in_blacklist(addr)</code>  <code>async</code>","text":"<p>Checks whether a given address is currently blacklisted.</p> <p>Parameters:</p> Name Type Description Default <code>addr</code> <code>str</code> <p>Node address.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the node is blacklisted, False otherwise.</p> Source code in <code>nebula/core/network/blacklist.py</code> <pre><code>async def node_in_blacklist(self, addr):\n    \"\"\"\n    Checks whether a given address is currently blacklisted.\n\n    Args:\n        addr (str): Node address.\n\n    Returns:\n        bool: True if the node is blacklisted, False otherwise.\n    \"\"\"\n    blacklisted = False\n    await self._blacklisted_nodes_lock.acquire_async()\n    if self._blacklisted_nodes:\n        blacklisted = addr in self._blacklisted_nodes.keys()\n    await self._blacklisted_nodes_lock.release_async()\n    return blacklisted\n</code></pre>"},{"location":"api/core/network/blacklist/#nebula.core.network.blacklist.BlackList.stop","title":"<code>stop()</code>  <code>async</code>","text":"<p>Stop the BlackList by clearing all data and stopping background tasks.</p> Source code in <code>nebula/core/network/blacklist.py</code> <pre><code>async def stop(self):\n    \"\"\"\n    Stop the BlackList by clearing all data and stopping background tasks.\n    \"\"\"\n    logging.info(\"\ud83d\uded1  Stopping BlackList...\")\n\n    # Stop the background cleaner\n    self._running.clear()\n\n    # Cancel all background tasks\n    if self._background_tasks:\n        logging.info(f\"\ud83d\uded1  Cancelling {len(self._background_tasks)} background tasks...\")\n        for task in self._background_tasks:\n            if not task.done():\n                task.cancel()\n                try:\n                    await task\n                except asyncio.CancelledError:\n                    pass\n        self._background_tasks.clear()\n        logging.info(\"\ud83d\uded1  All background tasks cancelled\")\n\n    # Clear all data\n    try:\n        async with self._blacklisted_nodes_lock:\n            self._blacklisted_nodes.clear()\n    except Exception as e:\n        logging.warning(f\"Error clearing blacklist: {e}\")\n\n    try:\n        async with self._recently_disconnected_lock:\n            self._recently_disconnected.clear()\n    except Exception as e:\n        logging.warning(f\"Error clearing recently disconnected: {e}\")\n\n    logging.info(\"\u2705  BlackList stopped successfully\")\n</code></pre>"},{"location":"api/core/network/blacklist/#nebula.core.network.blacklist.BlackList.verify_allowed_nodes","title":"<code>verify_allowed_nodes(nodes)</code>  <code>async</code>","text":"<p>Filters out blacklisted nodes from the given set.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>set</code> <p>Set of node addresses to check.</p> required <p>Returns:</p> Type Description <code>set | None</code> <p>set | None: Nodes not in the blacklist, or None if input is empty.</p> Source code in <code>nebula/core/network/blacklist.py</code> <pre><code>async def verify_allowed_nodes(self, nodes: set) -&gt; set | None:\n    \"\"\"\n    Filters out blacklisted nodes from the given set.\n\n    Args:\n        nodes (set): Set of node addresses to check.\n\n    Returns:\n        set | None: Nodes not in the blacklist, or None if input is empty.\n    \"\"\"\n    if not nodes:\n        return None\n    nodes_not_listed = nodes\n    await self._blacklisted_nodes_lock.acquire_async()\n    blacklist = self._blacklisted_nodes\n    if blacklist:\n        nodes_not_listed = nodes.difference(blacklist)\n    await self._blacklisted_nodes_lock.release_async()\n    return nodes_not_listed\n</code></pre>"},{"location":"api/core/network/blacklist/#nebula.core.network.blacklist.BlackList.verify_not_recently_disc","title":"<code>verify_not_recently_disc(nodes)</code>  <code>async</code>","text":"<p>Filters out recently disconnected nodes from the given set.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>set</code> <p>Set of node addresses to filter.</p> required <p>Returns:</p> Type Description <code>set | None</code> <p>set | None: Set of nodes not recently disconnected, or None if input is empty.</p> Source code in <code>nebula/core/network/blacklist.py</code> <pre><code>async def verify_not_recently_disc(self, nodes: set) -&gt; set | None:\n    \"\"\"\n    Filters out recently disconnected nodes from the given set.\n\n    Args:\n        nodes (set): Set of node addresses to filter.\n\n    Returns:\n        set | None: Set of nodes not recently disconnected, or None if input is empty.\n    \"\"\"\n    if not nodes:\n        return None\n    nodes_not_listed = nodes\n    await self._recently_disconnected_lock.acquire_async()\n    rec_disc = self._recently_disconnected\n    # logging.info(f\"recently disconencted nodes: {rec_disc}\")\n    if rec_disc:\n        nodes_not_listed = nodes.difference(rec_disc)\n    await self._recently_disconnected_lock.release_async()\n    return nodes_not_listed\n</code></pre>"},{"location":"api/core/network/communications/","title":"Documentation for Communications Module","text":""},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager","title":"<code>CommunicationsManager</code>","text":"<p>Singleton class responsible for managing all communications in the Nebula system.</p> <p>This class handles: - Sending and receiving protobuf messages between nodes. - Forwarding messages when acting as a proxy. - Managing known neighbors and communication topology. - Handling and dispatching incoming messages to the appropriate handlers. - Preventing message duplication via message hash tracking.</p> <p>It acts as a central coordinator for message-based interactions and is designed to work asynchronously to support non-blocking network operations.</p> Source code in <code>nebula/core/network/communications.py</code> <pre><code>class CommunicationsManager:\n    \"\"\"\n    Singleton class responsible for managing all communications in the Nebula system.\n\n    This class handles:\n    - Sending and receiving protobuf messages between nodes.\n    - Forwarding messages when acting as a proxy.\n    - Managing known neighbors and communication topology.\n    - Handling and dispatching incoming messages to the appropriate handlers.\n    - Preventing message duplication via message hash tracking.\n\n    It acts as a central coordinator for message-based interactions and is\n    designed to work asynchronously to support non-blocking network operations.\n    \"\"\"\n\n    _instance = None\n    _lock = Locker(\"communications_manager_lock\", async_lock=False)\n\n    def __new__(cls, engine: \"Engine\"):\n        with cls._lock:\n            if cls._instance is None:\n                cls._instance = super().__new__(cls)\n        return cls._instance\n\n    @classmethod\n    def get_instance(cls):\n        \"\"\"Obtain CommunicationsManager instance\"\"\"\n        if cls._instance is None:\n            raise ValueError(\"CommunicationsManager has not been initialized yet.\")\n        return cls._instance\n\n    def __init__(self, engine: \"Engine\"):\n        if hasattr(self, \"_initialized\") and self._initialized:\n            return  # Avoid reinicialization\n\n        logging.info(\"\ud83c\udf10  Initializing Communications Manager\")\n        self._engine = engine\n        self.addr = engine.get_addr()\n        self.host = self.addr.split(\":\")[0]\n        self.port = int(self.addr.split(\":\")[1])\n        self.config = engine.get_config()\n        self.id = str(self.config.participant[\"device_args\"][\"idx\"])\n\n        self.register_endpoint = f\"http://{self.config.participant['scenario_args']['controller']}/platform/dashboard/{self.config.participant['scenario_args']['name']}/node/register\"\n        self.wait_endpoint = f\"http://{self.config.participant['scenario_args']['controller']}/platform/dashboard/{self.config.participant['scenario_args']['name']}/node/wait\"\n\n        self._connections: dict[str, Connection] = {}\n        self.connections_lock = Locker(name=\"connections_lock\", async_lock=True)\n        self.connections_manager_lock = Locker(name=\"connections_manager_lock\", async_lock=True)\n        self.connection_attempt_lock_incoming = Locker(name=\"connection_attempt_lock_incoming\", async_lock=True)\n        self.connection_attempt_lock_outgoing = Locker(name=\"connection_attempt_lock_outgoing\", async_lock=True)\n        # Pending connections to be established\n        self.pending_connections = set()\n        self.incoming_connections = {}\n        self.outgoing_connections = {}\n        self.ready_connections = set()\n        self._ready_connections_lock = Locker(\"ready_connections_lock\", async_lock=True)\n\n        self._mm = MessagesManager(addr=self.addr, config=self.config)\n        self.received_messages_hashes = collections.deque(\n            maxlen=self.config.participant[\"message_args\"][\"max_local_messages\"]\n        )\n        self.receive_messages_lock = Locker(name=\"receive_messages_lock\", async_lock=True)\n\n        self._discoverer = Discoverer(addr=self.addr, config=self.config)\n        # self._health = Health(addr=self.addr, config=self.config)\n        self._health = None\n        self._forwarder = Forwarder(config=self.config)\n        self._propagator = Propagator()\n\n        # List of connections to reconnect {addr: addr, tries: 0}\n        self.connections_reconnect = []\n        self.max_connections = 1000\n        self.network_engine = None\n\n        self.stop_network_engine = asyncio.Event()\n        self.loop = asyncio.get_event_loop()\n        max_concurrent_tasks = 5\n        self.semaphore_send_model = asyncio.Semaphore(max_concurrent_tasks)\n\n        self._blacklist = BlackList()\n\n        # Connection service to communicate with external devices\n        self._external_connection_service = factory_connection_service(\"nebula\", self.addr)\n\n        self._initialized = True\n        self._running = asyncio.Event()\n        logging.info(\"Communication Manager initialization completed\")\n\n    @property\n    def engine(self):\n        \"\"\"\n        Returns the main engine responsible for coordinating local training and aggregation.\n        \"\"\"\n        return self._engine\n\n    @property\n    def connections(self):\n        \"\"\"\n        Returns the current list of active connections to neighboring nodes.\n        \"\"\"\n        return self._connections\n\n    @property\n    def mm(self):\n        \"\"\"\n        Returns the MessagesManager instance, used to create and process protocol messages.\n        \"\"\"\n        return self._mm\n\n    @property\n    def discoverer(self):\n        \"\"\"\n        Returns the component responsible for discovering new nodes in the network.\n        \"\"\"\n        return self._discoverer\n\n    @property\n    def health(self):\n        \"\"\"\n        Returns the HealthMonitor component that checks and maintains node health status.\n        \"\"\"\n        return self._health\n\n    @property\n    def forwarder(self):\n        \"\"\"\n        Returns the message forwarder, responsible for forwarding messages to other nodes.\n        \"\"\"\n        return self._forwarder\n\n    @property\n    def propagator(self):\n        \"\"\"\n        Returns the component responsible for propagating messages throughout the network.\n        \"\"\"\n        return self._propagator\n\n    @property\n    def ecs(self):\n        \"\"\"\n        Returns the ExternalConnectionService for handling external network interactions.\n        \"\"\"\n        return self._external_connection_service\n\n    @property\n    def bl(self):\n        \"\"\"\n        Returns the blacklist manager, used to track and filter banned or disconnected nodes.\n        \"\"\"\n        return self._blacklist\n\n    async def check_federation_ready(self):\n        # Check if all my connections are in ready_connections\n        logging.info(\n            f\"\ud83d\udd17  check_federation_ready | Ready connections: {self.ready_connections} | Connections: {self.connections.keys()}\"\n        )\n        async with self.connections_lock:\n            async with self._ready_connections_lock:\n                if set(self.connections.keys()) == self.ready_connections:\n                    return True\n\n    async def add_ready_connection(self, addr):\n        async with self._ready_connections_lock:\n            self.ready_connections.add(addr)\n\n    async def start_communications(self, initial_neighbors):\n        \"\"\"\n        Starts the communication services and connects to initial neighbors.\n\n        Args:\n            initial_neighbors (list): A list of neighbor addresses to connect to after startup.\n        \"\"\"\n        self._running.set()\n        logging.info(f\"Neighbors: {self.config.participant['network_args']['neighbors']}\")\n        logging.info(\n            f\"\ud83d\udca4  Cold start time: {self.config.participant['misc_args']['grace_time_connection']} seconds before connecting to the network\"\n        )\n        await asyncio.sleep(self.config.participant[\"misc_args\"][\"grace_time_connection\"])\n        await self.start()\n        neighbors = set(initial_neighbors)\n\n        if self.addr in neighbors:\n            neighbors.discard(self.addr)\n\n        for addr in neighbors:\n            await self.connect(addr, direct=True)\n            await asyncio.sleep(1)\n        while not await self.verify_connections(neighbors):\n            await asyncio.sleep(1)\n        current_connections = await self.get_addrs_current_connections()\n        logging.info(f\"Connections verified: {current_connections}\")\n        await self.deploy_additional_services()\n\n    \"\"\"                                                     ##############################\n                                                            #    PROCESSING MESSAGES     #\n                                                            ##############################\n    \"\"\"\n\n    async def handle_incoming_message(self, data, addr_from):\n        \"\"\"\n        Handles an incoming message if the sender is not blacklisted.\n\n        Args:\n            data (bytes): The raw message data.\n            addr_from (str): The address of the sender.\n        \"\"\"\n        if not await self.bl.node_in_blacklist(addr_from):\n            await self.mm.process_message(data, addr_from)\n\n    async def forward_message(self, data, addr_from):\n        \"\"\"\n        Forwards a message to other nodes.\n\n        Args:\n            data (bytes): The message to be forwarded.\n            addr_from (str): The address of the sender.\n        \"\"\"\n        logging.info(\"Forwarding message... \")\n        await self.forwarder.forward(data, addr_from=addr_from)\n\n    async def handle_message(self, message_event):\n        \"\"\"\n        Publishes a message event to the EventManager.\n\n        Args:\n            message_event (MessageEvent): The message event to publish.\n        \"\"\"\n        asyncio.create_task(EventManager.get_instance().publish(message_event))\n\n    async def handle_model_message(self, source, message):\n        \"\"\"\n        Handles a model-related message and routes it as either initialization or update.\n\n        Args:\n            source (str): The sender's address.\n            message (BaseMessage): The model message containing the round and payload.\n        \"\"\"\n        logging.info(f\"\ud83e\udd16  handle_model_message | Received model from {source} with round {message.round}\")\n        if message.round == -1:\n            model_init_event = MessageEvent((\"model\", \"initialization\"), source, message)\n            asyncio.create_task(EventManager.get_instance().publish(model_init_event))\n        else:\n            model_updt_event = MessageEvent((\"model\", \"update\"), source, message)\n            asyncio.create_task(EventManager.get_instance().publish(model_updt_event))\n\n    def create_message(self, message_type: str, action: str = \"\", *args, **kwargs):\n        \"\"\"\n        Creates a new protocol message using the MessagesManager.\n\n        Args:\n            message_type (str): The type of message (e.g., 'model', 'discover').\n            action (str, optional): An optional action to associate with the message.\n            *args: Positional arguments for the message.\n            **kwargs: Keyword arguments for the message.\n\n        Returns:\n            BaseMessage: The constructed message object.\n        \"\"\"\n        return self.mm.create_message(message_type, action, *args, **kwargs)\n\n    def get_messages_events(self):\n        \"\"\"\n        Returns the mapping of message types to their respective events.\n\n        Returns:\n            dict: A dictionary of message event associations.\n        \"\"\"\n        return self.mm.get_messages_events()\n\n    \"\"\"                                                     ##############################\n                                                            #          BLACKLIST         #\n                                                            ##############################\n    \"\"\"\n\n    async def add_to_recently_disconnected(self, addr):\n        \"\"\"\n        Adds the given address to the list of recently disconnected nodes.\n\n        This is typically used for temporary disconnection tracking before reattempting communication.\n\n        Args:\n            addr (str): The address of the node to mark as recently disconnected.\n        \"\"\"\n        await self.bl.add_recently_disconnected(addr)\n\n    async def add_to_blacklist(self, addr):\n        \"\"\"\n        Adds the given address to the blacklist, preventing any future connection attempts.\n\n        Args:\n            addr (str): The address of the node to blacklist.\n        \"\"\"\n        await self.bl.add_to_blacklist(addr)\n\n    async def get_blacklist(self):\n        \"\"\"\n        Retrieves the current set of blacklisted node addresses.\n\n        Returns:\n            set: A set of addresses currently in the blacklist.\n        \"\"\"\n        return await self.bl.get_blacklist()\n\n    async def apply_restrictions(self, nodes: set) -&gt; set | None:\n        \"\"\"\n        Filters a set of node addresses by removing any that are restricted (e.g., blacklisted).\n\n        Args:\n            nodes (set): A set of node addresses to filter.\n\n        Returns:\n            set or None: A filtered set of addresses, or None if all were restricted.\n        \"\"\"\n        return await self.bl.apply_restrictions(nodes)\n\n    async def clear_restrictions(self):\n        \"\"\"\n        Clears all temporary and permanent restrictions, including the blacklist and recently disconnected nodes.\n        \"\"\"\n        await self.bl.clear_restrictions()\n\n    \"\"\"                                                     ###############################\n                                                            # EXTERNAL CONNECTION SERVICE #\n                                                            ###############################\n    \"\"\"\n\n    async def start_external_connection_service(self, run_service=True):\n        \"\"\"\n        Initializes and optionally starts the external connection service (ECS).\n\n        Args:\n            run_service (bool): Whether to start the ECS immediately after initialization. Defaults to True.\n        \"\"\"\n        if self.ecs == None:\n            self._external_connection_service = factory_connection_service(self, self.addr)\n        if run_service:\n            await self.ecs.start()\n\n    async def stop_external_connection_service(self):\n        \"\"\"\n        Stops the external connection service if it is running.\n        \"\"\"\n        await self.ecs.stop()\n\n    async def init_external_connection_service(self):\n        \"\"\"\n        Initializes and starts the external connection service.\n        \"\"\"\n        await self.start_external_connection_service()\n\n    async def is_external_connection_service_running(self):\n        \"\"\"\n        Checks if the external connection service is currently running.\n\n        Returns:\n            bool: True if the ECS is running, False otherwise.\n        \"\"\"\n        return await self.ecs.is_running()\n\n    async def start_beacon(self):\n        \"\"\"\n        Starts the beacon emission process to announce the node's presence on the network.\n        \"\"\"\n        await self.ecs.start_beacon()\n\n    async def stop_beacon(self):\n        \"\"\"\n        Stops the beacon emission process.\n        \"\"\"\n        await self.ecs.stop_beacon()\n\n    async def modify_beacon_frequency(self, frequency):\n        \"\"\"\n        Modifies the frequency of the beacon emission.\n\n        Args:\n            frequency (float): The new frequency (in seconds) between beacon emissions.\n        \"\"\"\n        await self.ecs.modify_beacon_frequency(frequency)\n\n    async def stablish_connection_to_federation(self, msg_type=\"discover_join\", addrs_known=None) -&gt; tuple[int, set]:\n        \"\"\"\n        Uses the ExternalConnectionService to discover and establish connections with other nodes in the federation.\n\n        This method performs the following steps:\n        1. Discovers nodes on the network (if `addrs_known` is not provided).\n        2. Establishes TCP connections with discovered nodes.\n        3. Sends a federation discovery message to them.\n\n        Args:\n            msg_type (str): The type of discovery message to send (e.g., 'discover_join' or 'discover_nodes').\n            addrs_known (list, optional): A list of known addresses to use instead of performing discovery.\n\n        Returns:\n            tuple: A tuple containing:\n                - discovers_sent (int): Number of discovery messages sent.\n                - connections_made (set): Set of addresses to which connections were successfully initiated.\n        \"\"\"\n        addrs = []\n        if addrs_known == None:\n            logging.info(\"Searching federation process beginning...\")\n            addrs = await self.ecs.find_federation()\n            logging.info(f\"Found federation devices | addrs {addrs}\")\n        else:\n            logging.info(f\"Searching federation process beginning... | Using addrs previously known {addrs_known}\")\n            addrs = addrs_known\n\n        msg = self.create_message(\"discover\", msg_type)\n\n        # Remove neighbors\n        neighbors = await self.get_addrs_current_connections(only_direct=True, myself=True)\n        addrs = set(addrs)\n        if neighbors:\n            addrs.difference_update(neighbors)\n\n        discovers_sent = 0\n        connections_made = set()\n        if addrs:\n            logging.info(\"Starting communications with devices found\")\n            max_tries = 5\n            for addr in addrs:\n                await self.connect(addr, direct=False, priority=\"high\")\n                connections_made.add(addr)\n                await asyncio.sleep(1)\n            for i in range(0, max_tries):\n                if await self.verify_any_connections(addrs):\n                    break\n                await asyncio.sleep(1)\n            current_connections = await self.get_addrs_current_connections(only_undirected=True)\n            logging.info(f\"Connections verified after searching: {current_connections}\")\n\n            for addr in addrs:\n                logging.info(f\"Sending {msg_type} to addr: {addr}\")\n                asyncio.create_task(self.send_message(addr, msg))\n                await asyncio.sleep(1)\n                discovers_sent += 1\n        return (discovers_sent, connections_made)\n\n    \"\"\"                                                     ##############################\n                                                            #    OTHER FUNCTIONALITIES   #\n                                                            ##############################\n    \"\"\"\n\n    def get_connections_lock(self):\n        \"\"\"\n        Returns the asynchronous lock object used to synchronize access to the connections dictionary.\n\n        Returns:\n            asyncio.Lock: The lock protecting the connections data structure.\n        \"\"\"\n        return self.connections_lock\n\n    def get_config(self):\n        \"\"\"\n        Returns the configuration object associated with this communications manager.\n\n        Returns:\n            Config: The configuration instance containing settings and parameters.\n        \"\"\"\n        return self.config\n\n    def get_addr(self):\n        \"\"\"\n        Returns the network address (host:port) of this node.\n\n        Returns:\n            str: The node's own address.\n        \"\"\"\n        return self.addr\n\n    async def get_round(self):\n        \"\"\"\n        Retrieves the current training round number from the engine.\n\n        Returns:\n            int: The current round number in the federated learning process.\n        \"\"\"\n        return await self.engine.get_round()\n\n    async def start(self):\n        \"\"\"\n        Starts the communications manager by deploying the network engine to accept incoming connections.\n\n        This initializes the server and begins listening on the configured host and port.\n        \"\"\"\n        logging.info(\"\ud83c\udf10  Starting Communications Manager...\")\n        await self.deploy_network_engine()\n\n    async def deploy_network_engine(self):\n        \"\"\"\n        Deploys and starts the network engine server that listens for incoming connections.\n\n        Creates an asyncio server and schedules it to serve connections indefinitely.\n        \"\"\"\n        logging.info(\"\ud83c\udf10  Deploying Network engine...\")\n        self.network_engine = await asyncio.start_server(self.handle_connection_wrapper, self.host, self.port)\n        self.network_task = asyncio.create_task(self.network_engine.serve_forever(), name=\"Network Engine\")\n        logging.info(f\"\ud83c\udf10  Network engine deployed at host {self.host} and port {self.port}\")\n\n    async def handle_connection_wrapper(self, reader, writer):\n        asyncio.create_task(self.handle_connection(reader, writer))\n\n    async def handle_connection(self, reader, writer, priority=\"medium\"):\n        \"\"\"\n        Wrapper coroutine to handle a new incoming connection.\n\n        Schedules the actual connection handling coroutine as an asyncio task.\n\n        Args:\n            reader (asyncio.StreamReader): Stream reader for the connection.\n            writer (asyncio.StreamWriter): Stream writer for the connection.\n        \"\"\"\n\n        async def process_connection(reader, writer, priority=\"medium\"):\n            \"\"\"\n            Handles the lifecycle of a new incoming connection, including validation, authorization,\n            and adding the connection to the manager.\n\n            Performs checks such as blacklist verification, self-connection rejection, maximum connection limits,\n            duplicate connection detection, and manages pending connections.\n\n            Args:\n                reader (asyncio.StreamReader): Stream reader for the connection.\n                writer (asyncio.StreamWriter): Stream writer for the connection.\n                priority (str, optional): Priority level for processing the connection. Defaults to \"medium\".\n            \"\"\"\n            try:\n                addr = writer.get_extra_info(\"peername\")\n\n                # Check if learning cycle has finished - reject new connections\n                if await self.engine.learning_cycle_finished():\n                    logging.info(f\"\ud83d\udd17  [incoming] Rejecting connection from {addr} because learning cycle has finished\")\n                    writer.write(b\"CONNECTION//CLOSE\\n\")\n                    await writer.drain()\n                    writer.close()\n                    await writer.wait_closed()\n                    return\n\n                connected_node_id = await reader.readline()\n                connected_node_id = connected_node_id.decode(\"utf-8\").strip()\n                connected_node_port = addr[1]\n                if \":\" in connected_node_id:\n                    connected_node_id, connected_node_port = connected_node_id.split(\":\")\n                connection_addr = f\"{addr[0]}:{connected_node_port}\"\n                direct = await reader.readline()\n                direct = direct.decode(\"utf-8\").strip()\n                direct = direct == \"True\"\n                logging.info(\n                    f\"\ud83d\udd17  [incoming] Connection from {addr} - {connection_addr} [id {connected_node_id} | port {connected_node_port} | direct {direct}] (incoming)\"\n                )\n\n                blacklist = await self.bl.get_blacklist()\n                if blacklist:\n                    logging.info(f\"blacklist: {blacklist}, source trying to connect: {connection_addr}\")\n                    if connection_addr in blacklist:\n                        logging.info(f\"\ud83d\udd17  [incoming] Rejecting connection from {connection_addr}, it is blacklisted.\")\n                        writer.close()\n                        await writer.wait_closed()\n                        return\n\n                if self.id == connected_node_id:\n                    logging.info(\"\ud83d\udd17  [incoming] Connection with yourself is not allowed\")\n                    writer.write(b\"CONNECTION//CLOSE\\n\")\n                    await writer.drain()\n                    writer.close()\n                    await writer.wait_closed()\n                    return\n\n                async with self.connections_manager_lock:\n                    async with self.connections_lock:\n                        if len(self.connections) &gt;= self.max_connections:\n                            logging.info(\"\ud83d\udd17  [incoming] Maximum number of connections reached\")\n                            logging.info(f\"\ud83d\udd17  [incoming] Sending CONNECTION//CLOSE to {addr}\")\n                            writer.write(b\"CONNECTION//CLOSE\\n\")\n                            await writer.drain()\n                            writer.close()\n                            await writer.wait_closed()\n                            return\n\n                        logging.info(f\"\ud83d\udd17  [incoming] Connections: {self.connections}\")\n                        if connection_addr in self.connections:\n                            logging.info(f\"\ud83d\udd17  [incoming] Already connected with {self.connections[connection_addr]}\")\n                            logging.info(f\"\ud83d\udd17  [incoming] Sending CONNECTION//EXISTS to {addr}\")\n                            writer.write(b\"CONNECTION//EXISTS\\n\")\n                            await writer.drain()\n                            writer.close()\n                            await writer.wait_closed()\n                            return\n\n                    if connection_addr in self.pending_connections:\n                        logging.info(f\"\ud83d\udd17  [incoming] Connection with {connection_addr} is already pending\")\n                        if int(self.host.split(\".\")[3]) &lt; int(addr[0].split(\".\")[3]):\n                            logging.info(\n                                f\"\ud83d\udd17  [incoming] Closing incoming connection since self.host &lt; host  (from {connection_addr})\"\n                            )\n                            writer.write(b\"CONNECTION//CLOSE\\n\")\n                            await writer.drain()\n                            writer.close()\n                            await writer.wait_closed()\n                            return\n                        else:\n                            logging.info(\n                                f\"\ud83d\udd17  [incoming] Closing outgoing connection since self.host &gt;= host (from {connection_addr})\"\n                            )\n                            if connection_addr in self.outgoing_connections:\n                                out_reader, out_writer = self.outgoing_connections.pop(connection_addr)\n                                out_writer.write(b\"CONNECTION//CLOSE\\n\")\n                                await out_writer.drain()\n                                out_writer.close()\n                                await out_writer.wait_closed()\n\n                    logging.info(f\"\ud83d\udd17  [incoming] Including {connection_addr} in pending connections\")\n                    self.pending_connections.add(connection_addr)\n                    self.incoming_connections[connection_addr] = (reader, writer)\n\n                logging.info(f\"\ud83d\udd17  [incoming] Creating new connection with {addr} (id {connected_node_id})\")\n                await writer.drain()\n                connection = Connection(\n                    reader,\n                    writer,\n                    connected_node_id,\n                    addr[0],\n                    connected_node_port,\n                    direct=direct,\n                    config=self.config,\n                    prio=priority,\n                )\n                async with self.connections_manager_lock:\n                    async with self.connections_lock:\n                        logging.info(f\"\ud83d\udd17  [incoming] Including {connection_addr} in connections\")\n                        self.connections[connection_addr] = connection\n                        logging.info(f\"\ud83d\udd17  [incoming] Sending CONNECTION//NEW to {addr}\")\n                        writer.write(b\"CONNECTION//NEW\\n\")\n                        await writer.drain()\n                        writer.write(f\"{self.id}\\n\".encode())\n                        await writer.drain()\n                        await connection.start()\n\n            except Exception as e:\n                logging.exception(f\"\u2757\ufe0f  [incoming] Error while handling connection with {addr}: {e}\")\n            finally:\n                if connection_addr in self.pending_connections:\n                    logging.info(\n                        f\"\ud83d\udd17  [incoming] Removing {connection_addr} from pending connections: {self.pending_connections}\"\n                    )\n                    self.pending_connections.remove(connection_addr)\n                if connection_addr in self.incoming_connections:\n                    logging.info(\n                        f\"\ud83d\udd17  [incoming] Removing {connection_addr} from incoming connections: {self.incoming_connections.keys()}\"\n                    )\n                    self.incoming_connections.pop(connection_addr)\n\n        await process_connection(reader, writer, priority)\n\n    async def terminate_failed_reconnection(self, conn: Connection):\n        \"\"\"\n        Handles the termination of a failed reconnection attempt.\n\n        Marks the node as recently disconnected and closes the connection unilaterally\n        (i.e., without requiring a mutual disconnection handshake).\n\n        Args:\n            conn (Connection): The connection object representing the failed reconnection.\n        \"\"\"\n        connected_with = conn.addr\n        await self.bl.add_recently_disconnected(connected_with)\n        await self.disconnect(connected_with, mutual_disconnection=False)\n\n    async def stop(self):\n        logging.info(\"\ud83c\udf10  Stopping Communications Manager...\")\n\n        # Stop accepting new connections first\n        if self.network_engine:\n            logging.info(\"\ud83c\udf10  Closing network engine server...\")\n            self.network_engine.close()\n            await self.network_engine.wait_closed()\n            if hasattr(self, \"network_task\") and self.network_task:\n                self.network_task.cancel()\n                try:\n                    await self.network_task\n                except asyncio.CancelledError:\n                    pass\n\n        # Stop all existing connections\n        async with self.connections_lock:\n            connections = list(self.connections.values())\n            for node in connections:\n                await node.stop()\n\n        # Stop additional services\n        if self._forwarder:\n            await self._forwarder.stop()\n        if self.ecs:\n            await self.ecs.stop()\n        if self.discoverer:\n            await self.discoverer.stop()\n        if self.health:\n            try:\n                await self.health.stop()\n            except Exception as e:\n                logging.warning(f\"Error stopping health service: {e}\")\n        if self._propagator:\n            await self._propagator.stop()\n        if self._blacklist:\n            await self._blacklist.stop()\n\n        self._running.clear()\n\n        self.stop_network_engine.set()\n\n        logging.info(\"\ud83c\udf10  Communications Manager stopped successfully\")\n\n    async def run_reconnections(self):\n        for connection in self.connections_reconnect:\n            if connection[\"addr\"] in self.connections:\n                connection[\"tries\"] = 0\n                logging.info(f\"\ud83d\udd17  Node {connection.addr} is still connected!\")\n            else:\n                connection[\"tries\"] += 1\n                await self.connect(connection[\"addr\"])\n\n    async def clear_unused_undirect_connections(self):\n        \"\"\"\n        Cleans up inactive undirected connections.\n\n        Iterates over the current connections, identifies those marked as inactive,\n        and asynchronously disconnects them without requiring mutual disconnection.\n        \"\"\"\n        async with self.connections_lock:\n            inactive_connections = [conn for conn in self.connections.values() if await conn.is_inactive()]\n        for conn in inactive_connections:\n            logging.info(f\"Cleaning unused connection: {conn.addr}\")\n            asyncio.create_task(self.disconnect(conn.addr, mutual_disconnection=False))\n\n    async def verify_any_connections(self, neighbors):\n        \"\"\"\n        Checks if at least one of the given neighbors is currently connected.\n\n        Args:\n            neighbors (iterable): A list or set of neighbor addresses to check.\n\n        Returns:\n            bool: True if at least one neighbor is connected, False otherwise.\n        \"\"\"\n        # Return True if any neighbors are connected\n        async with self.connections_lock:\n            if any(neighbor in self.connections for neighbor in neighbors):\n                return True\n            return False\n\n    async def verify_connections(self, neighbors):\n        \"\"\"\n        Checks if all given neighbors are currently connected.\n\n        Args:\n            neighbors (iterable): A list or set of neighbor addresses to check.\n\n        Returns:\n            bool: True if all neighbors are connected, False otherwise.\n        \"\"\"\n        # Return True if all neighbors are connected\n        async with self.connections_lock:\n            return bool(all(neighbor in self.connections for neighbor in neighbors))\n\n    async def network_wait(self):\n        await self.stop_network_engine.wait()\n\n    async def deploy_additional_services(self):\n        \"\"\"\n        Starts additional network-related services required for the communications manager.\n\n        This includes asynchronously starting the forwarder service and synchronously starting the propagator service,\n        enabling message forwarding and propagation functionalities within the network.\n        \"\"\"\n        logging.info(\"\ud83c\udf10  Deploying additional services...\")\n        await self._forwarder.start()\n        await self._propagator.start()\n\n    async def include_received_message_hash(self, hash_message, source):\n        \"\"\"\n        Adds a received message hash to the tracking list if it hasn't been seen before.\n\n        This prevents processing the same message multiple times in the network.\n\n        Args:\n            hash_message (str): The hash of the received message.\n\n        Returns:\n            bool: True if the hash was added (i.e., the message is new), False if it was already received.\n        \"\"\"\n        try:\n            await self.receive_messages_lock.acquire_async()\n            if hash_message in self.received_messages_hashes:\n                logging.info(\"\u2757\ufe0f  handle_incoming_message | Ignoring message already received.\")\n                duplicated_event = DuplicatedMessageEvent(source, \"Duplicated message received\")\n                asyncio.create_task(EventManager.get_instance().publish_node_event(duplicated_event))\n                return False\n            self.received_messages_hashes.append(hash_message)\n            if len(self.received_messages_hashes) % 10000 == 0:\n                logging.info(f\"\ud83d\udce5  Received {len(self.received_messages_hashes)} messages\")\n            return True\n        except Exception as e:\n            logging.exception(f\"\u2757\ufe0f  handle_incoming_message | Error including message hash: {e}\")\n            return False\n        finally:\n            await self.receive_messages_lock.release_async()\n\n    async def send_message_to_neighbors(self, message, neighbors=None, interval=0):\n        \"\"\"\n        Sends a message to all or specific neighbors.\n\n        Args:\n            message (Any): The message to send.\n            neighbors (set, optional): A set of neighbor addresses to send the message to.\n                If None, the message is sent to all direct neighbors.\n            interval (float, optional): Delay in seconds between sending the message to each neighbor.\n        \"\"\"\n        if neighbors is None:\n            current_connections = await self.get_all_addrs_current_connections(only_direct=True)\n            neighbors = set(current_connections)\n            logging.info(f\"Sending message to ALL neighbors: {neighbors}\")\n        else:\n            logging.info(f\"Sending message to neighbors: {neighbors}\")\n\n        for neighbor in neighbors:\n            asyncio.create_task(self.send_message(neighbor, message))\n            if interval &gt; 0:\n                await asyncio.sleep(interval)\n\n    async def send_message(self, dest_addr, message, message_type=\"\"):\n        \"\"\"\n        Sends a message to a specific destination address, with optional compression for large messages.\n\n        Args:\n            dest_addr (str): The destination address of the message.\n            message (Any): The message to send.\n            message_type (str, optional): Type of message. If in _COMPRESSED_MESSAGES, it will be sent compressed.\n        \"\"\"\n        is_compressed = message_type in _COMPRESSED_MESSAGES\n        if not is_compressed:\n            try:\n                if dest_addr in self.connections:\n                    conn = self.connections[dest_addr]\n                    await conn.send(data=message)\n            except Exception as e:\n                logging.exception(f\"\u2757\ufe0f  Cannot send message {message} to {dest_addr}. Error: {e!s}\")\n                await self.disconnect(dest_addr, mutual_disconnection=False)\n        else:\n            async with self.semaphore_send_model:\n                try:\n                    conn = self.connections.get(dest_addr)\n                    if conn is None:\n                        logging.info(f\"\u2757\ufe0f  Connection with {dest_addr} not found\")\n                        return\n                    await conn.send(data=message, is_compressed=True)\n                except Exception as e:\n                    logging.exception(f\"\u2757\ufe0f  Cannot send model to {dest_addr}: {e!s}\")\n                    await self.disconnect(dest_addr, mutual_disconnection=False)\n\n    async def establish_connection(self, addr, direct=True, reconnect=False, priority=\"medium\"):\n        \"\"\"\n        Establishes a TCP connection to a remote node, handling blacklist checks, pending connection tracking,\n        and bidirectional handshake logic. Optionally upgrades an existing connection to direct, enforces\n        reconnection retries, and assigns a connection priority.\n\n        Args:\n            addr (str): The target node address in \"host:port\" format.\n            direct (bool, optional): Whether this connection should be marked as direct. Defaults to True.\n            reconnect (bool, optional): If True, enable reconnection tracking for this node. Defaults to False.\n            priority (str, optional): Priority level for this connection (\"low\", \"medium\", \"high\"). Defaults to \"medium\".\n\n        Returns:\n            bool: True if the connection action (new or upgrade) succeeded, False otherwise.\n        \"\"\"\n        # Check if learning cycle has finished - don't establish new connections\n        if await self.engine.learning_cycle_finished():\n            logging.info(f\"\ud83d\udd17  [outgoing] Not establishing connection to {addr} because learning cycle has finished\")\n            return False\n\n        logging.info(f\"\ud83d\udd17  [outgoing] Establishing connection with {addr} (direct: {direct})\")\n\n        async def process_establish_connection(addr, direct, reconnect, priority):\n            try:\n                host = str(addr.split(\":\")[0])\n                port = str(addr.split(\":\")[1])\n                if host == self.host and port == self.port:\n                    logging.info(\"\ud83d\udd17  [outgoing] Connection with yourself is not allowed\")\n                    return False\n\n                blacklist = await self.bl.get_blacklist()\n                if blacklist:\n                    logging.info(f\"blacklist: {blacklist}, source trying to connect: {addr}\")\n                    if addr in blacklist:\n                        logging.info(f\"\ud83d\udd17  [incoming] Rejecting connection from {addr}, it is blacklisted.\")\n                        return\n\n                async with self.connections_manager_lock:\n                    async with self.connections_lock:\n                        if addr in self.connections:\n                            logging.info(f\"\ud83d\udd17  [outgoing] Already connected with {self.connections[addr]}\")\n                            if not self.connections[addr].get_direct() and (direct == True):\n                                self.connections[addr].set_direct(direct)\n                                return True\n                            else:\n                                return False\n                    if addr in self.pending_connections:\n                        logging.info(f\"\ud83d\udd17  [outgoing] Connection with {addr} is already pending\")\n                        if int(self.host.split(\".\")[3]) &gt;= int(host.split(\".\")[3]):\n                            logging.info(\n                                f\"\ud83d\udd17  [outgoing] Closing outgoing connection since self.host &gt;= host (from {addr})\"\n                            )\n                            return False\n                        else:\n                            logging.info(\n                                f\"\ud83d\udd17  [outgoing] Closing incoming connection since self.host &lt; host (from {addr})\"\n                            )\n                            if addr in self.incoming_connections:\n                                inc_reader, inc_writer = self.incoming_connections.pop(addr)\n                                inc_writer.write(b\"CONNECTION//CLOSE\\n\")\n                                await inc_writer.drain()\n                                inc_writer.close()\n                                await inc_writer.wait_closed()\n\n                    self.pending_connections.add(addr)\n                    logging.info(f\"\ud83d\udd17  [outgoing] Including {addr} in pending connections: {self.pending_connections}\")\n\n                logging.info(f\"\ud83d\udd17  [outgoing] Openning connection with {host}:{port}\")\n                reader, writer = await asyncio.open_connection(host, port)\n                logging.info(f\"\ud83d\udd17  [outgoing] Connection opened with {writer.get_extra_info('peername')}\")\n\n                async with self.connections_manager_lock:\n                    self.outgoing_connections[addr] = (reader, writer)\n\n                writer.write(f\"{self.id}:{self.port}\\n\".encode())\n                await writer.drain()\n                writer.write(f\"{direct}\\n\".encode())\n                await writer.drain()\n\n                connection_status = await reader.readline()\n                connection_status = connection_status.decode(\"utf-8\").strip()\n\n                logging.info(f\"\ud83d\udd17  [outgoing] Received connection status {connection_status} (from {addr})\")\n                async with self.connections_lock:\n                    logging.info(f\"\ud83d\udd17  [outgoing] Connections: {self.connections}\")\n\n                if connection_status == \"CONNECTION//CLOSE\":\n                    logging.info(f\"\ud83d\udd17  [outgoing] Connection with {addr} closed\")\n                    if addr in self.pending_connections:\n                        logging.info(\n                            f\"\ud83d\udd17  [outgoing] Removing {addr} from pending connections: {self.pending_connections}\"\n                        )\n                        self.pending_connections.remove(addr)\n                    if addr in self.outgoing_connections:\n                        logging.info(\n                            f\"\ud83d\udd17  [outgoing] Removing {addr} from outgoing connections: {self.outgoing_connections.keys()}\"\n                        )\n                        self.outgoing_connections.pop(addr)\n                    if addr in self.incoming_connections:\n                        logging.info(\n                            f\"\ud83d\udd17  [outgoing] Removing {addr} from incoming connections: {self.incoming_connections.keys()}\"\n                        )\n                        self.incoming_connections.pop(addr)\n                    writer.close()\n                    await writer.wait_closed()\n                    return False\n                elif connection_status == \"CONNECTION//PENDING\":\n                    logging.info(f\"\ud83d\udd17  [outgoing] Connection with {addr} is already pending\")\n                    writer.close()\n                    await writer.wait_closed()\n                    return False\n                elif connection_status == \"CONNECTION//EXISTS\":\n                    async with self.connections_lock:\n                        logging.info(f\"\ud83d\udd17  [outgoing] Already connected {self.connections[addr]}\")\n                    writer.close()\n                    await writer.wait_closed()\n                    return True\n                elif connection_status == \"CONNECTION//NEW\":\n                    async with self.connections_manager_lock:\n                        connected_node_id = await reader.readline()\n                        connected_node_id = connected_node_id.decode(\"utf-8\").strip()\n                        logging.info(f\"\ud83d\udd17  [outgoing] Received connected node id: {connected_node_id} (from {addr})\")\n                        logging.info(\n                            f\"\ud83d\udd17  [outgoing] Creating new connection with {host}:{port} (id {connected_node_id})\"\n                        )\n                        connection = Connection(\n                            reader,\n                            writer,\n                            connected_node_id,\n                            host,\n                            port,\n                            direct=direct,\n                            config=self.config,\n                            prio=priority,\n                        )\n                        async with self.connections_lock:\n                            self.connections[addr] = connection\n                        await connection.start()\n                else:\n                    logging.info(f\"\ud83d\udd17  [outgoing] Unknown connection status {connection_status}\")\n                    writer.close()\n                    await writer.wait_closed()\n                    return False\n\n                if reconnect:\n                    logging.info(f\"\ud83d\udd17  [outgoing] Reconnection check is enabled on node {addr}\")\n                    self.connections_reconnect.append({\"addr\": addr, \"tries\": 0})\n\n                if direct:\n                    self.config.add_neighbor_from_config(addr)\n                return True\n            except Exception as e:\n                logging.info(f\"\u2757\ufe0f  [outgoing] Error adding direct connected neighbor {addr}: {e!s}\")\n                return False\n            finally:\n                if addr in self.pending_connections:\n                    logging.info(f\"\ud83d\udd17  [outgoing] Removing {addr} from pending connections: {self.pending_connections}\")\n                    self.pending_connections.remove(addr)\n                if addr in self.outgoing_connections:\n                    logging.info(\n                        f\"\ud83d\udd17  [outgoing] Removing {addr} from outgoing connections: {self.outgoing_connections.keys()}\"\n                    )\n                    self.outgoing_connections.pop(addr)\n                if addr in self.incoming_connections:\n                    logging.info(\n                        f\"\ud83d\udd17  [outgoing] Removing {addr} from incoming connections: {self.incoming_connections.keys()}\"\n                    )\n                    self.incoming_connections.pop(addr)\n\n        asyncio.create_task(process_establish_connection(addr, direct, reconnect, priority))\n\n    async def connect(self, addr, direct=True, priority=\"medium\"):\n        \"\"\"\n        Public method to initiate or upgrade a connection to a neighbor. Checks for existing connections,\n        avoids duplicates, and delegates the actual establishment logic to `establish_connection`.\n\n        Args:\n            addr (str): The neighbor address in \"host:port\" format.\n            direct (bool, optional): Whether the new connection should be direct. Defaults to True.\n            priority (str, optional): Priority level for establishing the connection. Defaults to \"medium\".\n\n        Returns:\n            bool: True if the connection action (new or upgrade) succeeded, False otherwise.\n        \"\"\"\n        async with self.connections_lock:\n            duplicated = addr in self.connections\n        if duplicated:\n            if direct:  # Upcoming direct connection\n                if not self.connections[addr].get_direct():\n                    logging.info(f\"\ud83d\udd17  [outgoing] Upgrading non direct connected neighbor {addr} to direct connection\")\n                    return await self.establish_connection(addr, direct=True, reconnect=False, priority=priority)\n                else:  # Upcoming undirected connection\n                    logging.info(f\"\ud83d\udd17  [outgoing] Already direct connected neighbor {addr}, reconnecting...\")\n                    return await self.establish_connection(addr, direct=True, reconnect=False, priority=priority)\n            else:\n                logging.info(f\"\u2757\ufe0f  Cannot add a duplicate {addr} (undirected connection), already connected\")\n                return False\n        else:\n            if direct:\n                return await self.establish_connection(addr, direct=True, reconnect=False, priority=priority)\n            else:\n                return await self.establish_connection(addr, direct=False, reconnect=False, priority=priority)\n\n    async def register(self):\n        data = {\"node\": self.addr}\n        logging.info(f\"Registering node {self.addr} in the controller\")\n        response = requests.post(self.register_endpoint, json=data)\n        if response.status_code == 200:\n            logging.info(f\"Node {self.addr} registered successfully in the controller\")\n        else:\n            logging.error(f\"Error registering node {self.addr} in the controller\")\n\n    async def wait_for_controller(self):\n        while await self.is_running():\n            response = requests.get(self.wait_endpoint)\n            if response.status_code == 200:\n                logging.info(\"Continue signal received from controller\")\n                break\n            else:\n                logging.info(\"Waiting for controller signal...\")\n            await asyncio.sleep(1)\n\n    async def is_running(self):\n        return self._running.is_set()\n\n    async def disconnect(self, dest_addr, mutual_disconnection=True, forced=False):\n        \"\"\"\n        Disconnects from a specified destination address and performs cleanup tasks.\n\n        Args:\n            dest_addr (str): The address of the node to disconnect from.\n            mutual_disconnection (bool, optional): Whether to notify the peer about the disconnection. Defaults to True.\n            forced (bool, optional): If True, the destination address will be blacklisted. Defaults to False.\n        \"\"\"\n        logging.info(f\"Trying to disconnect {dest_addr}\")\n\n        # Check if this is a direct neighbor before proceeding\n        is_neighbor = dest_addr in await self.get_addrs_current_connections(only_direct=True, myself=True)\n\n        # Add to blacklist if forced disconnection\n        if forced:\n            await self.add_to_blacklist(dest_addr)\n\n        # Get the connection under lock to prevent race conditions\n        async with self.connections_lock:\n            connection_to_remove = self.connections.get(dest_addr)\n            if not connection_to_remove:\n                logging.info(f\"Connection {dest_addr} not found\")\n                return\n            conn = self.connections[dest_addr]\n\n        try:\n            # Attempt mutual disconnection if requested\n            if mutual_disconnection:\n                try:\n                    await conn.send(data=self.create_message(\"connection\", \"disconnect\"))\n                    async with self.connections_lock:\n                        if dest_addr in self.connections:\n                            self.connections.pop(dest_addr)\n                    await conn.stop()\n                except Exception as e:\n                    logging.warning(f\"Failed to send disconnect message to {dest_addr}: {e!s}\")\n                    # Ensure connection is removed even if message sending fails\n                    async with self.connections_lock:\n                        if dest_addr in self.connections:\n                            self.connections.pop(dest_addr)\n                    await conn.stop()\n            else:\n                # For non-mutual disconnection, just stop and remove\n                async with self.connections_lock:\n                    if dest_addr in self.connections:\n                        self.connections.pop(dest_addr)\n                await conn.stop()\n\n            # Update configuration and neighbors\n            current_connections = await self.get_all_addrs_current_connections(only_direct=True)\n            current_connections = set(current_connections)\n            logging.info(f\"Current connections after disconnection: {current_connections}\")\n\n            # Update configuration\n            self.config.update_neighbors_from_config(current_connections, dest_addr)\n\n            # Update engine if this was a direct neighbor\n            if is_neighbor:\n                current_connections = await self.get_addrs_current_connections(only_direct=True, myself=True)\n                await self.engine.update_neighbors(dest_addr, current_connections, remove=True)\n\n        except Exception as e:\n            logging.exception(f\"Error during disconnection of {dest_addr}: {e!s}\")\n            # Ensure connection is removed even if there's an error\n            async with self.connections_lock:\n                if dest_addr in self.connections:\n                    self.connections.pop(dest_addr)\n            try:\n                await conn.stop()\n            except Exception as stop_error:\n                logging.warning(f\"Error stopping connection during cleanup: {stop_error!s}\")\n            raise\n\n    async def get_all_addrs_current_connections(self, only_direct=False, only_undirected=False):\n        \"\"\"\n        Retrieve the addresses of current connections with filtering options.\n\n        Args:\n            only_direct (bool, optional): If True, return only directly connected addresses. Defaults to False.\n            only_undirected (bool, optional): If True, return only undirected (non-direct) connections. Defaults to False.\n\n        Returns:\n            set: A set of connection addresses based on the filtering criteria.\n        \"\"\"\n        try:\n            await self.get_connections_lock().acquire_async()\n            if only_direct:\n                return {addr for addr, conn in self.connections.items() if conn.get_direct()}\n            elif only_undirected:\n                return {addr for addr, conn in self.connections.items() if not conn.get_direct()}\n            else:\n                return set(self.connections.keys())\n        finally:\n            await self.get_connections_lock().release_async()\n\n    async def get_addrs_current_connections(self, only_direct=False, only_undirected=False, myself=False):\n        \"\"\"\n        Get the addresses of current connections, optionally including self and filtering by connection type.\n\n        Args:\n            only_direct (bool, optional): If True, include only directly connected addresses. Defaults to False.\n            only_undirected (bool, optional): If True, include only undirected connections. Defaults to False.\n            myself (bool, optional): If True, include this node's own address in the result. Defaults to False.\n\n        Returns:\n            set: A set of connection addresses according to the specified filters.\n        \"\"\"\n        current_connections = await self.get_all_addrs_current_connections(\n            only_direct=only_direct, only_undirected=only_undirected\n        )\n        current_connections = set(current_connections)\n        if myself:\n            current_connections.add(self.addr)\n        return current_connections\n\n    def get_ready_connections(self):\n        return {addr for addr, conn in self.connections.items() if conn.get_ready()}\n\n    async def learning_finished(self):\n        return await self.engine.learning_cycle_finished()\n\n    def __str__(self):\n        return f\"Connections: {[str(conn) for conn in self.connections.values()]}\"\n</code></pre>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.bl","title":"<code>bl</code>  <code>property</code>","text":"<p>Returns the blacklist manager, used to track and filter banned or disconnected nodes.</p>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.connections","title":"<code>connections</code>  <code>property</code>","text":"<p>Returns the current list of active connections to neighboring nodes.</p>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.discoverer","title":"<code>discoverer</code>  <code>property</code>","text":"<p>Returns the component responsible for discovering new nodes in the network.</p>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.ecs","title":"<code>ecs</code>  <code>property</code>","text":"<p>Returns the ExternalConnectionService for handling external network interactions.</p>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.engine","title":"<code>engine</code>  <code>property</code>","text":"<p>Returns the main engine responsible for coordinating local training and aggregation.</p>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.forwarder","title":"<code>forwarder</code>  <code>property</code>","text":"<p>Returns the message forwarder, responsible for forwarding messages to other nodes.</p>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.health","title":"<code>health</code>  <code>property</code>","text":"<p>Returns the HealthMonitor component that checks and maintains node health status.</p>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.mm","title":"<code>mm</code>  <code>property</code>","text":"<p>Returns the MessagesManager instance, used to create and process protocol messages.</p>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.propagator","title":"<code>propagator</code>  <code>property</code>","text":"<p>Returns the component responsible for propagating messages throughout the network.</p>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.add_to_blacklist","title":"<code>add_to_blacklist(addr)</code>  <code>async</code>","text":"<p>Adds the given address to the blacklist, preventing any future connection attempts.</p> <p>Parameters:</p> Name Type Description Default <code>addr</code> <code>str</code> <p>The address of the node to blacklist.</p> required Source code in <code>nebula/core/network/communications.py</code> <pre><code>async def add_to_blacklist(self, addr):\n    \"\"\"\n    Adds the given address to the blacklist, preventing any future connection attempts.\n\n    Args:\n        addr (str): The address of the node to blacklist.\n    \"\"\"\n    await self.bl.add_to_blacklist(addr)\n</code></pre>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.add_to_recently_disconnected","title":"<code>add_to_recently_disconnected(addr)</code>  <code>async</code>","text":"<p>Adds the given address to the list of recently disconnected nodes.</p> <p>This is typically used for temporary disconnection tracking before reattempting communication.</p> <p>Parameters:</p> Name Type Description Default <code>addr</code> <code>str</code> <p>The address of the node to mark as recently disconnected.</p> required Source code in <code>nebula/core/network/communications.py</code> <pre><code>async def add_to_recently_disconnected(self, addr):\n    \"\"\"\n    Adds the given address to the list of recently disconnected nodes.\n\n    This is typically used for temporary disconnection tracking before reattempting communication.\n\n    Args:\n        addr (str): The address of the node to mark as recently disconnected.\n    \"\"\"\n    await self.bl.add_recently_disconnected(addr)\n</code></pre>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.apply_restrictions","title":"<code>apply_restrictions(nodes)</code>  <code>async</code>","text":"<p>Filters a set of node addresses by removing any that are restricted (e.g., blacklisted).</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>set</code> <p>A set of node addresses to filter.</p> required <p>Returns:</p> Type Description <code>set | None</code> <p>set or None: A filtered set of addresses, or None if all were restricted.</p> Source code in <code>nebula/core/network/communications.py</code> <pre><code>async def apply_restrictions(self, nodes: set) -&gt; set | None:\n    \"\"\"\n    Filters a set of node addresses by removing any that are restricted (e.g., blacklisted).\n\n    Args:\n        nodes (set): A set of node addresses to filter.\n\n    Returns:\n        set or None: A filtered set of addresses, or None if all were restricted.\n    \"\"\"\n    return await self.bl.apply_restrictions(nodes)\n</code></pre>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.clear_restrictions","title":"<code>clear_restrictions()</code>  <code>async</code>","text":"<p>Clears all temporary and permanent restrictions, including the blacklist and recently disconnected nodes.</p> Source code in <code>nebula/core/network/communications.py</code> <pre><code>async def clear_restrictions(self):\n    \"\"\"\n    Clears all temporary and permanent restrictions, including the blacklist and recently disconnected nodes.\n    \"\"\"\n    await self.bl.clear_restrictions()\n</code></pre>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.clear_unused_undirect_connections","title":"<code>clear_unused_undirect_connections()</code>  <code>async</code>","text":"<p>Cleans up inactive undirected connections.</p> <p>Iterates over the current connections, identifies those marked as inactive, and asynchronously disconnects them without requiring mutual disconnection.</p> Source code in <code>nebula/core/network/communications.py</code> <pre><code>async def clear_unused_undirect_connections(self):\n    \"\"\"\n    Cleans up inactive undirected connections.\n\n    Iterates over the current connections, identifies those marked as inactive,\n    and asynchronously disconnects them without requiring mutual disconnection.\n    \"\"\"\n    async with self.connections_lock:\n        inactive_connections = [conn for conn in self.connections.values() if await conn.is_inactive()]\n    for conn in inactive_connections:\n        logging.info(f\"Cleaning unused connection: {conn.addr}\")\n        asyncio.create_task(self.disconnect(conn.addr, mutual_disconnection=False))\n</code></pre>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.connect","title":"<code>connect(addr, direct=True, priority='medium')</code>  <code>async</code>","text":"<p>Public method to initiate or upgrade a connection to a neighbor. Checks for existing connections, avoids duplicates, and delegates the actual establishment logic to <code>establish_connection</code>.</p> <p>Parameters:</p> Name Type Description Default <code>addr</code> <code>str</code> <p>The neighbor address in \"host:port\" format.</p> required <code>direct</code> <code>bool</code> <p>Whether the new connection should be direct. Defaults to True.</p> <code>True</code> <code>priority</code> <code>str</code> <p>Priority level for establishing the connection. Defaults to \"medium\".</p> <code>'medium'</code> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the connection action (new or upgrade) succeeded, False otherwise.</p> Source code in <code>nebula/core/network/communications.py</code> <pre><code>async def connect(self, addr, direct=True, priority=\"medium\"):\n    \"\"\"\n    Public method to initiate or upgrade a connection to a neighbor. Checks for existing connections,\n    avoids duplicates, and delegates the actual establishment logic to `establish_connection`.\n\n    Args:\n        addr (str): The neighbor address in \"host:port\" format.\n        direct (bool, optional): Whether the new connection should be direct. Defaults to True.\n        priority (str, optional): Priority level for establishing the connection. Defaults to \"medium\".\n\n    Returns:\n        bool: True if the connection action (new or upgrade) succeeded, False otherwise.\n    \"\"\"\n    async with self.connections_lock:\n        duplicated = addr in self.connections\n    if duplicated:\n        if direct:  # Upcoming direct connection\n            if not self.connections[addr].get_direct():\n                logging.info(f\"\ud83d\udd17  [outgoing] Upgrading non direct connected neighbor {addr} to direct connection\")\n                return await self.establish_connection(addr, direct=True, reconnect=False, priority=priority)\n            else:  # Upcoming undirected connection\n                logging.info(f\"\ud83d\udd17  [outgoing] Already direct connected neighbor {addr}, reconnecting...\")\n                return await self.establish_connection(addr, direct=True, reconnect=False, priority=priority)\n        else:\n            logging.info(f\"\u2757\ufe0f  Cannot add a duplicate {addr} (undirected connection), already connected\")\n            return False\n    else:\n        if direct:\n            return await self.establish_connection(addr, direct=True, reconnect=False, priority=priority)\n        else:\n            return await self.establish_connection(addr, direct=False, reconnect=False, priority=priority)\n</code></pre>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.create_message","title":"<code>create_message(message_type, action='', *args, **kwargs)</code>","text":"<p>Creates a new protocol message using the MessagesManager.</p> <p>Parameters:</p> Name Type Description Default <code>message_type</code> <code>str</code> <p>The type of message (e.g., 'model', 'discover').</p> required <code>action</code> <code>str</code> <p>An optional action to associate with the message.</p> <code>''</code> <code>*args</code> <p>Positional arguments for the message.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments for the message.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>BaseMessage</code> <p>The constructed message object.</p> Source code in <code>nebula/core/network/communications.py</code> <pre><code>def create_message(self, message_type: str, action: str = \"\", *args, **kwargs):\n    \"\"\"\n    Creates a new protocol message using the MessagesManager.\n\n    Args:\n        message_type (str): The type of message (e.g., 'model', 'discover').\n        action (str, optional): An optional action to associate with the message.\n        *args: Positional arguments for the message.\n        **kwargs: Keyword arguments for the message.\n\n    Returns:\n        BaseMessage: The constructed message object.\n    \"\"\"\n    return self.mm.create_message(message_type, action, *args, **kwargs)\n</code></pre>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.deploy_additional_services","title":"<code>deploy_additional_services()</code>  <code>async</code>","text":"<p>Starts additional network-related services required for the communications manager.</p> <p>This includes asynchronously starting the forwarder service and synchronously starting the propagator service, enabling message forwarding and propagation functionalities within the network.</p> Source code in <code>nebula/core/network/communications.py</code> <pre><code>async def deploy_additional_services(self):\n    \"\"\"\n    Starts additional network-related services required for the communications manager.\n\n    This includes asynchronously starting the forwarder service and synchronously starting the propagator service,\n    enabling message forwarding and propagation functionalities within the network.\n    \"\"\"\n    logging.info(\"\ud83c\udf10  Deploying additional services...\")\n    await self._forwarder.start()\n    await self._propagator.start()\n</code></pre>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.deploy_network_engine","title":"<code>deploy_network_engine()</code>  <code>async</code>","text":"<p>Deploys and starts the network engine server that listens for incoming connections.</p> <p>Creates an asyncio server and schedules it to serve connections indefinitely.</p> Source code in <code>nebula/core/network/communications.py</code> <pre><code>async def deploy_network_engine(self):\n    \"\"\"\n    Deploys and starts the network engine server that listens for incoming connections.\n\n    Creates an asyncio server and schedules it to serve connections indefinitely.\n    \"\"\"\n    logging.info(\"\ud83c\udf10  Deploying Network engine...\")\n    self.network_engine = await asyncio.start_server(self.handle_connection_wrapper, self.host, self.port)\n    self.network_task = asyncio.create_task(self.network_engine.serve_forever(), name=\"Network Engine\")\n    logging.info(f\"\ud83c\udf10  Network engine deployed at host {self.host} and port {self.port}\")\n</code></pre>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.disconnect","title":"<code>disconnect(dest_addr, mutual_disconnection=True, forced=False)</code>  <code>async</code>","text":"<p>Disconnects from a specified destination address and performs cleanup tasks.</p> <p>Parameters:</p> Name Type Description Default <code>dest_addr</code> <code>str</code> <p>The address of the node to disconnect from.</p> required <code>mutual_disconnection</code> <code>bool</code> <p>Whether to notify the peer about the disconnection. Defaults to True.</p> <code>True</code> <code>forced</code> <code>bool</code> <p>If True, the destination address will be blacklisted. Defaults to False.</p> <code>False</code> Source code in <code>nebula/core/network/communications.py</code> <pre><code>async def disconnect(self, dest_addr, mutual_disconnection=True, forced=False):\n    \"\"\"\n    Disconnects from a specified destination address and performs cleanup tasks.\n\n    Args:\n        dest_addr (str): The address of the node to disconnect from.\n        mutual_disconnection (bool, optional): Whether to notify the peer about the disconnection. Defaults to True.\n        forced (bool, optional): If True, the destination address will be blacklisted. Defaults to False.\n    \"\"\"\n    logging.info(f\"Trying to disconnect {dest_addr}\")\n\n    # Check if this is a direct neighbor before proceeding\n    is_neighbor = dest_addr in await self.get_addrs_current_connections(only_direct=True, myself=True)\n\n    # Add to blacklist if forced disconnection\n    if forced:\n        await self.add_to_blacklist(dest_addr)\n\n    # Get the connection under lock to prevent race conditions\n    async with self.connections_lock:\n        connection_to_remove = self.connections.get(dest_addr)\n        if not connection_to_remove:\n            logging.info(f\"Connection {dest_addr} not found\")\n            return\n        conn = self.connections[dest_addr]\n\n    try:\n        # Attempt mutual disconnection if requested\n        if mutual_disconnection:\n            try:\n                await conn.send(data=self.create_message(\"connection\", \"disconnect\"))\n                async with self.connections_lock:\n                    if dest_addr in self.connections:\n                        self.connections.pop(dest_addr)\n                await conn.stop()\n            except Exception as e:\n                logging.warning(f\"Failed to send disconnect message to {dest_addr}: {e!s}\")\n                # Ensure connection is removed even if message sending fails\n                async with self.connections_lock:\n                    if dest_addr in self.connections:\n                        self.connections.pop(dest_addr)\n                await conn.stop()\n        else:\n            # For non-mutual disconnection, just stop and remove\n            async with self.connections_lock:\n                if dest_addr in self.connections:\n                    self.connections.pop(dest_addr)\n            await conn.stop()\n\n        # Update configuration and neighbors\n        current_connections = await self.get_all_addrs_current_connections(only_direct=True)\n        current_connections = set(current_connections)\n        logging.info(f\"Current connections after disconnection: {current_connections}\")\n\n        # Update configuration\n        self.config.update_neighbors_from_config(current_connections, dest_addr)\n\n        # Update engine if this was a direct neighbor\n        if is_neighbor:\n            current_connections = await self.get_addrs_current_connections(only_direct=True, myself=True)\n            await self.engine.update_neighbors(dest_addr, current_connections, remove=True)\n\n    except Exception as e:\n        logging.exception(f\"Error during disconnection of {dest_addr}: {e!s}\")\n        # Ensure connection is removed even if there's an error\n        async with self.connections_lock:\n            if dest_addr in self.connections:\n                self.connections.pop(dest_addr)\n        try:\n            await conn.stop()\n        except Exception as stop_error:\n            logging.warning(f\"Error stopping connection during cleanup: {stop_error!s}\")\n        raise\n</code></pre>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.establish_connection","title":"<code>establish_connection(addr, direct=True, reconnect=False, priority='medium')</code>  <code>async</code>","text":"<p>Establishes a TCP connection to a remote node, handling blacklist checks, pending connection tracking, and bidirectional handshake logic. Optionally upgrades an existing connection to direct, enforces reconnection retries, and assigns a connection priority.</p> <p>Parameters:</p> Name Type Description Default <code>addr</code> <code>str</code> <p>The target node address in \"host:port\" format.</p> required <code>direct</code> <code>bool</code> <p>Whether this connection should be marked as direct. Defaults to True.</p> <code>True</code> <code>reconnect</code> <code>bool</code> <p>If True, enable reconnection tracking for this node. Defaults to False.</p> <code>False</code> <code>priority</code> <code>str</code> <p>Priority level for this connection (\"low\", \"medium\", \"high\"). Defaults to \"medium\".</p> <code>'medium'</code> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the connection action (new or upgrade) succeeded, False otherwise.</p> Source code in <code>nebula/core/network/communications.py</code> <pre><code>async def establish_connection(self, addr, direct=True, reconnect=False, priority=\"medium\"):\n    \"\"\"\n    Establishes a TCP connection to a remote node, handling blacklist checks, pending connection tracking,\n    and bidirectional handshake logic. Optionally upgrades an existing connection to direct, enforces\n    reconnection retries, and assigns a connection priority.\n\n    Args:\n        addr (str): The target node address in \"host:port\" format.\n        direct (bool, optional): Whether this connection should be marked as direct. Defaults to True.\n        reconnect (bool, optional): If True, enable reconnection tracking for this node. Defaults to False.\n        priority (str, optional): Priority level for this connection (\"low\", \"medium\", \"high\"). Defaults to \"medium\".\n\n    Returns:\n        bool: True if the connection action (new or upgrade) succeeded, False otherwise.\n    \"\"\"\n    # Check if learning cycle has finished - don't establish new connections\n    if await self.engine.learning_cycle_finished():\n        logging.info(f\"\ud83d\udd17  [outgoing] Not establishing connection to {addr} because learning cycle has finished\")\n        return False\n\n    logging.info(f\"\ud83d\udd17  [outgoing] Establishing connection with {addr} (direct: {direct})\")\n\n    async def process_establish_connection(addr, direct, reconnect, priority):\n        try:\n            host = str(addr.split(\":\")[0])\n            port = str(addr.split(\":\")[1])\n            if host == self.host and port == self.port:\n                logging.info(\"\ud83d\udd17  [outgoing] Connection with yourself is not allowed\")\n                return False\n\n            blacklist = await self.bl.get_blacklist()\n            if blacklist:\n                logging.info(f\"blacklist: {blacklist}, source trying to connect: {addr}\")\n                if addr in blacklist:\n                    logging.info(f\"\ud83d\udd17  [incoming] Rejecting connection from {addr}, it is blacklisted.\")\n                    return\n\n            async with self.connections_manager_lock:\n                async with self.connections_lock:\n                    if addr in self.connections:\n                        logging.info(f\"\ud83d\udd17  [outgoing] Already connected with {self.connections[addr]}\")\n                        if not self.connections[addr].get_direct() and (direct == True):\n                            self.connections[addr].set_direct(direct)\n                            return True\n                        else:\n                            return False\n                if addr in self.pending_connections:\n                    logging.info(f\"\ud83d\udd17  [outgoing] Connection with {addr} is already pending\")\n                    if int(self.host.split(\".\")[3]) &gt;= int(host.split(\".\")[3]):\n                        logging.info(\n                            f\"\ud83d\udd17  [outgoing] Closing outgoing connection since self.host &gt;= host (from {addr})\"\n                        )\n                        return False\n                    else:\n                        logging.info(\n                            f\"\ud83d\udd17  [outgoing] Closing incoming connection since self.host &lt; host (from {addr})\"\n                        )\n                        if addr in self.incoming_connections:\n                            inc_reader, inc_writer = self.incoming_connections.pop(addr)\n                            inc_writer.write(b\"CONNECTION//CLOSE\\n\")\n                            await inc_writer.drain()\n                            inc_writer.close()\n                            await inc_writer.wait_closed()\n\n                self.pending_connections.add(addr)\n                logging.info(f\"\ud83d\udd17  [outgoing] Including {addr} in pending connections: {self.pending_connections}\")\n\n            logging.info(f\"\ud83d\udd17  [outgoing] Openning connection with {host}:{port}\")\n            reader, writer = await asyncio.open_connection(host, port)\n            logging.info(f\"\ud83d\udd17  [outgoing] Connection opened with {writer.get_extra_info('peername')}\")\n\n            async with self.connections_manager_lock:\n                self.outgoing_connections[addr] = (reader, writer)\n\n            writer.write(f\"{self.id}:{self.port}\\n\".encode())\n            await writer.drain()\n            writer.write(f\"{direct}\\n\".encode())\n            await writer.drain()\n\n            connection_status = await reader.readline()\n            connection_status = connection_status.decode(\"utf-8\").strip()\n\n            logging.info(f\"\ud83d\udd17  [outgoing] Received connection status {connection_status} (from {addr})\")\n            async with self.connections_lock:\n                logging.info(f\"\ud83d\udd17  [outgoing] Connections: {self.connections}\")\n\n            if connection_status == \"CONNECTION//CLOSE\":\n                logging.info(f\"\ud83d\udd17  [outgoing] Connection with {addr} closed\")\n                if addr in self.pending_connections:\n                    logging.info(\n                        f\"\ud83d\udd17  [outgoing] Removing {addr} from pending connections: {self.pending_connections}\"\n                    )\n                    self.pending_connections.remove(addr)\n                if addr in self.outgoing_connections:\n                    logging.info(\n                        f\"\ud83d\udd17  [outgoing] Removing {addr} from outgoing connections: {self.outgoing_connections.keys()}\"\n                    )\n                    self.outgoing_connections.pop(addr)\n                if addr in self.incoming_connections:\n                    logging.info(\n                        f\"\ud83d\udd17  [outgoing] Removing {addr} from incoming connections: {self.incoming_connections.keys()}\"\n                    )\n                    self.incoming_connections.pop(addr)\n                writer.close()\n                await writer.wait_closed()\n                return False\n            elif connection_status == \"CONNECTION//PENDING\":\n                logging.info(f\"\ud83d\udd17  [outgoing] Connection with {addr} is already pending\")\n                writer.close()\n                await writer.wait_closed()\n                return False\n            elif connection_status == \"CONNECTION//EXISTS\":\n                async with self.connections_lock:\n                    logging.info(f\"\ud83d\udd17  [outgoing] Already connected {self.connections[addr]}\")\n                writer.close()\n                await writer.wait_closed()\n                return True\n            elif connection_status == \"CONNECTION//NEW\":\n                async with self.connections_manager_lock:\n                    connected_node_id = await reader.readline()\n                    connected_node_id = connected_node_id.decode(\"utf-8\").strip()\n                    logging.info(f\"\ud83d\udd17  [outgoing] Received connected node id: {connected_node_id} (from {addr})\")\n                    logging.info(\n                        f\"\ud83d\udd17  [outgoing] Creating new connection with {host}:{port} (id {connected_node_id})\"\n                    )\n                    connection = Connection(\n                        reader,\n                        writer,\n                        connected_node_id,\n                        host,\n                        port,\n                        direct=direct,\n                        config=self.config,\n                        prio=priority,\n                    )\n                    async with self.connections_lock:\n                        self.connections[addr] = connection\n                    await connection.start()\n            else:\n                logging.info(f\"\ud83d\udd17  [outgoing] Unknown connection status {connection_status}\")\n                writer.close()\n                await writer.wait_closed()\n                return False\n\n            if reconnect:\n                logging.info(f\"\ud83d\udd17  [outgoing] Reconnection check is enabled on node {addr}\")\n                self.connections_reconnect.append({\"addr\": addr, \"tries\": 0})\n\n            if direct:\n                self.config.add_neighbor_from_config(addr)\n            return True\n        except Exception as e:\n            logging.info(f\"\u2757\ufe0f  [outgoing] Error adding direct connected neighbor {addr}: {e!s}\")\n            return False\n        finally:\n            if addr in self.pending_connections:\n                logging.info(f\"\ud83d\udd17  [outgoing] Removing {addr} from pending connections: {self.pending_connections}\")\n                self.pending_connections.remove(addr)\n            if addr in self.outgoing_connections:\n                logging.info(\n                    f\"\ud83d\udd17  [outgoing] Removing {addr} from outgoing connections: {self.outgoing_connections.keys()}\"\n                )\n                self.outgoing_connections.pop(addr)\n            if addr in self.incoming_connections:\n                logging.info(\n                    f\"\ud83d\udd17  [outgoing] Removing {addr} from incoming connections: {self.incoming_connections.keys()}\"\n                )\n                self.incoming_connections.pop(addr)\n\n    asyncio.create_task(process_establish_connection(addr, direct, reconnect, priority))\n</code></pre>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.forward_message","title":"<code>forward_message(data, addr_from)</code>  <code>async</code>","text":"<p>Forwards a message to other nodes.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>The message to be forwarded.</p> required <code>addr_from</code> <code>str</code> <p>The address of the sender.</p> required Source code in <code>nebula/core/network/communications.py</code> <pre><code>async def forward_message(self, data, addr_from):\n    \"\"\"\n    Forwards a message to other nodes.\n\n    Args:\n        data (bytes): The message to be forwarded.\n        addr_from (str): The address of the sender.\n    \"\"\"\n    logging.info(\"Forwarding message... \")\n    await self.forwarder.forward(data, addr_from=addr_from)\n</code></pre>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.get_addr","title":"<code>get_addr()</code>","text":"<p>Returns the network address (host:port) of this node.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>The node's own address.</p> Source code in <code>nebula/core/network/communications.py</code> <pre><code>def get_addr(self):\n    \"\"\"\n    Returns the network address (host:port) of this node.\n\n    Returns:\n        str: The node's own address.\n    \"\"\"\n    return self.addr\n</code></pre>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.get_addrs_current_connections","title":"<code>get_addrs_current_connections(only_direct=False, only_undirected=False, myself=False)</code>  <code>async</code>","text":"<p>Get the addresses of current connections, optionally including self and filtering by connection type.</p> <p>Parameters:</p> Name Type Description Default <code>only_direct</code> <code>bool</code> <p>If True, include only directly connected addresses. Defaults to False.</p> <code>False</code> <code>only_undirected</code> <code>bool</code> <p>If True, include only undirected connections. Defaults to False.</p> <code>False</code> <code>myself</code> <code>bool</code> <p>If True, include this node's own address in the result. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>set</code> <p>A set of connection addresses according to the specified filters.</p> Source code in <code>nebula/core/network/communications.py</code> <pre><code>async def get_addrs_current_connections(self, only_direct=False, only_undirected=False, myself=False):\n    \"\"\"\n    Get the addresses of current connections, optionally including self and filtering by connection type.\n\n    Args:\n        only_direct (bool, optional): If True, include only directly connected addresses. Defaults to False.\n        only_undirected (bool, optional): If True, include only undirected connections. Defaults to False.\n        myself (bool, optional): If True, include this node's own address in the result. Defaults to False.\n\n    Returns:\n        set: A set of connection addresses according to the specified filters.\n    \"\"\"\n    current_connections = await self.get_all_addrs_current_connections(\n        only_direct=only_direct, only_undirected=only_undirected\n    )\n    current_connections = set(current_connections)\n    if myself:\n        current_connections.add(self.addr)\n    return current_connections\n</code></pre>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.get_all_addrs_current_connections","title":"<code>get_all_addrs_current_connections(only_direct=False, only_undirected=False)</code>  <code>async</code>","text":"<p>Retrieve the addresses of current connections with filtering options.</p> <p>Parameters:</p> Name Type Description Default <code>only_direct</code> <code>bool</code> <p>If True, return only directly connected addresses. Defaults to False.</p> <code>False</code> <code>only_undirected</code> <code>bool</code> <p>If True, return only undirected (non-direct) connections. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>set</code> <p>A set of connection addresses based on the filtering criteria.</p> Source code in <code>nebula/core/network/communications.py</code> <pre><code>async def get_all_addrs_current_connections(self, only_direct=False, only_undirected=False):\n    \"\"\"\n    Retrieve the addresses of current connections with filtering options.\n\n    Args:\n        only_direct (bool, optional): If True, return only directly connected addresses. Defaults to False.\n        only_undirected (bool, optional): If True, return only undirected (non-direct) connections. Defaults to False.\n\n    Returns:\n        set: A set of connection addresses based on the filtering criteria.\n    \"\"\"\n    try:\n        await self.get_connections_lock().acquire_async()\n        if only_direct:\n            return {addr for addr, conn in self.connections.items() if conn.get_direct()}\n        elif only_undirected:\n            return {addr for addr, conn in self.connections.items() if not conn.get_direct()}\n        else:\n            return set(self.connections.keys())\n    finally:\n        await self.get_connections_lock().release_async()\n</code></pre>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.get_blacklist","title":"<code>get_blacklist()</code>  <code>async</code>","text":"<p>Retrieves the current set of blacklisted node addresses.</p> <p>Returns:</p> Name Type Description <code>set</code> <p>A set of addresses currently in the blacklist.</p> Source code in <code>nebula/core/network/communications.py</code> <pre><code>async def get_blacklist(self):\n    \"\"\"\n    Retrieves the current set of blacklisted node addresses.\n\n    Returns:\n        set: A set of addresses currently in the blacklist.\n    \"\"\"\n    return await self.bl.get_blacklist()\n</code></pre>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.get_config","title":"<code>get_config()</code>","text":"<p>Returns the configuration object associated with this communications manager.</p> <p>Returns:</p> Name Type Description <code>Config</code> <p>The configuration instance containing settings and parameters.</p> Source code in <code>nebula/core/network/communications.py</code> <pre><code>def get_config(self):\n    \"\"\"\n    Returns the configuration object associated with this communications manager.\n\n    Returns:\n        Config: The configuration instance containing settings and parameters.\n    \"\"\"\n    return self.config\n</code></pre>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.get_connections_lock","title":"<code>get_connections_lock()</code>","text":"<p>Returns the asynchronous lock object used to synchronize access to the connections dictionary.</p> <p>Returns:</p> Type Description <p>asyncio.Lock: The lock protecting the connections data structure.</p> Source code in <code>nebula/core/network/communications.py</code> <pre><code>def get_connections_lock(self):\n    \"\"\"\n    Returns the asynchronous lock object used to synchronize access to the connections dictionary.\n\n    Returns:\n        asyncio.Lock: The lock protecting the connections data structure.\n    \"\"\"\n    return self.connections_lock\n</code></pre>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.get_instance","title":"<code>get_instance()</code>  <code>classmethod</code>","text":"<p>Obtain CommunicationsManager instance</p> Source code in <code>nebula/core/network/communications.py</code> <pre><code>@classmethod\ndef get_instance(cls):\n    \"\"\"Obtain CommunicationsManager instance\"\"\"\n    if cls._instance is None:\n        raise ValueError(\"CommunicationsManager has not been initialized yet.\")\n    return cls._instance\n</code></pre>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.get_messages_events","title":"<code>get_messages_events()</code>","text":"<p>Returns the mapping of message types to their respective events.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary of message event associations.</p> Source code in <code>nebula/core/network/communications.py</code> <pre><code>def get_messages_events(self):\n    \"\"\"\n    Returns the mapping of message types to their respective events.\n\n    Returns:\n        dict: A dictionary of message event associations.\n    \"\"\"\n    return self.mm.get_messages_events()\n</code></pre>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.get_round","title":"<code>get_round()</code>  <code>async</code>","text":"<p>Retrieves the current training round number from the engine.</p> <p>Returns:</p> Name Type Description <code>int</code> <p>The current round number in the federated learning process.</p> Source code in <code>nebula/core/network/communications.py</code> <pre><code>async def get_round(self):\n    \"\"\"\n    Retrieves the current training round number from the engine.\n\n    Returns:\n        int: The current round number in the federated learning process.\n    \"\"\"\n    return await self.engine.get_round()\n</code></pre>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.handle_connection","title":"<code>handle_connection(reader, writer, priority='medium')</code>  <code>async</code>","text":"<p>Wrapper coroutine to handle a new incoming connection.</p> <p>Schedules the actual connection handling coroutine as an asyncio task.</p> <p>Parameters:</p> Name Type Description Default <code>reader</code> <code>StreamReader</code> <p>Stream reader for the connection.</p> required <code>writer</code> <code>StreamWriter</code> <p>Stream writer for the connection.</p> required Source code in <code>nebula/core/network/communications.py</code> <pre><code>async def handle_connection(self, reader, writer, priority=\"medium\"):\n    \"\"\"\n    Wrapper coroutine to handle a new incoming connection.\n\n    Schedules the actual connection handling coroutine as an asyncio task.\n\n    Args:\n        reader (asyncio.StreamReader): Stream reader for the connection.\n        writer (asyncio.StreamWriter): Stream writer for the connection.\n    \"\"\"\n\n    async def process_connection(reader, writer, priority=\"medium\"):\n        \"\"\"\n        Handles the lifecycle of a new incoming connection, including validation, authorization,\n        and adding the connection to the manager.\n\n        Performs checks such as blacklist verification, self-connection rejection, maximum connection limits,\n        duplicate connection detection, and manages pending connections.\n\n        Args:\n            reader (asyncio.StreamReader): Stream reader for the connection.\n            writer (asyncio.StreamWriter): Stream writer for the connection.\n            priority (str, optional): Priority level for processing the connection. Defaults to \"medium\".\n        \"\"\"\n        try:\n            addr = writer.get_extra_info(\"peername\")\n\n            # Check if learning cycle has finished - reject new connections\n            if await self.engine.learning_cycle_finished():\n                logging.info(f\"\ud83d\udd17  [incoming] Rejecting connection from {addr} because learning cycle has finished\")\n                writer.write(b\"CONNECTION//CLOSE\\n\")\n                await writer.drain()\n                writer.close()\n                await writer.wait_closed()\n                return\n\n            connected_node_id = await reader.readline()\n            connected_node_id = connected_node_id.decode(\"utf-8\").strip()\n            connected_node_port = addr[1]\n            if \":\" in connected_node_id:\n                connected_node_id, connected_node_port = connected_node_id.split(\":\")\n            connection_addr = f\"{addr[0]}:{connected_node_port}\"\n            direct = await reader.readline()\n            direct = direct.decode(\"utf-8\").strip()\n            direct = direct == \"True\"\n            logging.info(\n                f\"\ud83d\udd17  [incoming] Connection from {addr} - {connection_addr} [id {connected_node_id} | port {connected_node_port} | direct {direct}] (incoming)\"\n            )\n\n            blacklist = await self.bl.get_blacklist()\n            if blacklist:\n                logging.info(f\"blacklist: {blacklist}, source trying to connect: {connection_addr}\")\n                if connection_addr in blacklist:\n                    logging.info(f\"\ud83d\udd17  [incoming] Rejecting connection from {connection_addr}, it is blacklisted.\")\n                    writer.close()\n                    await writer.wait_closed()\n                    return\n\n            if self.id == connected_node_id:\n                logging.info(\"\ud83d\udd17  [incoming] Connection with yourself is not allowed\")\n                writer.write(b\"CONNECTION//CLOSE\\n\")\n                await writer.drain()\n                writer.close()\n                await writer.wait_closed()\n                return\n\n            async with self.connections_manager_lock:\n                async with self.connections_lock:\n                    if len(self.connections) &gt;= self.max_connections:\n                        logging.info(\"\ud83d\udd17  [incoming] Maximum number of connections reached\")\n                        logging.info(f\"\ud83d\udd17  [incoming] Sending CONNECTION//CLOSE to {addr}\")\n                        writer.write(b\"CONNECTION//CLOSE\\n\")\n                        await writer.drain()\n                        writer.close()\n                        await writer.wait_closed()\n                        return\n\n                    logging.info(f\"\ud83d\udd17  [incoming] Connections: {self.connections}\")\n                    if connection_addr in self.connections:\n                        logging.info(f\"\ud83d\udd17  [incoming] Already connected with {self.connections[connection_addr]}\")\n                        logging.info(f\"\ud83d\udd17  [incoming] Sending CONNECTION//EXISTS to {addr}\")\n                        writer.write(b\"CONNECTION//EXISTS\\n\")\n                        await writer.drain()\n                        writer.close()\n                        await writer.wait_closed()\n                        return\n\n                if connection_addr in self.pending_connections:\n                    logging.info(f\"\ud83d\udd17  [incoming] Connection with {connection_addr} is already pending\")\n                    if int(self.host.split(\".\")[3]) &lt; int(addr[0].split(\".\")[3]):\n                        logging.info(\n                            f\"\ud83d\udd17  [incoming] Closing incoming connection since self.host &lt; host  (from {connection_addr})\"\n                        )\n                        writer.write(b\"CONNECTION//CLOSE\\n\")\n                        await writer.drain()\n                        writer.close()\n                        await writer.wait_closed()\n                        return\n                    else:\n                        logging.info(\n                            f\"\ud83d\udd17  [incoming] Closing outgoing connection since self.host &gt;= host (from {connection_addr})\"\n                        )\n                        if connection_addr in self.outgoing_connections:\n                            out_reader, out_writer = self.outgoing_connections.pop(connection_addr)\n                            out_writer.write(b\"CONNECTION//CLOSE\\n\")\n                            await out_writer.drain()\n                            out_writer.close()\n                            await out_writer.wait_closed()\n\n                logging.info(f\"\ud83d\udd17  [incoming] Including {connection_addr} in pending connections\")\n                self.pending_connections.add(connection_addr)\n                self.incoming_connections[connection_addr] = (reader, writer)\n\n            logging.info(f\"\ud83d\udd17  [incoming] Creating new connection with {addr} (id {connected_node_id})\")\n            await writer.drain()\n            connection = Connection(\n                reader,\n                writer,\n                connected_node_id,\n                addr[0],\n                connected_node_port,\n                direct=direct,\n                config=self.config,\n                prio=priority,\n            )\n            async with self.connections_manager_lock:\n                async with self.connections_lock:\n                    logging.info(f\"\ud83d\udd17  [incoming] Including {connection_addr} in connections\")\n                    self.connections[connection_addr] = connection\n                    logging.info(f\"\ud83d\udd17  [incoming] Sending CONNECTION//NEW to {addr}\")\n                    writer.write(b\"CONNECTION//NEW\\n\")\n                    await writer.drain()\n                    writer.write(f\"{self.id}\\n\".encode())\n                    await writer.drain()\n                    await connection.start()\n\n        except Exception as e:\n            logging.exception(f\"\u2757\ufe0f  [incoming] Error while handling connection with {addr}: {e}\")\n        finally:\n            if connection_addr in self.pending_connections:\n                logging.info(\n                    f\"\ud83d\udd17  [incoming] Removing {connection_addr} from pending connections: {self.pending_connections}\"\n                )\n                self.pending_connections.remove(connection_addr)\n            if connection_addr in self.incoming_connections:\n                logging.info(\n                    f\"\ud83d\udd17  [incoming] Removing {connection_addr} from incoming connections: {self.incoming_connections.keys()}\"\n                )\n                self.incoming_connections.pop(connection_addr)\n\n    await process_connection(reader, writer, priority)\n</code></pre>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.handle_incoming_message","title":"<code>handle_incoming_message(data, addr_from)</code>  <code>async</code>","text":"<p>Handles an incoming message if the sender is not blacklisted.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>The raw message data.</p> required <code>addr_from</code> <code>str</code> <p>The address of the sender.</p> required Source code in <code>nebula/core/network/communications.py</code> <pre><code>async def handle_incoming_message(self, data, addr_from):\n    \"\"\"\n    Handles an incoming message if the sender is not blacklisted.\n\n    Args:\n        data (bytes): The raw message data.\n        addr_from (str): The address of the sender.\n    \"\"\"\n    if not await self.bl.node_in_blacklist(addr_from):\n        await self.mm.process_message(data, addr_from)\n</code></pre>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.handle_message","title":"<code>handle_message(message_event)</code>  <code>async</code>","text":"<p>Publishes a message event to the EventManager.</p> <p>Parameters:</p> Name Type Description Default <code>message_event</code> <code>MessageEvent</code> <p>The message event to publish.</p> required Source code in <code>nebula/core/network/communications.py</code> <pre><code>async def handle_message(self, message_event):\n    \"\"\"\n    Publishes a message event to the EventManager.\n\n    Args:\n        message_event (MessageEvent): The message event to publish.\n    \"\"\"\n    asyncio.create_task(EventManager.get_instance().publish(message_event))\n</code></pre>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.handle_model_message","title":"<code>handle_model_message(source, message)</code>  <code>async</code>","text":"<p>Handles a model-related message and routes it as either initialization or update.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The sender's address.</p> required <code>message</code> <code>BaseMessage</code> <p>The model message containing the round and payload.</p> required Source code in <code>nebula/core/network/communications.py</code> <pre><code>async def handle_model_message(self, source, message):\n    \"\"\"\n    Handles a model-related message and routes it as either initialization or update.\n\n    Args:\n        source (str): The sender's address.\n        message (BaseMessage): The model message containing the round and payload.\n    \"\"\"\n    logging.info(f\"\ud83e\udd16  handle_model_message | Received model from {source} with round {message.round}\")\n    if message.round == -1:\n        model_init_event = MessageEvent((\"model\", \"initialization\"), source, message)\n        asyncio.create_task(EventManager.get_instance().publish(model_init_event))\n    else:\n        model_updt_event = MessageEvent((\"model\", \"update\"), source, message)\n        asyncio.create_task(EventManager.get_instance().publish(model_updt_event))\n</code></pre>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.include_received_message_hash","title":"<code>include_received_message_hash(hash_message, source)</code>  <code>async</code>","text":"<p>Adds a received message hash to the tracking list if it hasn't been seen before.</p> <p>This prevents processing the same message multiple times in the network.</p> <p>Parameters:</p> Name Type Description Default <code>hash_message</code> <code>str</code> <p>The hash of the received message.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the hash was added (i.e., the message is new), False if it was already received.</p> Source code in <code>nebula/core/network/communications.py</code> <pre><code>async def include_received_message_hash(self, hash_message, source):\n    \"\"\"\n    Adds a received message hash to the tracking list if it hasn't been seen before.\n\n    This prevents processing the same message multiple times in the network.\n\n    Args:\n        hash_message (str): The hash of the received message.\n\n    Returns:\n        bool: True if the hash was added (i.e., the message is new), False if it was already received.\n    \"\"\"\n    try:\n        await self.receive_messages_lock.acquire_async()\n        if hash_message in self.received_messages_hashes:\n            logging.info(\"\u2757\ufe0f  handle_incoming_message | Ignoring message already received.\")\n            duplicated_event = DuplicatedMessageEvent(source, \"Duplicated message received\")\n            asyncio.create_task(EventManager.get_instance().publish_node_event(duplicated_event))\n            return False\n        self.received_messages_hashes.append(hash_message)\n        if len(self.received_messages_hashes) % 10000 == 0:\n            logging.info(f\"\ud83d\udce5  Received {len(self.received_messages_hashes)} messages\")\n        return True\n    except Exception as e:\n        logging.exception(f\"\u2757\ufe0f  handle_incoming_message | Error including message hash: {e}\")\n        return False\n    finally:\n        await self.receive_messages_lock.release_async()\n</code></pre>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.init_external_connection_service","title":"<code>init_external_connection_service()</code>  <code>async</code>","text":"<p>Initializes and starts the external connection service.</p> Source code in <code>nebula/core/network/communications.py</code> <pre><code>async def init_external_connection_service(self):\n    \"\"\"\n    Initializes and starts the external connection service.\n    \"\"\"\n    await self.start_external_connection_service()\n</code></pre>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.is_external_connection_service_running","title":"<code>is_external_connection_service_running()</code>  <code>async</code>","text":"<p>Checks if the external connection service is currently running.</p> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the ECS is running, False otherwise.</p> Source code in <code>nebula/core/network/communications.py</code> <pre><code>async def is_external_connection_service_running(self):\n    \"\"\"\n    Checks if the external connection service is currently running.\n\n    Returns:\n        bool: True if the ECS is running, False otherwise.\n    \"\"\"\n    return await self.ecs.is_running()\n</code></pre>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.modify_beacon_frequency","title":"<code>modify_beacon_frequency(frequency)</code>  <code>async</code>","text":"<p>Modifies the frequency of the beacon emission.</p> <p>Parameters:</p> Name Type Description Default <code>frequency</code> <code>float</code> <p>The new frequency (in seconds) between beacon emissions.</p> required Source code in <code>nebula/core/network/communications.py</code> <pre><code>async def modify_beacon_frequency(self, frequency):\n    \"\"\"\n    Modifies the frequency of the beacon emission.\n\n    Args:\n        frequency (float): The new frequency (in seconds) between beacon emissions.\n    \"\"\"\n    await self.ecs.modify_beacon_frequency(frequency)\n</code></pre>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.send_message","title":"<code>send_message(dest_addr, message, message_type='')</code>  <code>async</code>","text":"<p>Sends a message to a specific destination address, with optional compression for large messages.</p> <p>Parameters:</p> Name Type Description Default <code>dest_addr</code> <code>str</code> <p>The destination address of the message.</p> required <code>message</code> <code>Any</code> <p>The message to send.</p> required <code>message_type</code> <code>str</code> <p>Type of message. If in _COMPRESSED_MESSAGES, it will be sent compressed.</p> <code>''</code> Source code in <code>nebula/core/network/communications.py</code> <pre><code>async def send_message(self, dest_addr, message, message_type=\"\"):\n    \"\"\"\n    Sends a message to a specific destination address, with optional compression for large messages.\n\n    Args:\n        dest_addr (str): The destination address of the message.\n        message (Any): The message to send.\n        message_type (str, optional): Type of message. If in _COMPRESSED_MESSAGES, it will be sent compressed.\n    \"\"\"\n    is_compressed = message_type in _COMPRESSED_MESSAGES\n    if not is_compressed:\n        try:\n            if dest_addr in self.connections:\n                conn = self.connections[dest_addr]\n                await conn.send(data=message)\n        except Exception as e:\n            logging.exception(f\"\u2757\ufe0f  Cannot send message {message} to {dest_addr}. Error: {e!s}\")\n            await self.disconnect(dest_addr, mutual_disconnection=False)\n    else:\n        async with self.semaphore_send_model:\n            try:\n                conn = self.connections.get(dest_addr)\n                if conn is None:\n                    logging.info(f\"\u2757\ufe0f  Connection with {dest_addr} not found\")\n                    return\n                await conn.send(data=message, is_compressed=True)\n            except Exception as e:\n                logging.exception(f\"\u2757\ufe0f  Cannot send model to {dest_addr}: {e!s}\")\n                await self.disconnect(dest_addr, mutual_disconnection=False)\n</code></pre>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.send_message_to_neighbors","title":"<code>send_message_to_neighbors(message, neighbors=None, interval=0)</code>  <code>async</code>","text":"<p>Sends a message to all or specific neighbors.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Any</code> <p>The message to send.</p> required <code>neighbors</code> <code>set</code> <p>A set of neighbor addresses to send the message to. If None, the message is sent to all direct neighbors.</p> <code>None</code> <code>interval</code> <code>float</code> <p>Delay in seconds between sending the message to each neighbor.</p> <code>0</code> Source code in <code>nebula/core/network/communications.py</code> <pre><code>async def send_message_to_neighbors(self, message, neighbors=None, interval=0):\n    \"\"\"\n    Sends a message to all or specific neighbors.\n\n    Args:\n        message (Any): The message to send.\n        neighbors (set, optional): A set of neighbor addresses to send the message to.\n            If None, the message is sent to all direct neighbors.\n        interval (float, optional): Delay in seconds between sending the message to each neighbor.\n    \"\"\"\n    if neighbors is None:\n        current_connections = await self.get_all_addrs_current_connections(only_direct=True)\n        neighbors = set(current_connections)\n        logging.info(f\"Sending message to ALL neighbors: {neighbors}\")\n    else:\n        logging.info(f\"Sending message to neighbors: {neighbors}\")\n\n    for neighbor in neighbors:\n        asyncio.create_task(self.send_message(neighbor, message))\n        if interval &gt; 0:\n            await asyncio.sleep(interval)\n</code></pre>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.stablish_connection_to_federation","title":"<code>stablish_connection_to_federation(msg_type='discover_join', addrs_known=None)</code>  <code>async</code>","text":"<p>Uses the ExternalConnectionService to discover and establish connections with other nodes in the federation.</p> <p>This method performs the following steps: 1. Discovers nodes on the network (if <code>addrs_known</code> is not provided). 2. Establishes TCP connections with discovered nodes. 3. Sends a federation discovery message to them.</p> <p>Parameters:</p> Name Type Description Default <code>msg_type</code> <code>str</code> <p>The type of discovery message to send (e.g., 'discover_join' or 'discover_nodes').</p> <code>'discover_join'</code> <code>addrs_known</code> <code>list</code> <p>A list of known addresses to use instead of performing discovery.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[int, set]</code> <p>A tuple containing: - discovers_sent (int): Number of discovery messages sent. - connections_made (set): Set of addresses to which connections were successfully initiated.</p> Source code in <code>nebula/core/network/communications.py</code> <pre><code>async def stablish_connection_to_federation(self, msg_type=\"discover_join\", addrs_known=None) -&gt; tuple[int, set]:\n    \"\"\"\n    Uses the ExternalConnectionService to discover and establish connections with other nodes in the federation.\n\n    This method performs the following steps:\n    1. Discovers nodes on the network (if `addrs_known` is not provided).\n    2. Establishes TCP connections with discovered nodes.\n    3. Sends a federation discovery message to them.\n\n    Args:\n        msg_type (str): The type of discovery message to send (e.g., 'discover_join' or 'discover_nodes').\n        addrs_known (list, optional): A list of known addresses to use instead of performing discovery.\n\n    Returns:\n        tuple: A tuple containing:\n            - discovers_sent (int): Number of discovery messages sent.\n            - connections_made (set): Set of addresses to which connections were successfully initiated.\n    \"\"\"\n    addrs = []\n    if addrs_known == None:\n        logging.info(\"Searching federation process beginning...\")\n        addrs = await self.ecs.find_federation()\n        logging.info(f\"Found federation devices | addrs {addrs}\")\n    else:\n        logging.info(f\"Searching federation process beginning... | Using addrs previously known {addrs_known}\")\n        addrs = addrs_known\n\n    msg = self.create_message(\"discover\", msg_type)\n\n    # Remove neighbors\n    neighbors = await self.get_addrs_current_connections(only_direct=True, myself=True)\n    addrs = set(addrs)\n    if neighbors:\n        addrs.difference_update(neighbors)\n\n    discovers_sent = 0\n    connections_made = set()\n    if addrs:\n        logging.info(\"Starting communications with devices found\")\n        max_tries = 5\n        for addr in addrs:\n            await self.connect(addr, direct=False, priority=\"high\")\n            connections_made.add(addr)\n            await asyncio.sleep(1)\n        for i in range(0, max_tries):\n            if await self.verify_any_connections(addrs):\n                break\n            await asyncio.sleep(1)\n        current_connections = await self.get_addrs_current_connections(only_undirected=True)\n        logging.info(f\"Connections verified after searching: {current_connections}\")\n\n        for addr in addrs:\n            logging.info(f\"Sending {msg_type} to addr: {addr}\")\n            asyncio.create_task(self.send_message(addr, msg))\n            await asyncio.sleep(1)\n            discovers_sent += 1\n    return (discovers_sent, connections_made)\n</code></pre>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.start","title":"<code>start()</code>  <code>async</code>","text":"<p>Starts the communications manager by deploying the network engine to accept incoming connections.</p> <p>This initializes the server and begins listening on the configured host and port.</p> Source code in <code>nebula/core/network/communications.py</code> <pre><code>async def start(self):\n    \"\"\"\n    Starts the communications manager by deploying the network engine to accept incoming connections.\n\n    This initializes the server and begins listening on the configured host and port.\n    \"\"\"\n    logging.info(\"\ud83c\udf10  Starting Communications Manager...\")\n    await self.deploy_network_engine()\n</code></pre>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.start_beacon","title":"<code>start_beacon()</code>  <code>async</code>","text":"<p>Starts the beacon emission process to announce the node's presence on the network.</p> Source code in <code>nebula/core/network/communications.py</code> <pre><code>async def start_beacon(self):\n    \"\"\"\n    Starts the beacon emission process to announce the node's presence on the network.\n    \"\"\"\n    await self.ecs.start_beacon()\n</code></pre>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.start_communications","title":"<code>start_communications(initial_neighbors)</code>  <code>async</code>","text":"<p>Starts the communication services and connects to initial neighbors.</p> <p>Parameters:</p> Name Type Description Default <code>initial_neighbors</code> <code>list</code> <p>A list of neighbor addresses to connect to after startup.</p> required Source code in <code>nebula/core/network/communications.py</code> <pre><code>async def start_communications(self, initial_neighbors):\n    \"\"\"\n    Starts the communication services and connects to initial neighbors.\n\n    Args:\n        initial_neighbors (list): A list of neighbor addresses to connect to after startup.\n    \"\"\"\n    self._running.set()\n    logging.info(f\"Neighbors: {self.config.participant['network_args']['neighbors']}\")\n    logging.info(\n        f\"\ud83d\udca4  Cold start time: {self.config.participant['misc_args']['grace_time_connection']} seconds before connecting to the network\"\n    )\n    await asyncio.sleep(self.config.participant[\"misc_args\"][\"grace_time_connection\"])\n    await self.start()\n    neighbors = set(initial_neighbors)\n\n    if self.addr in neighbors:\n        neighbors.discard(self.addr)\n\n    for addr in neighbors:\n        await self.connect(addr, direct=True)\n        await asyncio.sleep(1)\n    while not await self.verify_connections(neighbors):\n        await asyncio.sleep(1)\n    current_connections = await self.get_addrs_current_connections()\n    logging.info(f\"Connections verified: {current_connections}\")\n    await self.deploy_additional_services()\n</code></pre>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.start_external_connection_service","title":"<code>start_external_connection_service(run_service=True)</code>  <code>async</code>","text":"<p>Initializes and optionally starts the external connection service (ECS).</p> <p>Parameters:</p> Name Type Description Default <code>run_service</code> <code>bool</code> <p>Whether to start the ECS immediately after initialization. Defaults to True.</p> <code>True</code> Source code in <code>nebula/core/network/communications.py</code> <pre><code>async def start_external_connection_service(self, run_service=True):\n    \"\"\"\n    Initializes and optionally starts the external connection service (ECS).\n\n    Args:\n        run_service (bool): Whether to start the ECS immediately after initialization. Defaults to True.\n    \"\"\"\n    if self.ecs == None:\n        self._external_connection_service = factory_connection_service(self, self.addr)\n    if run_service:\n        await self.ecs.start()\n</code></pre>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.stop_beacon","title":"<code>stop_beacon()</code>  <code>async</code>","text":"<p>Stops the beacon emission process.</p> Source code in <code>nebula/core/network/communications.py</code> <pre><code>async def stop_beacon(self):\n    \"\"\"\n    Stops the beacon emission process.\n    \"\"\"\n    await self.ecs.stop_beacon()\n</code></pre>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.stop_external_connection_service","title":"<code>stop_external_connection_service()</code>  <code>async</code>","text":"<p>Stops the external connection service if it is running.</p> Source code in <code>nebula/core/network/communications.py</code> <pre><code>async def stop_external_connection_service(self):\n    \"\"\"\n    Stops the external connection service if it is running.\n    \"\"\"\n    await self.ecs.stop()\n</code></pre>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.terminate_failed_reconnection","title":"<code>terminate_failed_reconnection(conn)</code>  <code>async</code>","text":"<p>Handles the termination of a failed reconnection attempt.</p> <p>Marks the node as recently disconnected and closes the connection unilaterally (i.e., without requiring a mutual disconnection handshake).</p> <p>Parameters:</p> Name Type Description Default <code>conn</code> <code>Connection</code> <p>The connection object representing the failed reconnection.</p> required Source code in <code>nebula/core/network/communications.py</code> <pre><code>async def terminate_failed_reconnection(self, conn: Connection):\n    \"\"\"\n    Handles the termination of a failed reconnection attempt.\n\n    Marks the node as recently disconnected and closes the connection unilaterally\n    (i.e., without requiring a mutual disconnection handshake).\n\n    Args:\n        conn (Connection): The connection object representing the failed reconnection.\n    \"\"\"\n    connected_with = conn.addr\n    await self.bl.add_recently_disconnected(connected_with)\n    await self.disconnect(connected_with, mutual_disconnection=False)\n</code></pre>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.verify_any_connections","title":"<code>verify_any_connections(neighbors)</code>  <code>async</code>","text":"<p>Checks if at least one of the given neighbors is currently connected.</p> <p>Parameters:</p> Name Type Description Default <code>neighbors</code> <code>iterable</code> <p>A list or set of neighbor addresses to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if at least one neighbor is connected, False otherwise.</p> Source code in <code>nebula/core/network/communications.py</code> <pre><code>async def verify_any_connections(self, neighbors):\n    \"\"\"\n    Checks if at least one of the given neighbors is currently connected.\n\n    Args:\n        neighbors (iterable): A list or set of neighbor addresses to check.\n\n    Returns:\n        bool: True if at least one neighbor is connected, False otherwise.\n    \"\"\"\n    # Return True if any neighbors are connected\n    async with self.connections_lock:\n        if any(neighbor in self.connections for neighbor in neighbors):\n            return True\n        return False\n</code></pre>"},{"location":"api/core/network/communications/#nebula.core.network.communications.CommunicationsManager.verify_connections","title":"<code>verify_connections(neighbors)</code>  <code>async</code>","text":"<p>Checks if all given neighbors are currently connected.</p> <p>Parameters:</p> Name Type Description Default <code>neighbors</code> <code>iterable</code> <p>A list or set of neighbor addresses to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if all neighbors are connected, False otherwise.</p> Source code in <code>nebula/core/network/communications.py</code> <pre><code>async def verify_connections(self, neighbors):\n    \"\"\"\n    Checks if all given neighbors are currently connected.\n\n    Args:\n        neighbors (iterable): A list or set of neighbor addresses to check.\n\n    Returns:\n        bool: True if all neighbors are connected, False otherwise.\n    \"\"\"\n    # Return True if all neighbors are connected\n    async with self.connections_lock:\n        return bool(all(neighbor in self.connections for neighbor in neighbors))\n</code></pre>"},{"location":"api/core/network/connection/","title":"Documentation for Connection Module","text":""},{"location":"api/core/network/connection/#nebula.core.network.connection.Connection","title":"<code>Connection</code>","text":"<p>Manages TCP communication channels using asyncio for asynchronous networking.</p> <p>This class encapsulates the logic for establishing, maintaining, and handling TCP connections between nodes in the distributed system.</p> Responsibilities <ul> <li>Creating and managing asynchronous TCP connections.</li> <li>Sending and receiving messages over the network.</li> <li>Handling connection lifecycle events (open, close, errors).</li> <li>Integrating with asyncio event loop for non-blocking I/O operations.</li> </ul> Usage <ul> <li>Used by nodes to communicate asynchronously with others.</li> <li>Supports concurrent message exchange via asyncio streams.</li> </ul> Note <p>This implementation leverages asyncio to enable scalable and efficient networking in distributed federated learning scenarios.</p> Source code in <code>nebula/core/network/connection.py</code> <pre><code>class Connection:\n    \"\"\"\n    Manages TCP communication channels using asyncio for asynchronous networking.\n\n    This class encapsulates the logic for establishing, maintaining,\n    and handling TCP connections between nodes in the distributed system.\n\n    Responsibilities:\n        - Creating and managing asynchronous TCP connections.\n        - Sending and receiving messages over the network.\n        - Handling connection lifecycle events (open, close, errors).\n        - Integrating with asyncio event loop for non-blocking I/O operations.\n\n    Usage:\n        - Used by nodes to communicate asynchronously with others.\n        - Supports concurrent message exchange via asyncio streams.\n\n    Note:\n        This implementation leverages asyncio to enable scalable\n        and efficient networking in distributed federated learning scenarios.\n    \"\"\"\n\n    DEFAULT_FEDERATED_ROUND = -1\n    INACTIVITY_TIMER = 120\n    INACTIVITY_DAEMON_SLEEP_TIME = 20\n\n    def __init__(\n        self,\n        reader,\n        writer,\n        id,\n        host,\n        port,\n        direct=True,\n        active=True,\n        compression=\"zlib\",\n        config=None,\n        prio=\"medium\",\n    ):\n        self.reader = reader\n        self.writer = writer\n        self.id = str(id)\n        self.host = host\n        self.port = port\n        self.addr = f\"{host}:{port}\"\n        self.direct = direct\n        self.active = active\n        self.last_active = time.time()\n        self.compression = compression\n        self.config = config\n        self._cm = None\n\n        self.federated_round = Connection.DEFAULT_FEDERATED_ROUND\n        self.loop = asyncio.get_event_loop()\n        self.read_task = None\n        self.process_task = None\n        self.inactivity_task = None\n        self.pending_messages_queue = asyncio.Queue(maxsize=100)\n        self.message_buffers: dict[bytes, dict[int, MessageChunk]] = {}\n        self._prio: ConnectionPriority = ConnectionPriority(prio)\n        self._inactivity = False\n        self._last_activity = time.time()\n        self._activity_lock = Locker(name=\"activity_lock\", async_lock=True)\n        self._activity_task = None\n\n        self.EOT_CHAR = b\"\\x00\\x00\\x00\\x04\"\n        self.COMPRESSION_CHAR = b\"\\x00\\x00\\x00\\x01\"\n        self.DATA_TYPE_PREFIXES = {\n            \"pb\": b\"\\x01\\x00\\x00\\x00\",\n            \"string\": b\"\\x02\\x00\\x00\\x00\",\n            \"json\": b\"\\x03\\x00\\x00\\x00\",\n            \"bytes\": b\"\\x04\\x00\\x00\\x00\",\n        }\n        self.HEADER_SIZE = 21\n        self.MAX_CHUNK_SIZE = 1024  # 1 KB\n        self.BUFFER_SIZE = 1024  # 1 KB\n\n        self.incompleted_reconnections = 0\n        self.forced_disconnection = False\n        self._running = asyncio.Event()\n\n        logging.info(\n            f\"Connection [established]: {self.addr} (id: {self.id}) (active: {self.active}) (direct: {self.direct})\"\n        )\n\n    def __str__(self):\n        return f\"Connection to {self.addr} (id: {self.id}) (active: {self.active}) (last active: {self.last_active}) (direct: {self.direct}) (priority: {self._prio.value})\"\n\n    def __repr__(self):\n        return self.__str__()\n\n    @property\n    def cm(self):\n        \"\"\"Communication Manager\"\"\"\n        if not self._cm:\n            from nebula.core.network.communications import CommunicationsManager\n\n            self._cm = CommunicationsManager.get_instance()\n            return self._cm\n        else:\n            return self._cm\n\n    def get_addr(self):\n        return self.addr\n\n    def get_prio(self):\n        \"\"\"Return Connection priority\"\"\"\n        return self._prio\n\n    async def is_inactive(self):\n        \"\"\"\n        Check if the connection is currently marked as inactive.\n\n        Returns:\n            bool: True if inactive, False otherwise.\n        \"\"\"\n        async with self._activity_lock:\n            return self._inactivity\n\n    async def _update_activity(self):\n        \"\"\"\n        Update the activity timestamp to the current time and mark the connection as active.\n        \"\"\"\n        async with self._activity_lock:\n            self._last_activity = time.time()\n            self._inactivity = False\n\n    async def _monitor_inactivity(self):\n        \"\"\"\n        Background task that monitors the connection for inactivity.\n\n        Runs indefinitely until the connection is marked as direct,\n        periodically checking if the last activity exceeds the inactivity threshold.\n        If inactive, marks the connection as inactive and logs a warning.\n        \"\"\"\n        try:\n            while await self.is_running():\n                if self.direct:\n                    break\n                await asyncio.sleep(self.INACTIVITY_DAEMON_SLEEP_TIME)\n                async with self._activity_lock:\n                    time_since_last = time.time() - self._last_activity\n                    if time_since_last &gt; self.INACTIVITY_TIMER:\n                        if not self._inactivity:\n                            self._inactivity = True\n                            logging.info(f\"[{self}] Connection marked as inactive.\")\n                    else:\n                        if self._inactivity:\n                            self._inactivity = False\n        except asyncio.CancelledError:\n            logging.info(\"_monitor_inactivity cancelled during shutdown.\")\n            return\n\n    def get_federated_round(self):\n        return self.federated_round\n\n    def get_tunnel_status(self):\n        return not (self.reader is None or self.writer is None)\n\n    def update_round(self, federated_round):\n        self.federated_round = federated_round\n\n    def get_ready(self):\n        return self.federated_round != Connection.DEFAULT_FEDERATED_ROUND\n\n    def get_direct(self):\n        \"\"\"\n        Check if the connection is marked as direct ( a.k.a neighbor ).\n\n        Returns:\n            bool: True if direct, False otherwise.\n        \"\"\"\n        return self.direct\n\n    def set_direct(self, direct):\n        # config.participant[\"network_args\"][\"neighbors\"] only contains direct neighbors (frotend purposes)\n        if direct:\n            self.config.add_neighbor_from_config(self.addr)\n        else:\n            self.config.remove_neighbor_from_config(self.addr)\n        self.last_active = time.time()\n        self.direct = direct\n\n    def set_active(self, active):\n        self.active = active\n        self.last_active = time.time()\n\n    def is_active(self):\n        return self.active\n\n    def get_last_active(self):\n        return self.last_active\n\n    async def is_running(self):\n        return self._running.is_set()\n\n    async def start(self):\n        \"\"\"\n        Start the connection by launching asynchronous tasks for handling incoming messages,\n        processing the message queue, and monitoring connection inactivity.\n\n        This method creates three asyncio tasks:\n        1. `handle_incoming_message` - reads and handles incoming data from the connection.\n        2. `process_message_queue` - processes messages queued for sending or further handling.\n        3. `_monitor_inactivity` - periodically checks if the connection has been inactive and updates its state accordingly.\n        \"\"\"\n        self._running.set()\n        self.read_task = asyncio.create_task(self.handle_incoming_message(), name=f\"Connection {self.addr} reader\")\n        self.process_task = asyncio.create_task(self.process_message_queue(), name=f\"Connection {self.addr} processor\")\n        self.inactivity_task = asyncio.create_task(self._monitor_inactivity())\n\n    async def stop(self):\n        \"\"\"\n        Stop the connection by cancelling all active asyncio tasks related to this connection\n        and closing the underlying writer stream.\n\n        This method performs the following steps:\n        - Sets a flag indicating the disconnection was forced.\n        - Cancels the read and process tasks if they exist, awaiting their cancellation and logging any cancellation exceptions.\n        - Closes the writer stream safely, awaiting its closure and logging any errors that occur during the closing process.\n        \"\"\"\n        self._running.clear()\n        logging.info(f\"\u2757\ufe0f  Connection [stopped]: {self.addr} (id: {self.id})\")\n        self.forced_disconnection = True\n        tasks = [self.read_task, self.process_task, self.inactivity_task]\n        for task in tasks:\n            if task is not None:\n                task.cancel()\n                try:\n                    await task\n                except asyncio.CancelledError:\n                    logging.exception(f\"\u2757\ufe0f  {self} cancelled...\")\n\n        if self.writer is not None:\n            try:\n                self.writer.close()\n                await self.writer.wait_closed()\n            except Exception as e:\n                logging.exception(f\"\u2757\ufe0f  Error ocurred when closing pipe: {e}\")\n\n    async def reconnect(self, max_retries: int = 5, delay: int = 5) -&gt; None:\n        \"\"\"\n        Attempt to reconnect to the remote address with a maximum number of retries and delay between attempts.\n\n        The method performs the following logic:\n        - Returns immediately if the disconnection was forced or the connection is not direct.\n        - Increments the count of incomplete reconnections and if the maximum allowed is reached, logs failure,\n        marks the disconnection as forced, and terminates the failed reconnection via the connection manager.\n        - Tries to reconnect up to `max_retries` times:\n            - On each attempt, it tries to establish a connection via the connection manager.\n            - Upon success, recreates the read and process asyncio tasks for this connection.\n            - Logs the successful reconnection if not forced to disconnect, then returns.\n        - If all retries fail, logs the failure and terminates the failed reconnection via the Communication manager.\n\n        Args:\n            max_retries (int): Maximum number of reconnection attempts. Defaults to 5.\n            delay (int): Delay in seconds between reconnection attempts. Defaults to 5.\n        \"\"\"\n        if self.forced_disconnection or not self.direct:\n            logging.info(f\"Not going to reconnect because: (forced: {self.forced_disconnection}, direct: {self.direct})\")\n            return\n\n        # Check if learning cycle has finished - don't reconnect\n        if await self.cm.learning_finished():\n            logging.info(f\"Not attempting reconnection to {self.addr} because learning cycle has finished\")\n            return\n\n        self.incompleted_reconnections += 1\n        if self.incompleted_reconnections == MAX_INCOMPLETED_RECONNECTIONS:\n            logging.info(f\"Reconnection with {self.addr} failed...\")\n            self.forced_disconnection = True\n            await self.cm.terminate_failed_reconnection(self)\n            return\n\n        for attempt in range(max_retries):\n            try:\n                logging.info(f\"Attempting to reconnect to {self.addr} (attempt {attempt + 1}/{max_retries})\")\n                await self.cm.connect(self.addr)\n                await asyncio.sleep(1)\n\n                self.read_task = asyncio.create_task(\n                    self.handle_incoming_message(),\n                    name=f\"Connection {self.addr} reader\",\n                )\n                self.process_task = asyncio.create_task(\n                    self.process_message_queue(),\n                    name=f\"Connection {self.addr} processor\",\n                )\n                if not self.forced_disconnection:\n                    logging.info(f\"Reconnected to {self.addr}\")\n                return\n            except Exception as e:\n                logging.exception(f\"Reconnection attempt {attempt + 1} failed: {e}\")\n                await asyncio.sleep(delay)\n        logging.error(f\"Failed to reconnect to {self.addr} after {max_retries} attempts. Stopping connection...\")\n        await self.cm.terminate_failed_reconnection(self)\n\n    async def send(\n        self,\n        data: Any,\n        pb: bool = True,\n        encoding_type: str = \"utf-8\",\n        is_compressed: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Sends data over the active connection.\n\n        This method handles:\n        - Preparing the data for transmission, including optional protobuf serialization, encoding, and compression.\n        - Appending a message ID and sending the data in chunks over the writer stream.\n        - Updating the activity timestamp before sending.\n        - Attempting reconnection in case of failure if the connection is direct.\n\n        Args:\n            data (Any): The data to be sent.\n            pb (bool): If True, data is serialized using Protobuf; otherwise, it is encoded as plain text. Defaults to True.\n            encoding_type (str): The character encoding used if pb is False. Defaults to \"utf-8\".\n            is_compressed (bool): If True, the encoded data will be compressed before sending. Defaults to False.\n        \"\"\"\n        if self.writer is None:\n            logging.error(\"Cannot send data, writer is None\")\n            return\n\n        # Check if learning cycle has finished - don't send messages\n        if await self.cm.learning_finished():\n            logging.info(f\"Not sending message to {self.addr} because learning cycle has finished\")\n            return\n\n        try:\n            message_id = uuid.uuid4().bytes\n            data_prefix, encoded_data = self._prepare_data(data, pb, encoding_type)\n\n            if is_compressed:\n                encoded_data = await asyncio.to_thread(self._compress, encoded_data, self.compression)\n                if encoded_data is None:\n                    return\n                data_to_send = data_prefix + encoded_data + self.COMPRESSION_CHAR\n            else:\n                data_to_send = data_prefix + encoded_data\n\n            await self._update_activity()\n            await self._send_chunks(message_id, data_to_send)\n        except Exception as e:\n            logging.exception(f\"Error sending data: {e}\")\n            if self.direct and not await self.cm.learning_finished():\n                await self.reconnect()\n            elif await self.cm.learning_finished():\n                logging.info(f\"Not attempting reconnection to {self.addr} because learning cycle has finished\")\n\n    def _prepare_data(self, data: Any, pb: bool, encoding_type: str) -&gt; tuple[bytes, bytes]:\n        \"\"\"\n        Prepares the data for transmission by determining its format and encoding it accordingly.\n\n        Args:\n            data (Any): The data to be sent (can be a string, dict, bytes, or serialized protobuf).\n            pb (bool): Whether the data is a pre-serialized protobuf. If True, no further encoding is performed.\n            encoding_type (str): Encoding to use for string or JSON data.\n\n        Returns:\n            tuple[bytes, bytes]: A tuple containing the prefix indicating the data type and the encoded data.\n\n        Raises:\n            ValueError: If the data type is unsupported.\n        \"\"\"\n        if pb:\n            return self.DATA_TYPE_PREFIXES[\"pb\"], data\n        elif isinstance(data, str):\n            return self.DATA_TYPE_PREFIXES[\"string\"], data.encode(encoding_type)\n        elif isinstance(data, dict):\n            return self.DATA_TYPE_PREFIXES[\"json\"], json.dumps(data).encode(encoding_type)\n        elif isinstance(data, bytes):\n            return self.DATA_TYPE_PREFIXES[\"bytes\"], data\n        else:\n            raise ValueError(f\"Unknown data type to send: {type(data)}\")\n\n    def _compress(self, data: bytes, compression: str) -&gt; bytes | None:\n        \"\"\"\n        Compresses the given byte data using the specified compression algorithm.\n\n        Args:\n            data (bytes): The raw data to compress.\n            compression (str): The compression method to use (\"lz4\", \"zlib\", \"bz2\", or \"lzma\").\n\n        Returns:\n            bytes | None: The compressed data, or None if the compression method is unsupported.\n        \"\"\"\n        if compression == \"lz4\":\n            return lz4.frame.compress(data)\n        elif compression == \"zlib\":\n            return zlib.compress(data)\n        elif compression == \"bz2\":\n            return bz2.compress(data)\n        elif compression == \"lzma\":\n            return lzma.compress(data)\n        else:\n            logging.error(f\"Unsupported compression method: {compression}\")\n            return None\n\n    async def _send_chunks(self, message_id: bytes, data: bytes) -&gt; None:\n        \"\"\"\n        Sends the encoded data over the connection in fixed-size chunks.\n\n        Each chunk is prefixed with a header containing the message ID, chunk index,\n        a flag indicating if it's the last chunk, and the size of the chunk.\n        An end-of-transmission (EOT) character is appended to each chunk.\n\n        Args:\n            message_id (bytes): Unique identifier for the message being sent.\n            data (bytes): The complete data payload to be split into chunks and transmitted.\n        \"\"\"\n        chunk_size = self._calculate_chunk_size(len(data))\n        num_chunks = (len(data) + chunk_size - 1) // chunk_size\n\n        for chunk_index in range(num_chunks):\n            start = chunk_index * chunk_size\n            end = min(start + chunk_size, len(data))\n            chunk = data[start:end]\n            is_last_chunk = chunk_index == num_chunks - 1\n\n            header = message_id + chunk_index.to_bytes(4, \"big\") + (b\"\\x01\" if is_last_chunk else b\"\\x00\")\n            chunk_size_bytes = len(chunk).to_bytes(4, \"big\")\n            chunk_with_header = header + chunk_size_bytes + chunk + self.EOT_CHAR\n\n            self.writer.write(chunk_with_header)\n            await self.writer.drain()\n\n            # logging.debug(f\"Sent message {message_id.hex()} | chunk {chunk_index+1}/{num_chunks} | size: {len(chunk)} bytes\")\n\n    def _calculate_chunk_size(self, data_size: int) -&gt; int:\n        return self.BUFFER_SIZE\n\n    async def handle_incoming_message(self) -&gt; None:\n        \"\"\"\n        Asynchronously handles incoming data chunks from the connection.\n\n        This method continuously reads incoming message headers and chunks,\n        stores the chunks until a complete message is assembled, and then\n        queues it for processing. It also updates the activity timestamp to\n        prevent false inactivity flags and resets reconnection counters.\n\n        If the message is complete (`is_last_chunk` is True), the full message\n        is processed. On errors, reconnection is attempted if appropriate.\n\n        Exceptions:\n            asyncio.CancelledError: Raised when the task is cancelled externally.\n            ConnectionError: Raised when the connection is unexpectedly closed.\n            BrokenPipeError: Raised when attempting to read from a broken connection.\n        \"\"\"\n        reusable_buffer = bytearray(self.MAX_CHUNK_SIZE)\n        try:\n            while await self.is_running():\n                if self.pending_messages_queue.full():\n                    await asyncio.sleep(0.1)\n                    continue\n                header = await self._read_exactly(self.HEADER_SIZE)\n                message_id, chunk_index, is_last_chunk = self._parse_header(header)\n                chunk_data = await self._read_chunk(reusable_buffer)\n                await self._update_activity()\n                self._store_chunk(message_id, chunk_index, chunk_data, is_last_chunk)\n                self.incompleted_reconnections = 0\n                if is_last_chunk:\n                    await self._process_complete_message(message_id)\n        except asyncio.CancelledError:\n            logging.info(\"handle_incoming_message cancelled during shutdown.\")\n            return\n        except ConnectionError as e:\n            logging.exception(f\"Connection closed while reading: {e}\")\n        except Exception as e:\n            logging.exception(f\"Error handling incoming message: {e}\")\n        finally:\n            if self.direct or self._prio == ConnectionPriority.HIGH: #and not await self.cm.learning_finished():\n                logging.info(\"ERROR: handling incoming message. Trying to reconnect..\")\n                await self.reconnect()\n            elif await self.cm.learning_finished():\n                logging.info(f\"Not attempting reconnection to {self.addr} because learning cycle has finished\")\n\n    async def _read_exactly(self, num_bytes: int, max_retries: int = 3) -&gt; bytes:\n        \"\"\"\n        Reads an exact number of bytes from the connection stream.\n\n        This method attempts to read exactly `num_bytes` bytes from the reader.\n        If the stream is closed or an error occurs, it retries up to `max_retries` times.\n\n        Args:\n            num_bytes (int): Number of bytes to read.\n            max_retries (int): Number of times to retry on failure (default is 3).\n\n        Returns:\n            bytes: The exact number of bytes read from the stream.\n\n        Raises:\n            ConnectionError: If the connection is closed before reading completes.\n            asyncio.IncompleteReadError: If the stream ends before enough bytes are read.\n            RuntimeError: If the maximum number of retries is exceeded.\n        \"\"\"\n        data = b\"\"\n        remaining = num_bytes\n        for _ in range(max_retries):\n            try:\n                while remaining &gt; 0:\n                    chunk = await self.reader.read(min(remaining, self.BUFFER_SIZE))\n                    if not chunk:\n                        raise ConnectionError(\"Connection closed while reading\")\n                    data += chunk\n                    remaining -= len(chunk)\n                return data\n            except asyncio.IncompleteReadError as e:\n                if _ == max_retries - 1:\n                    raise\n                logging.warning(f\"Retrying read after IncompleteReadError: {e}\")\n            except BrokenPipeError as e:\n                if not self.forced_disconnection:\n                    logging.exception(f\"Broken PIPE while reading: {e}\")\n        raise RuntimeError(\"Max retries reached in _read_exactly\")\n\n    def _parse_header(self, header: bytes) -&gt; tuple[bytes, int, bool]:\n        \"\"\"\n        Parses the message header to extract metadata.\n\n        Args:\n            header (bytes): The header bytes (expected length: 21 bytes).\n\n        Returns:\n            tuple:\n                - message_id (bytes): A 16-byte unique identifier for the message.\n                - chunk_index (int): The index of the current chunk.\n                - is_last_chunk (bool): True if this is the final chunk of the message.\n        \"\"\"\n        message_id = header[:16]\n        chunk_index = int.from_bytes(header[16:20], \"big\")\n        is_last_chunk = header[20] == 1\n        return message_id, chunk_index, is_last_chunk\n\n    async def _read_chunk(self, buffer: bytearray = None) -&gt; bytes:\n        \"\"\"\n        Reads a data chunk from the stream, validating its size and EOT marker.\n\n        Args:\n            buffer (bytearray, optional): A reusable buffer to store the chunk.\n                If not provided, a new buffer of MAX_CHUNK_SIZE will be created.\n\n        Returns:\n            bytes: The read chunk data (sliced from the buffer).\n\n        Raises:\n            ValueError: If the chunk size exceeds MAX_CHUNK_SIZE or if the EOT marker is invalid.\n            ConnectionError: If the connection is closed unexpectedly.\n        \"\"\"\n        if buffer is None:\n            buffer = bytearray(self.MAX_CHUNK_SIZE)\n\n        chunk_size_bytes = await self._read_exactly(4)\n        chunk_size = int.from_bytes(chunk_size_bytes, \"big\")\n\n        if chunk_size &gt; self.MAX_CHUNK_SIZE:\n            raise ValueError(f\"Chunk size {chunk_size} exceeds MAX_CHUNK_SIZE {self.MAX_CHUNK_SIZE}\")\n\n        chunk = await self._read_exactly(chunk_size)\n        buffer[:chunk_size] = chunk\n        eot = await self._read_exactly(len(self.EOT_CHAR))\n\n        if eot != self.EOT_CHAR:\n            raise ValueError(\"Invalid EOT character\")\n\n        return memoryview(buffer)[:chunk_size]\n\n    def _store_chunk(self, message_id: bytes, chunk_index: int, buffer: memoryview, is_last: bool) -&gt; None:\n        \"\"\"\n        Stores a received chunk in the internal message buffer for later assembly.\n\n        Args:\n            message_id (bytes): Unique identifier for the message.\n            chunk_index (int): Index of the current chunk in the message.\n            buffer (memoryview): The actual chunk data.\n            is_last (bool): Whether this chunk is the final part of the message.\n\n        Raises:\n            Exception: Logs and removes the message buffer if an error occurs while storing.\n        \"\"\"\n        if message_id not in self.message_buffers:\n            self.message_buffers[message_id] = {}\n        try:\n            self.message_buffers[message_id][chunk_index] = MessageChunk(chunk_index, buffer.tobytes(), is_last)\n            # logging.debug(f\"Stored chunk {chunk_index} of message {message_id.hex()} | size: {len(data)} bytes\")\n        except Exception as e:\n            if message_id in self.message_buffers:\n                del self.message_buffers[message_id]\n            logging.exception(f\"Error storing chunk {chunk_index} for message {message_id.hex()}: {e}\")\n\n    async def _process_complete_message(self, message_id: bytes) -&gt; None:\n        \"\"\"\n        Reconstructs and processes a complete message from its stored chunks.\n\n        Args:\n            message_id (bytes): Unique identifier of the message.\n\n        Behavior:\n            - Sorts and joins the chunks into a full message.\n            - Extracts the data type prefix and message content.\n            - Decompresses the message if necessary.\n            - Enqueues the message for further processing.\n        \"\"\"\n        chunks = sorted(self.message_buffers[message_id].values(), key=lambda x: x.index)\n        complete_message = b\"\".join(chunk.data for chunk in chunks)\n        del self.message_buffers[message_id]\n\n        data_type_prefix = complete_message[:4]\n        message_content = complete_message[4:]\n\n        if message_content.endswith(self.COMPRESSION_CHAR):\n            message_content = await asyncio.to_thread(\n                self._decompress,\n                message_content[: -len(self.COMPRESSION_CHAR)],\n                self.compression,\n            )\n            if message_content is None:\n                return\n\n        await self.pending_messages_queue.put((data_type_prefix, memoryview(message_content)))\n        # logging.debug(f\"Processed complete message {message_id.hex()} | total size: {len(complete_message)} bytes\")\n\n    def _decompress(self, data: bytes, compression: str) -&gt; bytes | None:\n        \"\"\"\n        Decompresses a byte stream using the specified compression algorithm.\n\n        Args:\n            data (bytes): The compressed data.\n            compression (str): The compression method (\"zlib\", \"bz2\", \"lzma\", \"lz4\").\n\n        Returns:\n            bytes | None: The decompressed data, or None if the method is unsupported or fails.\n        \"\"\"\n        if compression == \"zlib\":\n            return zlib.decompress(data)\n        elif compression == \"bz2\":\n            return bz2.decompress(data)\n        elif compression == \"lzma\":\n            return lzma.decompress(data)\n        elif compression == \"lz4\":\n            return lz4.frame.decompress(data)\n        else:\n            logging.error(f\"Unsupported compression method: {compression}\")\n            return None\n\n    async def process_message_queue(self) -&gt; None:\n        \"\"\"\n        Continuously processes messages from the pending queue.\n        \"\"\"\n        try:\n            while await self.is_running():\n                try:\n                    if self.pending_messages_queue is None:\n                        logging.error(\"Pending messages queue is not initialized\")\n                        return\n                    data_type_prefix, message = await self.pending_messages_queue.get()\n                    await self._handle_message(data_type_prefix, message)\n                    self.pending_messages_queue.task_done()\n                except Exception as e:\n                    logging.exception(f\"Error processing message queue: {e}\")\n                finally:\n                    await asyncio.sleep(0)\n        except asyncio.CancelledError:\n            logging.info(\"process_message_queue cancelled during shutdown.\")\n            return\n\n    async def _handle_message(self, data_type_prefix: bytes, message: bytes) -&gt; None:\n        \"\"\"\n        Dispatches a message to its corresponding handler based on the type prefix.\n\n        Args:\n            data_type_prefix (bytes): Indicates the format/type of the message.\n            message (bytes): The content of the message.\n\n        Behavior:\n            - Routes protobuf messages to the connection manager.\n            - Logs string, JSON, or raw byte messages.\n            - Logs an error for unknown message types.\n        \"\"\"\n        if data_type_prefix == self.DATA_TYPE_PREFIXES[\"pb\"]:\n            # logging.debug(\"Received a protobuf message\")\n            asyncio.create_task(\n                self.cm.handle_incoming_message(message, self.addr),\n                name=f\"Connection {self.addr} message handler\",\n            )\n        elif data_type_prefix == self.DATA_TYPE_PREFIXES[\"string\"]:\n            logging.debug(f\"Received string message: {message.decode('utf-8')}\")\n        elif data_type_prefix == self.DATA_TYPE_PREFIXES[\"json\"]:\n            logging.debug(f\"Received JSON message: {json.loads(message.decode('utf-8'))}\")\n        elif data_type_prefix == self.DATA_TYPE_PREFIXES[\"bytes\"]:\n            logging.debug(f\"Received bytes message of length: {len(message)}\")\n        else:\n            logging.error(f\"Unknown data type prefix: {data_type_prefix}\")\n</code></pre>"},{"location":"api/core/network/connection/#nebula.core.network.connection.Connection.cm","title":"<code>cm</code>  <code>property</code>","text":"<p>Communication Manager</p>"},{"location":"api/core/network/connection/#nebula.core.network.connection.Connection.get_direct","title":"<code>get_direct()</code>","text":"<p>Check if the connection is marked as direct ( a.k.a neighbor ).</p> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if direct, False otherwise.</p> Source code in <code>nebula/core/network/connection.py</code> <pre><code>def get_direct(self):\n    \"\"\"\n    Check if the connection is marked as direct ( a.k.a neighbor ).\n\n    Returns:\n        bool: True if direct, False otherwise.\n    \"\"\"\n    return self.direct\n</code></pre>"},{"location":"api/core/network/connection/#nebula.core.network.connection.Connection.get_prio","title":"<code>get_prio()</code>","text":"<p>Return Connection priority</p> Source code in <code>nebula/core/network/connection.py</code> <pre><code>def get_prio(self):\n    \"\"\"Return Connection priority\"\"\"\n    return self._prio\n</code></pre>"},{"location":"api/core/network/connection/#nebula.core.network.connection.Connection.handle_incoming_message","title":"<code>handle_incoming_message()</code>  <code>async</code>","text":"<p>Asynchronously handles incoming data chunks from the connection.</p> <p>This method continuously reads incoming message headers and chunks, stores the chunks until a complete message is assembled, and then queues it for processing. It also updates the activity timestamp to prevent false inactivity flags and resets reconnection counters.</p> <p>If the message is complete (<code>is_last_chunk</code> is True), the full message is processed. On errors, reconnection is attempted if appropriate.</p> <p>Raises:</p> Type Description <code>CancelledError</code> <p>Raised when the task is cancelled externally.</p> <code>ConnectionError</code> <p>Raised when the connection is unexpectedly closed.</p> <code>BrokenPipeError</code> <p>Raised when attempting to read from a broken connection.</p> Source code in <code>nebula/core/network/connection.py</code> <pre><code>async def handle_incoming_message(self) -&gt; None:\n    \"\"\"\n    Asynchronously handles incoming data chunks from the connection.\n\n    This method continuously reads incoming message headers and chunks,\n    stores the chunks until a complete message is assembled, and then\n    queues it for processing. It also updates the activity timestamp to\n    prevent false inactivity flags and resets reconnection counters.\n\n    If the message is complete (`is_last_chunk` is True), the full message\n    is processed. On errors, reconnection is attempted if appropriate.\n\n    Exceptions:\n        asyncio.CancelledError: Raised when the task is cancelled externally.\n        ConnectionError: Raised when the connection is unexpectedly closed.\n        BrokenPipeError: Raised when attempting to read from a broken connection.\n    \"\"\"\n    reusable_buffer = bytearray(self.MAX_CHUNK_SIZE)\n    try:\n        while await self.is_running():\n            if self.pending_messages_queue.full():\n                await asyncio.sleep(0.1)\n                continue\n            header = await self._read_exactly(self.HEADER_SIZE)\n            message_id, chunk_index, is_last_chunk = self._parse_header(header)\n            chunk_data = await self._read_chunk(reusable_buffer)\n            await self._update_activity()\n            self._store_chunk(message_id, chunk_index, chunk_data, is_last_chunk)\n            self.incompleted_reconnections = 0\n            if is_last_chunk:\n                await self._process_complete_message(message_id)\n    except asyncio.CancelledError:\n        logging.info(\"handle_incoming_message cancelled during shutdown.\")\n        return\n    except ConnectionError as e:\n        logging.exception(f\"Connection closed while reading: {e}\")\n    except Exception as e:\n        logging.exception(f\"Error handling incoming message: {e}\")\n    finally:\n        if self.direct or self._prio == ConnectionPriority.HIGH: #and not await self.cm.learning_finished():\n            logging.info(\"ERROR: handling incoming message. Trying to reconnect..\")\n            await self.reconnect()\n        elif await self.cm.learning_finished():\n            logging.info(f\"Not attempting reconnection to {self.addr} because learning cycle has finished\")\n</code></pre>"},{"location":"api/core/network/connection/#nebula.core.network.connection.Connection.is_inactive","title":"<code>is_inactive()</code>  <code>async</code>","text":"<p>Check if the connection is currently marked as inactive.</p> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if inactive, False otherwise.</p> Source code in <code>nebula/core/network/connection.py</code> <pre><code>async def is_inactive(self):\n    \"\"\"\n    Check if the connection is currently marked as inactive.\n\n    Returns:\n        bool: True if inactive, False otherwise.\n    \"\"\"\n    async with self._activity_lock:\n        return self._inactivity\n</code></pre>"},{"location":"api/core/network/connection/#nebula.core.network.connection.Connection.process_message_queue","title":"<code>process_message_queue()</code>  <code>async</code>","text":"<p>Continuously processes messages from the pending queue.</p> Source code in <code>nebula/core/network/connection.py</code> <pre><code>async def process_message_queue(self) -&gt; None:\n    \"\"\"\n    Continuously processes messages from the pending queue.\n    \"\"\"\n    try:\n        while await self.is_running():\n            try:\n                if self.pending_messages_queue is None:\n                    logging.error(\"Pending messages queue is not initialized\")\n                    return\n                data_type_prefix, message = await self.pending_messages_queue.get()\n                await self._handle_message(data_type_prefix, message)\n                self.pending_messages_queue.task_done()\n            except Exception as e:\n                logging.exception(f\"Error processing message queue: {e}\")\n            finally:\n                await asyncio.sleep(0)\n    except asyncio.CancelledError:\n        logging.info(\"process_message_queue cancelled during shutdown.\")\n        return\n</code></pre>"},{"location":"api/core/network/connection/#nebula.core.network.connection.Connection.reconnect","title":"<code>reconnect(max_retries=5, delay=5)</code>  <code>async</code>","text":"<p>Attempt to reconnect to the remote address with a maximum number of retries and delay between attempts.</p> <p>The method performs the following logic: - Returns immediately if the disconnection was forced or the connection is not direct. - Increments the count of incomplete reconnections and if the maximum allowed is reached, logs failure, marks the disconnection as forced, and terminates the failed reconnection via the connection manager. - Tries to reconnect up to <code>max_retries</code> times:     - On each attempt, it tries to establish a connection via the connection manager.     - Upon success, recreates the read and process asyncio tasks for this connection.     - Logs the successful reconnection if not forced to disconnect, then returns. - If all retries fail, logs the failure and terminates the failed reconnection via the Communication manager.</p> <p>Parameters:</p> Name Type Description Default <code>max_retries</code> <code>int</code> <p>Maximum number of reconnection attempts. Defaults to 5.</p> <code>5</code> <code>delay</code> <code>int</code> <p>Delay in seconds between reconnection attempts. Defaults to 5.</p> <code>5</code> Source code in <code>nebula/core/network/connection.py</code> <pre><code>async def reconnect(self, max_retries: int = 5, delay: int = 5) -&gt; None:\n    \"\"\"\n    Attempt to reconnect to the remote address with a maximum number of retries and delay between attempts.\n\n    The method performs the following logic:\n    - Returns immediately if the disconnection was forced or the connection is not direct.\n    - Increments the count of incomplete reconnections and if the maximum allowed is reached, logs failure,\n    marks the disconnection as forced, and terminates the failed reconnection via the connection manager.\n    - Tries to reconnect up to `max_retries` times:\n        - On each attempt, it tries to establish a connection via the connection manager.\n        - Upon success, recreates the read and process asyncio tasks for this connection.\n        - Logs the successful reconnection if not forced to disconnect, then returns.\n    - If all retries fail, logs the failure and terminates the failed reconnection via the Communication manager.\n\n    Args:\n        max_retries (int): Maximum number of reconnection attempts. Defaults to 5.\n        delay (int): Delay in seconds between reconnection attempts. Defaults to 5.\n    \"\"\"\n    if self.forced_disconnection or not self.direct:\n        logging.info(f\"Not going to reconnect because: (forced: {self.forced_disconnection}, direct: {self.direct})\")\n        return\n\n    # Check if learning cycle has finished - don't reconnect\n    if await self.cm.learning_finished():\n        logging.info(f\"Not attempting reconnection to {self.addr} because learning cycle has finished\")\n        return\n\n    self.incompleted_reconnections += 1\n    if self.incompleted_reconnections == MAX_INCOMPLETED_RECONNECTIONS:\n        logging.info(f\"Reconnection with {self.addr} failed...\")\n        self.forced_disconnection = True\n        await self.cm.terminate_failed_reconnection(self)\n        return\n\n    for attempt in range(max_retries):\n        try:\n            logging.info(f\"Attempting to reconnect to {self.addr} (attempt {attempt + 1}/{max_retries})\")\n            await self.cm.connect(self.addr)\n            await asyncio.sleep(1)\n\n            self.read_task = asyncio.create_task(\n                self.handle_incoming_message(),\n                name=f\"Connection {self.addr} reader\",\n            )\n            self.process_task = asyncio.create_task(\n                self.process_message_queue(),\n                name=f\"Connection {self.addr} processor\",\n            )\n            if not self.forced_disconnection:\n                logging.info(f\"Reconnected to {self.addr}\")\n            return\n        except Exception as e:\n            logging.exception(f\"Reconnection attempt {attempt + 1} failed: {e}\")\n            await asyncio.sleep(delay)\n    logging.error(f\"Failed to reconnect to {self.addr} after {max_retries} attempts. Stopping connection...\")\n    await self.cm.terminate_failed_reconnection(self)\n</code></pre>"},{"location":"api/core/network/connection/#nebula.core.network.connection.Connection.send","title":"<code>send(data, pb=True, encoding_type='utf-8', is_compressed=False)</code>  <code>async</code>","text":"<p>Sends data over the active connection.</p> <p>This method handles: - Preparing the data for transmission, including optional protobuf serialization, encoding, and compression. - Appending a message ID and sending the data in chunks over the writer stream. - Updating the activity timestamp before sending. - Attempting reconnection in case of failure if the connection is direct.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The data to be sent.</p> required <code>pb</code> <code>bool</code> <p>If True, data is serialized using Protobuf; otherwise, it is encoded as plain text. Defaults to True.</p> <code>True</code> <code>encoding_type</code> <code>str</code> <p>The character encoding used if pb is False. Defaults to \"utf-8\".</p> <code>'utf-8'</code> <code>is_compressed</code> <code>bool</code> <p>If True, the encoded data will be compressed before sending. Defaults to False.</p> <code>False</code> Source code in <code>nebula/core/network/connection.py</code> <pre><code>async def send(\n    self,\n    data: Any,\n    pb: bool = True,\n    encoding_type: str = \"utf-8\",\n    is_compressed: bool = False,\n) -&gt; None:\n    \"\"\"\n    Sends data over the active connection.\n\n    This method handles:\n    - Preparing the data for transmission, including optional protobuf serialization, encoding, and compression.\n    - Appending a message ID and sending the data in chunks over the writer stream.\n    - Updating the activity timestamp before sending.\n    - Attempting reconnection in case of failure if the connection is direct.\n\n    Args:\n        data (Any): The data to be sent.\n        pb (bool): If True, data is serialized using Protobuf; otherwise, it is encoded as plain text. Defaults to True.\n        encoding_type (str): The character encoding used if pb is False. Defaults to \"utf-8\".\n        is_compressed (bool): If True, the encoded data will be compressed before sending. Defaults to False.\n    \"\"\"\n    if self.writer is None:\n        logging.error(\"Cannot send data, writer is None\")\n        return\n\n    # Check if learning cycle has finished - don't send messages\n    if await self.cm.learning_finished():\n        logging.info(f\"Not sending message to {self.addr} because learning cycle has finished\")\n        return\n\n    try:\n        message_id = uuid.uuid4().bytes\n        data_prefix, encoded_data = self._prepare_data(data, pb, encoding_type)\n\n        if is_compressed:\n            encoded_data = await asyncio.to_thread(self._compress, encoded_data, self.compression)\n            if encoded_data is None:\n                return\n            data_to_send = data_prefix + encoded_data + self.COMPRESSION_CHAR\n        else:\n            data_to_send = data_prefix + encoded_data\n\n        await self._update_activity()\n        await self._send_chunks(message_id, data_to_send)\n    except Exception as e:\n        logging.exception(f\"Error sending data: {e}\")\n        if self.direct and not await self.cm.learning_finished():\n            await self.reconnect()\n        elif await self.cm.learning_finished():\n            logging.info(f\"Not attempting reconnection to {self.addr} because learning cycle has finished\")\n</code></pre>"},{"location":"api/core/network/connection/#nebula.core.network.connection.Connection.start","title":"<code>start()</code>  <code>async</code>","text":"<p>Start the connection by launching asynchronous tasks for handling incoming messages, processing the message queue, and monitoring connection inactivity.</p> <p>This method creates three asyncio tasks: 1. <code>handle_incoming_message</code> - reads and handles incoming data from the connection. 2. <code>process_message_queue</code> - processes messages queued for sending or further handling. 3. <code>_monitor_inactivity</code> - periodically checks if the connection has been inactive and updates its state accordingly.</p> Source code in <code>nebula/core/network/connection.py</code> <pre><code>async def start(self):\n    \"\"\"\n    Start the connection by launching asynchronous tasks for handling incoming messages,\n    processing the message queue, and monitoring connection inactivity.\n\n    This method creates three asyncio tasks:\n    1. `handle_incoming_message` - reads and handles incoming data from the connection.\n    2. `process_message_queue` - processes messages queued for sending or further handling.\n    3. `_monitor_inactivity` - periodically checks if the connection has been inactive and updates its state accordingly.\n    \"\"\"\n    self._running.set()\n    self.read_task = asyncio.create_task(self.handle_incoming_message(), name=f\"Connection {self.addr} reader\")\n    self.process_task = asyncio.create_task(self.process_message_queue(), name=f\"Connection {self.addr} processor\")\n    self.inactivity_task = asyncio.create_task(self._monitor_inactivity())\n</code></pre>"},{"location":"api/core/network/connection/#nebula.core.network.connection.Connection.stop","title":"<code>stop()</code>  <code>async</code>","text":"<p>Stop the connection by cancelling all active asyncio tasks related to this connection and closing the underlying writer stream.</p> <p>This method performs the following steps: - Sets a flag indicating the disconnection was forced. - Cancels the read and process tasks if they exist, awaiting their cancellation and logging any cancellation exceptions. - Closes the writer stream safely, awaiting its closure and logging any errors that occur during the closing process.</p> Source code in <code>nebula/core/network/connection.py</code> <pre><code>async def stop(self):\n    \"\"\"\n    Stop the connection by cancelling all active asyncio tasks related to this connection\n    and closing the underlying writer stream.\n\n    This method performs the following steps:\n    - Sets a flag indicating the disconnection was forced.\n    - Cancels the read and process tasks if they exist, awaiting their cancellation and logging any cancellation exceptions.\n    - Closes the writer stream safely, awaiting its closure and logging any errors that occur during the closing process.\n    \"\"\"\n    self._running.clear()\n    logging.info(f\"\u2757\ufe0f  Connection [stopped]: {self.addr} (id: {self.id})\")\n    self.forced_disconnection = True\n    tasks = [self.read_task, self.process_task, self.inactivity_task]\n    for task in tasks:\n        if task is not None:\n            task.cancel()\n            try:\n                await task\n            except asyncio.CancelledError:\n                logging.exception(f\"\u2757\ufe0f  {self} cancelled...\")\n\n    if self.writer is not None:\n        try:\n            self.writer.close()\n            await self.writer.wait_closed()\n        except Exception as e:\n            logging.exception(f\"\u2757\ufe0f  Error ocurred when closing pipe: {e}\")\n</code></pre>"},{"location":"api/core/network/discoverer/","title":"Documentation for Discoverer Module","text":""},{"location":"api/core/network/forwarder/","title":"Documentation for Forwarder Module","text":""},{"location":"api/core/network/forwarder/#nebula.core.network.forwarder.Forwarder","title":"<code>Forwarder</code>","text":"<p>Component responsible for forwarding incoming messages to appropriate peer nodes.</p> The Forwarder handles <ul> <li>Relaying messages received from one node to others in the federation.</li> <li>Applying any forwarding policies (e.g., proxy mode, rate limiting).</li> <li>Ensuring duplicate messages are not resent.</li> <li>Integrating with the CommunicationsManager to obtain current connections.</li> </ul> <p>This class is designed to run asynchronously, leveraging the existing connection pool and message routing logic to propagate messages reliably across the network.</p> Source code in <code>nebula/core/network/forwarder.py</code> <pre><code>class Forwarder:\n    \"\"\"\n    Component responsible for forwarding incoming messages to appropriate peer nodes.\n\n    The Forwarder handles:\n      - Relaying messages received from one node to others in the federation.\n      - Applying any forwarding policies (e.g., proxy mode, rate limiting).\n      - Ensuring duplicate messages are not resent.\n      - Integrating with the CommunicationsManager to obtain current connections.\n\n    This class is designed to run asynchronously, leveraging the existing connection pool\n    and message routing logic to propagate messages reliably across the network.\n    \"\"\"\n\n    def __init__(self, config):\n        \"\"\"\n        Initialize the Forwarder module.\n\n        Args:\n            config (dict): The global configuration, including forwarder parameters:\n                - forwarder_interval: Time between forwarding cycles.\n                - number_forwarded_messages: Max messages to forward per cycle.\n                - forward_messages_interval: Delay between individual message sends.\n        \"\"\"\n        print_msg_box(msg=\"Starting forwarder module...\", indent=2, title=\"Forwarder module\")\n        self.config = config\n        self._cm = None\n        self.pending_messages = asyncio.Queue()\n        self.pending_messages_lock = Locker(\"pending_messages_lock\", verbose=False, async_lock=True)\n        self._forwarder_task = None  # Track the background task\n\n        self.interval = self.config.participant[\"forwarder_args\"][\"forwarder_interval\"]\n        self.number_forwarded_messages = self.config.participant[\"forwarder_args\"][\"number_forwarded_messages\"]\n        self.messages_interval = self.config.participant[\"forwarder_args\"][\"forward_messages_interval\"]\n        self._running = asyncio.Event()\n\n    @property\n    def cm(self):\n        \"\"\"\n        Lazy-load and return the CommunicationsManager instance for sending messages.\n\n        Returns:\n            CommunicationsManager: The singleton communications manager.\n        \"\"\"\n        if not self._cm:\n            from nebula.core.network.communications import CommunicationsManager\n\n            self._cm = CommunicationsManager.get_instance()\n            return self._cm\n        else:\n            return self._cm\n\n    async def start(self):\n        \"\"\"\n        Start the forwarder by scheduling the forwarding loop as a background task.\n        \"\"\"\n        self._running.set()\n        self._forwarder_task = asyncio.create_task(self.run_forwarder(), name=\"Forwarder_run_forwarder\")\n\n    async def run_forwarder(self):\n        \"\"\"\n        Periodically process and dispatch pending messages.\n\n        Runs indefinitely (unless in CFL mode), acquiring a lock to safely\n        dequeue up to `number_forwarded_messages` and send them with appropriate timing.\n        \"\"\"\n        if self.config.participant[\"scenario_args\"][\"federation\"] == \"CFL\":\n            logging.info(\"\ud83d\udd01  Federation is CFL. Forwarder is disabled...\")\n            return\n        try:\n            while await self.is_running():\n                start_time = time.time()\n                await self.pending_messages_lock.acquire_async()\n                await self.process_pending_messages(messages_left=self.number_forwarded_messages)\n                await self.pending_messages_lock.release_async()\n                sleep_time = max(0, self.interval - (time.time() - start_time))\n                await asyncio.sleep(sleep_time)\n        except asyncio.CancelledError:\n            logging.info(\"run_forwarder cancelled during shutdown.\")\n            return\n\n    async def stop(self):\n        self._running.clear()\n        logging.info(\"\ud83d\udd01  Stopping Forwarder module...\")\n\n        # Cancel the background task\n        if self._forwarder_task and not self._forwarder_task.done():\n            logging.info(\"\ud83d\uded1  Cancelling Forwarder background task...\")\n            self._forwarder_task.cancel()\n            try:\n                await self._forwarder_task\n            except asyncio.CancelledError:\n                pass\n            self._forwarder_task = None\n            logging.info(\"\ud83d\uded1  Forwarder background task cancelled\")\n\n    async def is_running(self):\n        return self._running.is_set()\n\n    async def process_pending_messages(self, messages_left):\n        \"\"\"\n        Send up to `messages_left` messages from the pending queue to their target neighbors.\n\n        Args:\n            messages_left (int): The maximum number of messages to forward in this batch.\n        \"\"\"\n        while messages_left &gt; 0 and not self.pending_messages.empty():\n            msg, neighbors = await self.pending_messages.get()\n            for neighbor in neighbors[:messages_left]:\n                if neighbor not in self.cm.connections:\n                    continue\n                try:\n                    logging.debug(f\"\ud83d\udd01  Sending message (forwarding) --&gt; to {neighbor}\")\n                    await self.cm.send_message(neighbor, msg)\n                except Exception as e:\n                    logging.exception(f\"\ud83d\udd01  Error forwarding message to {neighbor}. Error: {e!s}\")\n                    pass\n                await asyncio.sleep(self.messages_interval)\n            messages_left -= len(neighbors)\n            if len(neighbors) &gt; messages_left:\n                logging.debug(\"\ud83d\udd01  Putting message back in queue for forwarding to the remaining neighbors\")\n                await self.pending_messages.put((msg, neighbors[messages_left:]))\n\n    async def forward(self, msg, addr_from):\n        \"\"\"\n        Enqueue a received message for forwarding to all other direct neighbors.\n\n        Excludes the original sender and acquires a lock to safely add to the queue.\n\n        Args:\n            msg (bytes): The serialized message to forward.\n            addr_from (str): The address of the node that originally sent the message.\n        \"\"\"\n        if self.config.participant[\"scenario_args\"][\"federation\"] == \"CFL\":\n            logging.info(\"\ud83d\udd01  Federation is CFL. Forwarder is disabled...\")\n            return\n        try:\n            await self.pending_messages_lock.acquire_async()\n            current_connections = await self.cm.get_addrs_current_connections(only_direct=True)\n            pending_nodes_to_send = [n for n in current_connections if n != addr_from]\n            logging.debug(f\"\ud83d\udd01  Puting message in queue for forwarding to {pending_nodes_to_send}\")\n            await self.pending_messages.put((msg, pending_nodes_to_send))\n        except Exception as e:\n            logging.exception(f\"\ud83d\udd01  Error forwarding message. Error: {e!s}\")\n        finally:\n            await self.pending_messages_lock.release_async()\n</code></pre>"},{"location":"api/core/network/forwarder/#nebula.core.network.forwarder.Forwarder.cm","title":"<code>cm</code>  <code>property</code>","text":"<p>Lazy-load and return the CommunicationsManager instance for sending messages.</p> <p>Returns:</p> Name Type Description <code>CommunicationsManager</code> <p>The singleton communications manager.</p>"},{"location":"api/core/network/forwarder/#nebula.core.network.forwarder.Forwarder.__init__","title":"<code>__init__(config)</code>","text":"<p>Initialize the Forwarder module.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>The global configuration, including forwarder parameters: - forwarder_interval: Time between forwarding cycles. - number_forwarded_messages: Max messages to forward per cycle. - forward_messages_interval: Delay between individual message sends.</p> required Source code in <code>nebula/core/network/forwarder.py</code> <pre><code>def __init__(self, config):\n    \"\"\"\n    Initialize the Forwarder module.\n\n    Args:\n        config (dict): The global configuration, including forwarder parameters:\n            - forwarder_interval: Time between forwarding cycles.\n            - number_forwarded_messages: Max messages to forward per cycle.\n            - forward_messages_interval: Delay between individual message sends.\n    \"\"\"\n    print_msg_box(msg=\"Starting forwarder module...\", indent=2, title=\"Forwarder module\")\n    self.config = config\n    self._cm = None\n    self.pending_messages = asyncio.Queue()\n    self.pending_messages_lock = Locker(\"pending_messages_lock\", verbose=False, async_lock=True)\n    self._forwarder_task = None  # Track the background task\n\n    self.interval = self.config.participant[\"forwarder_args\"][\"forwarder_interval\"]\n    self.number_forwarded_messages = self.config.participant[\"forwarder_args\"][\"number_forwarded_messages\"]\n    self.messages_interval = self.config.participant[\"forwarder_args\"][\"forward_messages_interval\"]\n    self._running = asyncio.Event()\n</code></pre>"},{"location":"api/core/network/forwarder/#nebula.core.network.forwarder.Forwarder.forward","title":"<code>forward(msg, addr_from)</code>  <code>async</code>","text":"<p>Enqueue a received message for forwarding to all other direct neighbors.</p> <p>Excludes the original sender and acquires a lock to safely add to the queue.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>bytes</code> <p>The serialized message to forward.</p> required <code>addr_from</code> <code>str</code> <p>The address of the node that originally sent the message.</p> required Source code in <code>nebula/core/network/forwarder.py</code> <pre><code>async def forward(self, msg, addr_from):\n    \"\"\"\n    Enqueue a received message for forwarding to all other direct neighbors.\n\n    Excludes the original sender and acquires a lock to safely add to the queue.\n\n    Args:\n        msg (bytes): The serialized message to forward.\n        addr_from (str): The address of the node that originally sent the message.\n    \"\"\"\n    if self.config.participant[\"scenario_args\"][\"federation\"] == \"CFL\":\n        logging.info(\"\ud83d\udd01  Federation is CFL. Forwarder is disabled...\")\n        return\n    try:\n        await self.pending_messages_lock.acquire_async()\n        current_connections = await self.cm.get_addrs_current_connections(only_direct=True)\n        pending_nodes_to_send = [n for n in current_connections if n != addr_from]\n        logging.debug(f\"\ud83d\udd01  Puting message in queue for forwarding to {pending_nodes_to_send}\")\n        await self.pending_messages.put((msg, pending_nodes_to_send))\n    except Exception as e:\n        logging.exception(f\"\ud83d\udd01  Error forwarding message. Error: {e!s}\")\n    finally:\n        await self.pending_messages_lock.release_async()\n</code></pre>"},{"location":"api/core/network/forwarder/#nebula.core.network.forwarder.Forwarder.process_pending_messages","title":"<code>process_pending_messages(messages_left)</code>  <code>async</code>","text":"<p>Send up to <code>messages_left</code> messages from the pending queue to their target neighbors.</p> <p>Parameters:</p> Name Type Description Default <code>messages_left</code> <code>int</code> <p>The maximum number of messages to forward in this batch.</p> required Source code in <code>nebula/core/network/forwarder.py</code> <pre><code>async def process_pending_messages(self, messages_left):\n    \"\"\"\n    Send up to `messages_left` messages from the pending queue to their target neighbors.\n\n    Args:\n        messages_left (int): The maximum number of messages to forward in this batch.\n    \"\"\"\n    while messages_left &gt; 0 and not self.pending_messages.empty():\n        msg, neighbors = await self.pending_messages.get()\n        for neighbor in neighbors[:messages_left]:\n            if neighbor not in self.cm.connections:\n                continue\n            try:\n                logging.debug(f\"\ud83d\udd01  Sending message (forwarding) --&gt; to {neighbor}\")\n                await self.cm.send_message(neighbor, msg)\n            except Exception as e:\n                logging.exception(f\"\ud83d\udd01  Error forwarding message to {neighbor}. Error: {e!s}\")\n                pass\n            await asyncio.sleep(self.messages_interval)\n        messages_left -= len(neighbors)\n        if len(neighbors) &gt; messages_left:\n            logging.debug(\"\ud83d\udd01  Putting message back in queue for forwarding to the remaining neighbors\")\n            await self.pending_messages.put((msg, neighbors[messages_left:]))\n</code></pre>"},{"location":"api/core/network/forwarder/#nebula.core.network.forwarder.Forwarder.run_forwarder","title":"<code>run_forwarder()</code>  <code>async</code>","text":"<p>Periodically process and dispatch pending messages.</p> <p>Runs indefinitely (unless in CFL mode), acquiring a lock to safely dequeue up to <code>number_forwarded_messages</code> and send them with appropriate timing.</p> Source code in <code>nebula/core/network/forwarder.py</code> <pre><code>async def run_forwarder(self):\n    \"\"\"\n    Periodically process and dispatch pending messages.\n\n    Runs indefinitely (unless in CFL mode), acquiring a lock to safely\n    dequeue up to `number_forwarded_messages` and send them with appropriate timing.\n    \"\"\"\n    if self.config.participant[\"scenario_args\"][\"federation\"] == \"CFL\":\n        logging.info(\"\ud83d\udd01  Federation is CFL. Forwarder is disabled...\")\n        return\n    try:\n        while await self.is_running():\n            start_time = time.time()\n            await self.pending_messages_lock.acquire_async()\n            await self.process_pending_messages(messages_left=self.number_forwarded_messages)\n            await self.pending_messages_lock.release_async()\n            sleep_time = max(0, self.interval - (time.time() - start_time))\n            await asyncio.sleep(sleep_time)\n    except asyncio.CancelledError:\n        logging.info(\"run_forwarder cancelled during shutdown.\")\n        return\n</code></pre>"},{"location":"api/core/network/forwarder/#nebula.core.network.forwarder.Forwarder.start","title":"<code>start()</code>  <code>async</code>","text":"<p>Start the forwarder by scheduling the forwarding loop as a background task.</p> Source code in <code>nebula/core/network/forwarder.py</code> <pre><code>async def start(self):\n    \"\"\"\n    Start the forwarder by scheduling the forwarding loop as a background task.\n    \"\"\"\n    self._running.set()\n    self._forwarder_task = asyncio.create_task(self.run_forwarder(), name=\"Forwarder_run_forwarder\")\n</code></pre>"},{"location":"api/core/network/health/","title":"Documentation for Health Module","text":""},{"location":"api/core/network/messages/","title":"Documentation for Messages Module","text":""},{"location":"api/core/network/messages/#nebula.core.network.messages.MessagesManager","title":"<code>MessagesManager</code>","text":"<p>Manages creation, processing, and whenever is neccesary to do forwarding of Nebula protobuf messages. Handles different message types defined in the protocol and coordinates with the CommunicationsManager.</p> Source code in <code>nebula/core/network/messages.py</code> <pre><code>class MessagesManager:\n    \"\"\"\n    Manages creation, processing, and whenever is neccesary to do forwarding of Nebula protobuf messages.\n    Handles different message types defined in the protocol and coordinates with the CommunicationsManager.\n    \"\"\"\n\n    def __init__(self, addr, config):\n        \"\"\"\n        Initialize MessagesManager with the node address and configuration.\n\n        Args:\n            addr (str): The network address of the current node.\n            config (dict): Configuration dictionary for the node.\n        \"\"\"\n        self.addr = addr\n        self.config = config\n        self._cm = None\n        self._message_templates = {}\n        self._define_message_templates()\n\n    @property\n    def cm(self):\n        \"\"\"\n        Lazy-load and return the singleton instance of CommunicationsManager.\n\n        Returns:\n            CommunicationsManager: The communications manager instance.\n        \"\"\"\n        if not self._cm:\n            from nebula.core.network.communications import CommunicationsManager\n\n            self._cm = CommunicationsManager.get_instance()\n            return self._cm\n        else:\n            return self._cm\n\n    def _define_message_templates(self):\n        \"\"\"\n        Define the message templates mapping message types to their parameters and default values.\n        This is used to dynamically create messages of different types.\n        \"\"\"\n        # Dictionary that maps message types to their required parameters and default values\n        self._message_templates = {\n            \"offer\": {\n                \"parameters\": [\"action\", \"n_neighbors\", \"loss\", \"parameters\", \"rounds\", \"round\", \"epochs\"],\n                \"defaults\": {\n                    \"parameters\": None,\n                    \"rounds\": 1,\n                    \"round\": -1,\n                    \"epochs\": 1,\n                },\n            },\n            \"connection\": {\"parameters\": [\"action\"], \"defaults\": {}},\n            \"discovery\": {\n                \"parameters\": [\"action\", \"latitude\", \"longitude\"],\n                \"defaults\": {\n                    \"latitude\": 0.0,\n                    \"longitude\": 0.0,\n                },\n            },\n            \"control\": {\n                \"parameters\": [\"action\", \"log\"],\n                \"defaults\": {\n                    \"log\": \"Control message\",\n                },\n            },\n            \"federation\": {\n                \"parameters\": [\"action\", \"arguments\", \"round\"],\n                \"defaults\": {\n                    \"arguments\": [],\n                    \"round\": None,\n                },\n            },\n            \"model\": {\n                \"parameters\": [\"round\", \"parameters\", \"weight\"],\n                \"defaults\": {\n                    \"weight\": 1,\n                },\n            },\n            \"reputation\": {\n                \"parameters\": [\"node_id\", \"score\", \"round\", \"action\"],\n                \"defaults\": {\n                    \"round\": None,\n                },\n            },\n            \"discover\": {\"parameters\": [\"action\"], \"defaults\": {}},\n            \"link\": {\"parameters\": [\"action\", \"addrs\"], \"defaults\": {}},\n            # Add additional message types here\n        }\n\n    def get_messages_events(self) -&gt; dict:\n        \"\"\"\n        Retrieve the available message event names and their corresponding actions.\n\n        Returns:\n            dict: Mapping of message names (excluding 'model') to their available action names.\n        \"\"\"\n        message_events = {}\n        for message_name in self._message_templates:\n            if message_name != \"model\":\n                message_events[message_name] = get_actions_names(message_name)\n        return message_events\n\n    async def process_message(self, data, addr_from):\n        \"\"\"\n        Asynchronously process an incoming serialized protobuf message.\n\n        Parses the message, verifies source, forwards or handles the message depending on its type,\n        and prevents duplicate processing using message hashes.\n\n        Args:\n            data (bytes): Serialized protobuf message bytes.\n            addr_from (str): Address from which the message was received.\n        \"\"\"\n        not_processing_messages = {\"control_message\", \"connection_message\"}\n        special_processing_messages = {\"discovery_message\", \"federation_message\", \"model_message\"}\n\n        try:\n            message_wrapper = nebula_pb2.Wrapper()\n            message_wrapper.ParseFromString(data)\n            source = message_wrapper.source\n            logging.debug(f\"\ud83d\udce5  handle_incoming_message | Received message from {addr_from} with source {source}\")\n            if source == self.addr:\n                return\n\n            # Extract the active message from the oneof field\n            message_type = message_wrapper.WhichOneof(\"message\")\n            msg_name = message_type.split(\"_\")[0]\n            if not message_type:\n                logging.warning(\"Received message with no active field in the 'oneof'\")\n                return\n\n            message_data = getattr(message_wrapper, message_type)\n\n            # Not required processing messages\n            if message_type in not_processing_messages:\n                # await self.cm.handle_message(source, message_type, message_data)\n                me = MessageEvent(\n                    (msg_name, get_action_name_from_value(msg_name, message_data.action)), source, message_data\n                )\n                await self.cm.handle_message(me)\n\n            # Message-specific forwarding and processing\n            elif message_type in special_processing_messages:\n                if await self.cm.include_received_message_hash(hashlib.md5(data).hexdigest(), addr_from):\n                    # Forward the message if required\n                    if self._should_forward_message(message_type, message_wrapper):\n                        await self.cm.forward_message(data, addr_from)\n\n                    if message_type == \"model_message\":\n                        await self.cm.handle_model_message(source, message_data)\n                    else:\n                        me = MessageEvent(\n                            (msg_name, get_action_name_from_value(msg_name, message_data.action)), source, message_data\n                        )\n                        await self.cm.handle_message(me)\n            # Rest of messages\n            else:\n                # if await self.cm.include_received_message_hash(hashlib.md5(data).hexdigest()):\n                me = MessageEvent(\n                    (msg_name, get_action_name_from_value(msg_name, message_data.action)), source, message_data\n                )\n                await self.cm.handle_message(me)\n        except Exception as e:\n            logging.exception(f\"\ud83d\udce5  handle_incoming_message | Error while processing: {e}\")\n            logging.exception(traceback.format_exc())\n\n    def _should_forward_message(self, message_type, message_wrapper):\n        \"\"\"\n        Determine if a received message should be forwarded to other nodes.\n\n        Forwarding is enabled for proxy devices or for specific message types\n        like initialization model messages or federation start actions.\n\n        Args:\n            message_type (str): Type of the message, e.g. 'model_message'.\n            message_wrapper (nebula_pb2.Wrapper): Parsed protobuf wrapper message.\n\n        Returns:\n            bool: True if the message should be forwarded, False otherwise.\n        \"\"\"\n        if self.cm.config.participant[\"device_args\"][\"proxy\"]:\n            return True\n        # TODO: Improve the technique. Now only forward model messages if the node is a proxy\n        # Need to update the expected model messages receiving during the round\n        # Round -1 is the initialization round --&gt; all nodes should receive the model\n        if message_type == \"model_message\" and message_wrapper.model_message.round == -1:\n            return True\n        if (\n            message_type == \"federation_message\"\n            and message_wrapper.federation_message.action\n            == nebula_pb2.FederationMessage.Action.Value(\"FEDERATION_START\")\n        ):\n            return True\n\n    def create_message(self, message_type: str, action: str = \"\", *args, **kwargs):\n        \"\"\"\n        Create and serialize a protobuf message of the given type and action.\n\n        Dynamically maps provided arguments to the protobuf message fields using predefined templates.\n        Wraps the message in a Nebula 'Wrapper' message with the node's address as source.\n\n        Args:\n            message_type (str): The type of message to create (e.g. 'offer', 'model', etc.).\n            action (str, optional): Action name for the message, converted to protobuf enum. Defaults to \"\".\n            *args: Positional arguments for message fields according to the template.\n            **kwargs: Keyword arguments for message fields.\n\n        Raises:\n            ValueError: If the message_type is invalid.\n            AttributeError: If the protobuf message class does not exist.\n\n        Returns:\n            bytes: Serialized protobuf 'Wrapper' message bytes ready for transmission.\n        \"\"\"\n        # logging.info(f\"Creating message | type: {message_type}, action: {action}, positionals: {args}, explicits: {kwargs.keys()}\")\n        # If an action is provided, convert it to its corresponding enum value using the factory\n        message_action = None\n        if action:\n            message_action = factory_message_action(message_type, action)\n\n        # Retrieve the template for the provided message type\n        message_template = self._message_templates.get(message_type)\n        if not message_template:\n            raise ValueError(f\"Invalid message type '{message_type}'\")\n\n        # Extract parameters and defaults from the template\n        template_params = message_template[\"parameters\"]\n        default_values: dict = message_template.get(\"defaults\", {})\n\n        # Dynamically retrieve the class for the protobuf message (e.g., OfferMessage)\n        class_name = message_type.capitalize() + \"Message\"\n        message_class = getattr(nebula_pb2, class_name, None)\n\n        if message_class is None:\n            raise AttributeError(f\"Message type {message_type} not found on the protocol\")\n\n        # Set the 'action' parameter if required and if the message_action is available\n        if \"action\" in template_params and message_action is not None:\n            kwargs[\"action\"] = message_action\n\n        # Map positional arguments to template parameters\n        remaining_params = [param_name for param_name in template_params if param_name not in kwargs]\n        if args:\n            for param_name, arg_value in zip(remaining_params, args, strict=False):\n                if param_name in kwargs:\n                    continue\n                kwargs[param_name] = arg_value\n\n        # Fill in missing parameters with their default values\n        # logging.info(f\"kwargs parameters: {kwargs.keys()}\")\n        for param_name in template_params:\n            if param_name not in kwargs:\n                # logging.info(f\"Filling parameter '{param_name}' with default value: {default_values.get(param_name)}\")\n                kwargs[param_name] = default_values.get(param_name)\n\n        # Create an instance of the protobuf message class using the constructed kwargs\n        message = message_class(**kwargs)\n\n        message_wrapper = nebula_pb2.Wrapper()\n        message_wrapper.source = self.addr\n        field_name = f\"{message_type}_message\"\n        getattr(message_wrapper, field_name).CopyFrom(message)\n        data = message_wrapper.SerializeToString()\n        return data\n</code></pre>"},{"location":"api/core/network/messages/#nebula.core.network.messages.MessagesManager.cm","title":"<code>cm</code>  <code>property</code>","text":"<p>Lazy-load and return the singleton instance of CommunicationsManager.</p> <p>Returns:</p> Name Type Description <code>CommunicationsManager</code> <p>The communications manager instance.</p>"},{"location":"api/core/network/messages/#nebula.core.network.messages.MessagesManager.__init__","title":"<code>__init__(addr, config)</code>","text":"<p>Initialize MessagesManager with the node address and configuration.</p> <p>Parameters:</p> Name Type Description Default <code>addr</code> <code>str</code> <p>The network address of the current node.</p> required <code>config</code> <code>dict</code> <p>Configuration dictionary for the node.</p> required Source code in <code>nebula/core/network/messages.py</code> <pre><code>def __init__(self, addr, config):\n    \"\"\"\n    Initialize MessagesManager with the node address and configuration.\n\n    Args:\n        addr (str): The network address of the current node.\n        config (dict): Configuration dictionary for the node.\n    \"\"\"\n    self.addr = addr\n    self.config = config\n    self._cm = None\n    self._message_templates = {}\n    self._define_message_templates()\n</code></pre>"},{"location":"api/core/network/messages/#nebula.core.network.messages.MessagesManager.create_message","title":"<code>create_message(message_type, action='', *args, **kwargs)</code>","text":"<p>Create and serialize a protobuf message of the given type and action.</p> <p>Dynamically maps provided arguments to the protobuf message fields using predefined templates. Wraps the message in a Nebula 'Wrapper' message with the node's address as source.</p> <p>Parameters:</p> Name Type Description Default <code>message_type</code> <code>str</code> <p>The type of message to create (e.g. 'offer', 'model', etc.).</p> required <code>action</code> <code>str</code> <p>Action name for the message, converted to protobuf enum. Defaults to \"\".</p> <code>''</code> <code>*args</code> <p>Positional arguments for message fields according to the template.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments for message fields.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the message_type is invalid.</p> <code>AttributeError</code> <p>If the protobuf message class does not exist.</p> <p>Returns:</p> Name Type Description <code>bytes</code> <p>Serialized protobuf 'Wrapper' message bytes ready for transmission.</p> Source code in <code>nebula/core/network/messages.py</code> <pre><code>def create_message(self, message_type: str, action: str = \"\", *args, **kwargs):\n    \"\"\"\n    Create and serialize a protobuf message of the given type and action.\n\n    Dynamically maps provided arguments to the protobuf message fields using predefined templates.\n    Wraps the message in a Nebula 'Wrapper' message with the node's address as source.\n\n    Args:\n        message_type (str): The type of message to create (e.g. 'offer', 'model', etc.).\n        action (str, optional): Action name for the message, converted to protobuf enum. Defaults to \"\".\n        *args: Positional arguments for message fields according to the template.\n        **kwargs: Keyword arguments for message fields.\n\n    Raises:\n        ValueError: If the message_type is invalid.\n        AttributeError: If the protobuf message class does not exist.\n\n    Returns:\n        bytes: Serialized protobuf 'Wrapper' message bytes ready for transmission.\n    \"\"\"\n    # logging.info(f\"Creating message | type: {message_type}, action: {action}, positionals: {args}, explicits: {kwargs.keys()}\")\n    # If an action is provided, convert it to its corresponding enum value using the factory\n    message_action = None\n    if action:\n        message_action = factory_message_action(message_type, action)\n\n    # Retrieve the template for the provided message type\n    message_template = self._message_templates.get(message_type)\n    if not message_template:\n        raise ValueError(f\"Invalid message type '{message_type}'\")\n\n    # Extract parameters and defaults from the template\n    template_params = message_template[\"parameters\"]\n    default_values: dict = message_template.get(\"defaults\", {})\n\n    # Dynamically retrieve the class for the protobuf message (e.g., OfferMessage)\n    class_name = message_type.capitalize() + \"Message\"\n    message_class = getattr(nebula_pb2, class_name, None)\n\n    if message_class is None:\n        raise AttributeError(f\"Message type {message_type} not found on the protocol\")\n\n    # Set the 'action' parameter if required and if the message_action is available\n    if \"action\" in template_params and message_action is not None:\n        kwargs[\"action\"] = message_action\n\n    # Map positional arguments to template parameters\n    remaining_params = [param_name for param_name in template_params if param_name not in kwargs]\n    if args:\n        for param_name, arg_value in zip(remaining_params, args, strict=False):\n            if param_name in kwargs:\n                continue\n            kwargs[param_name] = arg_value\n\n    # Fill in missing parameters with their default values\n    # logging.info(f\"kwargs parameters: {kwargs.keys()}\")\n    for param_name in template_params:\n        if param_name not in kwargs:\n            # logging.info(f\"Filling parameter '{param_name}' with default value: {default_values.get(param_name)}\")\n            kwargs[param_name] = default_values.get(param_name)\n\n    # Create an instance of the protobuf message class using the constructed kwargs\n    message = message_class(**kwargs)\n\n    message_wrapper = nebula_pb2.Wrapper()\n    message_wrapper.source = self.addr\n    field_name = f\"{message_type}_message\"\n    getattr(message_wrapper, field_name).CopyFrom(message)\n    data = message_wrapper.SerializeToString()\n    return data\n</code></pre>"},{"location":"api/core/network/messages/#nebula.core.network.messages.MessagesManager.get_messages_events","title":"<code>get_messages_events()</code>","text":"<p>Retrieve the available message event names and their corresponding actions.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Mapping of message names (excluding 'model') to their available action names.</p> Source code in <code>nebula/core/network/messages.py</code> <pre><code>def get_messages_events(self) -&gt; dict:\n    \"\"\"\n    Retrieve the available message event names and their corresponding actions.\n\n    Returns:\n        dict: Mapping of message names (excluding 'model') to their available action names.\n    \"\"\"\n    message_events = {}\n    for message_name in self._message_templates:\n        if message_name != \"model\":\n            message_events[message_name] = get_actions_names(message_name)\n    return message_events\n</code></pre>"},{"location":"api/core/network/messages/#nebula.core.network.messages.MessagesManager.process_message","title":"<code>process_message(data, addr_from)</code>  <code>async</code>","text":"<p>Asynchronously process an incoming serialized protobuf message.</p> <p>Parses the message, verifies source, forwards or handles the message depending on its type, and prevents duplicate processing using message hashes.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>Serialized protobuf message bytes.</p> required <code>addr_from</code> <code>str</code> <p>Address from which the message was received.</p> required Source code in <code>nebula/core/network/messages.py</code> <pre><code>async def process_message(self, data, addr_from):\n    \"\"\"\n    Asynchronously process an incoming serialized protobuf message.\n\n    Parses the message, verifies source, forwards or handles the message depending on its type,\n    and prevents duplicate processing using message hashes.\n\n    Args:\n        data (bytes): Serialized protobuf message bytes.\n        addr_from (str): Address from which the message was received.\n    \"\"\"\n    not_processing_messages = {\"control_message\", \"connection_message\"}\n    special_processing_messages = {\"discovery_message\", \"federation_message\", \"model_message\"}\n\n    try:\n        message_wrapper = nebula_pb2.Wrapper()\n        message_wrapper.ParseFromString(data)\n        source = message_wrapper.source\n        logging.debug(f\"\ud83d\udce5  handle_incoming_message | Received message from {addr_from} with source {source}\")\n        if source == self.addr:\n            return\n\n        # Extract the active message from the oneof field\n        message_type = message_wrapper.WhichOneof(\"message\")\n        msg_name = message_type.split(\"_\")[0]\n        if not message_type:\n            logging.warning(\"Received message with no active field in the 'oneof'\")\n            return\n\n        message_data = getattr(message_wrapper, message_type)\n\n        # Not required processing messages\n        if message_type in not_processing_messages:\n            # await self.cm.handle_message(source, message_type, message_data)\n            me = MessageEvent(\n                (msg_name, get_action_name_from_value(msg_name, message_data.action)), source, message_data\n            )\n            await self.cm.handle_message(me)\n\n        # Message-specific forwarding and processing\n        elif message_type in special_processing_messages:\n            if await self.cm.include_received_message_hash(hashlib.md5(data).hexdigest(), addr_from):\n                # Forward the message if required\n                if self._should_forward_message(message_type, message_wrapper):\n                    await self.cm.forward_message(data, addr_from)\n\n                if message_type == \"model_message\":\n                    await self.cm.handle_model_message(source, message_data)\n                else:\n                    me = MessageEvent(\n                        (msg_name, get_action_name_from_value(msg_name, message_data.action)), source, message_data\n                    )\n                    await self.cm.handle_message(me)\n        # Rest of messages\n        else:\n            # if await self.cm.include_received_message_hash(hashlib.md5(data).hexdigest()):\n            me = MessageEvent(\n                (msg_name, get_action_name_from_value(msg_name, message_data.action)), source, message_data\n            )\n            await self.cm.handle_message(me)\n    except Exception as e:\n        logging.exception(f\"\ud83d\udce5  handle_incoming_message | Error while processing: {e}\")\n        logging.exception(traceback.format_exc())\n</code></pre>"},{"location":"api/core/network/propagator/","title":"Documentation for Propagator Module","text":""},{"location":"api/core/network/propagator/#nebula.core.network.propagator.InitialModelPropagation","title":"<code>InitialModelPropagation</code>","text":"<p>               Bases: <code>PropagationStrategy</code></p> <p>Propagation strategy for sending the initial model to all newly connected nodes.</p> <p>Sends a fresh model initialized by the trainer with a default weight.</p> Source code in <code>nebula/core/network/propagator.py</code> <pre><code>class InitialModelPropagation(PropagationStrategy):\n    \"\"\"\n    Propagation strategy for sending the initial model to all newly connected nodes.\n\n    Sends a fresh model initialized by the trainer with a default weight.\n    \"\"\"\n\n    def __init__(self, aggregator: \"Aggregator\", trainer: \"Lightning\", engine: \"Engine\"):\n        \"\"\"\n        Args:\n            aggregator (Aggregator): The aggregator coordinating model rounds.\n            trainer (Lightning): The local trainer instance providing model parameters.\n            engine (Engine): The engine managing rounds and connections.\n        \"\"\"\n        self.aggregator = aggregator\n        self.trainer = trainer\n        self.engine = engine\n\n    async def get_round(self):\n        \"\"\"\n        Get the current training round number from the engine.\n\n        Returns:\n            int: The current round index.\n        \"\"\"\n        return await self.engine.get_round()\n\n    async def is_node_eligible(self, node: str) -&gt; bool:\n        \"\"\"\n        Determine if a node has not yet received the initial model.\n\n        Args:\n            node (str): The identifier of the target node.\n\n        Returns:\n            bool: True if the node is not already in the ready connections list.\n        \"\"\"\n        return node not in self.engine.cm.get_ready_connections()\n\n    def prepare_model_payload(self, node: str) -&gt; tuple[Any, float] | None:\n        \"\"\"\n        Prepare the initial model parameters and default weight.\n\n        Args:\n            node (str): The identifier of the target node (not used in payload).\n\n        Returns:\n            tuple[Any, float]: The initialized model parameters and default model weight.\n        \"\"\"\n        return (\n            self.trainer.get_model_parameters(initialize=True),\n            self.trainer.DEFAULT_MODEL_WEIGHT,\n        )\n</code></pre>"},{"location":"api/core/network/propagator/#nebula.core.network.propagator.InitialModelPropagation.__init__","title":"<code>__init__(aggregator, trainer, engine)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>aggregator</code> <code>Aggregator</code> <p>The aggregator coordinating model rounds.</p> required <code>trainer</code> <code>Lightning</code> <p>The local trainer instance providing model parameters.</p> required <code>engine</code> <code>Engine</code> <p>The engine managing rounds and connections.</p> required Source code in <code>nebula/core/network/propagator.py</code> <pre><code>def __init__(self, aggregator: \"Aggregator\", trainer: \"Lightning\", engine: \"Engine\"):\n    \"\"\"\n    Args:\n        aggregator (Aggregator): The aggregator coordinating model rounds.\n        trainer (Lightning): The local trainer instance providing model parameters.\n        engine (Engine): The engine managing rounds and connections.\n    \"\"\"\n    self.aggregator = aggregator\n    self.trainer = trainer\n    self.engine = engine\n</code></pre>"},{"location":"api/core/network/propagator/#nebula.core.network.propagator.InitialModelPropagation.get_round","title":"<code>get_round()</code>  <code>async</code>","text":"<p>Get the current training round number from the engine.</p> <p>Returns:</p> Name Type Description <code>int</code> <p>The current round index.</p> Source code in <code>nebula/core/network/propagator.py</code> <pre><code>async def get_round(self):\n    \"\"\"\n    Get the current training round number from the engine.\n\n    Returns:\n        int: The current round index.\n    \"\"\"\n    return await self.engine.get_round()\n</code></pre>"},{"location":"api/core/network/propagator/#nebula.core.network.propagator.InitialModelPropagation.is_node_eligible","title":"<code>is_node_eligible(node)</code>  <code>async</code>","text":"<p>Determine if a node has not yet received the initial model.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>str</code> <p>The identifier of the target node.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the node is not already in the ready connections list.</p> Source code in <code>nebula/core/network/propagator.py</code> <pre><code>async def is_node_eligible(self, node: str) -&gt; bool:\n    \"\"\"\n    Determine if a node has not yet received the initial model.\n\n    Args:\n        node (str): The identifier of the target node.\n\n    Returns:\n        bool: True if the node is not already in the ready connections list.\n    \"\"\"\n    return node not in self.engine.cm.get_ready_connections()\n</code></pre>"},{"location":"api/core/network/propagator/#nebula.core.network.propagator.InitialModelPropagation.prepare_model_payload","title":"<code>prepare_model_payload(node)</code>","text":"<p>Prepare the initial model parameters and default weight.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>str</code> <p>The identifier of the target node (not used in payload).</p> required <p>Returns:</p> Type Description <code>tuple[Any, float] | None</code> <p>tuple[Any, float]: The initialized model parameters and default model weight.</p> Source code in <code>nebula/core/network/propagator.py</code> <pre><code>def prepare_model_payload(self, node: str) -&gt; tuple[Any, float] | None:\n    \"\"\"\n    Prepare the initial model parameters and default weight.\n\n    Args:\n        node (str): The identifier of the target node (not used in payload).\n\n    Returns:\n        tuple[Any, float]: The initialized model parameters and default model weight.\n    \"\"\"\n    return (\n        self.trainer.get_model_parameters(initialize=True),\n        self.trainer.DEFAULT_MODEL_WEIGHT,\n    )\n</code></pre>"},{"location":"api/core/network/propagator/#nebula.core.network.propagator.PropagationStrategy","title":"<code>PropagationStrategy</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class defining the interface for model propagation strategies.</p> <p>Subclasses implement eligibility checks and payload preparation for sending model updates to specific nodes in the federation.</p> Source code in <code>nebula/core/network/propagator.py</code> <pre><code>class PropagationStrategy(ABC):\n    \"\"\"\n    Abstract base class defining the interface for model propagation strategies.\n\n    Subclasses implement eligibility checks and payload preparation for sending\n    model updates to specific nodes in the federation.\n    \"\"\"\n\n    @abstractmethod\n    async def is_node_eligible(self, node: str) -&gt; bool:\n        \"\"\"\n        Determine whether a given node should receive the model payload.\n\n        Args:\n            node (str): The address or identifier of the target node.\n\n        Returns:\n            bool: True if the node is eligible to receive the payload, False otherwise.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def prepare_model_payload(self, node: str) -&gt; tuple[Any, float] | None:\n        \"\"\"\n        Prepare the model data and weight for transmission to a node.\n\n        Args:\n            node (str): The address or identifier of the target node.\n\n        Returns:\n            tuple[Any, float] | None: A tuple containing the model object and its associated weight,\n                                       or None if no payload should be sent.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/core/network/propagator/#nebula.core.network.propagator.PropagationStrategy.is_node_eligible","title":"<code>is_node_eligible(node)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Determine whether a given node should receive the model payload.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>str</code> <p>The address or identifier of the target node.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the node is eligible to receive the payload, False otherwise.</p> Source code in <code>nebula/core/network/propagator.py</code> <pre><code>@abstractmethod\nasync def is_node_eligible(self, node: str) -&gt; bool:\n    \"\"\"\n    Determine whether a given node should receive the model payload.\n\n    Args:\n        node (str): The address or identifier of the target node.\n\n    Returns:\n        bool: True if the node is eligible to receive the payload, False otherwise.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/network/propagator/#nebula.core.network.propagator.PropagationStrategy.prepare_model_payload","title":"<code>prepare_model_payload(node)</code>  <code>abstractmethod</code>","text":"<p>Prepare the model data and weight for transmission to a node.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>str</code> <p>The address or identifier of the target node.</p> required <p>Returns:</p> Type Description <code>tuple[Any, float] | None</code> <p>tuple[Any, float] | None: A tuple containing the model object and its associated weight,                        or None if no payload should be sent.</p> Source code in <code>nebula/core/network/propagator.py</code> <pre><code>@abstractmethod\ndef prepare_model_payload(self, node: str) -&gt; tuple[Any, float] | None:\n    \"\"\"\n    Prepare the model data and weight for transmission to a node.\n\n    Args:\n        node (str): The address or identifier of the target node.\n\n    Returns:\n        tuple[Any, float] | None: A tuple containing the model object and its associated weight,\n                                   or None if no payload should be sent.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/network/propagator/#nebula.core.network.propagator.Propagator","title":"<code>Propagator</code>","text":"<p>Service responsible for propagating messages throughout the federation network.</p> The Propagator performs <ul> <li>Broadcasting discovery or control messages to all relevant peers.</li> <li>Managing propagation strategies (e.g., flood, gossip, or efficient spanning tree).</li> <li>Tracking propagation state to avoid infinite loops or redundant sends.</li> <li>Coordinating with the CommunicationsManager and Forwarder for message dispatch.</li> </ul> <p>Designed to work asynchronously, ensuring timely and scalable message dissemination across dynamically changing network topologies.</p> Source code in <code>nebula/core/network/propagator.py</code> <pre><code>class Propagator:\n    \"\"\"\n    Service responsible for propagating messages throughout the federation network.\n\n    The Propagator performs:\n      - Broadcasting discovery or control messages to all relevant peers.\n      - Managing propagation strategies (e.g., flood, gossip, or efficient spanning tree).\n      - Tracking propagation state to avoid infinite loops or redundant sends.\n      - Coordinating with the CommunicationsManager and Forwarder for message dispatch.\n\n    Designed to work asynchronously, ensuring timely and scalable message dissemination\n    across dynamically changing network topologies.\n    \"\"\"\n\n    def __init__(self):\n        self._cm = None\n        self._running = asyncio.Event()\n\n    @property\n    def cm(self):\n        \"\"\"\n        Lazy-load and return the CommunicationsManager instance for sending messages.\n\n        Returns:\n            CommunicationsManager: The singleton communications manager.\n        \"\"\"\n        if not self._cm:\n            from nebula.core.network.communications import CommunicationsManager\n\n            self._cm = CommunicationsManager.get_instance()\n            return self._cm\n        else:\n            return self._cm\n\n    async def start(self):\n        \"\"\"\n        Initialize the Propagator by retrieving core components and configuration,\n        setting up propagation intervals, history buffer, and strategy instances.\n\n        This method must be called before any propagation cycles to ensure that\n        all dependencies (engine, trainer, aggregator, etc.) are available.\n        \"\"\"\n        await EventManager.get_instance().subscribe_node_event(ModelPropagationEvent, self._propagate)\n        self.engine: Engine = self.cm.engine\n        self.config: Config = self.cm.get_config()\n        self.addr = self.cm.get_addr()\n        self.aggregator: Aggregator = self.engine.aggregator\n        self.trainer: Lightning = self.engine._trainer\n\n        self.status_history = deque(maxlen=self.config.participant[\"propagator_args\"][\"history_size\"])\n\n        self.interval = self.config.participant[\"propagator_args\"][\"propagate_interval\"]\n        self.model_interval = self.config.participant[\"propagator_args\"][\"propagate_model_interval\"]\n        self.early_stop = self.config.participant[\"propagator_args\"][\"propagation_early_stop\"]\n        self.stable_rounds_count = 0\n\n        # Propagation strategies (adapt to the specific use case)\n        self.strategies = {\n            \"initialization\": InitialModelPropagation(self.aggregator, self.trainer, self.engine),\n            \"stable\": StableModelPropagation(self.aggregator, self.trainer, self.engine),\n        }\n        print_msg_box(\n            msg=\"Starting propagator functionality...\\nModel propagation through the network\",\n            indent=2,\n            title=\"Propagator\",\n        )\n        self._running.set()\n\n    async def get_round(self):\n        \"\"\"\n        Retrieve the current federated learning round number.\n\n        Returns:\n            int: The current round index from the engine.\n        \"\"\"\n        return await self.engine.get_round()\n\n    def update_and_check_neighbors(self, strategy, eligible_neighbors):\n        \"\"\"\n        Update the history of eligible neighbors and determine if propagation should continue.\n\n        Appends the current list of eligible neighbors to a bounded history. If the history\n        buffer fills with identical entries, propagation is halted to prevent redundant sends.\n\n        Args:\n            strategy (PropagationStrategy): The propagation strategy in use.\n            eligible_neighbors (list): List of neighbor addresses eligible for propagation.\n\n        Returns:\n            bool: True if propagation should continue, False if it should stop due to repeated history.\n        \"\"\"\n        # Update the status of eligible neighbors\n        current_status = [n for n in eligible_neighbors]\n\n        # Check if the deque is full and the new status is different from the last one\n        if self.status_history and current_status != self.status_history[-1]:\n            logging.info(\n                f\"Status History deque is full and the new status is different from the last one: {list(self.status_history)}\"\n            )\n            self.status_history.append(current_status)\n            return True\n\n        # Add the current status to the deque\n        logging.info(f\"Adding current status to the deque: {current_status}\")\n        self.status_history.append(current_status)\n\n        # If the deque is full and all elements are the same, stop propagation\n        if len(self.status_history) == self.status_history.maxlen and all(\n            s == self.status_history[0] for s in self.status_history\n        ):\n            logging.info(\n                f\"Propagator exited for {self.status_history.maxlen} equal rounds: {list(self.status_history)}\"\n            )\n            return False\n\n        return True\n\n    def reset_status_history(self):\n        \"\"\"\n        Clear the history buffer of neighbor eligibility statuses.\n\n        This is typically done at the start of a new propagation cycle.\n        \"\"\"\n        self.status_history.clear()\n\n    async def _propagate(self, mpe: ModelPropagationEvent):\n        \"\"\"\n        Execute a single propagation cycle using the specified strategy.\n\n        1. Resets status history.\n        2. Validates the strategy and current round.\n        3. Identifies eligible neighbors.\n        4. Updates history and checks for repeated statuses.\n        5. Prepares and serializes the model payload.\n        6. Sends the model message to each eligible neighbor.\n        7. Waits for the configured interval before concluding.\n\n        Args:\n            strategy_id (str): Key identifying which propagation strategy to use\n                            (e.g., \"initialization\" or \"stable\").\n\n        Returns:\n            bool: True if propagation occurred (payload sent), False if halted early.\n        \"\"\"\n        eligible_neighbors, strategy_id = await mpe.get_event_data()\n\n        self.reset_status_history()\n        if strategy_id not in self.strategies:\n            logging.info(f\"Strategy {strategy_id} not found.\")\n            return False\n        if await self.get_round() is None:\n            logging.info(\"Propagation halted: round is not set.\")\n            return False\n\n        strategy = self.strategies[strategy_id]\n        logging.info(f\"Starting model propagation with strategy: {strategy_id}\")\n\n        # current_connections = await self.cm.get_addrs_current_connections(only_direct=True)\n        # eligible_neighbors = [\n        #     neighbor_addr for neighbor_addr in current_connections if await strategy.is_node_eligible(neighbor_addr)\n        # ]\n        logging.info(f\"Eligible neighbors for model propagation: {eligible_neighbors}\")\n        if not eligible_neighbors:\n            logging.info(\"Propagation complete: No eligible neighbors.\")\n            return False\n\n        logging.info(\"Checking repeated statuses during propagation\")\n        if not self.update_and_check_neighbors(strategy, eligible_neighbors):\n            logging.info(\"Exiting propagation due to repeated statuses.\")\n            return False\n\n        model_params, weight = strategy.prepare_model_payload(None)\n        if model_params:\n            serialized_model = (\n                model_params if isinstance(model_params, bytes) else self.trainer.serialize_model(model_params)\n            )\n        else:\n            serialized_model = None\n\n        current_round = await self.get_round()\n        round_number = -1 if strategy_id == \"initialization\" else current_round\n        parameters = serialized_model\n        message = self.cm.create_message(\"model\", \"\", round_number, parameters, weight)\n        for neighbor_addr in eligible_neighbors:\n            logging.info(\n                f\"Sending model to {neighbor_addr} with round {await self.get_round()}: weight={weight} |\u00a0size={sys.getsizeof(serialized_model) / (1024** 2) if serialized_model is not None else 0} MB\"\n            )\n            asyncio.create_task(self.cm.send_message(neighbor_addr, message, \"model\"))\n            # asyncio.create_task(self.cm.send_model(neighbor_addr, round_number, serialized_model, weight))\n\n        await asyncio.sleep(self.interval)\n        return True\n\n    async def get_model_information(self, dest_addr, strategy_id: str, init=False):\n        \"\"\"\n        Retrieve the serialized model payload and round metadata for making an offer to a node.\n\n        Args:\n            dest_addr (str): The address of the destination node.\n            strategy_id (str): Key identifying which propagation strategy to use.\n            init (bool, optional): If True, bypasses strategy and round validation (used for initial offers). Defaults to False.\n\n        Returns:\n            tuple(bytes, int, int) | None:\n                A tuple containing:\n                - serialized_model (bytes): The model payload ready for transmission.\n                - total_rounds (int): The configured total number of rounds.\n                - current_round (int): The current federated learning round.\n                Returns None if the strategy is invalid, the round is unset, or no payload is prepared.\n        \"\"\"\n        if not init:\n            if strategy_id not in self.strategies:\n                logging.info(f\"Strategy {strategy_id} not found.\")\n                return None\n            if await self.get_round() is None:\n                logging.info(\"Propagation halted: round is not set.\")\n                return None\n\n        strategy = self.strategies[strategy_id]\n        logging.info(f\"Preparing model information with strategy to make an offer: {strategy_id}\")\n\n        model_params, weight = strategy.prepare_model_payload(None)\n        rounds = self.engine.total_rounds\n\n        if model_params:\n            serialized_model = (\n                model_params if isinstance(model_params, bytes) else self.trainer.serialize_model(model_params)\n            )\n            return (serialized_model, rounds, await self.get_round())\n\n        return None\n\n    async def stop(self):\n        logging.info(\"\ud83c\udf10  Stopping Propagator module...\")\n        self._running.clear()\n\n    async def is_running(self):\n        return self._running.is_set()\n</code></pre>"},{"location":"api/core/network/propagator/#nebula.core.network.propagator.Propagator.cm","title":"<code>cm</code>  <code>property</code>","text":"<p>Lazy-load and return the CommunicationsManager instance for sending messages.</p> <p>Returns:</p> Name Type Description <code>CommunicationsManager</code> <p>The singleton communications manager.</p>"},{"location":"api/core/network/propagator/#nebula.core.network.propagator.Propagator.get_model_information","title":"<code>get_model_information(dest_addr, strategy_id, init=False)</code>  <code>async</code>","text":"<p>Retrieve the serialized model payload and round metadata for making an offer to a node.</p> <p>Parameters:</p> Name Type Description Default <code>dest_addr</code> <code>str</code> <p>The address of the destination node.</p> required <code>strategy_id</code> <code>str</code> <p>Key identifying which propagation strategy to use.</p> required <code>init</code> <code>bool</code> <p>If True, bypasses strategy and round validation (used for initial offers). Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>tuple(bytes, int, int) | None: A tuple containing: - serialized_model (bytes): The model payload ready for transmission. - total_rounds (int): The configured total number of rounds. - current_round (int): The current federated learning round. Returns None if the strategy is invalid, the round is unset, or no payload is prepared.</p> Source code in <code>nebula/core/network/propagator.py</code> <pre><code>async def get_model_information(self, dest_addr, strategy_id: str, init=False):\n    \"\"\"\n    Retrieve the serialized model payload and round metadata for making an offer to a node.\n\n    Args:\n        dest_addr (str): The address of the destination node.\n        strategy_id (str): Key identifying which propagation strategy to use.\n        init (bool, optional): If True, bypasses strategy and round validation (used for initial offers). Defaults to False.\n\n    Returns:\n        tuple(bytes, int, int) | None:\n            A tuple containing:\n            - serialized_model (bytes): The model payload ready for transmission.\n            - total_rounds (int): The configured total number of rounds.\n            - current_round (int): The current federated learning round.\n            Returns None if the strategy is invalid, the round is unset, or no payload is prepared.\n    \"\"\"\n    if not init:\n        if strategy_id not in self.strategies:\n            logging.info(f\"Strategy {strategy_id} not found.\")\n            return None\n        if await self.get_round() is None:\n            logging.info(\"Propagation halted: round is not set.\")\n            return None\n\n    strategy = self.strategies[strategy_id]\n    logging.info(f\"Preparing model information with strategy to make an offer: {strategy_id}\")\n\n    model_params, weight = strategy.prepare_model_payload(None)\n    rounds = self.engine.total_rounds\n\n    if model_params:\n        serialized_model = (\n            model_params if isinstance(model_params, bytes) else self.trainer.serialize_model(model_params)\n        )\n        return (serialized_model, rounds, await self.get_round())\n\n    return None\n</code></pre>"},{"location":"api/core/network/propagator/#nebula.core.network.propagator.Propagator.get_round","title":"<code>get_round()</code>  <code>async</code>","text":"<p>Retrieve the current federated learning round number.</p> <p>Returns:</p> Name Type Description <code>int</code> <p>The current round index from the engine.</p> Source code in <code>nebula/core/network/propagator.py</code> <pre><code>async def get_round(self):\n    \"\"\"\n    Retrieve the current federated learning round number.\n\n    Returns:\n        int: The current round index from the engine.\n    \"\"\"\n    return await self.engine.get_round()\n</code></pre>"},{"location":"api/core/network/propagator/#nebula.core.network.propagator.Propagator.reset_status_history","title":"<code>reset_status_history()</code>","text":"<p>Clear the history buffer of neighbor eligibility statuses.</p> <p>This is typically done at the start of a new propagation cycle.</p> Source code in <code>nebula/core/network/propagator.py</code> <pre><code>def reset_status_history(self):\n    \"\"\"\n    Clear the history buffer of neighbor eligibility statuses.\n\n    This is typically done at the start of a new propagation cycle.\n    \"\"\"\n    self.status_history.clear()\n</code></pre>"},{"location":"api/core/network/propagator/#nebula.core.network.propagator.Propagator.start","title":"<code>start()</code>  <code>async</code>","text":"<p>Initialize the Propagator by retrieving core components and configuration, setting up propagation intervals, history buffer, and strategy instances.</p> <p>This method must be called before any propagation cycles to ensure that all dependencies (engine, trainer, aggregator, etc.) are available.</p> Source code in <code>nebula/core/network/propagator.py</code> <pre><code>async def start(self):\n    \"\"\"\n    Initialize the Propagator by retrieving core components and configuration,\n    setting up propagation intervals, history buffer, and strategy instances.\n\n    This method must be called before any propagation cycles to ensure that\n    all dependencies (engine, trainer, aggregator, etc.) are available.\n    \"\"\"\n    await EventManager.get_instance().subscribe_node_event(ModelPropagationEvent, self._propagate)\n    self.engine: Engine = self.cm.engine\n    self.config: Config = self.cm.get_config()\n    self.addr = self.cm.get_addr()\n    self.aggregator: Aggregator = self.engine.aggregator\n    self.trainer: Lightning = self.engine._trainer\n\n    self.status_history = deque(maxlen=self.config.participant[\"propagator_args\"][\"history_size\"])\n\n    self.interval = self.config.participant[\"propagator_args\"][\"propagate_interval\"]\n    self.model_interval = self.config.participant[\"propagator_args\"][\"propagate_model_interval\"]\n    self.early_stop = self.config.participant[\"propagator_args\"][\"propagation_early_stop\"]\n    self.stable_rounds_count = 0\n\n    # Propagation strategies (adapt to the specific use case)\n    self.strategies = {\n        \"initialization\": InitialModelPropagation(self.aggregator, self.trainer, self.engine),\n        \"stable\": StableModelPropagation(self.aggregator, self.trainer, self.engine),\n    }\n    print_msg_box(\n        msg=\"Starting propagator functionality...\\nModel propagation through the network\",\n        indent=2,\n        title=\"Propagator\",\n    )\n    self._running.set()\n</code></pre>"},{"location":"api/core/network/propagator/#nebula.core.network.propagator.Propagator.update_and_check_neighbors","title":"<code>update_and_check_neighbors(strategy, eligible_neighbors)</code>","text":"<p>Update the history of eligible neighbors and determine if propagation should continue.</p> <p>Appends the current list of eligible neighbors to a bounded history. If the history buffer fills with identical entries, propagation is halted to prevent redundant sends.</p> <p>Parameters:</p> Name Type Description Default <code>strategy</code> <code>PropagationStrategy</code> <p>The propagation strategy in use.</p> required <code>eligible_neighbors</code> <code>list</code> <p>List of neighbor addresses eligible for propagation.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if propagation should continue, False if it should stop due to repeated history.</p> Source code in <code>nebula/core/network/propagator.py</code> <pre><code>def update_and_check_neighbors(self, strategy, eligible_neighbors):\n    \"\"\"\n    Update the history of eligible neighbors and determine if propagation should continue.\n\n    Appends the current list of eligible neighbors to a bounded history. If the history\n    buffer fills with identical entries, propagation is halted to prevent redundant sends.\n\n    Args:\n        strategy (PropagationStrategy): The propagation strategy in use.\n        eligible_neighbors (list): List of neighbor addresses eligible for propagation.\n\n    Returns:\n        bool: True if propagation should continue, False if it should stop due to repeated history.\n    \"\"\"\n    # Update the status of eligible neighbors\n    current_status = [n for n in eligible_neighbors]\n\n    # Check if the deque is full and the new status is different from the last one\n    if self.status_history and current_status != self.status_history[-1]:\n        logging.info(\n            f\"Status History deque is full and the new status is different from the last one: {list(self.status_history)}\"\n        )\n        self.status_history.append(current_status)\n        return True\n\n    # Add the current status to the deque\n    logging.info(f\"Adding current status to the deque: {current_status}\")\n    self.status_history.append(current_status)\n\n    # If the deque is full and all elements are the same, stop propagation\n    if len(self.status_history) == self.status_history.maxlen and all(\n        s == self.status_history[0] for s in self.status_history\n    ):\n        logging.info(\n            f\"Propagator exited for {self.status_history.maxlen} equal rounds: {list(self.status_history)}\"\n        )\n        return False\n\n    return True\n</code></pre>"},{"location":"api/core/network/propagator/#nebula.core.network.propagator.StableModelPropagation","title":"<code>StableModelPropagation</code>","text":"<p>               Bases: <code>PropagationStrategy</code></p> <p>Propagation strategy for sending model updates after the initial round.</p> <p>Sends the latest trained model to neighbors.</p> Source code in <code>nebula/core/network/propagator.py</code> <pre><code>class StableModelPropagation(PropagationStrategy):\n    \"\"\"\n    Propagation strategy for sending model updates after the initial round.\n\n    Sends the latest trained model to neighbors.\n    \"\"\"\n\n    def __init__(self, aggregator: \"Aggregator\", trainer: \"Lightning\", engine: \"Engine\"):\n        \"\"\"\n        Args:\n            aggregator (Aggregator): The aggregator coordinating model rounds.\n            trainer (Lightning): The local trainer instance providing model parameters and weight.\n            engine (Engine): The engine managing rounds, connections, and addresses.\n        \"\"\"\n        self.aggregator = aggregator\n        self.trainer = trainer\n        self.engine = engine\n        self.addr = self.engine.get_addr()\n\n    async def get_round(self):\n        \"\"\"\n        Get the current training round number from the engine.\n\n        Returns:\n            int: The current round index.\n        \"\"\"\n        return await self.engine.get_round()\n\n    async def is_node_eligible(self, node: str) -&gt; bool:\n        \"\"\"\n        Determine if a node requires a model update based on aggregation state.\n\n        Args:\n            node (str): The identifier of the target node.\n\n        Returns:\n            bool: True if the node is pending aggregation or its last federated round\n                  is less than the current round.\n        \"\"\"\n        return (node not in self.aggregator.get_nodes_pending_models_to_aggregate()) or (\n            self.engine.cm.connections[node].get_federated_round() &lt; await self.get_round()\n        )\n\n    def prepare_model_payload(self, node: str) -&gt; tuple[Any, float] | None:\n        \"\"\"\n        Prepare the current model parameters and their corresponding weight.\n\n        Args:\n            node (str): The identifier of the target node (not used in payload).\n\n        Returns:\n            tuple[Any, float]: The model parameters and model weight for propagation.\n        \"\"\"\n        return self.trainer.get_model_parameters(), self.trainer.get_model_weight()\n</code></pre>"},{"location":"api/core/network/propagator/#nebula.core.network.propagator.StableModelPropagation.__init__","title":"<code>__init__(aggregator, trainer, engine)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>aggregator</code> <code>Aggregator</code> <p>The aggregator coordinating model rounds.</p> required <code>trainer</code> <code>Lightning</code> <p>The local trainer instance providing model parameters and weight.</p> required <code>engine</code> <code>Engine</code> <p>The engine managing rounds, connections, and addresses.</p> required Source code in <code>nebula/core/network/propagator.py</code> <pre><code>def __init__(self, aggregator: \"Aggregator\", trainer: \"Lightning\", engine: \"Engine\"):\n    \"\"\"\n    Args:\n        aggregator (Aggregator): The aggregator coordinating model rounds.\n        trainer (Lightning): The local trainer instance providing model parameters and weight.\n        engine (Engine): The engine managing rounds, connections, and addresses.\n    \"\"\"\n    self.aggregator = aggregator\n    self.trainer = trainer\n    self.engine = engine\n    self.addr = self.engine.get_addr()\n</code></pre>"},{"location":"api/core/network/propagator/#nebula.core.network.propagator.StableModelPropagation.get_round","title":"<code>get_round()</code>  <code>async</code>","text":"<p>Get the current training round number from the engine.</p> <p>Returns:</p> Name Type Description <code>int</code> <p>The current round index.</p> Source code in <code>nebula/core/network/propagator.py</code> <pre><code>async def get_round(self):\n    \"\"\"\n    Get the current training round number from the engine.\n\n    Returns:\n        int: The current round index.\n    \"\"\"\n    return await self.engine.get_round()\n</code></pre>"},{"location":"api/core/network/propagator/#nebula.core.network.propagator.StableModelPropagation.is_node_eligible","title":"<code>is_node_eligible(node)</code>  <code>async</code>","text":"<p>Determine if a node requires a model update based on aggregation state.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>str</code> <p>The identifier of the target node.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the node is pending aggregation or its last federated round   is less than the current round.</p> Source code in <code>nebula/core/network/propagator.py</code> <pre><code>async def is_node_eligible(self, node: str) -&gt; bool:\n    \"\"\"\n    Determine if a node requires a model update based on aggregation state.\n\n    Args:\n        node (str): The identifier of the target node.\n\n    Returns:\n        bool: True if the node is pending aggregation or its last federated round\n              is less than the current round.\n    \"\"\"\n    return (node not in self.aggregator.get_nodes_pending_models_to_aggregate()) or (\n        self.engine.cm.connections[node].get_federated_round() &lt; await self.get_round()\n    )\n</code></pre>"},{"location":"api/core/network/propagator/#nebula.core.network.propagator.StableModelPropagation.prepare_model_payload","title":"<code>prepare_model_payload(node)</code>","text":"<p>Prepare the current model parameters and their corresponding weight.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>str</code> <p>The identifier of the target node (not used in payload).</p> required <p>Returns:</p> Type Description <code>tuple[Any, float] | None</code> <p>tuple[Any, float]: The model parameters and model weight for propagation.</p> Source code in <code>nebula/core/network/propagator.py</code> <pre><code>def prepare_model_payload(self, node: str) -&gt; tuple[Any, float] | None:\n    \"\"\"\n    Prepare the current model parameters and their corresponding weight.\n\n    Args:\n        node (str): The identifier of the target node (not used in payload).\n\n    Returns:\n        tuple[Any, float]: The model parameters and model weight for propagation.\n    \"\"\"\n    return self.trainer.get_model_parameters(), self.trainer.get_model_weight()\n</code></pre>"},{"location":"api/core/network/externalconnection/","title":"Documentation for Externalconnection Module","text":""},{"location":"api/core/network/externalconnection/externalconnectionservice/","title":"Documentation for Externalconnectionservice Module","text":""},{"location":"api/core/network/externalconnection/externalconnectionservice/#nebula.core.network.externalconnection.externalconnectionservice.ExternalConnectionService","title":"<code>ExternalConnectionService</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for an external connection service in a DFL federation.</p> <p>This interface defines the required methods for any service responsible for discovering federations and managing beacon signals that announce node presence in the network.</p> Source code in <code>nebula/core/network/externalconnection/externalconnectionservice.py</code> <pre><code>class ExternalConnectionService(ABC):\n    \"\"\"\n    Abstract base class for an external connection service in a DFL federation.\n\n    This interface defines the required methods for any service responsible\n    for discovering federations and managing beacon signals that announce\n    node presence in the network.\n    \"\"\"\n\n    @abstractmethod\n    async def start(self):\n        \"\"\"\n        Start the external connection service.\n\n        This typically involves initializing discovery mechanisms\n        and preparing to receive or send messages related to federation discovery.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def stop(self):\n        \"\"\"\n        Stop the external connection service.\n\n        This should gracefully shut down any background tasks or sockets\n        associated with discovery or beaconing.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def is_running(self):\n        \"\"\"\n        Check whether the external connection service is currently active.\n\n        Returns:\n            bool: True if the service is running, False otherwise.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def find_federation(self):\n        \"\"\"\n        Attempt to discover other federations or nodes in the network.\n\n        This method is used by a node to actively search for potential\n        neighbors to join a federation or to bootstrap its own.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def start_beacon(self):\n        \"\"\"\n        Start periodically sending beacon messages to announce node presence.\n\n        Beacon messages help other nodes detect and identify this node's\n        existence and availability on the network.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def stop_beacon(self):\n        \"\"\"\n        Stop sending beacon messages.\n\n        This disables periodic presence announcements, making the node\n        temporarily invisible to passive discovery.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def modify_beacon_frequency(self, frequency):\n        \"\"\"\n        Modify the frequency at which beacon messages are sent.\n\n        Args:\n            frequency (float): New beacon interval in seconds.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/core/network/externalconnection/externalconnectionservice/#nebula.core.network.externalconnection.externalconnectionservice.ExternalConnectionService.find_federation","title":"<code>find_federation()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Attempt to discover other federations or nodes in the network.</p> <p>This method is used by a node to actively search for potential neighbors to join a federation or to bootstrap its own.</p> Source code in <code>nebula/core/network/externalconnection/externalconnectionservice.py</code> <pre><code>@abstractmethod\nasync def find_federation(self):\n    \"\"\"\n    Attempt to discover other federations or nodes in the network.\n\n    This method is used by a node to actively search for potential\n    neighbors to join a federation or to bootstrap its own.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/network/externalconnection/externalconnectionservice/#nebula.core.network.externalconnection.externalconnectionservice.ExternalConnectionService.is_running","title":"<code>is_running()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Check whether the external connection service is currently active.</p> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the service is running, False otherwise.</p> Source code in <code>nebula/core/network/externalconnection/externalconnectionservice.py</code> <pre><code>@abstractmethod\nasync def is_running(self):\n    \"\"\"\n    Check whether the external connection service is currently active.\n\n    Returns:\n        bool: True if the service is running, False otherwise.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/network/externalconnection/externalconnectionservice/#nebula.core.network.externalconnection.externalconnectionservice.ExternalConnectionService.modify_beacon_frequency","title":"<code>modify_beacon_frequency(frequency)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Modify the frequency at which beacon messages are sent.</p> <p>Parameters:</p> Name Type Description Default <code>frequency</code> <code>float</code> <p>New beacon interval in seconds.</p> required Source code in <code>nebula/core/network/externalconnection/externalconnectionservice.py</code> <pre><code>@abstractmethod\nasync def modify_beacon_frequency(self, frequency):\n    \"\"\"\n    Modify the frequency at which beacon messages are sent.\n\n    Args:\n        frequency (float): New beacon interval in seconds.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/network/externalconnection/externalconnectionservice/#nebula.core.network.externalconnection.externalconnectionservice.ExternalConnectionService.start","title":"<code>start()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Start the external connection service.</p> <p>This typically involves initializing discovery mechanisms and preparing to receive or send messages related to federation discovery.</p> Source code in <code>nebula/core/network/externalconnection/externalconnectionservice.py</code> <pre><code>@abstractmethod\nasync def start(self):\n    \"\"\"\n    Start the external connection service.\n\n    This typically involves initializing discovery mechanisms\n    and preparing to receive or send messages related to federation discovery.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/network/externalconnection/externalconnectionservice/#nebula.core.network.externalconnection.externalconnectionservice.ExternalConnectionService.start_beacon","title":"<code>start_beacon()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Start periodically sending beacon messages to announce node presence.</p> <p>Beacon messages help other nodes detect and identify this node's existence and availability on the network.</p> Source code in <code>nebula/core/network/externalconnection/externalconnectionservice.py</code> <pre><code>@abstractmethod\nasync def start_beacon(self):\n    \"\"\"\n    Start periodically sending beacon messages to announce node presence.\n\n    Beacon messages help other nodes detect and identify this node's\n    existence and availability on the network.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/network/externalconnection/externalconnectionservice/#nebula.core.network.externalconnection.externalconnectionservice.ExternalConnectionService.stop","title":"<code>stop()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Stop the external connection service.</p> <p>This should gracefully shut down any background tasks or sockets associated with discovery or beaconing.</p> Source code in <code>nebula/core/network/externalconnection/externalconnectionservice.py</code> <pre><code>@abstractmethod\nasync def stop(self):\n    \"\"\"\n    Stop the external connection service.\n\n    This should gracefully shut down any background tasks or sockets\n    associated with discovery or beaconing.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/network/externalconnection/externalconnectionservice/#nebula.core.network.externalconnection.externalconnectionservice.ExternalConnectionService.stop_beacon","title":"<code>stop_beacon()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Stop sending beacon messages.</p> <p>This disables periodic presence announcements, making the node temporarily invisible to passive discovery.</p> Source code in <code>nebula/core/network/externalconnection/externalconnectionservice.py</code> <pre><code>@abstractmethod\nasync def stop_beacon(self):\n    \"\"\"\n    Stop sending beacon messages.\n\n    This disables periodic presence announcements, making the node\n    temporarily invisible to passive discovery.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/network/externalconnection/externalconnectionservice/#nebula.core.network.externalconnection.externalconnectionservice.ExternalConnectionServiceException","title":"<code>ExternalConnectionServiceException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised for errors related to external connection services.</p> Source code in <code>nebula/core/network/externalconnection/externalconnectionservice.py</code> <pre><code>class ExternalConnectionServiceException(Exception):\n    \"\"\"\n    Exception raised for errors related to external connection services.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/core/network/externalconnection/externalconnectionservice/#nebula.core.network.externalconnection.externalconnectionservice.factory_connection_service","title":"<code>factory_connection_service(con_serv, addr)</code>","text":"<p>Factory method to instantiate the appropriate external connection service.</p> <p>Parameters:</p> Name Type Description Default <code>con_serv</code> <code>str</code> <p>Identifier of the connection service to use.</p> required <code>addr</code> <code>str</code> <p>Address of the node.</p> required <p>Returns:</p> Name Type Description <code>ExternalConnectionService</code> <code>ExternalConnectionService</code> <p>An instance of the requested service.</p> <p>Raises:</p> Type Description <code>ExternalConnectionServiceException</code> <p>If the service identifier is not recognized.</p> Source code in <code>nebula/core/network/externalconnection/externalconnectionservice.py</code> <pre><code>def factory_connection_service(con_serv, addr) -&gt; ExternalConnectionService:\n    \"\"\"\n    Factory method to instantiate the appropriate external connection service.\n\n    Args:\n        con_serv (str): Identifier of the connection service to use.\n        addr (str): Address of the node.\n\n    Returns:\n        ExternalConnectionService: An instance of the requested service.\n\n    Raises:\n        ExternalConnectionServiceException: If the service identifier is not recognized.\n    \"\"\"\n    from nebula.core.network.externalconnection.nebuladiscoveryservice import NebulaConnectionService\n\n    CONNECTION_SERVICES = {\n        \"nebula\": NebulaConnectionService,\n    }\n\n    con_serv = CONNECTION_SERVICES.get(con_serv, NebulaConnectionService)\n\n    if con_serv:\n        return con_serv(addr)\n    else:\n        raise ExternalConnectionServiceException(f\"Connection Service {con_serv} not found\")\n</code></pre>"},{"location":"api/core/network/externalconnection/nebuladiscoveryservice/","title":"Documentation for Nebuladiscoveryservice Module","text":""},{"location":"api/core/network/externalconnection/nebuladiscoveryservice/#nebula.core.network.externalconnection.nebuladiscoveryservice.NebulaClientProtocol","title":"<code>NebulaClientProtocol</code>","text":"<p>               Bases: <code>DatagramProtocol</code></p> Source code in <code>nebula/core/network/externalconnection/nebuladiscoveryservice.py</code> <pre><code>class NebulaClientProtocol(asyncio.DatagramProtocol):\n    BCAST_IP = \"239.255.255.250\"\n    BCAST_PORT = 1900\n    SEARCH_TRIES = 3\n    SEARCH_INTERVAL = 3\n\n    def __init__(self, nebula_service):\n        self.nebula_service: NebulaConnectionService = nebula_service\n        self.transport = None\n        self.search_done = asyncio.Event()\n\n    def connection_made(self, transport):\n        self.transport = transport\n        sock = self.transport.get_extra_info(\"socket\")\n        if sock is not None:\n            sock.setsockopt(socket.IPPROTO_IP, socket.IP_MULTICAST_TTL, 2)\n        asyncio.create_task(self.keep_search())\n\n    async def stop(self):\n        \"\"\"\n        Stop the client protocol by setting the search_done event to release any waiting tasks.\n        \"\"\"\n        self.search_done.set()\n\n    async def keep_search(self):\n        \"\"\"\n        Periodically broadcast search requests to discover other nodes in the federation.\n\n        This loop runs a fixed number of times, each time sending a multicast\n        discovery request and waiting for a predefined interval before repeating.\n\n        When the loop completes, a synchronization event (`search_done`) is set\n        to indicate that the search phase is finished.\n        \"\"\"\n        logging.info(\"Federation searching loop started\")\n        for _ in range(self.SEARCH_TRIES):\n            await self.search()\n            await asyncio.sleep(self.SEARCH_INTERVAL)\n        self.search_done.set()\n\n    async def wait_for_search(self):\n        \"\"\"\n        Wait for the search phase to complete.\n\n        This coroutine blocks until the `search_done` event is set,\n        signaling that the search loop has finished.\n        \"\"\"\n        await self.search_done.wait()\n\n    async def search(self):\n        \"\"\"\n        Send a multicast discovery message to locate other Nebula nodes.\n\n        Constructs and sends an SSDP-like M-SEARCH request targeted to\n        all devices on the local multicast group. This message indicates\n        interest in finding other participants in the Nebula DFL federation.\n\n        If an error occurs during sending, it is logged as an exception.\n        \"\"\"\n        logging.info(\"Searching for nodes...\")\n        try:\n            search_request = (\n                \"M-SEARCH * HTTP/1.1\\r\\n\"\n                \"HOST: 239.255.255.250:1900\\r\\n\"\n                'MAN: \"ssdp:discover\"\\r\\n'\n                \"MX: 1\\r\\n\"\n                \"ST: urn:nebula-service\\r\\n\"\n                \"TYPE: discover\\r\\n\"\n                \"\\r\\n\"\n            )\n            self.transport.sendto(search_request.encode(\"ASCII\"), (self.BCAST_IP, self.BCAST_PORT))\n        except Exception as e:\n            logging.exception(f\"Error sending search request: {e}\")\n\n    def datagram_received(self, data, addr):\n        try:\n            if \"ST: urn:nebula-service\" in data.decode(\"utf-8\"):\n                # logging.info(\"Received response from Node server-service\")\n                self.nebula_service.response_received(data, addr)\n        except UnicodeDecodeError:\n            logging.warning(f\"Received malformed message from {addr}, ignoring.\")\n</code></pre>"},{"location":"api/core/network/externalconnection/nebuladiscoveryservice/#nebula.core.network.externalconnection.nebuladiscoveryservice.NebulaClientProtocol.keep_search","title":"<code>keep_search()</code>  <code>async</code>","text":"<p>Periodically broadcast search requests to discover other nodes in the federation.</p> <p>This loop runs a fixed number of times, each time sending a multicast discovery request and waiting for a predefined interval before repeating.</p> <p>When the loop completes, a synchronization event (<code>search_done</code>) is set to indicate that the search phase is finished.</p> Source code in <code>nebula/core/network/externalconnection/nebuladiscoveryservice.py</code> <pre><code>async def keep_search(self):\n    \"\"\"\n    Periodically broadcast search requests to discover other nodes in the federation.\n\n    This loop runs a fixed number of times, each time sending a multicast\n    discovery request and waiting for a predefined interval before repeating.\n\n    When the loop completes, a synchronization event (`search_done`) is set\n    to indicate that the search phase is finished.\n    \"\"\"\n    logging.info(\"Federation searching loop started\")\n    for _ in range(self.SEARCH_TRIES):\n        await self.search()\n        await asyncio.sleep(self.SEARCH_INTERVAL)\n    self.search_done.set()\n</code></pre>"},{"location":"api/core/network/externalconnection/nebuladiscoveryservice/#nebula.core.network.externalconnection.nebuladiscoveryservice.NebulaClientProtocol.search","title":"<code>search()</code>  <code>async</code>","text":"<p>Send a multicast discovery message to locate other Nebula nodes.</p> <p>Constructs and sends an SSDP-like M-SEARCH request targeted to all devices on the local multicast group. This message indicates interest in finding other participants in the Nebula DFL federation.</p> <p>If an error occurs during sending, it is logged as an exception.</p> Source code in <code>nebula/core/network/externalconnection/nebuladiscoveryservice.py</code> <pre><code>async def search(self):\n    \"\"\"\n    Send a multicast discovery message to locate other Nebula nodes.\n\n    Constructs and sends an SSDP-like M-SEARCH request targeted to\n    all devices on the local multicast group. This message indicates\n    interest in finding other participants in the Nebula DFL federation.\n\n    If an error occurs during sending, it is logged as an exception.\n    \"\"\"\n    logging.info(\"Searching for nodes...\")\n    try:\n        search_request = (\n            \"M-SEARCH * HTTP/1.1\\r\\n\"\n            \"HOST: 239.255.255.250:1900\\r\\n\"\n            'MAN: \"ssdp:discover\"\\r\\n'\n            \"MX: 1\\r\\n\"\n            \"ST: urn:nebula-service\\r\\n\"\n            \"TYPE: discover\\r\\n\"\n            \"\\r\\n\"\n        )\n        self.transport.sendto(search_request.encode(\"ASCII\"), (self.BCAST_IP, self.BCAST_PORT))\n    except Exception as e:\n        logging.exception(f\"Error sending search request: {e}\")\n</code></pre>"},{"location":"api/core/network/externalconnection/nebuladiscoveryservice/#nebula.core.network.externalconnection.nebuladiscoveryservice.NebulaClientProtocol.stop","title":"<code>stop()</code>  <code>async</code>","text":"<p>Stop the client protocol by setting the search_done event to release any waiting tasks.</p> Source code in <code>nebula/core/network/externalconnection/nebuladiscoveryservice.py</code> <pre><code>async def stop(self):\n    \"\"\"\n    Stop the client protocol by setting the search_done event to release any waiting tasks.\n    \"\"\"\n    self.search_done.set()\n</code></pre>"},{"location":"api/core/network/externalconnection/nebuladiscoveryservice/#nebula.core.network.externalconnection.nebuladiscoveryservice.NebulaClientProtocol.wait_for_search","title":"<code>wait_for_search()</code>  <code>async</code>","text":"<p>Wait for the search phase to complete.</p> <p>This coroutine blocks until the <code>search_done</code> event is set, signaling that the search loop has finished.</p> Source code in <code>nebula/core/network/externalconnection/nebuladiscoveryservice.py</code> <pre><code>async def wait_for_search(self):\n    \"\"\"\n    Wait for the search phase to complete.\n\n    This coroutine blocks until the `search_done` event is set,\n    signaling that the search loop has finished.\n    \"\"\"\n    await self.search_done.wait()\n</code></pre>"},{"location":"api/core/network/externalconnection/nebuladiscoveryservice/#nebula.core.network.externalconnection.nebuladiscoveryservice.NebulaServerProtocol","title":"<code>NebulaServerProtocol</code>","text":"<p>               Bases: <code>DatagramProtocol</code></p> Source code in <code>nebula/core/network/externalconnection/nebuladiscoveryservice.py</code> <pre><code>class NebulaServerProtocol(asyncio.DatagramProtocol):\n    BCAST_IP = \"239.255.255.250\"\n    UPNP_PORT = 1900\n    DISCOVER_MESSAGE = \"TYPE: discover\"\n    BEACON_MESSAGE = \"TYPE: beacon\"\n\n    def __init__(self, nebula_service, addr):\n        self.nebula_service: NebulaConnectionService = nebula_service\n        self.addr = addr\n        self.transport = None\n\n    def connection_made(self, transport):\n        self.transport = transport\n        logging.info(\"Nebula UPnP server is listening...\")\n\n    def datagram_received(self, data, addr):\n        msg = data.decode(\"utf-8\")\n        if self._is_nebula_message(msg):\n            # logging.info(\"Nebula message received...\")\n            if self.DISCOVER_MESSAGE in msg:\n                logging.info(\"Discovery request received, responding...\")\n                asyncio.create_task(self.respond(addr))\n            elif self.BEACON_MESSAGE in msg:\n                asyncio.create_task(self.handle_beacon_received(msg))\n\n    async def respond(self, addr):\n        \"\"\"\n        Send a unicast HTTP-like response message to a given address.\n\n        This method is typically called when a discovery request is received.\n        It returns metadata indicating that this node is available for\n        participation in a DFL federation.\n\n        Args:\n            addr (tuple): The address (IP, port) to send the response to.\n        \"\"\"\n        try:\n            response = (\n                \"HTTP/1.1 200 OK\\r\\n\"\n                \"CACHE-CONTROL: max-age=1800\\r\\n\"\n                \"ST: urn:nebula-service\\r\\n\"\n                \"TYPE: response\\r\\n\"\n                f\"LOCATION: {self.addr}\\r\\n\"\n                \"\\r\\n\"\n            )\n            self.transport.sendto(response.encode(\"ASCII\"), addr)\n        except Exception as e:\n            logging.exception(f\"Error responding to client: {e}\")\n\n    async def handle_beacon_received(self, msg):\n        \"\"\"\n        Process a received beacon message from another node.\n\n        Extracts and parses the beacon content, validates it is not from\n        this same node, and then notifies the associated Nebula service\n        about the presence of a neighbor.\n\n        Args:\n            msg (str): The raw message string received via multicast.\n        \"\"\"\n        lines = msg.split(\"\\r\\n\")\n        beacon_data = {}\n\n        for line in lines:\n            if \": \" in line:\n                key, value = line.split(\": \", 1)\n                beacon_data[key] = value\n\n        # Verify that it is not the beacon itself\n        beacon_addr = beacon_data.get(\"LOCATION\")\n        if beacon_addr == self.addr:\n            return\n\n        latitude = float(beacon_data.get(\"LATITUDE\", 0.0))\n        longitude = float(beacon_data.get(\"LONGITUDE\", 0.0))\n        await self.nebula_service.notify_beacon_received(beacon_addr, (latitude, longitude))\n\n    def _is_nebula_message(self, msg):\n        \"\"\"\n        Determine if a message corresponds to the Nebula discovery protocol.\n\n        Args:\n            msg (str): The raw message string to evaluate.\n\n        Returns:\n            bool: True if the message follows the Nebula service format, False otherwise.\n        \"\"\"\n        return \"ST: urn:nebula-service\" in msg\n</code></pre>"},{"location":"api/core/network/externalconnection/nebuladiscoveryservice/#nebula.core.network.externalconnection.nebuladiscoveryservice.NebulaServerProtocol.handle_beacon_received","title":"<code>handle_beacon_received(msg)</code>  <code>async</code>","text":"<p>Process a received beacon message from another node.</p> <p>Extracts and parses the beacon content, validates it is not from this same node, and then notifies the associated Nebula service about the presence of a neighbor.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>The raw message string received via multicast.</p> required Source code in <code>nebula/core/network/externalconnection/nebuladiscoveryservice.py</code> <pre><code>async def handle_beacon_received(self, msg):\n    \"\"\"\n    Process a received beacon message from another node.\n\n    Extracts and parses the beacon content, validates it is not from\n    this same node, and then notifies the associated Nebula service\n    about the presence of a neighbor.\n\n    Args:\n        msg (str): The raw message string received via multicast.\n    \"\"\"\n    lines = msg.split(\"\\r\\n\")\n    beacon_data = {}\n\n    for line in lines:\n        if \": \" in line:\n            key, value = line.split(\": \", 1)\n            beacon_data[key] = value\n\n    # Verify that it is not the beacon itself\n    beacon_addr = beacon_data.get(\"LOCATION\")\n    if beacon_addr == self.addr:\n        return\n\n    latitude = float(beacon_data.get(\"LATITUDE\", 0.0))\n    longitude = float(beacon_data.get(\"LONGITUDE\", 0.0))\n    await self.nebula_service.notify_beacon_received(beacon_addr, (latitude, longitude))\n</code></pre>"},{"location":"api/core/network/externalconnection/nebuladiscoveryservice/#nebula.core.network.externalconnection.nebuladiscoveryservice.NebulaServerProtocol.respond","title":"<code>respond(addr)</code>  <code>async</code>","text":"<p>Send a unicast HTTP-like response message to a given address.</p> <p>This method is typically called when a discovery request is received. It returns metadata indicating that this node is available for participation in a DFL federation.</p> <p>Parameters:</p> Name Type Description Default <code>addr</code> <code>tuple</code> <p>The address (IP, port) to send the response to.</p> required Source code in <code>nebula/core/network/externalconnection/nebuladiscoveryservice.py</code> <pre><code>async def respond(self, addr):\n    \"\"\"\n    Send a unicast HTTP-like response message to a given address.\n\n    This method is typically called when a discovery request is received.\n    It returns metadata indicating that this node is available for\n    participation in a DFL federation.\n\n    Args:\n        addr (tuple): The address (IP, port) to send the response to.\n    \"\"\"\n    try:\n        response = (\n            \"HTTP/1.1 200 OK\\r\\n\"\n            \"CACHE-CONTROL: max-age=1800\\r\\n\"\n            \"ST: urn:nebula-service\\r\\n\"\n            \"TYPE: response\\r\\n\"\n            f\"LOCATION: {self.addr}\\r\\n\"\n            \"\\r\\n\"\n        )\n        self.transport.sendto(response.encode(\"ASCII\"), addr)\n    except Exception as e:\n        logging.exception(f\"Error responding to client: {e}\")\n</code></pre>"},{"location":"api/core/pb/","title":"Documentation for Pb Module","text":""},{"location":"api/core/pb/nebula_pb2/","title":"Documentation for Nebula_pb2 Module","text":"<p>Generated protocol buffer code.</p>"},{"location":"api/core/situationalawareness/","title":"Documentation for Situationalawareness Module","text":""},{"location":"api/core/situationalawareness/situationalawareness/","title":"Documentation for Situationalawareness Module","text":""},{"location":"api/core/situationalawareness/situationalawareness/#nebula.core.situationalawareness.situationalawareness.ISADiscovery","title":"<code>ISADiscovery</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Interface for Situational Awareness discovery components.</p> <p>Defines methods for initializing discovery, handling late connection processes, and retrieving training-related information.</p> Source code in <code>nebula/core/situationalawareness/situationalawareness.py</code> <pre><code>class ISADiscovery(ABC):\n    \"\"\"\n    Interface for Situational Awareness discovery components.\n\n    Defines methods for initializing discovery, handling late connection processes,\n    and retrieving training-related information.\n    \"\"\"\n\n    @abstractmethod\n    async def init(self, sa_reasoner):\n        \"\"\"\n        Initialize the discovery component with a corresponding reasoner.\n\n        Args:\n            sa_reasoner (ISAReasoner): The reasoner instance to coordinate with.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    async def start_late_connection_process(self, connected=False, msg_type=\"discover_join\", addrs_known=None):\n        \"\"\"\n        Begin the late-connection discovery process for situational awareness.\n\n        Args:\n            connected (bool, optional): Whether the node is already connected. Defaults to False.\n            msg_type (str, optional): Type of discovery message to send. Defaults to \"discover_join\".\n            addrs_known (list, optional): Known addresses to use instead of active discovery.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    async def get_trainning_info(self):\n        \"\"\"\n        Retrieve information necessary for training initialization.\n\n        Returns:\n            Any: Training information produced by the discovery component.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api/core/situationalawareness/situationalawareness/#nebula.core.situationalawareness.situationalawareness.ISADiscovery.get_trainning_info","title":"<code>get_trainning_info()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Retrieve information necessary for training initialization.</p> <p>Returns:</p> Name Type Description <code>Any</code> <p>Training information produced by the discovery component.</p> Source code in <code>nebula/core/situationalawareness/situationalawareness.py</code> <pre><code>@abstractmethod\nasync def get_trainning_info(self):\n    \"\"\"\n    Retrieve information necessary for training initialization.\n\n    Returns:\n        Any: Training information produced by the discovery component.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/core/situationalawareness/situationalawareness/#nebula.core.situationalawareness.situationalawareness.ISADiscovery.init","title":"<code>init(sa_reasoner)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Initialize the discovery component with a corresponding reasoner.</p> <p>Parameters:</p> Name Type Description Default <code>sa_reasoner</code> <code>ISAReasoner</code> <p>The reasoner instance to coordinate with.</p> required Source code in <code>nebula/core/situationalawareness/situationalawareness.py</code> <pre><code>@abstractmethod\nasync def init(self, sa_reasoner):\n    \"\"\"\n    Initialize the discovery component with a corresponding reasoner.\n\n    Args:\n        sa_reasoner (ISAReasoner): The reasoner instance to coordinate with.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/core/situationalawareness/situationalawareness/#nebula.core.situationalawareness.situationalawareness.ISADiscovery.start_late_connection_process","title":"<code>start_late_connection_process(connected=False, msg_type='discover_join', addrs_known=None)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Begin the late-connection discovery process for situational awareness.</p> <p>Parameters:</p> Name Type Description Default <code>connected</code> <code>bool</code> <p>Whether the node is already connected. Defaults to False.</p> <code>False</code> <code>msg_type</code> <code>str</code> <p>Type of discovery message to send. Defaults to \"discover_join\".</p> <code>'discover_join'</code> <code>addrs_known</code> <code>list</code> <p>Known addresses to use instead of active discovery.</p> <code>None</code> Source code in <code>nebula/core/situationalawareness/situationalawareness.py</code> <pre><code>@abstractmethod\nasync def start_late_connection_process(self, connected=False, msg_type=\"discover_join\", addrs_known=None):\n    \"\"\"\n    Begin the late-connection discovery process for situational awareness.\n\n    Args:\n        connected (bool, optional): Whether the node is already connected. Defaults to False.\n        msg_type (str, optional): Type of discovery message to send. Defaults to \"discover_join\".\n        addrs_known (list, optional): Known addresses to use instead of active discovery.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/core/situationalawareness/situationalawareness/#nebula.core.situationalawareness.situationalawareness.ISAReasoner","title":"<code>ISAReasoner</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Interface for Situational Awareness reasoning components.</p> <p>Defines methods for initializing the reasoner, accepting or rejecting connections, and querying known nodes and available actions.</p> Source code in <code>nebula/core/situationalawareness/situationalawareness.py</code> <pre><code>class ISAReasoner(ABC):\n    \"\"\"\n    Interface for Situational Awareness reasoning components.\n\n    Defines methods for initializing the reasoner, accepting or rejecting connections,\n    and querying known nodes and available actions.\n    \"\"\"\n\n    @abstractmethod\n    async def init(self, sa_discovery):\n        \"\"\"\n        Initialize the reasoner with a corresponding discovery component.\n\n        Args:\n            sa_discovery (ISADiscovery): The discovery instance to coordinate with.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    async def accept_connection(self, source, joining=False):\n        \"\"\"\n        Decide whether to accept a connection from a given source node.\n\n        Args:\n            source (str): The address or identifier of the requesting node.\n            joining (bool, optional): Whether the connection is part of a join process. Defaults to False.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def get_nodes_known(self, neighbors_too=False, neighbors_only=False):\n        \"\"\"\n        Get the set of nodes known to the reasoner.\n\n        Args:\n            neighbors_too (bool, optional): Include neighbors in the result. Defaults to False.\n            neighbors_only (bool, optional): Return only neighbors. Defaults to False.\n\n        Returns:\n            set: Identifiers of known nodes based on the provided filters.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def get_actions(self):\n        \"\"\"\n        Get the list of situational awareness actions the reasoner can perform in\n        response to late connections process.\n\n        Returns:\n            list: Available action identifiers.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api/core/situationalawareness/situationalawareness/#nebula.core.situationalawareness.situationalawareness.ISAReasoner.accept_connection","title":"<code>accept_connection(source, joining=False)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Decide whether to accept a connection from a given source node.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The address or identifier of the requesting node.</p> required <code>joining</code> <code>bool</code> <p>Whether the connection is part of a join process. Defaults to False.</p> <code>False</code> Source code in <code>nebula/core/situationalawareness/situationalawareness.py</code> <pre><code>@abstractmethod\nasync def accept_connection(self, source, joining=False):\n    \"\"\"\n    Decide whether to accept a connection from a given source node.\n\n    Args:\n        source (str): The address or identifier of the requesting node.\n        joining (bool, optional): Whether the connection is part of a join process. Defaults to False.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/core/situationalawareness/situationalawareness/#nebula.core.situationalawareness.situationalawareness.ISAReasoner.get_actions","title":"<code>get_actions()</code>  <code>abstractmethod</code>","text":"<p>Get the list of situational awareness actions the reasoner can perform in response to late connections process.</p> <p>Returns:</p> Name Type Description <code>list</code> <p>Available action identifiers.</p> Source code in <code>nebula/core/situationalawareness/situationalawareness.py</code> <pre><code>@abstractmethod\ndef get_actions(self):\n    \"\"\"\n    Get the list of situational awareness actions the reasoner can perform in\n    response to late connections process.\n\n    Returns:\n        list: Available action identifiers.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/core/situationalawareness/situationalawareness/#nebula.core.situationalawareness.situationalawareness.ISAReasoner.get_nodes_known","title":"<code>get_nodes_known(neighbors_too=False, neighbors_only=False)</code>  <code>abstractmethod</code>","text":"<p>Get the set of nodes known to the reasoner.</p> <p>Parameters:</p> Name Type Description Default <code>neighbors_too</code> <code>bool</code> <p>Include neighbors in the result. Defaults to False.</p> <code>False</code> <code>neighbors_only</code> <code>bool</code> <p>Return only neighbors. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>set</code> <p>Identifiers of known nodes based on the provided filters.</p> Source code in <code>nebula/core/situationalawareness/situationalawareness.py</code> <pre><code>@abstractmethod\ndef get_nodes_known(self, neighbors_too=False, neighbors_only=False):\n    \"\"\"\n    Get the set of nodes known to the reasoner.\n\n    Args:\n        neighbors_too (bool, optional): Include neighbors in the result. Defaults to False.\n        neighbors_only (bool, optional): Return only neighbors. Defaults to False.\n\n    Returns:\n        set: Identifiers of known nodes based on the provided filters.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/core/situationalawareness/situationalawareness/#nebula.core.situationalawareness.situationalawareness.ISAReasoner.init","title":"<code>init(sa_discovery)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Initialize the reasoner with a corresponding discovery component.</p> <p>Parameters:</p> Name Type Description Default <code>sa_discovery</code> <code>ISADiscovery</code> <p>The discovery instance to coordinate with.</p> required Source code in <code>nebula/core/situationalawareness/situationalawareness.py</code> <pre><code>@abstractmethod\nasync def init(self, sa_discovery):\n    \"\"\"\n    Initialize the reasoner with a corresponding discovery component.\n\n    Args:\n        sa_discovery (ISADiscovery): The discovery instance to coordinate with.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/core/situationalawareness/situationalawareness/#nebula.core.situationalawareness.situationalawareness.SituationalAwareness","title":"<code>SituationalAwareness</code>","text":"<p>High-level coordinator for Situational Awareness in the DFL federation.</p> <p>Manages discovery and reasoning components, wiring them together and exposing simple methods for initialization and late-connection handling.</p> Source code in <code>nebula/core/situationalawareness/situationalawareness.py</code> <pre><code>class SituationalAwareness:\n    \"\"\"\n    High-level coordinator for Situational Awareness in the DFL federation.\n\n    Manages discovery and reasoning components, wiring them together\n    and exposing simple methods for initialization and late-connection handling.\n    \"\"\"\n\n    def __init__(self, config, engine):\n        \"\"\"\n        Initialize Situational Awareness module by creating discovery and reasoner instances.\n\n        Args:\n            config (Config): Configuration containing situational awareness settings.\n            engine (Engine): The core engine of the federation for coordination.\n        \"\"\"\n        print_msg_box(\n            msg=\"Starting Situational Awareness module...\",\n            indent=2,\n            title=\"Situational Awareness module\",\n        )\n        self._config = config\n        selector = self._config.participant[\"situational_awareness\"][\"sa_discovery\"][\"candidate_selector\"]\n        selector = selector.lower()\n        model_handler = config.participant[\"situational_awareness\"][\"sa_discovery\"][\"model_handler\"]\n        self._sad = factory_sa_discovery(\n            \"nebula\",\n            self._config.participant[\"mobility_args\"][\"additional_node\"][\"status\"],\n            selector,\n            model_handler,\n            engine=engine,\n            verbose=config.participant[\"situational_awareness\"][\"sa_discovery\"][\"verbose\"],\n        )\n        self._sareasoner = factory_sa_reasoner(\n            \"nebula\",\n            self._config,\n        )\n\n    @property\n    def sad(self):\n        \"\"\"\n        Access the Situational Awareness discovery component.\n\n        Returns:\n            ISADiscovery: The discovery instance.\n        \"\"\"\n        return self._sad\n\n    @property\n    def sar(self):\n        \"\"\"\n        Access the Situational Awareness reasoner component.\n\n        Returns:\n            ISAReasoner: The reasoner instance.\n        \"\"\"\n        return self._sareasoner\n\n    async def init(self):\n        \"\"\"\n        Initialize both discovery and reasoner components, linking them together.\n        \"\"\"\n        await self.sad.init(self.sar)\n        await self.sar.init(self.sad)\n\n    async def start_late_connection_process(self):\n        \"\"\"\n        Start the late-connection process via the discovery component.\n        \"\"\"\n        await self.sad.start_late_connection_process()\n\n    async def get_trainning_info(self):\n        \"\"\"\n        Retrieve training information from the discovery component.\n\n        Returns:\n            Any: Information relevant to training decisions.\n        \"\"\"\n        return await self.sad.get_trainning_info()\n\n    async def stop(self):\n        \"\"\"\n        Stop both discovery and reasoner components if they implement a stop method.\n        \"\"\"\n        sad_stop = getattr(self.sad, \"stop\", None)\n        if callable(sad_stop):\n            if asyncio.iscoroutinefunction(sad_stop):\n                await sad_stop()\n            else:\n                sad_stop()\n        sar_stop = getattr(self.sar, \"stop\", None)\n        if callable(sar_stop):\n            if asyncio.iscoroutinefunction(sar_stop):\n                await sar_stop()\n            else:\n                sar_stop()\n</code></pre>"},{"location":"api/core/situationalawareness/situationalawareness/#nebula.core.situationalawareness.situationalawareness.SituationalAwareness.sad","title":"<code>sad</code>  <code>property</code>","text":"<p>Access the Situational Awareness discovery component.</p> <p>Returns:</p> Name Type Description <code>ISADiscovery</code> <p>The discovery instance.</p>"},{"location":"api/core/situationalawareness/situationalawareness/#nebula.core.situationalawareness.situationalawareness.SituationalAwareness.sar","title":"<code>sar</code>  <code>property</code>","text":"<p>Access the Situational Awareness reasoner component.</p> <p>Returns:</p> Name Type Description <code>ISAReasoner</code> <p>The reasoner instance.</p>"},{"location":"api/core/situationalawareness/situationalawareness/#nebula.core.situationalawareness.situationalawareness.SituationalAwareness.__init__","title":"<code>__init__(config, engine)</code>","text":"<p>Initialize Situational Awareness module by creating discovery and reasoner instances.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>Configuration containing situational awareness settings.</p> required <code>engine</code> <code>Engine</code> <p>The core engine of the federation for coordination.</p> required Source code in <code>nebula/core/situationalawareness/situationalawareness.py</code> <pre><code>def __init__(self, config, engine):\n    \"\"\"\n    Initialize Situational Awareness module by creating discovery and reasoner instances.\n\n    Args:\n        config (Config): Configuration containing situational awareness settings.\n        engine (Engine): The core engine of the federation for coordination.\n    \"\"\"\n    print_msg_box(\n        msg=\"Starting Situational Awareness module...\",\n        indent=2,\n        title=\"Situational Awareness module\",\n    )\n    self._config = config\n    selector = self._config.participant[\"situational_awareness\"][\"sa_discovery\"][\"candidate_selector\"]\n    selector = selector.lower()\n    model_handler = config.participant[\"situational_awareness\"][\"sa_discovery\"][\"model_handler\"]\n    self._sad = factory_sa_discovery(\n        \"nebula\",\n        self._config.participant[\"mobility_args\"][\"additional_node\"][\"status\"],\n        selector,\n        model_handler,\n        engine=engine,\n        verbose=config.participant[\"situational_awareness\"][\"sa_discovery\"][\"verbose\"],\n    )\n    self._sareasoner = factory_sa_reasoner(\n        \"nebula\",\n        self._config,\n    )\n</code></pre>"},{"location":"api/core/situationalawareness/situationalawareness/#nebula.core.situationalawareness.situationalawareness.SituationalAwareness.get_trainning_info","title":"<code>get_trainning_info()</code>  <code>async</code>","text":"<p>Retrieve training information from the discovery component.</p> <p>Returns:</p> Name Type Description <code>Any</code> <p>Information relevant to training decisions.</p> Source code in <code>nebula/core/situationalawareness/situationalawareness.py</code> <pre><code>async def get_trainning_info(self):\n    \"\"\"\n    Retrieve training information from the discovery component.\n\n    Returns:\n        Any: Information relevant to training decisions.\n    \"\"\"\n    return await self.sad.get_trainning_info()\n</code></pre>"},{"location":"api/core/situationalawareness/situationalawareness/#nebula.core.situationalawareness.situationalawareness.SituationalAwareness.init","title":"<code>init()</code>  <code>async</code>","text":"<p>Initialize both discovery and reasoner components, linking them together.</p> Source code in <code>nebula/core/situationalawareness/situationalawareness.py</code> <pre><code>async def init(self):\n    \"\"\"\n    Initialize both discovery and reasoner components, linking them together.\n    \"\"\"\n    await self.sad.init(self.sar)\n    await self.sar.init(self.sad)\n</code></pre>"},{"location":"api/core/situationalawareness/situationalawareness/#nebula.core.situationalawareness.situationalawareness.SituationalAwareness.start_late_connection_process","title":"<code>start_late_connection_process()</code>  <code>async</code>","text":"<p>Start the late-connection process via the discovery component.</p> Source code in <code>nebula/core/situationalawareness/situationalawareness.py</code> <pre><code>async def start_late_connection_process(self):\n    \"\"\"\n    Start the late-connection process via the discovery component.\n    \"\"\"\n    await self.sad.start_late_connection_process()\n</code></pre>"},{"location":"api/core/situationalawareness/situationalawareness/#nebula.core.situationalawareness.situationalawareness.SituationalAwareness.stop","title":"<code>stop()</code>  <code>async</code>","text":"<p>Stop both discovery and reasoner components if they implement a stop method.</p> Source code in <code>nebula/core/situationalawareness/situationalawareness.py</code> <pre><code>async def stop(self):\n    \"\"\"\n    Stop both discovery and reasoner components if they implement a stop method.\n    \"\"\"\n    sad_stop = getattr(self.sad, \"stop\", None)\n    if callable(sad_stop):\n        if asyncio.iscoroutinefunction(sad_stop):\n            await sad_stop()\n        else:\n            sad_stop()\n    sar_stop = getattr(self.sar, \"stop\", None)\n    if callable(sar_stop):\n        if asyncio.iscoroutinefunction(sar_stop):\n            await sar_stop()\n        else:\n            sar_stop()\n</code></pre>"},{"location":"api/core/situationalawareness/situationalawareness/#nebula.core.situationalawareness.situationalawareness.factory_sa_discovery","title":"<code>factory_sa_discovery(sa_discovery, additional, selector, model_handler, engine, verbose)</code>","text":"<p>Factory function to create an ISADiscovery implementation.</p> <p>Parameters:</p> Name Type Description Default <code>sa_discovery</code> <code>str</code> <p>Identifier of the discovery backend (e.g., \"nebula\").</p> required <code>additional</code> <code>bool</code> <p>Additional status of the node.</p> required <code>selector</code> <code>str</code> <p>Candidate selector strategy name.</p> required <code>model_handler</code> <code>str</code> <p>Model handler strategy name.</p> required <code>engine</code> <code>Engine</code> <p>Reference to the engine.</p> required <code>verbose</code> <code>bool</code> <p>Enable verbose logging or output.</p> required <p>Returns:</p> Name Type Description <code>ISADiscovery</code> <code>ISADiscovery</code> <p>An instance of the requested discovery implementation.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the specified discovery service identifier is not found.</p> Source code in <code>nebula/core/situationalawareness/situationalawareness.py</code> <pre><code>def factory_sa_discovery(sa_discovery, additional, selector, model_handler, engine, verbose) -&gt; ISADiscovery:\n    \"\"\"\n    Factory function to create an ISADiscovery implementation.\n\n    Args:\n        sa_discovery (str): Identifier of the discovery backend (e.g., \"nebula\").\n        additional (bool): Additional status of the node.\n        selector (str): Candidate selector strategy name.\n        model_handler (str): Model handler strategy name.\n        engine (Engine): Reference to the engine.\n        verbose (bool): Enable verbose logging or output.\n\n    Returns:\n        ISADiscovery: An instance of the requested discovery implementation.\n\n    Raises:\n        Exception: If the specified discovery service identifier is not found.\n    \"\"\"\n    from nebula.core.situationalawareness.discovery.federationconnector import FederationConnector\n\n    DISCOVERY = {\n        \"nebula\": FederationConnector,\n    }\n    sad = DISCOVERY.get(sa_discovery)\n    if sad:\n        return sad(additional, selector, model_handler, engine, verbose)\n    else:\n        raise Exception(f\"SA Discovery service {sa_discovery} not found.\")\n</code></pre>"},{"location":"api/core/situationalawareness/situationalawareness/#nebula.core.situationalawareness.situationalawareness.factory_sa_reasoner","title":"<code>factory_sa_reasoner(sa_reasoner, config)</code>","text":"<p>Factory function to create an ISAReasoner implementation.</p> <p>Parameters:</p> Name Type Description Default <code>sa_reasoner</code> <code>str</code> <p>Identifier of the reasoner backend (e.g., \"nebula\").</p> required <code>config</code> <code>Config</code> <p>The configuration object for initializing the reasoner.</p> required <p>Returns:</p> Name Type Description <code>ISAReasoner</code> <code>ISAReasoner</code> <p>An instance of the requested reasoner implementation.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the specified reasoner service identifier is not found.</p> Source code in <code>nebula/core/situationalawareness/situationalawareness.py</code> <pre><code>def factory_sa_reasoner(sa_reasoner, config) -&gt; ISAReasoner:\n    \"\"\"\n    Factory function to create an ISAReasoner implementation.\n\n    Args:\n        sa_reasoner (str): Identifier of the reasoner backend (e.g., \"nebula\").\n        config (Config): The configuration object for initializing the reasoner.\n\n    Returns:\n        ISAReasoner: An instance of the requested reasoner implementation.\n\n    Raises:\n        Exception: If the specified reasoner service identifier is not found.\n    \"\"\"\n    from nebula.core.situationalawareness.awareness.sareasoner import SAReasoner\n\n    REASONER = {\n        \"nebula\": SAReasoner,\n    }\n    sar = REASONER.get(sa_reasoner)\n    if sar:\n        return sar(config)\n    else:\n        raise Exception(f\"SA Reasoner service {sa_reasoner} not found.\")\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/","title":"Documentation for Awareness Module","text":""},{"location":"api/core/situationalawareness/awareness/sareasoner/","title":"Documentation for Sareasoner Module","text":""},{"location":"api/core/situationalawareness/awareness/sareasoner/#nebula.core.situationalawareness.awareness.sareasoner.SAMComponent","title":"<code>SAMComponent</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class representing a Situational Awareness Module Component (SAMComponent).</p> <p>Each SAMComponent is responsible for analyzing specific aspects of the system's state and proposing relevant actions. These components act as internal reasoning units within the SAReasoner and contribute suggestions to the command arbitration process.</p> <p>Methods: - init(): Initialize internal state and resources required by the component. - sa_component_actions(): Generate and return actions based on local analysis.</p> Source code in <code>nebula/core/situationalawareness/awareness/sareasoner.py</code> <pre><code>class SAMComponent(ABC):\n    \"\"\"\n    Abstract base class representing a Situational Awareness Module Component (SAMComponent).\n\n    Each SAMComponent is responsible for analyzing specific aspects of the system's state and\n    proposing relevant actions. These components act as internal reasoning units within the\n    SAReasoner and contribute suggestions to the command arbitration process.\n\n    Methods:\n    - init(): Initialize internal state and resources required by the component.\n    - sa_component_actions(): Generate and return actions based on local analysis.\n    \"\"\"\n\n    @abstractmethod\n    async def init(self):\n        \"\"\"\n        Initialize the SAMComponent.\n\n        This method should prepare any internal state, models, or resources required\n        before the component starts analyzing and proposing actions.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    async def sa_component_actions(self):\n        \"\"\"\n        Analyze system state and generate a list of SACommand suggestions.\n        It uses the SuggestionBuffer to send a list of SACommands.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sareasoner/#nebula.core.situationalawareness.awareness.sareasoner.SAMComponent.init","title":"<code>init()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Initialize the SAMComponent.</p> <p>This method should prepare any internal state, models, or resources required before the component starts analyzing and proposing actions.</p> Source code in <code>nebula/core/situationalawareness/awareness/sareasoner.py</code> <pre><code>@abstractmethod\nasync def init(self):\n    \"\"\"\n    Initialize the SAMComponent.\n\n    This method should prepare any internal state, models, or resources required\n    before the component starts analyzing and proposing actions.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sareasoner/#nebula.core.situationalawareness.awareness.sareasoner.SAMComponent.sa_component_actions","title":"<code>sa_component_actions()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Analyze system state and generate a list of SACommand suggestions. It uses the SuggestionBuffer to send a list of SACommands.</p> Source code in <code>nebula/core/situationalawareness/awareness/sareasoner.py</code> <pre><code>@abstractmethod\nasync def sa_component_actions(self):\n    \"\"\"\n    Analyze system state and generate a list of SACommand suggestions.\n    It uses the SuggestionBuffer to send a list of SACommands.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sareasoner/#nebula.core.situationalawareness.awareness.sareasoner.SAReasoner","title":"<code>SAReasoner</code>","text":"<p>               Bases: <code>ISAReasoner</code></p> <p>Core implementation of the Situational Awareness Reasoner (SAReasoner).</p> <p>This class coordinates the lifecycle and interactions of all internal components in the SA module, including SAMComponents (reasoning units), the suggestion buffer, and the arbitration policy. It is responsible for:</p> <ul> <li>Initializing and managing all registered SAMComponents.</li> <li>Collecting suggestions from each component in response to events.</li> <li>Registering and notifying the SuggestionBuffer of suggestions.</li> <li>Triggering arbitration when multiple conflicting commands are proposed.</li> <li>Interfacing with the wider system through the ISAReasoner interface.</li> </ul> <p>This class acts as the central controller for decision-making based on local or global awareness in distributed systems.</p> Source code in <code>nebula/core/situationalawareness/awareness/sareasoner.py</code> <pre><code>class SAReasoner(ISAReasoner):\n    \"\"\"\n    Core implementation of the Situational Awareness Reasoner (SAReasoner).\n\n    This class coordinates the lifecycle and interactions of all internal components\n    in the SA module, including SAMComponents (reasoning units), the suggestion buffer,\n    and the arbitration policy. It is responsible for:\n\n    - Initializing and managing all registered SAMComponents.\n    - Collecting suggestions from each component in response to events.\n    - Registering and notifying the SuggestionBuffer of suggestions.\n    - Triggering arbitration when multiple conflicting commands are proposed.\n    - Interfacing with the wider system through the ISAReasoner interface.\n\n    This class acts as the central controller for decision-making based on local\n    or global awareness in distributed systems.\n    \"\"\"\n\n    MODULE_PATH = \"nebula/nebula/core/situationalawareness/awareness\"\n\n    def __init__(\n        self,\n        config,\n    ):\n        print_msg_box(\n            msg=\"Starting Situational Awareness Reasoner module...\",\n            indent=2,\n            title=\"SA Reasoner\",\n        )\n        logging.info(\"\ud83c\udf10  Initializing SAReasoner\")\n        self._config = copy.deepcopy(config.participant)\n        self._addr = config.participant[\"network_args\"][\"addr\"]\n        self._topology = config.participant[\"mobility_args\"][\"topology_type\"]\n        self._situational_awareness_network: SANetwork | None = None\n        self._situational_awareness_training = None\n        self._restructure_process_lock = Locker(name=\"restructure_process_lock\", async_lock=True)\n        self._restructure_cooldown = 0\n        self._arbitrator_notification = asyncio.Event()\n        self._suggestion_buffer = SuggestionBuffer(self._arbitrator_notification, verbose=True)\n        self._communciation_manager = CommunicationsManager.get_instance()\n        self._sys_monitor = SystemMonitor()\n        arb_pol = config.participant[\"situational_awareness\"][\"sa_reasoner\"][\"arbitration_policy\"]\n        self._arbitatrion_policy = factory_arbitration_policy(arb_pol, True)\n        self._sa_components: dict[str, SAMComponent] = {}\n        self._sa_discovery: ISADiscovery | None = None\n        self._verbose = config.participant[\"situational_awareness\"][\"sa_reasoner\"][\"verbose\"]\n\n    @property\n    def san(self) -&gt; SANetwork | None:\n        \"\"\"Situational Awareness Network\"\"\"\n        return self._situational_awareness_network\n\n    @property\n    def cm(self):\n        \"\"\"Communicaiton Manager\"\"\"\n        return self._communciation_manager\n\n    @property\n    def sb(self):\n        \"\"\"Suggestion Buffer\"\"\"\n        return self._suggestion_buffer\n\n    @property\n    def ab(self):\n        \"\"\"Arbitatrion Policy\"\"\"\n        return self._arbitatrion_policy\n\n    @property\n    def sad(self) -&gt; ISADiscovery | None:\n        \"\"\"SA Discovery\"\"\"\n        return self._sa_discovery\n\n    async def init(self, sa_discovery):\n        \"\"\"\n        Initialize the SAReasoner by loading components and subscribing to relevant events.\n\n        Args:\n            sa_discovery (ISADiscovery): The discovery component to coordinate with.\n        \"\"\"\n        self._sa_discovery = sa_discovery\n        await self._loading_sa_components()\n        await EventManager.get_instance().subscribe_node_event(RoundEndEvent, self._process_round_end_event)\n        await EventManager.get_instance().subscribe_node_event(AggregationEvent, self._process_aggregation_event)\n\n    def is_additional_participant(self):\n        \"\"\"\n        Determine if this node is configured as an additional (mobile) participant.\n\n        Returns:\n            bool: True if the node is marked as an additional participant, False otherwise.\n        \"\"\"\n        return self._config[\"mobility_args\"][\"additional_node\"][\"status\"]\n\n    \"\"\"                                                     ###############################\n                                                            #    REESTRUCTURE TOPOLOGY    #\n                                                            ###############################\n    \"\"\"\n\n    def get_restructure_process_lock(self):\n        if self.san is None:\n            raise RuntimeError(\"Situational Awareness Network (san) is not initialized.\")\n        return self.san.get_restructure_process_lock()\n\n    \"\"\"                                                     ###############################\n                                                            #          SA NETWORK         #\n                                                            ###############################\n    \"\"\"\n\n    async def get_nodes_known(self, neighbors_too=False, neighbors_only=False):\n        if self.san is None:\n            raise RuntimeError(\"Situational Awareness Network (san) is not initialized.\")\n        return await self.san.get_nodes_known(neighbors_too, neighbors_only)\n\n    async def accept_connection(self, source, joining=False):\n        if self.san is None:\n            raise RuntimeError(\"Situational Awareness Network (san) is not initialized.\")\n        return await self.san.accept_connection(source, joining)\n\n    async def get_actions(self):\n        if self.san is None:\n            raise RuntimeError(\"Situational Awareness Network (san) is not initialized.\")\n        return await self.san.get_actions()\n\n    \"\"\"                                                     ###############################\n                                                            #         ARBITRATION         #\n                                                            ###############################\n    \"\"\"\n\n    async def _process_round_end_event(self, ree: RoundEndEvent):\n        \"\"\"\n        Handle the end of a federated learning round by gathering situational awareness actions\n        and executing arbitration commands.\n\n        1. Trigger each SA component to propose actions asynchronously.\n        2. Run arbitration to select valid SACommand instances.\n        3. Execute parallelizable commands concurrently and sequential commands one by one.\n\n        Args:\n            ree (RoundEndEvent): The event signaling the end of the current training round.\n        \"\"\"\n        logging.info(\"\ud83d\udd04 Arbitration | Round End Event...\")\n        for sa_comp in self._sa_components.values():\n            asyncio.create_task(sa_comp.sa_component_actions())\n        valid_commands = await self._arbitatrion_suggestions(RoundEndEvent)\n\n        # Execute SACommand selected\n        if self._verbose:\n            logging.info(f\"Going to execute {len(valid_commands)} SACommands\")\n        for cmd in valid_commands:\n            if cmd.is_parallelizable():\n                if self._verbose:\n                    logging.info(\n                        f\"going to execute parallelizable action: {cmd.get_action()} made by: {await cmd.get_owner()}\"\n                    )\n                asyncio.create_task(cmd.execute())\n            else:\n                if self._verbose:\n                    logging.info(f\"going to execute action: {cmd.get_action()} made by: {await cmd.get_owner()}\")\n                await cmd.execute()\n\n    async def _process_aggregation_event(self, age: AggregationEvent):\n        \"\"\"\n        Handle an aggregation event by selecting and executing an SACommand to adjust aggregation behavior.\n\n        1. Run arbitration to retrieve suggestions specific to aggregation.\n        2. If any commands are returned, pick the first one.\n        3. Execute the chosen command and apply its resulting updates to the aggregation event.\n\n        Args:\n            age (AggregationEvent): The event containing updates ready for federation aggregation.\n        \"\"\"\n        logging.info(\"\ud83d\udd04 Arbitration | Aggregation Event...\")\n        aggregation_command = await self._arbitatrion_suggestions(AggregationEvent)\n        if len(aggregation_command):\n            if self._verbose:\n                logging.info(\n                    f\"Aggregation event resolved. SA Agente that suggest action: {await aggregation_command[0].get_owner}\"\n                )\n            final_updates = await aggregation_command[0].execute()\n            age.update_updates(final_updates)\n\n    async def _arbitatrion_suggestions(self, event_type):\n        \"\"\"\n        Perform arbitration over a set of agent suggestions for a given event type.\n\n        This method waits for all suggestions to be submitted, detects and resolves\n        conflicts based on command priorities and optional tie-breaking, and\n        returns a list of valid, non-conflicting commands.\n\n        Parameters:\n            event_type: The identifier or type of the event for which suggestions are being arbitrated.\n\n        Returns:\n            list[SACommand]: A list of validated and conflict-free commands after arbitration.\n        \"\"\"\n        if self._verbose:\n            logging.info(\"Waiting for all suggestions done\")\n        await self.sb.set_event_waited(event_type)\n        await self._arbitrator_notification.wait()\n        if self._verbose:\n            logging.info(\"waiting released\")\n        suggestions = await self.sb.get_suggestions(event_type)\n        self._arbitrator_notification.clear()\n        if not len(suggestions):\n            if self._verbose:\n                logging.info(\"No suggestions for this event | Arbitatrion not required\")\n            return []\n\n        if self._verbose:\n            logging.info(f\"Starting arbitatrion | Number of suggestions received: {len(suggestions)}\")\n\n        valid_commands: list[SACommand] = []\n\n        for agent, cmd in suggestions:\n            has_conflict = False\n            to_remove: list[SACommand] = []\n\n            for other in valid_commands:\n                if await cmd.conflicts_with(other):\n                    if self._verbose:\n                        logging.info(\n                            f\"Conflict detected between -- {await cmd.get_owner()} and {await other.get_owner()} --\"\n                        )\n                    if self._verbose:\n                        logging.info(f\"Action in conflict ({cmd.get_action()}, {other.get_action()})\")\n                    if cmd.got_higher_priority_than(other.get_prio()):\n                        to_remove.append(other)\n                    elif cmd.get_prio() == other.get_prio():\n                        if await self.ab.tie_break(cmd, other):\n                            to_remove.append(other)\n                        else:\n                            has_conflict = True\n                            break\n                    else:\n                        has_conflict = True\n                        break\n\n            if not has_conflict:\n                for r in to_remove:\n                    await r.discard_command()\n                    valid_commands.remove(r)\n                valid_commands.append(cmd)\n\n        logging.info(\"Arbitatrion finished\")\n        return valid_commands\n\n    \"\"\"                                                     ###############################\n                                                            #    SA COMPONENT LOADING     #\n                                                            ###############################\n    \"\"\"\n\n    def _to_pascal_case(self, name: str) -&gt; str:\n        \"\"\"Converts a snake_case or compact lowercase name into PascalCase with 'SA' prefix.\"\"\"\n        if name.startswith(\"sa_\"):\n            name = name[3:]  # remove 'sa_' prefix\n        elif name.startswith(\"sa\"):\n            name = name[2:]  # remove 'sa' prefix\n        parts = name.split(\"_\") if \"_\" in name else [name]\n        return \"SA\" + \"\".join(part.capitalize() for part in parts)\n\n    async def _loading_sa_components(self):\n        \"\"\"Dynamically loads the SA Components defined in the JSON configuration.\"\"\"\n        self._load_minimal_requirement_config()\n        sa_section = self._config[\"situational_awareness\"][\"sa_reasoner\"]\n        components: dict = sa_section[\"sar_components\"]\n\n        for component_name, is_enabled in components.items():\n            if is_enabled:\n                component_config = sa_section[component_name]\n                component_name = component_name.replace(\"_\", \"\")\n                class_name = self._to_pascal_case(component_name)\n                module_path = os.path.join(self.MODULE_PATH, component_name)\n                module_file = os.path.join(module_path, f\"{component_name}.py\")\n\n                if os.path.exists(module_file):\n                    module = await self._load_component(class_name, module_file, component_config)\n                    if module:\n                        self._sa_components[component_name] = module\n                else:\n                    logging.error(f\"\u26a0\ufe0f SA Component {component_name} not found on {module_file}\")\n\n        await self._set_minimal_requirements()\n        await self._initialize_sa_components()\n\n    async def _load_component(self, class_name, component_file, config):\n        \"\"\"Loads a SA Component dynamically and initializes it with its configuration.\"\"\"\n        spec = importlib.util.spec_from_file_location(class_name, component_file)\n        if spec and spec.loader:\n            component = importlib.util.module_from_spec(spec)\n            spec.loader.exec_module(component)\n            if hasattr(component, class_name):  # Verify if class exists\n                return getattr(component, class_name)(config)  # Create and instance using component config\n            else:\n                logging.error(f\"\u26a0\ufe0f Cannot create {class_name} SA Component, class not found on {component_file}\")\n        return None\n\n    async def _initialize_sa_components(self):\n        if self._sa_components:\n            for sacomp in self._sa_components.values():\n                await sacomp.init()\n\n    def _load_minimal_requirement_config(self):\n        # self._config[\"situational_awareness\"][\"sa_reasoner\"][\"sa_network\"][\"addr\"] = self._addr\n        # self._config[\"situational_awareness\"][\"sa_reasoner\"][\"sa_network\"][\"sar\"] = self\n        self._config[\"situational_awareness\"][\"sa_reasoner\"][\"sa_network\"][\"strict_topology\"] = self._config[\n            \"situational_awareness\"\n        ][\"strict_topology\"]\n\n        # SA Reasoner instance for all SA Reasoner Components\n        sar_components: dict = self._config[\"situational_awareness\"][\"sa_reasoner\"][\"sar_components\"]\n        for sar_comp in sar_components:\n            self._config[\"situational_awareness\"][\"sa_reasoner\"][sar_comp][\"sar\"] = self\n            self._config[\"situational_awareness\"][\"sa_reasoner\"][sar_comp][\"addr\"] = self._addr\n\n    async def _set_minimal_requirements(self):\n        \"\"\"Set minimal requirements to setup the SA Reasoner\"\"\"\n        if self._sa_components:\n            self._situational_awareness_network = self._sa_components[\"sanetwork\"]\n        else:\n            raise ValueError(\"SA Network not found\")\n\n    async def stop(self):\n        \"\"\"\n        Stop the SAReasoner by stopping all SA components and clearing any pending operations.\n        \"\"\"\n        logging.info(\"\ud83d\uded1  Stopping SAReasoner...\")\n        self._arbitrator_notification.set()\n\n        # Stop all SA components\n        if self._sa_components:\n            for component_name, component in self._sa_components.items():\n                try:\n                    # Check if component has a stop method\n                    stop_method = getattr(component, \"stop\", None)\n                    if stop_method and callable(stop_method):\n                        if asyncio.iscoroutinefunction(stop_method):\n                            await stop_method()\n                        else:\n                            stop_method()\n                        logging.info(f\"\u2705  Stopped SA component: {component_name}\")\n                except Exception as e:\n                    logging.warning(f\"Error stopping SA component {component_name}: {e}\")\n\n        logging.info(\"\u2705  SAReasoner stopped successfully\")\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sareasoner/#nebula.core.situationalawareness.awareness.sareasoner.SAReasoner.ab","title":"<code>ab</code>  <code>property</code>","text":"<p>Arbitatrion Policy</p>"},{"location":"api/core/situationalawareness/awareness/sareasoner/#nebula.core.situationalawareness.awareness.sareasoner.SAReasoner.cm","title":"<code>cm</code>  <code>property</code>","text":"<p>Communicaiton Manager</p>"},{"location":"api/core/situationalawareness/awareness/sareasoner/#nebula.core.situationalawareness.awareness.sareasoner.SAReasoner.sad","title":"<code>sad</code>  <code>property</code>","text":"<p>SA Discovery</p>"},{"location":"api/core/situationalawareness/awareness/sareasoner/#nebula.core.situationalawareness.awareness.sareasoner.SAReasoner.san","title":"<code>san</code>  <code>property</code>","text":"<p>Situational Awareness Network</p>"},{"location":"api/core/situationalawareness/awareness/sareasoner/#nebula.core.situationalawareness.awareness.sareasoner.SAReasoner.sb","title":"<code>sb</code>  <code>property</code>","text":"<p>Suggestion Buffer</p>"},{"location":"api/core/situationalawareness/awareness/sareasoner/#nebula.core.situationalawareness.awareness.sareasoner.SAReasoner.init","title":"<code>init(sa_discovery)</code>  <code>async</code>","text":"<p>Initialize the SAReasoner by loading components and subscribing to relevant events.</p> <p>Parameters:</p> Name Type Description Default <code>sa_discovery</code> <code>ISADiscovery</code> <p>The discovery component to coordinate with.</p> required Source code in <code>nebula/core/situationalawareness/awareness/sareasoner.py</code> <pre><code>async def init(self, sa_discovery):\n    \"\"\"\n    Initialize the SAReasoner by loading components and subscribing to relevant events.\n\n    Args:\n        sa_discovery (ISADiscovery): The discovery component to coordinate with.\n    \"\"\"\n    self._sa_discovery = sa_discovery\n    await self._loading_sa_components()\n    await EventManager.get_instance().subscribe_node_event(RoundEndEvent, self._process_round_end_event)\n    await EventManager.get_instance().subscribe_node_event(AggregationEvent, self._process_aggregation_event)\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sareasoner/#nebula.core.situationalawareness.awareness.sareasoner.SAReasoner.is_additional_participant","title":"<code>is_additional_participant()</code>","text":"<p>Determine if this node is configured as an additional (mobile) participant.</p> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the node is marked as an additional participant, False otherwise.</p> Source code in <code>nebula/core/situationalawareness/awareness/sareasoner.py</code> <pre><code>def is_additional_participant(self):\n    \"\"\"\n    Determine if this node is configured as an additional (mobile) participant.\n\n    Returns:\n        bool: True if the node is marked as an additional participant, False otherwise.\n    \"\"\"\n    return self._config[\"mobility_args\"][\"additional_node\"][\"status\"]\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sareasoner/#nebula.core.situationalawareness.awareness.sareasoner.SAReasoner.stop","title":"<code>stop()</code>  <code>async</code>","text":"<p>Stop the SAReasoner by stopping all SA components and clearing any pending operations.</p> Source code in <code>nebula/core/situationalawareness/awareness/sareasoner.py</code> <pre><code>async def stop(self):\n    \"\"\"\n    Stop the SAReasoner by stopping all SA components and clearing any pending operations.\n    \"\"\"\n    logging.info(\"\ud83d\uded1  Stopping SAReasoner...\")\n    self._arbitrator_notification.set()\n\n    # Stop all SA components\n    if self._sa_components:\n        for component_name, component in self._sa_components.items():\n            try:\n                # Check if component has a stop method\n                stop_method = getattr(component, \"stop\", None)\n                if stop_method and callable(stop_method):\n                    if asyncio.iscoroutinefunction(stop_method):\n                        await stop_method()\n                    else:\n                        stop_method()\n                    logging.info(f\"\u2705  Stopped SA component: {component_name}\")\n            except Exception as e:\n                logging.warning(f\"Error stopping SA component {component_name}: {e}\")\n\n    logging.info(\"\u2705  SAReasoner stopped successfully\")\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/suggestionbuffer/","title":"Documentation for Suggestionbuffer Module","text":""},{"location":"api/core/situationalawareness/awareness/suggestionbuffer/#nebula.core.situationalawareness.awareness.suggestionbuffer.SuggestionBuffer","title":"<code>SuggestionBuffer</code>","text":"<p>Singleton class that manages the coordination of suggestions from Situational Awareness (SA) agents.</p> <p>The SuggestionBuffer stores, synchronizes, and tracks command suggestions issued by agents in response to specific node events. It ensures that all expected agents have submitted their input before triggering arbitration. Internally, it maintains buffers for suggestions, synchronization locks, and agent-specific notifications to guarantee consistency in distributed settings.</p> <p>Main Responsibilities: - Register expected agents for an event and track their completion. - Store and retrieve suggestions for arbitration. - Signal the arbitrator once all expected suggestions have been received. - Support safe concurrent access through async-aware locking mechanisms.</p> Source code in <code>nebula/core/situationalawareness/awareness/suggestionbuffer.py</code> <pre><code>class SuggestionBuffer:\n    \"\"\"\n    Singleton class that manages the coordination of suggestions from Situational Awareness (SA) agents.\n\n    The SuggestionBuffer stores, synchronizes, and tracks command suggestions issued by agents in\n    response to specific node events. It ensures that all expected agents have submitted their input\n    before triggering arbitration. Internally, it maintains buffers for suggestions, synchronization\n    locks, and agent-specific notifications to guarantee consistency in distributed settings.\n\n    Main Responsibilities:\n    - Register expected agents for an event and track their completion.\n    - Store and retrieve suggestions for arbitration.\n    - Signal the arbitrator once all expected suggestions have been received.\n    - Support safe concurrent access through async-aware locking mechanisms.\n    \"\"\"\n\n    _instance = None\n    _lock = Locker(\"initialize_sb_lock\", async_lock=False)\n\n    def __new__(cls, arbitrator_notification, verbose):\n        with cls._lock:\n            if cls._instance is None:\n                cls._instance = super().__new__(cls)\n        return cls._instance\n\n    @classmethod\n    def get_instance(cls):\n        \"\"\"Obtain SuggestionBuffer instance\"\"\"\n        if cls._instance is None:\n            raise ValueError(\"SuggestionBuffer has not been initialized yet.\")\n        return cls._instance\n\n    def __init__(self, arbitrator_notification: asyncio.Event, verbose):\n        \"\"\"Initializes the suggestion buffer with thread-safe synchronization.\"\"\"\n        self._arbitrator_notification = arbitrator_notification\n        self._arbitrator_notification_lock = Locker(\"arbitrator_notification_lock\", async_lock=True)\n        self._verbose = verbose\n        self._buffer: dict[type[NodeEvent], list[tuple[SAModuleAgent, SACommand]]] = defaultdict(list)\n        self._suggestion_buffer_lock = Locker(\"suggestion_buffer_lock\", async_lock=True)\n        self._expected_agents: dict[type[NodeEvent], list[SAModuleAgent]] = defaultdict(list)\n        self._expected_agents_lock = Locker(\"expected_agents_lock\", async_lock=True)\n        self._event_notifications: dict[type[NodeEvent], list[tuple[SAModuleAgent, asyncio.Event]]] = defaultdict(list)\n        self._event_waited = None\n\n    async def register_event_agents(self, event_type, agent: SAModuleAgent):\n        \"\"\"\n        Register a Situational Awareness (SA) agent as an expected participant for a given event type.\n\n        Parameters:\n            event_type (Type[NodeEvent]): The type of event being registered.\n            agent (SAModuleAgent): The agent expected to submit suggestions for the event.\n        \"\"\"\n        async with self._expected_agents_lock:\n            if self._verbose:\n                logging.info(f\"Registering SA Agent: {await agent.get_agent()} for event: {event_type.__name__}\")\n\n            if event_type not in self._event_notifications:\n                self._event_notifications[event_type] = []\n\n            self._expected_agents[event_type].append(agent)\n\n            existing_agents = {a for a, _ in self._event_notifications[event_type]}\n            if agent not in existing_agents:\n                self._event_notifications[event_type].append((agent, asyncio.Event()))\n\n    async def register_suggestion(self, event_type, agent: SAModuleAgent, suggestion: SACommand):\n        \"\"\"\n        Register a suggestion issued by a specific SA agent for a given event.\n\n        Parameters:\n            event_type (Type[NodeEvent]): The event type for which the suggestion is made.\n            agent (SAModuleAgent): The agent submitting the suggestion.\n            suggestion (SACommand): The command being suggested.\n        \"\"\"\n        async with self._suggestion_buffer_lock:\n            if self._verbose:\n                logging.info(\n                    f\"Registering Suggestion from SA Agent: {await agent.get_agent()} for event: {event_type.__name__}\"\n                )\n            self._buffer[event_type].append((agent, suggestion))\n\n    async def set_event_waited(self, event_type):\n        \"\"\"\n        Set the event type that the SuggestionBuffer will wait for.\n\n        Used to indicate that arbitration should proceed when all suggestions for this event are received.\n\n        Parameters:\n            event_type (Type[NodeEvent]): The event type to monitor.\n        \"\"\"\n        if not self._event_waited:\n            if self._verbose:\n                logging.info(\n                    f\"Set notification when all suggestions have being received for event: {event_type.__name__}\"\n                )\n            self._event_waited = event_type\n            await self._notify_arbitrator(event_type)\n\n    async def notify_all_suggestions_done_for_agent(self, saa: SAModuleAgent, event_type):\n        \"\"\"\n        Notify that a specific SA agent has completed its suggestion submission for an event.\n\n        Parameters:\n            saa (SAModuleAgent): The notifying agent.\n            event_type (Type[NodeEvent]): The related event type.\n        \"\"\"\n        async with self._expected_agents_lock:\n            agent_found = False\n            for agent, event in self._event_notifications.get(event_type, []):\n                if agent == saa:\n                    event.set()\n                    agent_found = True\n                    if self._verbose:\n                        logging.info(\n                            f\"SA Agent: {await saa.get_agent()} notifies all suggestions registered for event: {event_type.__name__}\"\n                        )\n                    break\n            if not agent_found and self._verbose:\n                logging.error(\n                    f\"SAModuleAgent: {await saa.get_agent()} not found on notifications awaited for event {event_type.__name__}\"\n                )\n        await self._notify_arbitrator(event_type)\n\n    async def _notify_arbitrator(self, event_type):\n        \"\"\"\n        Check if all expected agents have submitted their suggestions for the current awaited event.\n\n        If so, notifies the arbitrator via the provided asyncio event.\n        \"\"\"\n        if event_type != self._event_waited:\n            return\n\n        async with self._arbitrator_notification_lock:\n            async with self._expected_agents_lock:\n                expected_agents = self._expected_agents.get(event_type, [])\n                notifications = self._event_notifications.get(event_type, list())\n\n                agent_event_map = {a: e for a, e in notifications}\n                all_received = all(\n                    agent in agent_event_map and agent_event_map[agent].is_set() for agent in expected_agents\n                )\n\n                if all_received:\n                    self._arbitrator_notification.set()\n                    self._event_waited = None\n                    await self._reset_notifications_for_agents(event_type, expected_agents)\n\n    async def _reset_notifications_for_agents(self, event_type, agents):\n        \"\"\"\n        Reset all notification events for the given agents tied to a specific event.\n\n        Parameters:\n            event_type (Type[NodeEvent]): The event for which to reset agent notifications.\n            agents (list[SAModuleAgent]): The list of agents to reset.\n        \"\"\"\n        notifications = self._event_notifications.get(event_type, set())\n        for agent, event in notifications:\n            if agent in agents:\n                event.clear()\n\n    async def get_suggestions(self, event_type) -&gt; list[tuple[SAModuleAgent, SACommand]]:\n        \"\"\"\n        Retrieve and return all suggestions for a given event type.\n\n        Also clears the buffer after reading.\n\n        Parameters:\n            event_type (Type[NodeEvent]): The event whose suggestions are requested.\n\n        Returns:\n            list[tuple[SAModuleAgent, SACommand]]: List of (agent, suggestion) pairs.\n        \"\"\"\n        async with self._suggestion_buffer_lock:\n            async with self._expected_agents_lock:\n                suggestions = list(self._buffer.get(event_type, []))\n                if self._verbose:\n                    logging.info(f\"Retrieving all sugestions for event: {event_type.__name__}\")\n                await self._clear_suggestions(event_type)\n                return suggestions\n\n    async def _clear_suggestions(self, event_type):\n        \"\"\"\n        Clear the buffer and associated data for a specific event type.\n\n        Parameters:\n            event_type (Type[NodeEvent]): The event whose stored suggestions are to be removed.\n        \"\"\"\n        self._buffer[event_type].clear()\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/suggestionbuffer/#nebula.core.situationalawareness.awareness.suggestionbuffer.SuggestionBuffer.__init__","title":"<code>__init__(arbitrator_notification, verbose)</code>","text":"<p>Initializes the suggestion buffer with thread-safe synchronization.</p> Source code in <code>nebula/core/situationalawareness/awareness/suggestionbuffer.py</code> <pre><code>def __init__(self, arbitrator_notification: asyncio.Event, verbose):\n    \"\"\"Initializes the suggestion buffer with thread-safe synchronization.\"\"\"\n    self._arbitrator_notification = arbitrator_notification\n    self._arbitrator_notification_lock = Locker(\"arbitrator_notification_lock\", async_lock=True)\n    self._verbose = verbose\n    self._buffer: dict[type[NodeEvent], list[tuple[SAModuleAgent, SACommand]]] = defaultdict(list)\n    self._suggestion_buffer_lock = Locker(\"suggestion_buffer_lock\", async_lock=True)\n    self._expected_agents: dict[type[NodeEvent], list[SAModuleAgent]] = defaultdict(list)\n    self._expected_agents_lock = Locker(\"expected_agents_lock\", async_lock=True)\n    self._event_notifications: dict[type[NodeEvent], list[tuple[SAModuleAgent, asyncio.Event]]] = defaultdict(list)\n    self._event_waited = None\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/suggestionbuffer/#nebula.core.situationalawareness.awareness.suggestionbuffer.SuggestionBuffer.get_instance","title":"<code>get_instance()</code>  <code>classmethod</code>","text":"<p>Obtain SuggestionBuffer instance</p> Source code in <code>nebula/core/situationalawareness/awareness/suggestionbuffer.py</code> <pre><code>@classmethod\ndef get_instance(cls):\n    \"\"\"Obtain SuggestionBuffer instance\"\"\"\n    if cls._instance is None:\n        raise ValueError(\"SuggestionBuffer has not been initialized yet.\")\n    return cls._instance\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/suggestionbuffer/#nebula.core.situationalawareness.awareness.suggestionbuffer.SuggestionBuffer.get_suggestions","title":"<code>get_suggestions(event_type)</code>  <code>async</code>","text":"<p>Retrieve and return all suggestions for a given event type.</p> <p>Also clears the buffer after reading.</p> <p>Parameters:</p> Name Type Description Default <code>event_type</code> <code>Type[NodeEvent]</code> <p>The event whose suggestions are requested.</p> required <p>Returns:</p> Type Description <code>list[tuple[SAModuleAgent, SACommand]]</code> <p>list[tuple[SAModuleAgent, SACommand]]: List of (agent, suggestion) pairs.</p> Source code in <code>nebula/core/situationalawareness/awareness/suggestionbuffer.py</code> <pre><code>async def get_suggestions(self, event_type) -&gt; list[tuple[SAModuleAgent, SACommand]]:\n    \"\"\"\n    Retrieve and return all suggestions for a given event type.\n\n    Also clears the buffer after reading.\n\n    Parameters:\n        event_type (Type[NodeEvent]): The event whose suggestions are requested.\n\n    Returns:\n        list[tuple[SAModuleAgent, SACommand]]: List of (agent, suggestion) pairs.\n    \"\"\"\n    async with self._suggestion_buffer_lock:\n        async with self._expected_agents_lock:\n            suggestions = list(self._buffer.get(event_type, []))\n            if self._verbose:\n                logging.info(f\"Retrieving all sugestions for event: {event_type.__name__}\")\n            await self._clear_suggestions(event_type)\n            return suggestions\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/suggestionbuffer/#nebula.core.situationalawareness.awareness.suggestionbuffer.SuggestionBuffer.notify_all_suggestions_done_for_agent","title":"<code>notify_all_suggestions_done_for_agent(saa, event_type)</code>  <code>async</code>","text":"<p>Notify that a specific SA agent has completed its suggestion submission for an event.</p> <p>Parameters:</p> Name Type Description Default <code>saa</code> <code>SAModuleAgent</code> <p>The notifying agent.</p> required <code>event_type</code> <code>Type[NodeEvent]</code> <p>The related event type.</p> required Source code in <code>nebula/core/situationalawareness/awareness/suggestionbuffer.py</code> <pre><code>async def notify_all_suggestions_done_for_agent(self, saa: SAModuleAgent, event_type):\n    \"\"\"\n    Notify that a specific SA agent has completed its suggestion submission for an event.\n\n    Parameters:\n        saa (SAModuleAgent): The notifying agent.\n        event_type (Type[NodeEvent]): The related event type.\n    \"\"\"\n    async with self._expected_agents_lock:\n        agent_found = False\n        for agent, event in self._event_notifications.get(event_type, []):\n            if agent == saa:\n                event.set()\n                agent_found = True\n                if self._verbose:\n                    logging.info(\n                        f\"SA Agent: {await saa.get_agent()} notifies all suggestions registered for event: {event_type.__name__}\"\n                    )\n                break\n        if not agent_found and self._verbose:\n            logging.error(\n                f\"SAModuleAgent: {await saa.get_agent()} not found on notifications awaited for event {event_type.__name__}\"\n            )\n    await self._notify_arbitrator(event_type)\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/suggestionbuffer/#nebula.core.situationalawareness.awareness.suggestionbuffer.SuggestionBuffer.register_event_agents","title":"<code>register_event_agents(event_type, agent)</code>  <code>async</code>","text":"<p>Register a Situational Awareness (SA) agent as an expected participant for a given event type.</p> <p>Parameters:</p> Name Type Description Default <code>event_type</code> <code>Type[NodeEvent]</code> <p>The type of event being registered.</p> required <code>agent</code> <code>SAModuleAgent</code> <p>The agent expected to submit suggestions for the event.</p> required Source code in <code>nebula/core/situationalawareness/awareness/suggestionbuffer.py</code> <pre><code>async def register_event_agents(self, event_type, agent: SAModuleAgent):\n    \"\"\"\n    Register a Situational Awareness (SA) agent as an expected participant for a given event type.\n\n    Parameters:\n        event_type (Type[NodeEvent]): The type of event being registered.\n        agent (SAModuleAgent): The agent expected to submit suggestions for the event.\n    \"\"\"\n    async with self._expected_agents_lock:\n        if self._verbose:\n            logging.info(f\"Registering SA Agent: {await agent.get_agent()} for event: {event_type.__name__}\")\n\n        if event_type not in self._event_notifications:\n            self._event_notifications[event_type] = []\n\n        self._expected_agents[event_type].append(agent)\n\n        existing_agents = {a for a, _ in self._event_notifications[event_type]}\n        if agent not in existing_agents:\n            self._event_notifications[event_type].append((agent, asyncio.Event()))\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/suggestionbuffer/#nebula.core.situationalawareness.awareness.suggestionbuffer.SuggestionBuffer.register_suggestion","title":"<code>register_suggestion(event_type, agent, suggestion)</code>  <code>async</code>","text":"<p>Register a suggestion issued by a specific SA agent for a given event.</p> <p>Parameters:</p> Name Type Description Default <code>event_type</code> <code>Type[NodeEvent]</code> <p>The event type for which the suggestion is made.</p> required <code>agent</code> <code>SAModuleAgent</code> <p>The agent submitting the suggestion.</p> required <code>suggestion</code> <code>SACommand</code> <p>The command being suggested.</p> required Source code in <code>nebula/core/situationalawareness/awareness/suggestionbuffer.py</code> <pre><code>async def register_suggestion(self, event_type, agent: SAModuleAgent, suggestion: SACommand):\n    \"\"\"\n    Register a suggestion issued by a specific SA agent for a given event.\n\n    Parameters:\n        event_type (Type[NodeEvent]): The event type for which the suggestion is made.\n        agent (SAModuleAgent): The agent submitting the suggestion.\n        suggestion (SACommand): The command being suggested.\n    \"\"\"\n    async with self._suggestion_buffer_lock:\n        if self._verbose:\n            logging.info(\n                f\"Registering Suggestion from SA Agent: {await agent.get_agent()} for event: {event_type.__name__}\"\n            )\n        self._buffer[event_type].append((agent, suggestion))\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/suggestionbuffer/#nebula.core.situationalawareness.awareness.suggestionbuffer.SuggestionBuffer.set_event_waited","title":"<code>set_event_waited(event_type)</code>  <code>async</code>","text":"<p>Set the event type that the SuggestionBuffer will wait for.</p> <p>Used to indicate that arbitration should proceed when all suggestions for this event are received.</p> <p>Parameters:</p> Name Type Description Default <code>event_type</code> <code>Type[NodeEvent]</code> <p>The event type to monitor.</p> required Source code in <code>nebula/core/situationalawareness/awareness/suggestionbuffer.py</code> <pre><code>async def set_event_waited(self, event_type):\n    \"\"\"\n    Set the event type that the SuggestionBuffer will wait for.\n\n    Used to indicate that arbitration should proceed when all suggestions for this event are received.\n\n    Parameters:\n        event_type (Type[NodeEvent]): The event type to monitor.\n    \"\"\"\n    if not self._event_waited:\n        if self._verbose:\n            logging.info(\n                f\"Set notification when all suggestions have being received for event: {event_type.__name__}\"\n            )\n        self._event_waited = event_type\n        await self._notify_arbitrator(event_type)\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/arbitrationpolicies/","title":"Documentation for Arbitrationpolicies Module","text":""},{"location":"api/core/situationalawareness/awareness/arbitrationpolicies/arbitrationpolicy/","title":"Documentation for Arbitrationpolicy Module","text":""},{"location":"api/core/situationalawareness/awareness/arbitrationpolicies/arbitrationpolicy/#nebula.core.situationalawareness.awareness.arbitrationpolicies.arbitrationpolicy.ArbitrationPolicy","title":"<code>ArbitrationPolicy</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class defining the arbitration policy for resolving conflicts between SA commands.</p> <p>This class establishes the interface for implementing arbitration logic used in the Situational Awareness module. It includes initialization and a tie-breaking mechanism when two commands have the same priority or conflict.</p> <p>Methods: - init(config): Initialize the arbitration policy with a configuration object. - tie_break(sac1, sac2): Decide which command to keep when two conflict and have equal priority.</p> Source code in <code>nebula/core/situationalawareness/awareness/arbitrationpolicies/arbitrationpolicy.py</code> <pre><code>class ArbitrationPolicy(ABC):\n    \"\"\"\n    Abstract base class defining the arbitration policy for resolving conflicts between SA commands.\n\n    This class establishes the interface for implementing arbitration logic used in the\n    Situational Awareness module. It includes initialization and a tie-breaking mechanism\n    when two commands have the same priority or conflict.\n\n    Methods:\n    - init(config): Initialize the arbitration policy with a configuration object.\n    - tie_break(sac1, sac2): Decide which command to keep when two conflict and have equal priority.\n    \"\"\"\n\n    @abstractmethod\n    async def init(self, config):\n        \"\"\"\n        Initialize the arbitration policy with the provided configuration.\n\n        Parameters:\n            config (Any): A configuration object or dictionary to set up internal parameters.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    async def tie_break(self, sac1: SACommand, sac2: SACommand) -&gt; bool:\n        \"\"\"\n        Resolve a conflict between two commands with equal priority.\n\n        Parameters:\n            sac1 (SACommand): First command in conflict.\n            sac2 (SACommand): Second command in conflict.\n\n        Returns:\n            bool: True if sac1 should be kept over sac2, False if sac2 is preferred.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/arbitrationpolicies/arbitrationpolicy/#nebula.core.situationalawareness.awareness.arbitrationpolicies.arbitrationpolicy.ArbitrationPolicy.init","title":"<code>init(config)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Initialize the arbitration policy with the provided configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Any</code> <p>A configuration object or dictionary to set up internal parameters.</p> required Source code in <code>nebula/core/situationalawareness/awareness/arbitrationpolicies/arbitrationpolicy.py</code> <pre><code>@abstractmethod\nasync def init(self, config):\n    \"\"\"\n    Initialize the arbitration policy with the provided configuration.\n\n    Parameters:\n        config (Any): A configuration object or dictionary to set up internal parameters.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/arbitrationpolicies/arbitrationpolicy/#nebula.core.situationalawareness.awareness.arbitrationpolicies.arbitrationpolicy.ArbitrationPolicy.tie_break","title":"<code>tie_break(sac1, sac2)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Resolve a conflict between two commands with equal priority.</p> <p>Parameters:</p> Name Type Description Default <code>sac1</code> <code>SACommand</code> <p>First command in conflict.</p> required <code>sac2</code> <code>SACommand</code> <p>Second command in conflict.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if sac1 should be kept over sac2, False if sac2 is preferred.</p> Source code in <code>nebula/core/situationalawareness/awareness/arbitrationpolicies/arbitrationpolicy.py</code> <pre><code>@abstractmethod\nasync def tie_break(self, sac1: SACommand, sac2: SACommand) -&gt; bool:\n    \"\"\"\n    Resolve a conflict between two commands with equal priority.\n\n    Parameters:\n        sac1 (SACommand): First command in conflict.\n        sac2 (SACommand): Second command in conflict.\n\n    Returns:\n        bool: True if sac1 should be kept over sac2, False if sac2 is preferred.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/arbitrationpolicies/staticarbitrationpolicy/","title":"Documentation for Staticarbitrationpolicy Module","text":""},{"location":"api/core/situationalawareness/awareness/arbitrationpolicies/staticarbitrationpolicy/#nebula.core.situationalawareness.awareness.arbitrationpolicies.staticarbitrationpolicy.SAP","title":"<code>SAP</code>","text":"<p>               Bases: <code>ArbitrationPolicy</code></p> <p>Static Arbitration Policy for the Reasoner module.</p> <p>This class implements a fixed priority arbitration mechanism for  SA (Situational Awareness) components. Each SA component category  is assigned a static weight representing its priority level.</p> <p>In case of conflicting SA commands, the policy selects the command  whose originating component has the highest priority weight.</p> <p>Attributes:</p> Name Type Description <code>_verbose</code> <code>bool</code> <p>Enables verbose logging for debugging and tracing.</p> <code>agent_weights</code> <code>dict</code> <p>Mapping of SA component categories to static weights.</p> <p>Methods:</p> Name Description <code>init</code> <p>Placeholder for initialization with external configuration.</p> <code>tie_break</code> <p>Resolves conflicts between two SA commands by  comparing their category weights, returning True if sac1 wins.</p> Source code in <code>nebula/core/situationalawareness/awareness/arbitrationpolicies/staticarbitrationpolicy.py</code> <pre><code>class SAP(ArbitrationPolicy):  # Static Arbitatrion Policy\n    \"\"\"\n    Static Arbitration Policy for the Reasoner module.\n\n    This class implements a fixed priority arbitration mechanism for \n    SA (Situational Awareness) components. Each SA component category \n    is assigned a static weight representing its priority level.\n\n    In case of conflicting SA commands, the policy selects the command \n    whose originating component has the highest priority weight.\n\n    Attributes:\n        _verbose (bool): Enables verbose logging for debugging and tracing.\n        agent_weights (dict): Mapping of SA component categories to static weights.\n\n    Methods:\n        init(config): Placeholder for initialization with external configuration.\n        tie_break(sac1, sac2): Resolves conflicts between two SA commands by \n            comparing their category weights, returning True if sac1 wins.\n    \"\"\"\n    def __init__(self, verbose):\n        self._verbose = verbose\n        # Define static weights for SA Agents from SA Components\n        self.agent_weights = {\"SATraining\": 1, \"SANetwork\": 2, \"SAReputation\": 3}\n\n    async def init(self, config):\n        pass\n\n    async def _get_agent_category(self, sa_command: SACommand) -&gt; str:\n        \"\"\"\n        Extract agent category name.\n        Example: \"SATraining_Agent1\" \u2192 \"SATraining\"\n        \"\"\"\n        full_name = await sa_command.get_owner()\n        return full_name.split(\"_\")[0] if \"_\" in full_name else full_name\n\n    async def tie_break(self, sac1: SACommand, sac2: SACommand) -&gt; bool:\n        \"\"\"\n        Tie break conflcited SA Commands\n        \"\"\"\n        if self._verbose:\n            logging.info(\n                f\"Tie break between ({await sac1.get_owner()}, {sac1.get_action().value}) &amp; ({await sac2.get_owner()}, {sac2.get_action().value})\"\n            )\n\n        async def get_weight(cmd):\n            category = await self._get_agent_category(cmd)\n            return self.agent_weights.get(category, 0)\n\n        if await get_weight(sac1) &gt; await get_weight(sac2):\n            if self._verbose:\n                logging.info(\n                    f\"Tie break resolved, SA Command choosen ({await sac1.get_owner()}, {sac1.get_action().value})\"\n                )\n            return True\n        else:\n            if self._verbose:\n                logging.info(\n                    f\"Tie break resolved, SA Command choosen ({await sac2.get_owner()}, {sac2.get_action().value})\"\n                )\n            return False\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/arbitrationpolicies/staticarbitrationpolicy/#nebula.core.situationalawareness.awareness.arbitrationpolicies.staticarbitrationpolicy.SAP.tie_break","title":"<code>tie_break(sac1, sac2)</code>  <code>async</code>","text":"<p>Tie break conflcited SA Commands</p> Source code in <code>nebula/core/situationalawareness/awareness/arbitrationpolicies/staticarbitrationpolicy.py</code> <pre><code>async def tie_break(self, sac1: SACommand, sac2: SACommand) -&gt; bool:\n    \"\"\"\n    Tie break conflcited SA Commands\n    \"\"\"\n    if self._verbose:\n        logging.info(\n            f\"Tie break between ({await sac1.get_owner()}, {sac1.get_action().value}) &amp; ({await sac2.get_owner()}, {sac2.get_action().value})\"\n        )\n\n    async def get_weight(cmd):\n        category = await self._get_agent_category(cmd)\n        return self.agent_weights.get(category, 0)\n\n    if await get_weight(sac1) &gt; await get_weight(sac2):\n        if self._verbose:\n            logging.info(\n                f\"Tie break resolved, SA Command choosen ({await sac1.get_owner()}, {sac1.get_action().value})\"\n            )\n        return True\n    else:\n        if self._verbose:\n            logging.info(\n                f\"Tie break resolved, SA Command choosen ({await sac2.get_owner()}, {sac2.get_action().value})\"\n            )\n        return False\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/","title":"Documentation for Sanetwork Module","text":""},{"location":"api/core/situationalawareness/awareness/sanetwork/sanetwork/","title":"Documentation for Sanetwork Module","text":""},{"location":"api/core/situationalawareness/awareness/sanetwork/sanetwork/#nebula.core.situationalawareness.awareness.sanetwork.sanetwork.SANetwork","title":"<code>SANetwork</code>","text":"<p>               Bases: <code>SAMComponent</code></p> <p>Network situational awareness component responsible for monitoring and managing the communication context within the federation.</p> This component handles <ul> <li>Tracking active and potential peer nodes.</li> <li>Evaluating network conditions for situational awareness decisions.</li> <li>Integrating with discovery and reasoning modules for dynamic topology updates.</li> </ul> <p>Inherits from SAMComponent to participate in the broader Situational Awareness pipeline.</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/sanetwork.py</code> <pre><code>class SANetwork(SAMComponent):\n    \"\"\"\n    Network situational awareness component responsible for monitoring and managing\n    the communication context within the federation.\n\n    This component handles:\n      - Tracking active and potential peer nodes.\n      - Evaluating network conditions for situational awareness decisions.\n      - Integrating with discovery and reasoning modules for dynamic topology updates.\n\n    Inherits from SAMComponent to participate in the broader Situational Awareness pipeline.\n    \"\"\"\n\n    NEIGHBOR_VERIFICATION_TIMEOUT = 30\n\n    def __init__(self, config):\n        self._neighbor_policy = config[\"neighbor_policy\"]  # topology\n        self._neighbor_policy = self._neighbor_policy.lower()\n        self._strict_topology = config[\"strict_topology\"]  # strict_topology\n        print_msg_box(\n            msg=f\"Starting Network SA\\nNeighbor Policy: {self._neighbor_policy}\\nStrict: {self._strict_topology}\",\n            indent=2,\n            title=\"Network SA module\",\n        )\n        self._sar = config[\"sar\"]  # sar\n        self._addr = config[\"addr\"]  # addr\n        self._neighbor_policy = factory_NeighborPolicy(self._neighbor_policy)\n        self._restructure_process_lock = Locker(name=\"restructure_process_lock\", async_lock=True)\n        self._restructure_cooldown = 0\n        self._verbose = config[\"verbose\"]  # verbose\n        self._cm = CommunicationsManager.get_instance()\n        self._sa_network_agent = SANetworkAgent(self)\n\n        # Track verification tasks for proper cleanup during shutdown\n        self._verification_tasks = set()\n        self._verification_tasks_lock = asyncio.Lock()\n\n    @property\n    def sar(self) -&gt; SAReasoner:\n        \"\"\"SA Reasoner\"\"\"\n        return self._sar\n\n    @property\n    def cm(self):\n        \"\"\"Communication Manager\"\"\"\n        return self._cm\n\n    @property\n    def np(self):\n        \"\"\"Neighbor Policy\"\"\"\n        return self._neighbor_policy\n\n    @property\n    def sana(self):\n        \"\"\"SA Network Agent\"\"\"\n        return self._sa_network_agent\n\n    async def init(self):\n        \"\"\"\n        Initialize the SANetwork component by deploying external connection services,\n        subscribing to relevant events, starting beaconing, and configuring neighbor policies.\n\n        Actions performed:\n        1. If not an additional participant, start and subscribe to beacon and finish events.\n        2. Otherwise, initialize ECS without running it.\n        3. Build and apply the neighbor policy using current direct and undirected connections.\n        4. Subscribe to node discovery and neighbor update events.\n        5. Register this agent with the situational awareness network agent.\n        \"\"\"\n        if not self.sar.is_additional_participant():\n            logging.info(\"Deploying External Connection Service\")\n            await self.cm.start_external_connection_service()\n            await EventManager.get_instance().subscribe_node_event(BeaconRecievedEvent, self.beacon_received)\n            await EventManager.get_instance().subscribe_node_event(ExperimentFinishEvent, self.experiment_finish)\n            await self.cm.start_beacon()\n        else:\n            logging.info(\"Deploying External Connection Service | No running\")\n            await self.cm.start_external_connection_service(run_service=False)\n\n        logging.info(\"Building neighbor policy configuration..\")\n        await self.np.set_config([\n            await self.cm.get_addrs_current_connections(only_direct=True, myself=False),\n            await self.cm.get_addrs_current_connections(only_direct=False, only_undirected=False, myself=False),\n            self._addr,\n            self._strict_topology,\n        ])\n\n        await EventManager.get_instance().subscribe_node_event(NodeFoundEvent, self._process_node_found_event)\n        await EventManager.get_instance().subscribe_node_event(UpdateNeighborEvent, self._process_update_neighbor_event)\n        await self.sana.register_sa_agent()\n\n    async def sa_component_actions(self):\n        \"\"\"\n        Perform periodic situational awareness checks for network conditions.\n\n        This method evaluates the external connection service status and analyzes\n        the robustness of the current network topology.\n        \"\"\"\n        logging.info(\"SA Network evaluating current scenario\")\n        await self._check_external_connection_service_status()\n        await self._analize_topology_robustness()\n\n    \"\"\"                                                     ###############################\n                                                            #       NEIGHBOR POLICY       #\n                                                            ###############################\n    \"\"\"\n\n    async def _process_node_found_event(self, nfe: NodeFoundEvent):\n        \"\"\"\n        Handle an event indicating a new node has been discovered.\n\n        Args:\n            nfe (NodeFoundEvent): The event containing the discovered node's address.\n        \"\"\"\n        node_addr = await nfe.get_event_data()\n        await self.np.meet_node(node_addr)\n\n    async def _process_update_neighbor_event(self, une: UpdateNeighborEvent):\n        \"\"\"\n        Handle an update to the neighbor set, such as node join or leave.\n\n        Args:\n            une (UpdateNeighborEvent): The event containing the neighbor address and removal flag.\n        \"\"\"\n        node_addr, removed = await une.get_event_data()\n        if self._verbose:\n            logging.info(f\"Processing Update Neighbor Event, node addr: {node_addr}, remove: {removed}\")\n        await self.np.update_neighbors(node_addr, removed)\n\n    async def meet_node(self, node):\n        \"\"\"\n        Propose a meeting (connection) with a newly discovered node if it is not self.\n\n        Args:\n            node (str): The address of the node to meet.\n        \"\"\"\n        if node != self._addr:\n            await self.np.meet_node(node)\n\n    async def get_nodes_known(self, neighbors_too=False, neighbors_only=False):\n        \"\"\"\n        Retrieve the list of known nodes in the network.\n\n        Args:\n            neighbors_too (bool, optional): Include neighbors in the result. Defaults to False.\n            neighbors_only (bool, optional): Return only neighbors. Defaults to False.\n\n        Returns:\n            set: Addresses of known nodes based on the provided filters.\n        \"\"\"\n        return await self.np.get_nodes_known(neighbors_too, neighbors_only)\n\n    async def neighbors_left(self):\n        \"\"\"\n        Check whether any direct neighbor connections remain.\n\n        Returns:\n            bool: True if there are one or more direct neighbor connections, False otherwise.\n        \"\"\"\n        return len(await self.cm.get_addrs_current_connections(only_direct=True, myself=False)) &gt; 0\n\n    async def accept_connection(self, source, joining=False):\n        \"\"\"\n        Decide whether to accept an incoming connection request from a source node.\n\n        Args:\n            source (str): The address of the requesting node.\n            joining (bool, optional): True if this is part of a join process. Defaults to False.\n\n        Returns:\n            bool: True if the connection should be accepted, False otherwise.\n        \"\"\"\n        accepted = await self.np.accept_connection(source, joining)\n        return accepted\n\n    async def need_more_neighbors(self):\n        \"\"\"\n        Determine if the network requires additional neighbor connections.\n\n        Returns:\n            bool: True if more neighbors are needed, False otherwise.\n        \"\"\"\n        return await self.np.need_more_neighbors()\n\n    async def get_actions(self):\n        \"\"\"\n        Retrieve the set of situational awareness actions applicable to the current network state.\n\n        Returns:\n            list: Identifiers of available network actions.\n        \"\"\"\n        return await self.np.get_actions()\n\n    \"\"\"                                                     ###############################\n                                                            # EXTERNAL CONNECTION SERVICE #\n                                                            ###############################\n    \"\"\"\n\n    async def _check_external_connection_service_status(self):\n        \"\"\"\n        Ensure the external connection service is running; if not, initialize and start beaconing.\n\n        This method checks the ECS status, starts it if necessary,\n        subscribes to beacon events, and initiates beacon transmission.\n        \"\"\"\n        if not await self.cm.is_external_connection_service_running():\n            logging.info(\"\ud83d\udd04 External Service not running | Starting service...\")\n            await self.cm.init_external_connection_service()\n            await EventManager.get_instance().subscribe_node_event(BeaconRecievedEvent, self.beacon_received)\n            await self.cm.start_beacon()\n\n    async def experiment_finish(self, efe: ExperimentFinishEvent):\n        \"\"\"\n        Handle the completion of an experiment by shutting down the external connection service.\n\n        Args:\n            efe (ExperimentFinishEvent): The event indicating the experiment has finished.\n        \"\"\"\n        await self.cm.stop_external_connection_service()\n\n    async def beacon_received(self, beacon_recieved_event: BeaconRecievedEvent):\n        \"\"\"\n        Process a received beacon event by publishing a NodeFoundEvent for the given address.\n\n        Extracts the address and geolocation from the beacon event and notifies\n        the system that a new node has been discovered.\n\n        Args:\n            beacon_recieved_event (BeaconRecievedEvent): The event containing beacon data.\n        \"\"\"\n        addr, geoloc = await beacon_recieved_event.get_event_data()\n        latitude, longitude = geoloc\n        nfe = NodeFoundEvent(addr)\n        asyncio.create_task(EventManager.get_instance().publish_node_event(nfe))\n\n    \"\"\"                                                     ###############################\n                                                            #    REESTRUCTURE TOPOLOGY    #\n                                                            ###############################\n    \"\"\"\n\n    def _update_restructure_cooldown(self):\n        \"\"\"\n        Decrement or wrap the restructure cooldown counter.\n\n        Uses modulo arithmetic to ensure the cooldown cycles correctly,\n        preventing frequent restructuring operations.\n        \"\"\"\n        if self._restructure_cooldown &gt; 0:\n            self._restructure_cooldown = (self._restructure_cooldown + 1) % RESTRUCTURE_COOLDOWN\n\n    def _restructure_available(self):\n        \"\"\"\n        Check if restructuring is currently allowed based on the cooldown.\n\n        Returns:\n            bool: True if cooldown is zero (restructure allowed), False otherwise.\n        \"\"\"\n        if self._restructure_cooldown:\n            if self._verbose:\n                logging.info(\"Reestructure on cooldown\")\n        return self._restructure_cooldown == 0\n\n    def get_restructure_process_lock(self):\n        \"\"\"\n        Retrieve the asynchronous lock protecting the restructure process.\n\n        Returns:\n            asyncio.Lock: Lock to ensure only one restructure operation runs at a time.\n        \"\"\"\n        return self._restructure_process_lock\n\n    async def _analize_topology_robustness(self):\n        \"\"\"\n        Analyze the current network topology to assess robustness and suggest SA actions.\n\n        Performs the following checks:\n        1. If no neighbors remain, suggest reconnection to the federation.\n        2. If more neighbors are needed and restructuring is off cooldown, suggest removing or searching for neighbors.\n        3. If excess neighbors exist, suggest disconnecting according to policy.\n        4. Otherwise, suggest maintaining current connections.\n        5. If a restructure is already in progress, suggest idling.\n\n        Uses neighbor policy decisions and cooldown logic to produce situational awareness commands.\n        \"\"\"\n        # TODO update the way of checking\n        logging.info(\"\ud83d\udd04 Analizing node network robustness...\")\n        if not self._restructure_process_lock.locked():\n            if not await self.neighbors_left():\n                if self._verbose:\n                    logging.info(\"No Neighbors left | reconnecting with Federation\")\n                await self.sana.create_and_suggest_action(\n                    SACommandAction.RECONNECT, self.reconnect_to_federation, False, None\n                )\n            elif await self.np.need_more_neighbors() and self._restructure_available():\n                if self._verbose:\n                    logging.info(\"Suggesting to Remove neighbors according to policy...\")\n                if await self.np.any_leftovers_neighbors():\n                    nodes_to_remove = await self.np.get_neighbors_to_remove()\n                    await self.sana.create_and_suggest_action(\n                        SACommandAction.DISCONNECT, self.cm.disconnect, True, nodes_to_remove\n                    )\n                if self._verbose:\n                    logging.info(\"Insufficient Robustness | Upgrading robustness | Searching for more connections\")\n                self._update_restructure_cooldown()\n                possible_neighbors = await self.np.get_posible_neighbors()\n                possible_neighbors = await self.cm.apply_restrictions(possible_neighbors)\n                if not possible_neighbors:\n                    if self._verbose:\n                        logging.info(\"All possible neighbors using nodes known are restricted...\")\n                else:\n                    pass\n                await self.sana.create_and_suggest_action(\n                    SACommandAction.SEARCH_CONNECTIONS, self.upgrade_connection_robustness, False, possible_neighbors\n                )\n            elif await self.np.any_leftovers_neighbors():\n                nodes_to_remove = await self.np.get_neighbors_to_remove()\n                if self._verbose:\n                    logging.info(f\"Excess neighbors | removing: {list(nodes_to_remove)}\")\n                await self.sana.create_and_suggest_action(\n                    SACommandAction.DISCONNECT, self.cm.disconnect, False, nodes_to_remove\n                )\n            else:\n                if self._verbose:\n                    logging.info(\"Sufficient Robustness | no actions required\")\n                await self.sana.create_and_suggest_action(\n                    SACommandAction.MAINTAIN_CONNECTIONS,\n                    self.cm.clear_unused_undirect_connections,\n                    more_suggestions=False,\n                )\n        else:\n            if self._verbose:\n                logging.info(\"\u2757\ufe0f Reestructure/Reconnecting process already running...\")\n            await self.sana.create_and_suggest_action(SACommandAction.IDLE, more_suggestions=False)\n\n    async def reconnect_to_federation(self):\n        \"\"\"\n        Clear any connection restrictions and initiate a late\u2010connection discovery process\n        to rejoin the federation.\n\n        Steps:\n        1. Acquire the restructure lock.\n        2. Clear blacklist and recently disconnected restrictions.\n        3. If known node addresses exist, use them for discovery; otherwise, perform a fresh discovery.\n        4. Release the restructure lock.\n        \"\"\"\n        logging.info(\"Going to reconnect with federation...\")\n        await self._restructure_process_lock.acquire_async()\n        await self.cm.clear_restrictions()\n        # If we got some refs, try to reconnect to them\n        if len(await self.np.get_nodes_known()) &gt; 0:\n            if self._verbose:\n                logging.info(\"Reconnecting | Addrs availables\")\n            await self.sar.sad.start_late_connection_process(\n                connected=False, msg_type=\"discover_nodes\", addrs_known=await self.np.get_nodes_known()\n            )\n        else:\n            if self._verbose:\n                logging.info(\"Reconnecting | NO Addrs availables\")\n            await self.sar.sad.start_late_connection_process(connected=False, msg_type=\"discover_nodes\")\n        await self._restructure_process_lock.release_async()\n\n    async def upgrade_connection_robustness(self, possible_neighbors):\n        \"\"\"\n        Attempt to strengthen network robustness by discovering or reconnecting to additional neighbors.\n\n        Steps:\n        1. Acquire the restructure lock.\n        2. If possible_neighbors is non\u2010empty, use them for a targeted late\u2010connection discovery.\n        3. Otherwise, perform a generic discovery of federation nodes.\n        4. Release the restructure lock.\n\n        Args:\n            possible_neighbors (set): Addresses of candidate nodes for connection enhancement.\n        \"\"\"\n        await self._restructure_process_lock.acquire_async()\n        # If we got some refs, try to connect to them\n        if possible_neighbors and len(possible_neighbors) &gt; 0:\n            if self._verbose:\n                logging.info(f\"Reestructuring | Addrs availables | addr list: {possible_neighbors}\")\n            await self.sar.sad.start_late_connection_process(\n                connected=True, msg_type=\"discover_nodes\", addrs_known=possible_neighbors\n            )\n        else:\n            if self._verbose:\n                logging.info(\"Reestructuring | NO Addrs availables\")\n            await self.sar.sad.start_late_connection_process(connected=True, msg_type=\"discover_nodes\")\n        await self._restructure_process_lock.release_async()\n\n    async def stop_connections_with_federation(self):\n        \"\"\"\n        Disconnect from all current federation neighbors after a short delay.\n\n        1. Waits for a predefined sleep period (to allow in\u2010flight messages to complete).\n        2. Blacklists each direct neighbor.\n        3. Disconnects from each neighbor without mutual handshake.\n        \"\"\"\n        await asyncio.sleep(10)\n        logging.info(\"### DISCONNECTING FROM FEDERATON ###\")\n        neighbors = await self.np.get_nodes_known(neighbors_only=True)\n        for n in neighbors:\n            await self.cm.add_to_blacklist(n)\n        for n in neighbors:\n            await self.cm.disconnect(n, mutual_disconnection=False, forced=True)\n\n    async def verify_neighbors_stablished(self, nodes: set):\n        \"\"\"\n        Verify that a set of connection attempts has succeeded within a timeout.\n\n        Args:\n            nodes (set): The set of node addresses for which connections were attempted.\n\n        Behavior:\n        1. Sleeps for NEIGHBOR_VERIFICATION_TIMEOUT seconds.\n        2. Compares the originally requested nodes against the currently known neighbors.\n        3. Logs any addresses that failed to establish and instructs the policy to forget them.\n        \"\"\"\n        if not nodes:\n            return\n\n        await asyncio.sleep(self.NEIGHBOR_VERIFICATION_TIMEOUT)\n        logging.info(\"Verifyng all connections were stablished\")\n        nodes_to_forget = nodes.copy()\n        neighbors = await self.np.get_nodes_known(neighbors_only=True)\n        if neighbors:\n            nodes_to_forget.difference_update(neighbors)\n        logging.info(f\"Connections dont stablished: {nodes_to_forget}\")\n        await self.forget_nodes(nodes_to_forget)\n\n    async def create_verification_task(self, nodes: set):\n        \"\"\"\n        Create and track a verification task for neighbor establishment.\n\n        Args:\n            nodes (set): The set of node addresses for which connections were attempted.\n\n        Returns:\n            asyncio.Task: The created verification task.\n        \"\"\"\n        verification_task = asyncio.create_task(self.verify_neighbors_stablished(nodes))\n\n        async with self._verification_tasks_lock:\n            self._verification_tasks.add(verification_task)\n\n        return verification_task\n\n    async def forget_nodes(self, nodes_to_forget):\n        \"\"\"\n        Instruct the neighbor policy to remove specified nodes from its known set.\n\n        Args:\n            nodes_to_forget (set): Addresses of nodes to be purged from policy memory.\n        \"\"\"\n        await self.np.forget_nodes(nodes_to_forget)\n\n    async def stop(self):\n        \"\"\"\n        Stop the SANetwork component by releasing locks and clearing any pending operations.\n        \"\"\"\n        logging.info(\"\ud83d\uded1  Stopping SANetwork...\")\n\n        # Cancel all verification tasks\n        async with self._verification_tasks_lock:\n            if self._verification_tasks:\n                tasks_to_cancel = [task for task in self._verification_tasks if not task.done()]\n                logging.info(f\"\ud83d\uded1  Cancelling {len(tasks_to_cancel)} verification tasks...\")\n                for task in tasks_to_cancel:\n                    task.cancel()\n                    try:\n                        await task\n                    except asyncio.CancelledError:\n                        pass\n                self._verification_tasks.clear()\n                logging.info(\"\ud83d\uded1  All verification tasks cancelled\")\n\n        # Release any held locks\n        try:\n            if self._restructure_process_lock.locked():\n                self._restructure_process_lock.release()\n        except Exception as e:\n            logging.warning(f\"Error releasing restructure_process_lock: {e}\")\n\n        logging.info(\"\u2705  SANetwork stopped successfully\")\n\n    \"\"\"                                                     ###############################\n                                                            #       SA NETWORK AGENT      #\n                                                            ###############################\n    \"\"\"\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/sanetwork/#nebula.core.situationalawareness.awareness.sanetwork.sanetwork.SANetwork.cm","title":"<code>cm</code>  <code>property</code>","text":"<p>Communication Manager</p>"},{"location":"api/core/situationalawareness/awareness/sanetwork/sanetwork/#nebula.core.situationalawareness.awareness.sanetwork.sanetwork.SANetwork.np","title":"<code>np</code>  <code>property</code>","text":"<p>Neighbor Policy</p>"},{"location":"api/core/situationalawareness/awareness/sanetwork/sanetwork/#nebula.core.situationalawareness.awareness.sanetwork.sanetwork.SANetwork.sana","title":"<code>sana</code>  <code>property</code>","text":"<p>SA Network Agent</p>"},{"location":"api/core/situationalawareness/awareness/sanetwork/sanetwork/#nebula.core.situationalawareness.awareness.sanetwork.sanetwork.SANetwork.sar","title":"<code>sar</code>  <code>property</code>","text":"<p>SA Reasoner</p>"},{"location":"api/core/situationalawareness/awareness/sanetwork/sanetwork/#nebula.core.situationalawareness.awareness.sanetwork.sanetwork.SANetwork.accept_connection","title":"<code>accept_connection(source, joining=False)</code>  <code>async</code>","text":"<p>Decide whether to accept an incoming connection request from a source node.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The address of the requesting node.</p> required <code>joining</code> <code>bool</code> <p>True if this is part of a join process. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the connection should be accepted, False otherwise.</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/sanetwork.py</code> <pre><code>async def accept_connection(self, source, joining=False):\n    \"\"\"\n    Decide whether to accept an incoming connection request from a source node.\n\n    Args:\n        source (str): The address of the requesting node.\n        joining (bool, optional): True if this is part of a join process. Defaults to False.\n\n    Returns:\n        bool: True if the connection should be accepted, False otherwise.\n    \"\"\"\n    accepted = await self.np.accept_connection(source, joining)\n    return accepted\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/sanetwork/#nebula.core.situationalawareness.awareness.sanetwork.sanetwork.SANetwork.beacon_received","title":"<code>beacon_received(beacon_recieved_event)</code>  <code>async</code>","text":"<p>Process a received beacon event by publishing a NodeFoundEvent for the given address.</p> <p>Extracts the address and geolocation from the beacon event and notifies the system that a new node has been discovered.</p> <p>Parameters:</p> Name Type Description Default <code>beacon_recieved_event</code> <code>BeaconRecievedEvent</code> <p>The event containing beacon data.</p> required Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/sanetwork.py</code> <pre><code>async def beacon_received(self, beacon_recieved_event: BeaconRecievedEvent):\n    \"\"\"\n    Process a received beacon event by publishing a NodeFoundEvent for the given address.\n\n    Extracts the address and geolocation from the beacon event and notifies\n    the system that a new node has been discovered.\n\n    Args:\n        beacon_recieved_event (BeaconRecievedEvent): The event containing beacon data.\n    \"\"\"\n    addr, geoloc = await beacon_recieved_event.get_event_data()\n    latitude, longitude = geoloc\n    nfe = NodeFoundEvent(addr)\n    asyncio.create_task(EventManager.get_instance().publish_node_event(nfe))\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/sanetwork/#nebula.core.situationalawareness.awareness.sanetwork.sanetwork.SANetwork.create_verification_task","title":"<code>create_verification_task(nodes)</code>  <code>async</code>","text":"<p>Create and track a verification task for neighbor establishment.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>set</code> <p>The set of node addresses for which connections were attempted.</p> required <p>Returns:</p> Type Description <p>asyncio.Task: The created verification task.</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/sanetwork.py</code> <pre><code>async def create_verification_task(self, nodes: set):\n    \"\"\"\n    Create and track a verification task for neighbor establishment.\n\n    Args:\n        nodes (set): The set of node addresses for which connections were attempted.\n\n    Returns:\n        asyncio.Task: The created verification task.\n    \"\"\"\n    verification_task = asyncio.create_task(self.verify_neighbors_stablished(nodes))\n\n    async with self._verification_tasks_lock:\n        self._verification_tasks.add(verification_task)\n\n    return verification_task\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/sanetwork/#nebula.core.situationalawareness.awareness.sanetwork.sanetwork.SANetwork.experiment_finish","title":"<code>experiment_finish(efe)</code>  <code>async</code>","text":"<p>Handle the completion of an experiment by shutting down the external connection service.</p> <p>Parameters:</p> Name Type Description Default <code>efe</code> <code>ExperimentFinishEvent</code> <p>The event indicating the experiment has finished.</p> required Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/sanetwork.py</code> <pre><code>async def experiment_finish(self, efe: ExperimentFinishEvent):\n    \"\"\"\n    Handle the completion of an experiment by shutting down the external connection service.\n\n    Args:\n        efe (ExperimentFinishEvent): The event indicating the experiment has finished.\n    \"\"\"\n    await self.cm.stop_external_connection_service()\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/sanetwork/#nebula.core.situationalawareness.awareness.sanetwork.sanetwork.SANetwork.forget_nodes","title":"<code>forget_nodes(nodes_to_forget)</code>  <code>async</code>","text":"<p>Instruct the neighbor policy to remove specified nodes from its known set.</p> <p>Parameters:</p> Name Type Description Default <code>nodes_to_forget</code> <code>set</code> <p>Addresses of nodes to be purged from policy memory.</p> required Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/sanetwork.py</code> <pre><code>async def forget_nodes(self, nodes_to_forget):\n    \"\"\"\n    Instruct the neighbor policy to remove specified nodes from its known set.\n\n    Args:\n        nodes_to_forget (set): Addresses of nodes to be purged from policy memory.\n    \"\"\"\n    await self.np.forget_nodes(nodes_to_forget)\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/sanetwork/#nebula.core.situationalawareness.awareness.sanetwork.sanetwork.SANetwork.get_actions","title":"<code>get_actions()</code>  <code>async</code>","text":"<p>Retrieve the set of situational awareness actions applicable to the current network state.</p> <p>Returns:</p> Name Type Description <code>list</code> <p>Identifiers of available network actions.</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/sanetwork.py</code> <pre><code>async def get_actions(self):\n    \"\"\"\n    Retrieve the set of situational awareness actions applicable to the current network state.\n\n    Returns:\n        list: Identifiers of available network actions.\n    \"\"\"\n    return await self.np.get_actions()\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/sanetwork/#nebula.core.situationalawareness.awareness.sanetwork.sanetwork.SANetwork.get_nodes_known","title":"<code>get_nodes_known(neighbors_too=False, neighbors_only=False)</code>  <code>async</code>","text":"<p>Retrieve the list of known nodes in the network.</p> <p>Parameters:</p> Name Type Description Default <code>neighbors_too</code> <code>bool</code> <p>Include neighbors in the result. Defaults to False.</p> <code>False</code> <code>neighbors_only</code> <code>bool</code> <p>Return only neighbors. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>set</code> <p>Addresses of known nodes based on the provided filters.</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/sanetwork.py</code> <pre><code>async def get_nodes_known(self, neighbors_too=False, neighbors_only=False):\n    \"\"\"\n    Retrieve the list of known nodes in the network.\n\n    Args:\n        neighbors_too (bool, optional): Include neighbors in the result. Defaults to False.\n        neighbors_only (bool, optional): Return only neighbors. Defaults to False.\n\n    Returns:\n        set: Addresses of known nodes based on the provided filters.\n    \"\"\"\n    return await self.np.get_nodes_known(neighbors_too, neighbors_only)\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/sanetwork/#nebula.core.situationalawareness.awareness.sanetwork.sanetwork.SANetwork.get_restructure_process_lock","title":"<code>get_restructure_process_lock()</code>","text":"<p>Retrieve the asynchronous lock protecting the restructure process.</p> <p>Returns:</p> Type Description <p>asyncio.Lock: Lock to ensure only one restructure operation runs at a time.</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/sanetwork.py</code> <pre><code>def get_restructure_process_lock(self):\n    \"\"\"\n    Retrieve the asynchronous lock protecting the restructure process.\n\n    Returns:\n        asyncio.Lock: Lock to ensure only one restructure operation runs at a time.\n    \"\"\"\n    return self._restructure_process_lock\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/sanetwork/#nebula.core.situationalawareness.awareness.sanetwork.sanetwork.SANetwork.init","title":"<code>init()</code>  <code>async</code>","text":"<p>Initialize the SANetwork component by deploying external connection services, subscribing to relevant events, starting beaconing, and configuring neighbor policies.</p> <p>Actions performed: 1. If not an additional participant, start and subscribe to beacon and finish events. 2. Otherwise, initialize ECS without running it. 3. Build and apply the neighbor policy using current direct and undirected connections. 4. Subscribe to node discovery and neighbor update events. 5. Register this agent with the situational awareness network agent.</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/sanetwork.py</code> <pre><code>async def init(self):\n    \"\"\"\n    Initialize the SANetwork component by deploying external connection services,\n    subscribing to relevant events, starting beaconing, and configuring neighbor policies.\n\n    Actions performed:\n    1. If not an additional participant, start and subscribe to beacon and finish events.\n    2. Otherwise, initialize ECS without running it.\n    3. Build and apply the neighbor policy using current direct and undirected connections.\n    4. Subscribe to node discovery and neighbor update events.\n    5. Register this agent with the situational awareness network agent.\n    \"\"\"\n    if not self.sar.is_additional_participant():\n        logging.info(\"Deploying External Connection Service\")\n        await self.cm.start_external_connection_service()\n        await EventManager.get_instance().subscribe_node_event(BeaconRecievedEvent, self.beacon_received)\n        await EventManager.get_instance().subscribe_node_event(ExperimentFinishEvent, self.experiment_finish)\n        await self.cm.start_beacon()\n    else:\n        logging.info(\"Deploying External Connection Service | No running\")\n        await self.cm.start_external_connection_service(run_service=False)\n\n    logging.info(\"Building neighbor policy configuration..\")\n    await self.np.set_config([\n        await self.cm.get_addrs_current_connections(only_direct=True, myself=False),\n        await self.cm.get_addrs_current_connections(only_direct=False, only_undirected=False, myself=False),\n        self._addr,\n        self._strict_topology,\n    ])\n\n    await EventManager.get_instance().subscribe_node_event(NodeFoundEvent, self._process_node_found_event)\n    await EventManager.get_instance().subscribe_node_event(UpdateNeighborEvent, self._process_update_neighbor_event)\n    await self.sana.register_sa_agent()\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/sanetwork/#nebula.core.situationalawareness.awareness.sanetwork.sanetwork.SANetwork.meet_node","title":"<code>meet_node(node)</code>  <code>async</code>","text":"<p>Propose a meeting (connection) with a newly discovered node if it is not self.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>str</code> <p>The address of the node to meet.</p> required Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/sanetwork.py</code> <pre><code>async def meet_node(self, node):\n    \"\"\"\n    Propose a meeting (connection) with a newly discovered node if it is not self.\n\n    Args:\n        node (str): The address of the node to meet.\n    \"\"\"\n    if node != self._addr:\n        await self.np.meet_node(node)\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/sanetwork/#nebula.core.situationalawareness.awareness.sanetwork.sanetwork.SANetwork.need_more_neighbors","title":"<code>need_more_neighbors()</code>  <code>async</code>","text":"<p>Determine if the network requires additional neighbor connections.</p> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if more neighbors are needed, False otherwise.</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/sanetwork.py</code> <pre><code>async def need_more_neighbors(self):\n    \"\"\"\n    Determine if the network requires additional neighbor connections.\n\n    Returns:\n        bool: True if more neighbors are needed, False otherwise.\n    \"\"\"\n    return await self.np.need_more_neighbors()\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/sanetwork/#nebula.core.situationalawareness.awareness.sanetwork.sanetwork.SANetwork.neighbors_left","title":"<code>neighbors_left()</code>  <code>async</code>","text":"<p>Check whether any direct neighbor connections remain.</p> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if there are one or more direct neighbor connections, False otherwise.</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/sanetwork.py</code> <pre><code>async def neighbors_left(self):\n    \"\"\"\n    Check whether any direct neighbor connections remain.\n\n    Returns:\n        bool: True if there are one or more direct neighbor connections, False otherwise.\n    \"\"\"\n    return len(await self.cm.get_addrs_current_connections(only_direct=True, myself=False)) &gt; 0\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/sanetwork/#nebula.core.situationalawareness.awareness.sanetwork.sanetwork.SANetwork.reconnect_to_federation","title":"<code>reconnect_to_federation()</code>  <code>async</code>","text":"<p>Clear any connection restrictions and initiate a late\u2010connection discovery process to rejoin the federation.</p> <p>Steps: 1. Acquire the restructure lock. 2. Clear blacklist and recently disconnected restrictions. 3. If known node addresses exist, use them for discovery; otherwise, perform a fresh discovery. 4. Release the restructure lock.</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/sanetwork.py</code> <pre><code>async def reconnect_to_federation(self):\n    \"\"\"\n    Clear any connection restrictions and initiate a late\u2010connection discovery process\n    to rejoin the federation.\n\n    Steps:\n    1. Acquire the restructure lock.\n    2. Clear blacklist and recently disconnected restrictions.\n    3. If known node addresses exist, use them for discovery; otherwise, perform a fresh discovery.\n    4. Release the restructure lock.\n    \"\"\"\n    logging.info(\"Going to reconnect with federation...\")\n    await self._restructure_process_lock.acquire_async()\n    await self.cm.clear_restrictions()\n    # If we got some refs, try to reconnect to them\n    if len(await self.np.get_nodes_known()) &gt; 0:\n        if self._verbose:\n            logging.info(\"Reconnecting | Addrs availables\")\n        await self.sar.sad.start_late_connection_process(\n            connected=False, msg_type=\"discover_nodes\", addrs_known=await self.np.get_nodes_known()\n        )\n    else:\n        if self._verbose:\n            logging.info(\"Reconnecting | NO Addrs availables\")\n        await self.sar.sad.start_late_connection_process(connected=False, msg_type=\"discover_nodes\")\n    await self._restructure_process_lock.release_async()\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/sanetwork/#nebula.core.situationalawareness.awareness.sanetwork.sanetwork.SANetwork.sa_component_actions","title":"<code>sa_component_actions()</code>  <code>async</code>","text":"<p>Perform periodic situational awareness checks for network conditions.</p> <p>This method evaluates the external connection service status and analyzes the robustness of the current network topology.</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/sanetwork.py</code> <pre><code>async def sa_component_actions(self):\n    \"\"\"\n    Perform periodic situational awareness checks for network conditions.\n\n    This method evaluates the external connection service status and analyzes\n    the robustness of the current network topology.\n    \"\"\"\n    logging.info(\"SA Network evaluating current scenario\")\n    await self._check_external_connection_service_status()\n    await self._analize_topology_robustness()\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/sanetwork/#nebula.core.situationalawareness.awareness.sanetwork.sanetwork.SANetwork.stop","title":"<code>stop()</code>  <code>async</code>","text":"<p>Stop the SANetwork component by releasing locks and clearing any pending operations.</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/sanetwork.py</code> <pre><code>async def stop(self):\n    \"\"\"\n    Stop the SANetwork component by releasing locks and clearing any pending operations.\n    \"\"\"\n    logging.info(\"\ud83d\uded1  Stopping SANetwork...\")\n\n    # Cancel all verification tasks\n    async with self._verification_tasks_lock:\n        if self._verification_tasks:\n            tasks_to_cancel = [task for task in self._verification_tasks if not task.done()]\n            logging.info(f\"\ud83d\uded1  Cancelling {len(tasks_to_cancel)} verification tasks...\")\n            for task in tasks_to_cancel:\n                task.cancel()\n                try:\n                    await task\n                except asyncio.CancelledError:\n                    pass\n            self._verification_tasks.clear()\n            logging.info(\"\ud83d\uded1  All verification tasks cancelled\")\n\n    # Release any held locks\n    try:\n        if self._restructure_process_lock.locked():\n            self._restructure_process_lock.release()\n    except Exception as e:\n        logging.warning(f\"Error releasing restructure_process_lock: {e}\")\n\n    logging.info(\"\u2705  SANetwork stopped successfully\")\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/sanetwork/#nebula.core.situationalawareness.awareness.sanetwork.sanetwork.SANetwork.stop_connections_with_federation","title":"<code>stop_connections_with_federation()</code>  <code>async</code>","text":"<p>Disconnect from all current federation neighbors after a short delay.</p> <ol> <li>Waits for a predefined sleep period (to allow in\u2010flight messages to complete).</li> <li>Blacklists each direct neighbor.</li> <li>Disconnects from each neighbor without mutual handshake.</li> </ol> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/sanetwork.py</code> <pre><code>async def stop_connections_with_federation(self):\n    \"\"\"\n    Disconnect from all current federation neighbors after a short delay.\n\n    1. Waits for a predefined sleep period (to allow in\u2010flight messages to complete).\n    2. Blacklists each direct neighbor.\n    3. Disconnects from each neighbor without mutual handshake.\n    \"\"\"\n    await asyncio.sleep(10)\n    logging.info(\"### DISCONNECTING FROM FEDERATON ###\")\n    neighbors = await self.np.get_nodes_known(neighbors_only=True)\n    for n in neighbors:\n        await self.cm.add_to_blacklist(n)\n    for n in neighbors:\n        await self.cm.disconnect(n, mutual_disconnection=False, forced=True)\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/sanetwork/#nebula.core.situationalawareness.awareness.sanetwork.sanetwork.SANetwork.upgrade_connection_robustness","title":"<code>upgrade_connection_robustness(possible_neighbors)</code>  <code>async</code>","text":"<p>Attempt to strengthen network robustness by discovering or reconnecting to additional neighbors.</p> <p>Steps: 1. Acquire the restructure lock. 2. If possible_neighbors is non\u2010empty, use them for a targeted late\u2010connection discovery. 3. Otherwise, perform a generic discovery of federation nodes. 4. Release the restructure lock.</p> <p>Parameters:</p> Name Type Description Default <code>possible_neighbors</code> <code>set</code> <p>Addresses of candidate nodes for connection enhancement.</p> required Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/sanetwork.py</code> <pre><code>async def upgrade_connection_robustness(self, possible_neighbors):\n    \"\"\"\n    Attempt to strengthen network robustness by discovering or reconnecting to additional neighbors.\n\n    Steps:\n    1. Acquire the restructure lock.\n    2. If possible_neighbors is non\u2010empty, use them for a targeted late\u2010connection discovery.\n    3. Otherwise, perform a generic discovery of federation nodes.\n    4. Release the restructure lock.\n\n    Args:\n        possible_neighbors (set): Addresses of candidate nodes for connection enhancement.\n    \"\"\"\n    await self._restructure_process_lock.acquire_async()\n    # If we got some refs, try to connect to them\n    if possible_neighbors and len(possible_neighbors) &gt; 0:\n        if self._verbose:\n            logging.info(f\"Reestructuring | Addrs availables | addr list: {possible_neighbors}\")\n        await self.sar.sad.start_late_connection_process(\n            connected=True, msg_type=\"discover_nodes\", addrs_known=possible_neighbors\n        )\n    else:\n        if self._verbose:\n            logging.info(\"Reestructuring | NO Addrs availables\")\n        await self.sar.sad.start_late_connection_process(connected=True, msg_type=\"discover_nodes\")\n    await self._restructure_process_lock.release_async()\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/sanetwork/#nebula.core.situationalawareness.awareness.sanetwork.sanetwork.SANetwork.verify_neighbors_stablished","title":"<code>verify_neighbors_stablished(nodes)</code>  <code>async</code>","text":"<p>Verify that a set of connection attempts has succeeded within a timeout.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>set</code> <p>The set of node addresses for which connections were attempted.</p> required <p>Behavior: 1. Sleeps for NEIGHBOR_VERIFICATION_TIMEOUT seconds. 2. Compares the originally requested nodes against the currently known neighbors. 3. Logs any addresses that failed to establish and instructs the policy to forget them.</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/sanetwork.py</code> <pre><code>async def verify_neighbors_stablished(self, nodes: set):\n    \"\"\"\n    Verify that a set of connection attempts has succeeded within a timeout.\n\n    Args:\n        nodes (set): The set of node addresses for which connections were attempted.\n\n    Behavior:\n    1. Sleeps for NEIGHBOR_VERIFICATION_TIMEOUT seconds.\n    2. Compares the originally requested nodes against the currently known neighbors.\n    3. Logs any addresses that failed to establish and instructs the policy to forget them.\n    \"\"\"\n    if not nodes:\n        return\n\n    await asyncio.sleep(self.NEIGHBOR_VERIFICATION_TIMEOUT)\n    logging.info(\"Verifyng all connections were stablished\")\n    nodes_to_forget = nodes.copy()\n    neighbors = await self.np.get_nodes_known(neighbors_only=True)\n    if neighbors:\n        nodes_to_forget.difference_update(neighbors)\n    logging.info(f\"Connections dont stablished: {nodes_to_forget}\")\n    await self.forget_nodes(nodes_to_forget)\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/sanetwork/#nebula.core.situationalawareness.awareness.sanetwork.sanetwork.SANetworkAgent","title":"<code>SANetworkAgent</code>","text":"<p>               Bases: <code>SAModuleAgent</code></p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/sanetwork.py</code> <pre><code>class SANetworkAgent(SAModuleAgent):\n    def __init__(self, sanetwork: SANetwork):\n        self._san = sanetwork\n\n    async def get_agent(self) -&gt; str:\n        return \"SANetwork_MainNetworkAgent\"\n\n    async def register_sa_agent(self):\n        await SuggestionBuffer.get_instance().register_event_agents(RoundEndEvent, self)\n\n    async def suggest_action(self, sac: SACommand):\n        await SuggestionBuffer.get_instance().register_suggestion(RoundEndEvent, self, sac)\n\n    async def notify_all_suggestions_done(self, event_type):\n        await SuggestionBuffer.get_instance().notify_all_suggestions_done_for_agent(self, event_type)\n\n    async def create_and_suggest_action(\n        self, saca: SACommandAction, function: Callable = None, more_suggestions=False, *args\n    ):\n        \"\"\"\n        Create a situational awareness command based on the specified action and suggest it for arbitration.\n\n        Depending on the SACommandAction provided, this method:\n        - Instantiates the appropriate SACommand via the factory.\n        - Submits the command to the arbitration process (`suggest_action`).\n        - Optionally finalizes suggestion collection (`notify_all_suggestions_done`).\n        - In some cases waits for execution.\n\n        Args:\n            saca (SACommandAction): The situational awareness action to suggest (e.g., SEARCH_CONNECTIONS, RECONNECT).\n            function (Callable, optional): The function to execute if the command is chosen. Defaults to None.\n            more_suggestions (bool, optional): If False, marks the end of suggestion gathering. Defaults to False.\n            *args: Additional positional arguments passed to the SACommand constructor to be used as function parameters.\n        \"\"\"\n        sac = None\n        if saca == SACommandAction.MAINTAIN_CONNECTIONS:\n            sac = factory_sa_command(\n                \"connectivity\", SACommandAction.MAINTAIN_CONNECTIONS, self, \"\", SACommandPRIO.MEDIUM, False, function\n            )\n            await self.suggest_action(sac)\n            await self.notify_all_suggestions_done(RoundEndEvent)\n        elif saca == SACommandAction.SEARCH_CONNECTIONS:\n            sac = factory_sa_command(\n                \"connectivity\",\n                SACommandAction.SEARCH_CONNECTIONS,\n                self,\n                \"\",\n                SACommandPRIO.MEDIUM,\n                True,\n                function,\n                *args,\n            )\n            await self.suggest_action(sac)\n            if not more_suggestions:\n                await self.notify_all_suggestions_done(RoundEndEvent)\n            sa_command_state = await sac.get_state_future()  # By using 'await' we get future.set_result()\n            if sa_command_state == SACommandState.EXECUTED:\n                (nodes_to_forget,) = args\n                await self._san.create_verification_task(nodes_to_forget)\n        elif saca == SACommandAction.RECONNECT:\n            sac = factory_sa_command(\n                \"connectivity\", SACommandAction.RECONNECT, self, \"\", SACommandPRIO.HIGH, True, function\n            )\n            await self.suggest_action(sac)\n            if not more_suggestions:\n                await self.notify_all_suggestions_done(RoundEndEvent)\n        elif saca == SACommandAction.DISCONNECT:\n            nodes = args[0] if isinstance(args[0], set) else set(args)\n            for node in nodes:\n                sac = factory_sa_command(\n                    \"connectivity\",\n                    SACommandAction.DISCONNECT,\n                    self,\n                    node,\n                    SACommandPRIO.HIGH,\n                    True,\n                    function,\n                    node,\n                    True,\n                )\n                # TODO Check executed state to ensure node is removed\n                await self.suggest_action(sac)\n            if not more_suggestions:\n                await self.notify_all_suggestions_done(RoundEndEvent)\n        elif saca == SACommandAction.IDLE:\n            sac = factory_sa_command(\n                \"connectivity\", SACommandAction.IDLE, self, \"\", SACommandPRIO.LOW, False, function, None\n            )\n            await self.suggest_action(sac)\n            if not more_suggestions:\n                await self.notify_all_suggestions_done(RoundEndEvent)\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/sanetwork/#nebula.core.situationalawareness.awareness.sanetwork.sanetwork.SANetworkAgent.create_and_suggest_action","title":"<code>create_and_suggest_action(saca, function=None, more_suggestions=False, *args)</code>  <code>async</code>","text":"<p>Create a situational awareness command based on the specified action and suggest it for arbitration.</p> <p>Depending on the SACommandAction provided, this method: - Instantiates the appropriate SACommand via the factory. - Submits the command to the arbitration process (<code>suggest_action</code>). - Optionally finalizes suggestion collection (<code>notify_all_suggestions_done</code>). - In some cases waits for execution.</p> <p>Parameters:</p> Name Type Description Default <code>saca</code> <code>SACommandAction</code> <p>The situational awareness action to suggest (e.g., SEARCH_CONNECTIONS, RECONNECT).</p> required <code>function</code> <code>Callable</code> <p>The function to execute if the command is chosen. Defaults to None.</p> <code>None</code> <code>more_suggestions</code> <code>bool</code> <p>If False, marks the end of suggestion gathering. Defaults to False.</p> <code>False</code> <code>*args</code> <p>Additional positional arguments passed to the SACommand constructor to be used as function parameters.</p> <code>()</code> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/sanetwork.py</code> <pre><code>async def create_and_suggest_action(\n    self, saca: SACommandAction, function: Callable = None, more_suggestions=False, *args\n):\n    \"\"\"\n    Create a situational awareness command based on the specified action and suggest it for arbitration.\n\n    Depending on the SACommandAction provided, this method:\n    - Instantiates the appropriate SACommand via the factory.\n    - Submits the command to the arbitration process (`suggest_action`).\n    - Optionally finalizes suggestion collection (`notify_all_suggestions_done`).\n    - In some cases waits for execution.\n\n    Args:\n        saca (SACommandAction): The situational awareness action to suggest (e.g., SEARCH_CONNECTIONS, RECONNECT).\n        function (Callable, optional): The function to execute if the command is chosen. Defaults to None.\n        more_suggestions (bool, optional): If False, marks the end of suggestion gathering. Defaults to False.\n        *args: Additional positional arguments passed to the SACommand constructor to be used as function parameters.\n    \"\"\"\n    sac = None\n    if saca == SACommandAction.MAINTAIN_CONNECTIONS:\n        sac = factory_sa_command(\n            \"connectivity\", SACommandAction.MAINTAIN_CONNECTIONS, self, \"\", SACommandPRIO.MEDIUM, False, function\n        )\n        await self.suggest_action(sac)\n        await self.notify_all_suggestions_done(RoundEndEvent)\n    elif saca == SACommandAction.SEARCH_CONNECTIONS:\n        sac = factory_sa_command(\n            \"connectivity\",\n            SACommandAction.SEARCH_CONNECTIONS,\n            self,\n            \"\",\n            SACommandPRIO.MEDIUM,\n            True,\n            function,\n            *args,\n        )\n        await self.suggest_action(sac)\n        if not more_suggestions:\n            await self.notify_all_suggestions_done(RoundEndEvent)\n        sa_command_state = await sac.get_state_future()  # By using 'await' we get future.set_result()\n        if sa_command_state == SACommandState.EXECUTED:\n            (nodes_to_forget,) = args\n            await self._san.create_verification_task(nodes_to_forget)\n    elif saca == SACommandAction.RECONNECT:\n        sac = factory_sa_command(\n            \"connectivity\", SACommandAction.RECONNECT, self, \"\", SACommandPRIO.HIGH, True, function\n        )\n        await self.suggest_action(sac)\n        if not more_suggestions:\n            await self.notify_all_suggestions_done(RoundEndEvent)\n    elif saca == SACommandAction.DISCONNECT:\n        nodes = args[0] if isinstance(args[0], set) else set(args)\n        for node in nodes:\n            sac = factory_sa_command(\n                \"connectivity\",\n                SACommandAction.DISCONNECT,\n                self,\n                node,\n                SACommandPRIO.HIGH,\n                True,\n                function,\n                node,\n                True,\n            )\n            # TODO Check executed state to ensure node is removed\n            await self.suggest_action(sac)\n        if not more_suggestions:\n            await self.notify_all_suggestions_done(RoundEndEvent)\n    elif saca == SACommandAction.IDLE:\n        sac = factory_sa_command(\n            \"connectivity\", SACommandAction.IDLE, self, \"\", SACommandPRIO.LOW, False, function, None\n        )\n        await self.suggest_action(sac)\n        if not more_suggestions:\n            await self.notify_all_suggestions_done(RoundEndEvent)\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/","title":"Documentation for Neighborpolicies Module","text":""},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/distanceneighborpolicy/","title":"Documentation for Distanceneighborpolicy Module","text":""},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/distanceneighborpolicy/#nebula.core.situationalawareness.awareness.sanetwork.neighborpolicies.distanceneighborpolicy.DistanceNeighborPolicy","title":"<code>DistanceNeighborPolicy</code>","text":"<p>               Bases: <code>NeighborPolicy</code></p> <p>Neighbor policy based on physical distance between nodes.</p> <p>This policy governs decisions related to neighbor management, including: - When to initiate discovery for new neighbors. - Whether to accept a new incoming neighbor connection. - When to discard or replace existing neighbors. - Keeping track of current neighbors and known nodes with their distances.</p> <p>The policy operates under the assumption that physical proximity  can be beneficial for performance and robustness in the network.</p> <p>Attributes:</p> Name Type Description <code>max_neighbors</code> <code>int | None</code> <p>Maximum number of neighbors allowed for this node.</p> <code>nodes_known</code> <code>set[str]</code> <p>Set of all known node IDs, including potential neighbors.</p> <code>neighbors</code> <code>set[str]</code> <p>Set of currently accepted neighbor node IDs.</p> <code>addr</code> <code>str | None</code> <p>The address of this node (used for self-identification).</p> <code>neighbors_lock</code> <code>Locker</code> <p>Async lock for safe access to <code>neighbors</code>.</p> <code>nodes_known_lock</code> <code>Locker</code> <p>Async lock for safe access to <code>nodes_known</code>.</p> <code>nodes_distances</code> <code>dict[str, tuple[float, tuple[float, float]]] | None</code> <p>Mapping from node IDs to a tuple containing (distance, (latitude, longitude)).</p> <code>nodes_distances_lock</code> <code>Locker</code> <p>Async lock for safe access to <code>nodes_distances</code>.</p> <code>_verbose</code> <code>bool</code> <p>Whether to enable verbose logging for debugging purposes.</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/neighborpolicies/distanceneighborpolicy.py</code> <pre><code>class DistanceNeighborPolicy(NeighborPolicy):\n    \"\"\"\n    Neighbor policy based on physical distance between nodes.\n\n    This policy governs decisions related to neighbor management, including:\n    - When to initiate discovery for new neighbors.\n    - Whether to accept a new incoming neighbor connection.\n    - When to discard or replace existing neighbors.\n    - Keeping track of current neighbors and known nodes with their distances.\n\n    The policy operates under the assumption that physical proximity \n    can be beneficial for performance and robustness in the network.\n\n    Attributes:\n        max_neighbors (int | None): Maximum number of neighbors allowed for this node.\n        nodes_known (set[str]): Set of all known node IDs, including potential neighbors.\n        neighbors (set[str]): Set of currently accepted neighbor node IDs.\n        addr (str | None): The address of this node (used for self-identification).\n        neighbors_lock (Locker): Async lock for safe access to `neighbors`.\n        nodes_known_lock (Locker): Async lock for safe access to `nodes_known`.\n        nodes_distances (dict[str, tuple[float, tuple[float, float]]] | None): \n            Mapping from node IDs to a tuple containing (distance, (latitude, longitude)).\n        nodes_distances_lock (Locker): Async lock for safe access to `nodes_distances`.\n        _verbose (bool): Whether to enable verbose logging for debugging purposes.\n    \"\"\"\n    # INFO: This value may change according to the needs of the federation\n    MAX_DISTANCE_THRESHOLD = 200\n\n    def __init__(self):\n        self.max_neighbors = None\n        self.nodes_known = set()\n        self.neighbors = set()\n        self.addr = None\n        self.neighbors_lock = Locker(name=\"neighbors_lock\", async_lock=True)\n        self.nodes_known_lock = Locker(name=\"nodes_known_lock\", async_lock=True)\n        self.nodes_distances: dict[str, tuple[float, tuple[float, float]]] = None\n        self.nodes_distances_lock = Locker(\"nodes_distances_lock\", async_lock=True)\n        self._verbose = False\n\n    async def set_config(self, config):\n        \"\"\"\n        Args:\n            config[0] -&gt; list of self neighbors\n            config[1] -&gt; list of nodes known on federation\n            config[2] -&gt; self addr\n            config[3] -&gt; stricted_topology\n        \"\"\"\n        logging.info(\"Initializing Distance Topology Neighbor Policy\")\n        async with self.neighbors_lock:\n            self.neighbors = config[0]\n        for addr in config[1]:\n            self.nodes_known.add(addr)\n        self.addr\n\n        await EventManager.get_instance().subscribe_addonevent(GPSEvent, self._udpate_distances)\n\n    async def _udpate_distances(self, gpsevent: GPSEvent):\n        async with self.nodes_distances_lock:\n            distances = await gpsevent.get_event_data()\n            self.nodes_distances = distances\n\n    async def need_more_neighbors(self):\n        async with self.neighbors_lock:\n            async with self.nodes_distances_lock:\n                if not self.nodes_distances:\n                    return False\n\n                closest_nodes: set[str] = {\n                    nodo_id\n                    for nodo_id, (distancia, _) in self.nodes_distances.items()\n                    if distancia &lt; self.MAX_DISTANCE_THRESHOLD\n                }\n                available_nodes = closest_nodes.difference(self.neighbors)\n                if self._verbose:\n                    logging.info(f\"Available neighbors based on distance: {available_nodes}\")\n                return len(available_nodes) &gt; 0\n\n    async def accept_connection(self, source, joining=False):\n        \"\"\"\n        return true if connection is accepted\n        \"\"\"\n        async with self.neighbors_lock:\n            ac = source not in self.neighbors\n        return ac\n\n    async def meet_node(self, node):\n        \"\"\"\n        Update the list of nodes known on federation\n        \"\"\"\n        async with self.nodes_known_lock:\n            if node != self.addr:\n                if node not in self.nodes_known:\n                    logging.info(f\"Update nodes known | addr: {node}\")\n                self.nodes_known.add(node)\n\n    async def get_nodes_known(self, neighbors_too=False, neighbors_only=False):\n        if neighbors_only:\n            async with self.neighbors_lock:\n                no = self.neighbors.copy()\n                return no\n\n        async with self.nodes_known_lock:\n            nk = self.nodes_known.copy()\n            if not neighbors_too:\n                async with self.neighbors_lock:\n                    nk = self.nodes_known - self.neighbors\n        return nk\n\n    async def forget_nodes(self, nodes, forget_all=False):\n        async with self.nodes_known_lock:\n            if forget_all:\n                self.nodes_known.clear()\n            else:\n                for node in nodes:\n                    self.nodes_known.discard(node)\n\n    async def get_actions(self):\n        \"\"\"\n        return list of actions to do in response to connection\n            - First list represents addrs argument to LinkMessage to connect to\n            - Second one represents the same but for disconnect from LinkMessage\n        \"\"\"\n        return [await self._connect_to(), await self._disconnect_from()]\n\n    async def _disconnect_from(self):\n        return \"\"\n\n    async def _connect_to(self):\n        ct = \"\"\n        async with self.neighbors_lock:\n            ct = \" \".join(self.neighbors)\n        return ct\n\n    async def update_neighbors(self, node, remove=False):\n        if node == self.addr:\n            return\n        async with self.neighbors_lock:\n            if remove:\n                try:\n                    self.neighbors.remove(node)\n                    if self._verbose:\n                        logging.info(f\"Remove neighbor | addr: {node}\")\n                except KeyError:\n                    pass\n            else:\n                self.neighbors.add(node)\n                if self._verbose:\n                    logging.info(f\"Add neighbor | addr: {node}\")\n\n    async def get_posible_neighbors(self):\n        \"\"\"Return set of posible neighbors to connect to.\"\"\"\n        async with self.neighbors_lock:\n            async with self.nodes_distances_lock:\n                closest_nodes: set[str] = {\n                    nodo_id\n                    for nodo_id, (distancia, _) in self.nodes_distances.items()\n                    if distancia &lt; self.MAX_DISTANCE_THRESHOLD - 20\n                }\n                if self._verbose:\n                    logging.info(f\"Closest nodes: {closest_nodes}, neighbors: {self.neighbors}\")\n                available_nodes = closest_nodes.difference(self.neighbors)\n                if self._verbose:\n                    logging.info(f\"Available neighbors based on distance: {available_nodes}\")\n                return available_nodes\n\n    async def any_leftovers_neighbors(self):\n        distant_nodes = set()\n        async with self.neighbors_lock:\n            async with self.nodes_distances_lock:\n                if not self.nodes_distances:\n                    return False\n\n                distant_nodes: set[str] = {\n                    nodo_id\n                    for nodo_id, (distancia, _) in self.nodes_distances.items()\n                    if distancia &gt; self.MAX_DISTANCE_THRESHOLD\n                }\n                distant_nodes = self.neighbors.intersection(distant_nodes)\n                if self._verbose:\n                    logging.info(f\"Distant neighbors based on distance: {distant_nodes}\")\n        return len(distant_nodes) &gt; 0\n\n    async def get_neighbors_to_remove(self):\n        distant_nodes = set()\n        async with self.neighbors_lock:\n            async with self.nodes_distances_lock:\n                distant_nodes: set[str] = {\n                    nodo_id\n                    for nodo_id, (distancia, _) in self.nodes_distances.items()\n                    if distancia &gt; self.MAX_DISTANCE_THRESHOLD\n                }\n                distant_nodes = self.neighbors.intersection(distant_nodes)\n                if self._verbose:\n                    logging.info(f\"Remove neighbors based on distance: {distant_nodes}\")\n        return distant_nodes\n\n    def stricted_topology_status(stricted_topology: bool):\n        pass\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/distanceneighborpolicy/#nebula.core.situationalawareness.awareness.sanetwork.neighborpolicies.distanceneighborpolicy.DistanceNeighborPolicy.accept_connection","title":"<code>accept_connection(source, joining=False)</code>  <code>async</code>","text":"<p>return true if connection is accepted</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/neighborpolicies/distanceneighborpolicy.py</code> <pre><code>async def accept_connection(self, source, joining=False):\n    \"\"\"\n    return true if connection is accepted\n    \"\"\"\n    async with self.neighbors_lock:\n        ac = source not in self.neighbors\n    return ac\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/distanceneighborpolicy/#nebula.core.situationalawareness.awareness.sanetwork.neighborpolicies.distanceneighborpolicy.DistanceNeighborPolicy.get_actions","title":"<code>get_actions()</code>  <code>async</code>","text":"<p>return list of actions to do in response to connection     - First list represents addrs argument to LinkMessage to connect to     - Second one represents the same but for disconnect from LinkMessage</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/neighborpolicies/distanceneighborpolicy.py</code> <pre><code>async def get_actions(self):\n    \"\"\"\n    return list of actions to do in response to connection\n        - First list represents addrs argument to LinkMessage to connect to\n        - Second one represents the same but for disconnect from LinkMessage\n    \"\"\"\n    return [await self._connect_to(), await self._disconnect_from()]\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/distanceneighborpolicy/#nebula.core.situationalawareness.awareness.sanetwork.neighborpolicies.distanceneighborpolicy.DistanceNeighborPolicy.get_posible_neighbors","title":"<code>get_posible_neighbors()</code>  <code>async</code>","text":"<p>Return set of posible neighbors to connect to.</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/neighborpolicies/distanceneighborpolicy.py</code> <pre><code>async def get_posible_neighbors(self):\n    \"\"\"Return set of posible neighbors to connect to.\"\"\"\n    async with self.neighbors_lock:\n        async with self.nodes_distances_lock:\n            closest_nodes: set[str] = {\n                nodo_id\n                for nodo_id, (distancia, _) in self.nodes_distances.items()\n                if distancia &lt; self.MAX_DISTANCE_THRESHOLD - 20\n            }\n            if self._verbose:\n                logging.info(f\"Closest nodes: {closest_nodes}, neighbors: {self.neighbors}\")\n            available_nodes = closest_nodes.difference(self.neighbors)\n            if self._verbose:\n                logging.info(f\"Available neighbors based on distance: {available_nodes}\")\n            return available_nodes\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/distanceneighborpolicy/#nebula.core.situationalawareness.awareness.sanetwork.neighborpolicies.distanceneighborpolicy.DistanceNeighborPolicy.meet_node","title":"<code>meet_node(node)</code>  <code>async</code>","text":"<p>Update the list of nodes known on federation</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/neighborpolicies/distanceneighborpolicy.py</code> <pre><code>async def meet_node(self, node):\n    \"\"\"\n    Update the list of nodes known on federation\n    \"\"\"\n    async with self.nodes_known_lock:\n        if node != self.addr:\n            if node not in self.nodes_known:\n                logging.info(f\"Update nodes known | addr: {node}\")\n            self.nodes_known.add(node)\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/distanceneighborpolicy/#nebula.core.situationalawareness.awareness.sanetwork.neighborpolicies.distanceneighborpolicy.DistanceNeighborPolicy.set_config","title":"<code>set_config(config)</code>  <code>async</code>","text":"Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/neighborpolicies/distanceneighborpolicy.py</code> <pre><code>async def set_config(self, config):\n    \"\"\"\n    Args:\n        config[0] -&gt; list of self neighbors\n        config[1] -&gt; list of nodes known on federation\n        config[2] -&gt; self addr\n        config[3] -&gt; stricted_topology\n    \"\"\"\n    logging.info(\"Initializing Distance Topology Neighbor Policy\")\n    async with self.neighbors_lock:\n        self.neighbors = config[0]\n    for addr in config[1]:\n        self.nodes_known.add(addr)\n    self.addr\n\n    await EventManager.get_instance().subscribe_addonevent(GPSEvent, self._udpate_distances)\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/fcneighborpolicy/","title":"Documentation for Fcneighborpolicy Module","text":""},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/fcneighborpolicy/#nebula.core.situationalawareness.awareness.sanetwork.neighborpolicies.fcneighborpolicy.FCNeighborPolicy","title":"<code>FCNeighborPolicy</code>","text":"<p>               Bases: <code>NeighborPolicy</code></p> <p>Neighbor policy for fully-connected (FC) structured topologies.</p> <p>This policy assumes a fully-connected topology where every node should attempt  to connect to all known nodes. It always accepts incoming neighbor connections  and considers the neighbor list incomplete if there are known nodes that are not yet connected.</p> <p>The goal is to maintain full connectivity across all known nodes in the federation.</p> <p>Attributes:</p> Name Type Description <code>max_neighbors</code> <code>int | None</code> <p>Unused in FC topology, but kept for compatibility.</p> <code>nodes_known</code> <code>set[str]</code> <p>Set of all known node IDs discovered in the federation.</p> <code>neighbors</code> <code>set[str]</code> <p>Set of currently connected neighbor node IDs.</p> <code>addr</code> <code>str | None</code> <p>The address of this node (used for self-identification).</p> <code>neighbors_lock</code> <code>Locker</code> <p>Async lock for safe access to <code>neighbors</code>.</p> <code>nodes_known_lock</code> <code>Locker</code> <p>Async lock for safe access to <code>nodes_known</code>.</p> <code>_verbose</code> <code>bool</code> <p>Whether to enable verbose logging for debugging purposes.</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/neighborpolicies/fcneighborpolicy.py</code> <pre><code>class FCNeighborPolicy(NeighborPolicy):\n    \"\"\"\n    Neighbor policy for fully-connected (FC) structured topologies.\n\n    This policy assumes a fully-connected topology where every node should attempt \n    to connect to all known nodes. It always accepts incoming neighbor connections \n    and considers the neighbor list incomplete if there are known nodes that are not yet connected.\n\n    The goal is to maintain full connectivity across all known nodes in the federation.\n\n    Attributes:\n        max_neighbors (int | None): Unused in FC topology, but kept for compatibility.\n        nodes_known (set[str]): Set of all known node IDs discovered in the federation.\n        neighbors (set[str]): Set of currently connected neighbor node IDs.\n        addr (str | None): The address of this node (used for self-identification).\n        neighbors_lock (Locker): Async lock for safe access to `neighbors`.\n        nodes_known_lock (Locker): Async lock for safe access to `nodes_known`.\n        _verbose (bool): Whether to enable verbose logging for debugging purposes.\n    \"\"\"\n\n    def __init__(self):\n        self.max_neighbors = None\n        self.nodes_known = set()\n        self.neighbors = set()\n        self.addr = None\n        self.neighbors_lock = Locker(name=\"neighbors_lock\")\n        self.nodes_known_lock = Locker(name=\"nodes_known_lock\")\n        self._verbose = False\n\n    async def need_more_neighbors(self):\n        \"\"\"\n        Fully connected network requires to be connected to all devices, therefore,\n        if there are more nodes known that self.neighbors, more neighbors are required\n        \"\"\"\n        self.neighbors_lock.acquire()\n        need_more = len(self.neighbors) &lt; len(self.nodes_known)\n        self.neighbors_lock.release()\n        return need_more\n\n    async def set_config(self, config):\n        \"\"\"\n        Args:\n            config[0] -&gt; list of self neighbors\n            config[1] -&gt; list of nodes known on federation\n            config[2] -&gt; self addr\n            config[3] -&gt; stricted_topology\n        \"\"\"\n        logging.info(\"Initializing Fully-Connected Topology Neighbor Policy\")\n        self.neighbors_lock.acquire()\n        self.neighbors = config[0]\n        self.neighbors_lock.release()\n        for addr in config[1]:\n            self.nodes_known.add(addr)\n        self.addr\n\n    async def accept_connection(self, source, joining=False):\n        \"\"\"\n        return true if connection is accepted\n        \"\"\"\n        self.neighbors_lock.acquire()\n        ac = source not in self.neighbors\n        self.neighbors_lock.release()\n        return ac\n\n    async def meet_node(self, node):\n        \"\"\"\n        Update the list of nodes known on federation\n        \"\"\"\n        self.nodes_known_lock.acquire()\n        if node != self.addr:\n            if node not in self.nodes_known:\n                logging.info(f\"Update nodes known | addr: {node}\")\n            self.nodes_known.add(node)\n        self.nodes_known_lock.release()\n\n    async def get_nodes_known(self, neighbors_too=False, neighbors_only=False):\n        if neighbors_only:\n            self.neighbors_lock.acquire()\n            no = self.neighbors.copy()\n            self.neighbors_lock.release()\n            return no\n\n        self.nodes_known_lock.acquire()\n        nk = self.nodes_known.copy()\n        if not neighbors_too:\n            self.neighbors_lock.acquire()\n            nk = self.nodes_known - self.neighbors\n            self.neighbors_lock.release()\n        self.nodes_known_lock.release()\n        return nk\n\n    async def forget_nodes(self, nodes, forget_all=False):\n        self.nodes_known_lock.acquire()\n        if forget_all:\n            self.nodes_known.clear()\n        else:\n            for node in nodes:\n                self.nodes_known.discard(node)\n        self.nodes_known_lock.release()\n\n    async def get_actions(self):\n        \"\"\"\n        return list of actions to do in response to connection\n            - First list represents addrs argument to LinkMessage to connect to\n            - Second one represents the same but for disconnect from LinkMessage\n        \"\"\"\n        return [await self._connect_to(), await self._disconnect_from()]\n\n    async def _disconnect_from(self):\n        return \"\"\n\n    async def _connect_to(self):\n        ct = \"\"\n        self.neighbors_lock.acquire()\n        ct = \" \".join(self.neighbors)\n        self.neighbors_lock.release()\n        return ct\n\n    async def update_neighbors(self, node, remove=False):\n        if node == self.addr:\n            return\n        self.neighbors_lock.acquire()\n        if remove:\n            try:\n                self.neighbors.remove(node)\n                if self._verbose:\n                    logging.info(f\"Remove neighbor | addr: {node}\")\n            except KeyError:\n                pass\n        else:\n            self.neighbors.add(node)\n            if self._verbose:\n                logging.info(f\"Add neighbor | addr: {node}\")\n        self.neighbors_lock.release()\n\n    async def any_leftovers_neighbors(self):\n        return False\n\n    async def get_neighbors_to_remove(self):\n        return set()\n\n    async def get_posible_neighbors(self):\n        \"\"\"Return set of posible neighbors to connect to.\"\"\"\n        return await self.get_nodes_known(neighbors_too=False)\n\n    async def stricted_topology_status(stricted_topology: bool):\n        pass\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/fcneighborpolicy/#nebula.core.situationalawareness.awareness.sanetwork.neighborpolicies.fcneighborpolicy.FCNeighborPolicy.accept_connection","title":"<code>accept_connection(source, joining=False)</code>  <code>async</code>","text":"<p>return true if connection is accepted</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/neighborpolicies/fcneighborpolicy.py</code> <pre><code>async def accept_connection(self, source, joining=False):\n    \"\"\"\n    return true if connection is accepted\n    \"\"\"\n    self.neighbors_lock.acquire()\n    ac = source not in self.neighbors\n    self.neighbors_lock.release()\n    return ac\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/fcneighborpolicy/#nebula.core.situationalawareness.awareness.sanetwork.neighborpolicies.fcneighborpolicy.FCNeighborPolicy.get_actions","title":"<code>get_actions()</code>  <code>async</code>","text":"<p>return list of actions to do in response to connection     - First list represents addrs argument to LinkMessage to connect to     - Second one represents the same but for disconnect from LinkMessage</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/neighborpolicies/fcneighborpolicy.py</code> <pre><code>async def get_actions(self):\n    \"\"\"\n    return list of actions to do in response to connection\n        - First list represents addrs argument to LinkMessage to connect to\n        - Second one represents the same but for disconnect from LinkMessage\n    \"\"\"\n    return [await self._connect_to(), await self._disconnect_from()]\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/fcneighborpolicy/#nebula.core.situationalawareness.awareness.sanetwork.neighborpolicies.fcneighborpolicy.FCNeighborPolicy.get_posible_neighbors","title":"<code>get_posible_neighbors()</code>  <code>async</code>","text":"<p>Return set of posible neighbors to connect to.</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/neighborpolicies/fcneighborpolicy.py</code> <pre><code>async def get_posible_neighbors(self):\n    \"\"\"Return set of posible neighbors to connect to.\"\"\"\n    return await self.get_nodes_known(neighbors_too=False)\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/fcneighborpolicy/#nebula.core.situationalawareness.awareness.sanetwork.neighborpolicies.fcneighborpolicy.FCNeighborPolicy.meet_node","title":"<code>meet_node(node)</code>  <code>async</code>","text":"<p>Update the list of nodes known on federation</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/neighborpolicies/fcneighborpolicy.py</code> <pre><code>async def meet_node(self, node):\n    \"\"\"\n    Update the list of nodes known on federation\n    \"\"\"\n    self.nodes_known_lock.acquire()\n    if node != self.addr:\n        if node not in self.nodes_known:\n            logging.info(f\"Update nodes known | addr: {node}\")\n        self.nodes_known.add(node)\n    self.nodes_known_lock.release()\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/fcneighborpolicy/#nebula.core.situationalawareness.awareness.sanetwork.neighborpolicies.fcneighborpolicy.FCNeighborPolicy.need_more_neighbors","title":"<code>need_more_neighbors()</code>  <code>async</code>","text":"<p>Fully connected network requires to be connected to all devices, therefore, if there are more nodes known that self.neighbors, more neighbors are required</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/neighborpolicies/fcneighborpolicy.py</code> <pre><code>async def need_more_neighbors(self):\n    \"\"\"\n    Fully connected network requires to be connected to all devices, therefore,\n    if there are more nodes known that self.neighbors, more neighbors are required\n    \"\"\"\n    self.neighbors_lock.acquire()\n    need_more = len(self.neighbors) &lt; len(self.nodes_known)\n    self.neighbors_lock.release()\n    return need_more\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/fcneighborpolicy/#nebula.core.situationalawareness.awareness.sanetwork.neighborpolicies.fcneighborpolicy.FCNeighborPolicy.set_config","title":"<code>set_config(config)</code>  <code>async</code>","text":"Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/neighborpolicies/fcneighborpolicy.py</code> <pre><code>async def set_config(self, config):\n    \"\"\"\n    Args:\n        config[0] -&gt; list of self neighbors\n        config[1] -&gt; list of nodes known on federation\n        config[2] -&gt; self addr\n        config[3] -&gt; stricted_topology\n    \"\"\"\n    logging.info(\"Initializing Fully-Connected Topology Neighbor Policy\")\n    self.neighbors_lock.acquire()\n    self.neighbors = config[0]\n    self.neighbors_lock.release()\n    for addr in config[1]:\n        self.nodes_known.add(addr)\n    self.addr\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/idleneighborpolicy/","title":"Documentation for Idleneighborpolicy Module","text":""},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/idleneighborpolicy/#nebula.core.situationalawareness.awareness.sanetwork.neighborpolicies.idleneighborpolicy.IDLENeighborPolicy","title":"<code>IDLENeighborPolicy</code>","text":"<p>               Bases: <code>NeighborPolicy</code></p> <p>Neighbor policy for minimal connectivity scenarios.</p> <p>This policy only attempts to discover or establish new neighbor connections  if the node is currently isolated (i.e., has no neighbors). All incoming  connection requests are accepted regardless of the current neighbor state.</p> <p>This policy is suitable for scenarios where minimal intervention is preferred,  and connections are formed opportunistically rather than proactively.</p> <p>Attributes:</p> Name Type Description <code>max_neighbors</code> <code>int | None</code> <p>Unused in this policy but maintained for compatibility.</p> <code>nodes_known</code> <code>set[str]</code> <p>Set of known node IDs discovered during federation.</p> <code>neighbors</code> <code>set[str]</code> <p>Set of currently connected neighbor node IDs.</p> <code>addr</code> <code>str | None</code> <p>This node's own address.</p> <code>neighbors_lock</code> <code>Locker</code> <p>Async lock for thread-safe access to <code>neighbors</code>.</p> <code>nodes_known_lock</code> <code>Locker</code> <p>Async lock for thread-safe access to <code>nodes_known</code>.</p> <code>_verbose</code> <code>bool</code> <p>Enables verbose logging for debugging and traceability.</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/neighborpolicies/idleneighborpolicy.py</code> <pre><code>class IDLENeighborPolicy(NeighborPolicy):\n    \"\"\"\n    Neighbor policy for minimal connectivity scenarios.\n\n    This policy only attempts to discover or establish new neighbor connections \n    if the node is currently isolated (i.e., has no neighbors). All incoming \n    connection requests are accepted regardless of the current neighbor state.\n\n    This policy is suitable for scenarios where minimal intervention is preferred, \n    and connections are formed opportunistically rather than proactively.\n\n    Attributes:\n        max_neighbors (int | None): Unused in this policy but maintained for compatibility.\n        nodes_known (set[str]): Set of known node IDs discovered during federation.\n        neighbors (set[str]): Set of currently connected neighbor node IDs.\n        addr (str | None): This node's own address.\n        neighbors_lock (Locker): Async lock for thread-safe access to `neighbors`.\n        nodes_known_lock (Locker): Async lock for thread-safe access to `nodes_known`.\n        _verbose (bool): Enables verbose logging for debugging and traceability.\n    \"\"\"\n\n    def __init__(self):\n        self.max_neighbors = None\n        self.nodes_known = set()\n        self.neighbors = set()\n        self.addr = None\n        self.neighbors_lock = Locker(name=\"neighbors_lock\")\n        self.nodes_known_lock = Locker(name=\"nodes_known_lock\")\n        self._verbose = False\n\n    async def need_more_neighbors(self):\n        \"\"\"\n        Fully connected network requires to be connected to all devices, therefore,\n        if there are more nodes known that self.neighbors, more neighbors are required\n        \"\"\"\n        self.neighbors_lock.acquire()\n        need_more = len(self.neighbors) &lt;= 0\n        self.neighbors_lock.release()\n        return need_more\n\n    async def set_config(self, config):\n        \"\"\"\n        Args:\n            config[0] -&gt; list of self neighbors\n            config[1] -&gt; list of nodes known on federation\n            config[2] -&gt; self addr\n            config[3] -&gt; stricted_topology\n        \"\"\"\n        logging.info(\"Initializing Random Topology Neighbor Policy\")\n        self.neighbors_lock.acquire()\n        self.neighbors = config[0]\n        self.neighbors_lock.release()\n        for addr in config[1]:\n            self.nodes_known.add(addr)\n        self.addr\n\n    async def accept_connection(self, source, joining=False):\n        \"\"\"\n        return true if connection is accepted\n        \"\"\"\n        self.neighbors_lock.acquire()\n        ac = source not in self.neighbors\n        self.neighbors_lock.release()\n        return ac\n\n    async def meet_node(self, node):\n        \"\"\"\n        Update the list of nodes known on federation\n        \"\"\"\n        self.nodes_known_lock.acquire()\n        if node != self.addr:\n            if node not in self.nodes_known:\n                logging.info(f\"Update nodes known | addr: {node}\")\n            self.nodes_known.add(node)\n        self.nodes_known_lock.release()\n\n    async def get_nodes_known(self, neighbors_too=False, neighbors_only=False):\n        if neighbors_only:\n            self.neighbors_lock.acquire()\n            no = self.neighbors.copy()\n            self.neighbors_lock.release()\n            return no\n\n        self.nodes_known_lock.acquire()\n        nk = self.nodes_known.copy()\n        if not neighbors_too:\n            self.neighbors_lock.acquire()\n            nk = self.nodes_known - self.neighbors\n            self.neighbors_lock.release()\n        self.nodes_known_lock.release()\n        return nk\n\n    async def forget_nodes(self, nodes, forget_all=False):\n        self.nodes_known_lock.acquire()\n        if forget_all:\n            self.nodes_known.clear()\n        else:\n            for node in nodes:\n                self.nodes_known.discard(node)\n        self.nodes_known_lock.release()\n\n    async def get_actions(self):\n        \"\"\"\n        return list of actions to do in response to connection\n            - First list represents addrs argument to LinkMessage to connect to\n            - Second one represents the same but for disconnect from LinkMessage\n        \"\"\"\n        return [await self._connect_to(), await self._disconnect_from()]\n\n    async def _disconnect_from(self):\n        return \"\"\n\n    async def _connect_to(self):\n        ct = \"\"\n        self.neighbors_lock.acquire()\n        ct = \" \".join(self.neighbors)\n        self.neighbors_lock.release()\n        return ct\n\n    async def update_neighbors(self, node, remove=False):\n        if node == self.addr:\n            return\n        self.neighbors_lock.acquire()\n        if remove:\n            try:\n                self.neighbors.remove(node)\n                if self._verbose:\n                    logging.info(f\"Remove neighbor | addr: {node}\")\n            except KeyError:\n                pass\n        else:\n            self.neighbors.add(node)\n            if self._verbose:\n                logging.info(f\"Add neighbor | addr: {node}\")\n        self.neighbors_lock.release()\n\n    async def any_leftovers_neighbors(self):\n        return False\n\n    async def get_neighbors_to_remove(self):\n        return set()\n\n    async def get_posible_neighbors(self):\n        \"\"\"Return set of posible neighbors to connect to.\"\"\"\n        return await self.get_nodes_known(neighbors_too=False)\n\n    async def stricted_topology_status(stricted_topology: bool):\n        pass\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/idleneighborpolicy/#nebula.core.situationalawareness.awareness.sanetwork.neighborpolicies.idleneighborpolicy.IDLENeighborPolicy.accept_connection","title":"<code>accept_connection(source, joining=False)</code>  <code>async</code>","text":"<p>return true if connection is accepted</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/neighborpolicies/idleneighborpolicy.py</code> <pre><code>async def accept_connection(self, source, joining=False):\n    \"\"\"\n    return true if connection is accepted\n    \"\"\"\n    self.neighbors_lock.acquire()\n    ac = source not in self.neighbors\n    self.neighbors_lock.release()\n    return ac\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/idleneighborpolicy/#nebula.core.situationalawareness.awareness.sanetwork.neighborpolicies.idleneighborpolicy.IDLENeighborPolicy.get_actions","title":"<code>get_actions()</code>  <code>async</code>","text":"<p>return list of actions to do in response to connection     - First list represents addrs argument to LinkMessage to connect to     - Second one represents the same but for disconnect from LinkMessage</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/neighborpolicies/idleneighborpolicy.py</code> <pre><code>async def get_actions(self):\n    \"\"\"\n    return list of actions to do in response to connection\n        - First list represents addrs argument to LinkMessage to connect to\n        - Second one represents the same but for disconnect from LinkMessage\n    \"\"\"\n    return [await self._connect_to(), await self._disconnect_from()]\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/idleneighborpolicy/#nebula.core.situationalawareness.awareness.sanetwork.neighborpolicies.idleneighborpolicy.IDLENeighborPolicy.get_posible_neighbors","title":"<code>get_posible_neighbors()</code>  <code>async</code>","text":"<p>Return set of posible neighbors to connect to.</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/neighborpolicies/idleneighborpolicy.py</code> <pre><code>async def get_posible_neighbors(self):\n    \"\"\"Return set of posible neighbors to connect to.\"\"\"\n    return await self.get_nodes_known(neighbors_too=False)\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/idleneighborpolicy/#nebula.core.situationalawareness.awareness.sanetwork.neighborpolicies.idleneighborpolicy.IDLENeighborPolicy.meet_node","title":"<code>meet_node(node)</code>  <code>async</code>","text":"<p>Update the list of nodes known on federation</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/neighborpolicies/idleneighborpolicy.py</code> <pre><code>async def meet_node(self, node):\n    \"\"\"\n    Update the list of nodes known on federation\n    \"\"\"\n    self.nodes_known_lock.acquire()\n    if node != self.addr:\n        if node not in self.nodes_known:\n            logging.info(f\"Update nodes known | addr: {node}\")\n        self.nodes_known.add(node)\n    self.nodes_known_lock.release()\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/idleneighborpolicy/#nebula.core.situationalawareness.awareness.sanetwork.neighborpolicies.idleneighborpolicy.IDLENeighborPolicy.need_more_neighbors","title":"<code>need_more_neighbors()</code>  <code>async</code>","text":"<p>Fully connected network requires to be connected to all devices, therefore, if there are more nodes known that self.neighbors, more neighbors are required</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/neighborpolicies/idleneighborpolicy.py</code> <pre><code>async def need_more_neighbors(self):\n    \"\"\"\n    Fully connected network requires to be connected to all devices, therefore,\n    if there are more nodes known that self.neighbors, more neighbors are required\n    \"\"\"\n    self.neighbors_lock.acquire()\n    need_more = len(self.neighbors) &lt;= 0\n    self.neighbors_lock.release()\n    return need_more\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/idleneighborpolicy/#nebula.core.situationalawareness.awareness.sanetwork.neighborpolicies.idleneighborpolicy.IDLENeighborPolicy.set_config","title":"<code>set_config(config)</code>  <code>async</code>","text":"Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/neighborpolicies/idleneighborpolicy.py</code> <pre><code>async def set_config(self, config):\n    \"\"\"\n    Args:\n        config[0] -&gt; list of self neighbors\n        config[1] -&gt; list of nodes known on federation\n        config[2] -&gt; self addr\n        config[3] -&gt; stricted_topology\n    \"\"\"\n    logging.info(\"Initializing Random Topology Neighbor Policy\")\n    self.neighbors_lock.acquire()\n    self.neighbors = config[0]\n    self.neighbors_lock.release()\n    for addr in config[1]:\n        self.nodes_known.add(addr)\n    self.addr\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/neighborpolicy/","title":"Documentation for Neighborpolicy Module","text":""},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/neighborpolicy/#nebula.core.situationalawareness.awareness.sanetwork.neighborpolicies.neighborpolicy.NeighborPolicy","title":"<code>NeighborPolicy</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/neighborpolicies/neighborpolicy.py</code> <pre><code>class NeighborPolicy(ABC):\n    @abstractmethod\n    async def set_config(self, config):\n        \"\"\"Set internal configuration parameters for the neighbor policy, typically from a shared configuration object.\"\"\"\n        pass\n\n    @abstractmethod\n    async def need_more_neighbors(self):\n        \"\"\"Return True if the current node requires additional neighbors to fulfill its connectivity policy.\"\"\"\n        pass\n\n    @abstractmethod\n    async def get_posible_neighbors(self):\n        \"\"\"Return set of posible neighbors to connect to.\"\"\"\n        pass\n\n    @abstractmethod\n    async def any_leftovers_neighbors(self):\n        \"\"\"Return True if there are any neighbors that are no longer needed or should be replaced.\"\"\"\n        pass\n\n    @abstractmethod\n    async def get_neighbors_to_remove(self):\n        \"\"\"Return a list of neighbors that should be removed based on current policy constraints or evaluation.\"\"\"\n        pass\n\n    @abstractmethod\n    async def accept_connection(self, source, joining=False):\n        \"\"\"\n        Determine whether to accept a connection request from a given node.\n\n        Parameters:\n            source: The identifier of the node requesting the connection.\n            joining (bool): Whether this is an initial joining request.\n\n        Returns:\n            bool: True if the connection is accepted, False otherwise.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def get_actions(self):\n        \"\"\"Return a list of actions (e.g., add or remove neighbors) that should be executed to maintain the policy.\"\"\"\n        pass\n\n    @abstractmethod\n    async def meet_node(self, node):\n        \"\"\"\n        Register the discovery or interaction with a new node.\n\n        Parameters:\n            node: The node being encountered or added to internal memory.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def forget_nodes(self, nodes, forget_all=False):\n        \"\"\"\n        Remove the specified nodes from internal memory.\n\n        Parameters:\n            nodes: A list of node identifiers to forget.\n            forget_all (bool): If True, forget all nodes.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def get_nodes_known(self, neighbors_too=False, neighbors_only=False):\n        \"\"\"\n        Retrieve a list of nodes known by the current policy.\n\n        Parameters:\n            neighbors_too (bool): If True, include current neighbors in the result.\n            neighbors_only (bool): If True, return only current neighbors.\n\n        Returns:\n            list: A list of node identifiers.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def update_neighbors(self, node, remove=False):\n        \"\"\"\n        Add or remove a neighbor in the current neighbor set.\n\n        Parameters:\n            node: The node to be added or removed.\n            remove (bool): If True, remove the node instead of adding.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def stricted_topology_status(stricted_topology: bool):\n        \"\"\"\n        Update the policy with the current strict topology status.\n\n        Parameters:\n            stricted_topology (bool): True if the topology should be preserved.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/neighborpolicy/#nebula.core.situationalawareness.awareness.sanetwork.neighborpolicies.neighborpolicy.NeighborPolicy.accept_connection","title":"<code>accept_connection(source, joining=False)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Determine whether to accept a connection request from a given node.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <p>The identifier of the node requesting the connection.</p> required <code>joining</code> <code>bool</code> <p>Whether this is an initial joining request.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the connection is accepted, False otherwise.</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/neighborpolicies/neighborpolicy.py</code> <pre><code>@abstractmethod\nasync def accept_connection(self, source, joining=False):\n    \"\"\"\n    Determine whether to accept a connection request from a given node.\n\n    Parameters:\n        source: The identifier of the node requesting the connection.\n        joining (bool): Whether this is an initial joining request.\n\n    Returns:\n        bool: True if the connection is accepted, False otherwise.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/neighborpolicy/#nebula.core.situationalawareness.awareness.sanetwork.neighborpolicies.neighborpolicy.NeighborPolicy.any_leftovers_neighbors","title":"<code>any_leftovers_neighbors()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Return True if there are any neighbors that are no longer needed or should be replaced.</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/neighborpolicies/neighborpolicy.py</code> <pre><code>@abstractmethod\nasync def any_leftovers_neighbors(self):\n    \"\"\"Return True if there are any neighbors that are no longer needed or should be replaced.\"\"\"\n    pass\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/neighborpolicy/#nebula.core.situationalawareness.awareness.sanetwork.neighborpolicies.neighborpolicy.NeighborPolicy.forget_nodes","title":"<code>forget_nodes(nodes, forget_all=False)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Remove the specified nodes from internal memory.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <p>A list of node identifiers to forget.</p> required <code>forget_all</code> <code>bool</code> <p>If True, forget all nodes.</p> <code>False</code> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/neighborpolicies/neighborpolicy.py</code> <pre><code>@abstractmethod\nasync def forget_nodes(self, nodes, forget_all=False):\n    \"\"\"\n    Remove the specified nodes from internal memory.\n\n    Parameters:\n        nodes: A list of node identifiers to forget.\n        forget_all (bool): If True, forget all nodes.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/neighborpolicy/#nebula.core.situationalawareness.awareness.sanetwork.neighborpolicies.neighborpolicy.NeighborPolicy.get_actions","title":"<code>get_actions()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Return a list of actions (e.g., add or remove neighbors) that should be executed to maintain the policy.</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/neighborpolicies/neighborpolicy.py</code> <pre><code>@abstractmethod\nasync def get_actions(self):\n    \"\"\"Return a list of actions (e.g., add or remove neighbors) that should be executed to maintain the policy.\"\"\"\n    pass\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/neighborpolicy/#nebula.core.situationalawareness.awareness.sanetwork.neighborpolicies.neighborpolicy.NeighborPolicy.get_neighbors_to_remove","title":"<code>get_neighbors_to_remove()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Return a list of neighbors that should be removed based on current policy constraints or evaluation.</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/neighborpolicies/neighborpolicy.py</code> <pre><code>@abstractmethod\nasync def get_neighbors_to_remove(self):\n    \"\"\"Return a list of neighbors that should be removed based on current policy constraints or evaluation.\"\"\"\n    pass\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/neighborpolicy/#nebula.core.situationalawareness.awareness.sanetwork.neighborpolicies.neighborpolicy.NeighborPolicy.get_nodes_known","title":"<code>get_nodes_known(neighbors_too=False, neighbors_only=False)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Retrieve a list of nodes known by the current policy.</p> <p>Parameters:</p> Name Type Description Default <code>neighbors_too</code> <code>bool</code> <p>If True, include current neighbors in the result.</p> <code>False</code> <code>neighbors_only</code> <code>bool</code> <p>If True, return only current neighbors.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of node identifiers.</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/neighborpolicies/neighborpolicy.py</code> <pre><code>@abstractmethod\nasync def get_nodes_known(self, neighbors_too=False, neighbors_only=False):\n    \"\"\"\n    Retrieve a list of nodes known by the current policy.\n\n    Parameters:\n        neighbors_too (bool): If True, include current neighbors in the result.\n        neighbors_only (bool): If True, return only current neighbors.\n\n    Returns:\n        list: A list of node identifiers.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/neighborpolicy/#nebula.core.situationalawareness.awareness.sanetwork.neighborpolicies.neighborpolicy.NeighborPolicy.get_posible_neighbors","title":"<code>get_posible_neighbors()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Return set of posible neighbors to connect to.</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/neighborpolicies/neighborpolicy.py</code> <pre><code>@abstractmethod\nasync def get_posible_neighbors(self):\n    \"\"\"Return set of posible neighbors to connect to.\"\"\"\n    pass\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/neighborpolicy/#nebula.core.situationalawareness.awareness.sanetwork.neighborpolicies.neighborpolicy.NeighborPolicy.meet_node","title":"<code>meet_node(node)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Register the discovery or interaction with a new node.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <p>The node being encountered or added to internal memory.</p> required Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/neighborpolicies/neighborpolicy.py</code> <pre><code>@abstractmethod\nasync def meet_node(self, node):\n    \"\"\"\n    Register the discovery or interaction with a new node.\n\n    Parameters:\n        node: The node being encountered or added to internal memory.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/neighborpolicy/#nebula.core.situationalawareness.awareness.sanetwork.neighborpolicies.neighborpolicy.NeighborPolicy.need_more_neighbors","title":"<code>need_more_neighbors()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Return True if the current node requires additional neighbors to fulfill its connectivity policy.</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/neighborpolicies/neighborpolicy.py</code> <pre><code>@abstractmethod\nasync def need_more_neighbors(self):\n    \"\"\"Return True if the current node requires additional neighbors to fulfill its connectivity policy.\"\"\"\n    pass\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/neighborpolicy/#nebula.core.situationalawareness.awareness.sanetwork.neighborpolicies.neighborpolicy.NeighborPolicy.set_config","title":"<code>set_config(config)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Set internal configuration parameters for the neighbor policy, typically from a shared configuration object.</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/neighborpolicies/neighborpolicy.py</code> <pre><code>@abstractmethod\nasync def set_config(self, config):\n    \"\"\"Set internal configuration parameters for the neighbor policy, typically from a shared configuration object.\"\"\"\n    pass\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/neighborpolicy/#nebula.core.situationalawareness.awareness.sanetwork.neighborpolicies.neighborpolicy.NeighborPolicy.stricted_topology_status","title":"<code>stricted_topology_status(stricted_topology)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Update the policy with the current strict topology status.</p> <p>Parameters:</p> Name Type Description Default <code>stricted_topology</code> <code>bool</code> <p>True if the topology should be preserved.</p> required Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/neighborpolicies/neighborpolicy.py</code> <pre><code>@abstractmethod\nasync def stricted_topology_status(stricted_topology: bool):\n    \"\"\"\n    Update the policy with the current strict topology status.\n\n    Parameters:\n        stricted_topology (bool): True if the topology should be preserved.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/neighborpolicy/#nebula.core.situationalawareness.awareness.sanetwork.neighborpolicies.neighborpolicy.NeighborPolicy.update_neighbors","title":"<code>update_neighbors(node, remove=False)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Add or remove a neighbor in the current neighbor set.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <p>The node to be added or removed.</p> required <code>remove</code> <code>bool</code> <p>If True, remove the node instead of adding.</p> <code>False</code> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/neighborpolicies/neighborpolicy.py</code> <pre><code>@abstractmethod\nasync def update_neighbors(self, node, remove=False):\n    \"\"\"\n    Add or remove a neighbor in the current neighbor set.\n\n    Parameters:\n        node: The node to be added or removed.\n        remove (bool): If True, remove the node instead of adding.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/ringneighborpolicy/","title":"Documentation for Ringneighborpolicy Module","text":""},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/ringneighborpolicy/#nebula.core.situationalawareness.awareness.sanetwork.neighborpolicies.ringneighborpolicy.RINGNeighborPolicy","title":"<code>RINGNeighborPolicy</code>","text":"<p>               Bases: <code>NeighborPolicy</code></p> <p>Neighbor policy for ring topologies.</p> <p>This policy maintains a strict limit on the number of neighbors per node,  enforcing a ring-like structure. Each node connects to a fixed number of  neighbors (by default 2), and excess connections are detected and marked  for removal.</p> The policy ensures <ul> <li>No node connects to more than <code>max_neighbors</code>.</li> <li>New connections are accepted only if the node has not reached its limit   or the incomming connection is made by a joinning node.</li> <li>When a node joins, it's accepted only if not already connected.</li> <li>Excess neighbors (due to dynamic changes) can be identified and pruned,   ensuring that incomers dont get pruned way to fast.</li> </ul> <p>Attributes:</p> Name Type Description <code>max_neighbors</code> <code>int</code> <p>Maximum number of neighbors allowed (default is 2).</p> <code>nodes_known</code> <code>set[str]</code> <p>Set of node IDs discovered in the network.</p> <code>neighbors</code> <code>set[str]</code> <p>Set of current neighbor node IDs.</p> <code>neighbors_lock</code> <code>Locker</code> <p>Lock for thread-safe access to the neighbors set.</p> <code>nodes_known_lock</code> <code>Locker</code> <p>Lock for managing access to the known nodes set.</p> <code>addr</code> <code>str</code> <p>This node's own address.</p> <code>_excess_neighbors_removed</code> <code>set[str]</code> <p>Recently removed nodes due to excess connections.</p> <code>_excess_neighbors_removed_lock</code> <code>Locker</code> <p>Lock for accessing the removal tracking set.</p> <code>_verbose</code> <code>bool</code> <p>Enables verbose logging.</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/neighborpolicies/ringneighborpolicy.py</code> <pre><code>class RINGNeighborPolicy(NeighborPolicy):\n    \"\"\"\n    Neighbor policy for ring topologies.\n\n    This policy maintains a strict limit on the number of neighbors per node, \n    enforcing a ring-like structure. Each node connects to a fixed number of \n    neighbors (by default 2), and excess connections are detected and marked \n    for removal.\n\n    The policy ensures:\n      - No node connects to more than `max_neighbors`.\n      - New connections are accepted only if the node has not reached its limit\n        or the incomming connection is made by a joinning node.\n      - When a node joins, it's accepted only if not already connected.\n      - Excess neighbors (due to dynamic changes) can be identified and pruned,\n        ensuring that incomers dont get pruned way to fast.\n\n    Attributes:\n        max_neighbors (int): Maximum number of neighbors allowed (default is 2).\n        nodes_known (set[str]): Set of node IDs discovered in the network.\n        neighbors (set[str]): Set of current neighbor node IDs.\n        neighbors_lock (Locker): Lock for thread-safe access to the neighbors set.\n        nodes_known_lock (Locker): Lock for managing access to the known nodes set.\n        addr (str): This node's own address.\n        _excess_neighbors_removed (set[str]): Recently removed nodes due to excess connections.\n        _excess_neighbors_removed_lock (Locker): Lock for accessing the removal tracking set.\n        _verbose (bool): Enables verbose logging.\n    \"\"\"\n\n    RECENTLY_REMOVED_BAN_TIME = 20\n\n    def __init__(self):\n        self.max_neighbors = 2\n        self.nodes_known = set()\n        self.neighbors = set()\n        self.neighbors_lock = Locker(name=\"neighbors_lock\")\n        self.nodes_known_lock = Locker(name=\"nodes_known_lock\")\n        self.addr = \"\"\n        self._excess_neighbors_removed = set()\n        self._excess_neighbors_removed_lock = Locker(\"excess_neighbors_removed_lock\", async_lock=True)\n        self._verbose = False\n\n    async def need_more_neighbors(self):\n        self.neighbors_lock.acquire()\n        need_more = len(self.neighbors) &lt; self.max_neighbors\n        self.neighbors_lock.release()\n        return need_more\n\n    async def set_config(self, config):\n        \"\"\"\n        Args:\n            config[0] -&gt; list of self neighbors\n            config[1] -&gt; list of nodes known on federation\n            config[2] -&gt; self.addr\n            config[3] -&gt; stricted_topology\n        \"\"\"\n        logging.info(\"Initializing Ring Topology Neighbor Policy\")\n        self.neighbors_lock.acquire()\n        if self._verbose:\n            logging.info(f\"neighbors: {config[0]}\")\n        self.neighbors = config[0]\n        self.neighbors_lock.release()\n        for addr in config[1]:\n            self.nodes_known.add(addr)\n        self.addr = config[2]\n\n    async def accept_connection(self, source, joining=False):\n        \"\"\"\n        return true if connection is accepted\n        \"\"\"\n        ac = False\n        if await self._is_recently_removed(source):\n            return ac\n\n        with self.neighbors_lock:\n            if joining:\n                ac = source not in self.neighbors\n            else:\n                ac = not len(self.neighbors) &gt;= self.max_neighbors\n        return ac\n\n    async def meet_node(self, node):\n        self.nodes_known_lock.acquire()\n        if node != self.addr:\n            if node not in self.nodes_known:\n                logging.info(f\"Update nodes known | addr: {node}\")\n            self.nodes_known.add(node)\n        self.nodes_known_lock.release()\n\n    async def forget_nodes(self, nodes, forget_all=False):\n        self.nodes_known_lock.acquire()\n        if forget_all:\n            self.nodes_known.clear()\n        else:\n            for node in nodes:\n                self.nodes_known.discard(node)\n        self.nodes_known_lock.release()\n\n    async def get_nodes_known(self, neighbors_too=False, neighbors_only=False):\n        if neighbors_only:\n            self.neighbors_lock.acquire()\n            no = self.neighbors.copy()\n            self.neighbors_lock.release()\n            return no\n\n        self.nodes_known_lock.acquire()\n        nk = self.nodes_known.copy()\n        if not neighbors_too:\n            self.neighbors_lock.acquire()\n            nk = self.nodes_known - self.neighbors\n            self.neighbors_lock.release()\n        self.nodes_known_lock.release()\n        return nk\n\n    async def get_actions(self):\n        \"\"\"\n        return list of actions to do in response to connection\n            - First list represents addrs argument to LinkMessage to connect to\n            - Second one represents the same but for disconnect from LinkMessage\n        \"\"\"\n        self.neighbors_lock.acquire()\n        ct_actions = \"\"\n        df_actions = \"\"\n        if len(self.neighbors) == self.max_neighbors:\n            list_neighbors = list(self.neighbors)\n            index = random.randint(0, len(list_neighbors) - 1)\n            node = list_neighbors[index]\n            ct_actions = node  # connect to\n            df_actions = node  # disconnect from\n        self.neighbors_lock.release()\n        return [ct_actions, df_actions]\n\n    async def update_neighbors(self, node, remove=False):\n        self.neighbors_lock.acquire()\n        if remove:\n            if node in self.neighbors:\n                self.neighbors.remove(node)\n        else:\n            self.neighbors.add(node)\n        self.neighbors_lock.release()\n\n    async def get_posible_neighbors(self):\n        \"\"\"Return set of posible neighbors to connect to.\"\"\"\n        return await self.get_nodes_known(neighbors_too=False)\n\n    async def any_leftovers_neighbors(self):\n        self.neighbors_lock.acquire()\n        aln = len(self.neighbors) &gt; self.max_neighbors\n        self.neighbors_lock.release()\n        return aln\n\n    async def get_neighbors_to_remove(self):\n        neighbors = list()\n        self.neighbors_lock.acquire()\n        if self.neighbors:\n            neighbors = set(self.neighbors)\n            neighbors_to_remove = len(self.neighbors) - self.max_neighbors\n            neighbors = set(random.sample(list(neighbors), neighbors_to_remove))\n            self.neighbors_lock.release()\n        await self._add_removed_ban(neighbors)\n        return neighbors\n\n    async def stricted_topology_status(stricted_topology: bool):\n        pass\n\n    async def _is_recently_removed(self, source):\n        async with self._excess_neighbors_removed_lock:\n            return source in self._excess_neighbors_removed\n\n    async def _add_removed_ban(self, sources):\n        async with self._excess_neighbors_removed_lock:\n            for source in sources:\n                self._excess_neighbors_removed.add(source)\n                asyncio.create_task(self._clear_ban(source))\n\n    async def _clear_ban(self, source):\n        asyncio.sleep(self.RECENTLY_REMOVED_BAN_TIME)\n        async with self._excess_neighbors_removed_lock:\n            self._excess_neighbors_removed.discard(source)\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/ringneighborpolicy/#nebula.core.situationalawareness.awareness.sanetwork.neighborpolicies.ringneighborpolicy.RINGNeighborPolicy.accept_connection","title":"<code>accept_connection(source, joining=False)</code>  <code>async</code>","text":"<p>return true if connection is accepted</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/neighborpolicies/ringneighborpolicy.py</code> <pre><code>async def accept_connection(self, source, joining=False):\n    \"\"\"\n    return true if connection is accepted\n    \"\"\"\n    ac = False\n    if await self._is_recently_removed(source):\n        return ac\n\n    with self.neighbors_lock:\n        if joining:\n            ac = source not in self.neighbors\n        else:\n            ac = not len(self.neighbors) &gt;= self.max_neighbors\n    return ac\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/ringneighborpolicy/#nebula.core.situationalawareness.awareness.sanetwork.neighborpolicies.ringneighborpolicy.RINGNeighborPolicy.get_actions","title":"<code>get_actions()</code>  <code>async</code>","text":"<p>return list of actions to do in response to connection     - First list represents addrs argument to LinkMessage to connect to     - Second one represents the same but for disconnect from LinkMessage</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/neighborpolicies/ringneighborpolicy.py</code> <pre><code>async def get_actions(self):\n    \"\"\"\n    return list of actions to do in response to connection\n        - First list represents addrs argument to LinkMessage to connect to\n        - Second one represents the same but for disconnect from LinkMessage\n    \"\"\"\n    self.neighbors_lock.acquire()\n    ct_actions = \"\"\n    df_actions = \"\"\n    if len(self.neighbors) == self.max_neighbors:\n        list_neighbors = list(self.neighbors)\n        index = random.randint(0, len(list_neighbors) - 1)\n        node = list_neighbors[index]\n        ct_actions = node  # connect to\n        df_actions = node  # disconnect from\n    self.neighbors_lock.release()\n    return [ct_actions, df_actions]\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/ringneighborpolicy/#nebula.core.situationalawareness.awareness.sanetwork.neighborpolicies.ringneighborpolicy.RINGNeighborPolicy.get_posible_neighbors","title":"<code>get_posible_neighbors()</code>  <code>async</code>","text":"<p>Return set of posible neighbors to connect to.</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/neighborpolicies/ringneighborpolicy.py</code> <pre><code>async def get_posible_neighbors(self):\n    \"\"\"Return set of posible neighbors to connect to.\"\"\"\n    return await self.get_nodes_known(neighbors_too=False)\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/ringneighborpolicy/#nebula.core.situationalawareness.awareness.sanetwork.neighborpolicies.ringneighborpolicy.RINGNeighborPolicy.set_config","title":"<code>set_config(config)</code>  <code>async</code>","text":"Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/neighborpolicies/ringneighborpolicy.py</code> <pre><code>async def set_config(self, config):\n    \"\"\"\n    Args:\n        config[0] -&gt; list of self neighbors\n        config[1] -&gt; list of nodes known on federation\n        config[2] -&gt; self.addr\n        config[3] -&gt; stricted_topology\n    \"\"\"\n    logging.info(\"Initializing Ring Topology Neighbor Policy\")\n    self.neighbors_lock.acquire()\n    if self._verbose:\n        logging.info(f\"neighbors: {config[0]}\")\n    self.neighbors = config[0]\n    self.neighbors_lock.release()\n    for addr in config[1]:\n        self.nodes_known.add(addr)\n    self.addr = config[2]\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/starneighborpolicy/","title":"Documentation for Starneighborpolicy Module","text":""},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/starneighborpolicy/#nebula.core.situationalawareness.awareness.sanetwork.neighborpolicies.starneighborpolicy.STARNeighborPolicy","title":"<code>STARNeighborPolicy</code>","text":"<p>               Bases: <code>NeighborPolicy</code></p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/neighborpolicies/starneighborpolicy.py</code> <pre><code>class STARNeighborPolicy(NeighborPolicy):\n    def __init__(self):\n        self.max_neighbors = 1\n        self.nodes_known = set()\n        self.neighbors = set()\n        self.neighbors_lock = Locker(name=\"neighbors_lock\")\n        self.nodes_known_lock = Locker(name=\"nodes_known_lock\")\n        self.addr = \"\"\n        self._verbose = False\n\n    async def need_more_neighbors(self):\n        self.neighbors_lock.acquire()\n        need_more = len(self.neighbors) &lt; self.max_neighbors\n        self.neighbors_lock.release()\n        return need_more\n\n    async def set_config(self, config):\n        \"\"\"\n        Args:\n            config[0] -&gt; list of self neighbors, in this case, the star point\n            config[1] -&gt; list of nodes known on federation\n            config[2] -&gt; self.addr\n            config[3] -&gt; stricted_topology\n        \"\"\"\n        self.neighbors_lock.acquire()\n        self.neighbors = config[0]\n        self.neighbors_lock.release()\n        for addr in config[1]:\n            self.nodes_known.add(addr)\n        self.addr = config[2]\n\n    async def accept_connection(self, source, joining=False):\n        \"\"\"\n        return true if connection is accepted\n        \"\"\"\n        ac = joining\n        return ac\n\n    async def meet_node(self, node):\n        self.nodes_known_lock.acquire()\n        if node != self.addr:\n            if node not in self.nodes_known:\n                logging.info(f\"Update nodes known | addr: {node}\")\n            self.nodes_known.add(node)\n        self.nodes_known_lock.release()\n\n    async def forget_nodes(self, nodes, forget_all=False):\n        self.nodes_known_lock.acquire()\n        if forget_all:\n            self.nodes_known.clear()\n        else:\n            for node in nodes:\n                self.nodes_known.discard(node)\n        self.nodes_known_lock.release()\n\n    async def get_nodes_known(self, neighbors_too=False, neighbors_only=False):\n        self.nodes_known_lock.acquire()\n        nk = self.nodes_known.copy()\n        if not neighbors_too:\n            self.neighbors_lock.acquire()\n            nk = self.nodes_known - self.neighbors\n            self.neighbors_lock.release()\n        self.nodes_known_lock.release()\n        return nk\n\n    async def get_actions(self):\n        \"\"\"\n        return list of actions to do in response to connection\n            - First list represents addrs argument to LinkMessage to connect to\n            - Second one represents the same but for disconnect from LinkMessage\n        \"\"\"\n        self.neighbors_lock.acquire()\n        ct_actions = []\n        df_actions = []\n        if len(self.neighbors) &lt; self.max_neighbors:\n            ct_actions.append(self.neighbors[0])  # connect to star point\n            df_actions.append(self.addr)  # disconnect from me\n        self.neighbors_lock.release()\n        return [ct_actions, df_actions]\n\n    async def update_neighbors(self, node, remove=False):\n        pass\n\n    async def stricted_topology_status(stricted_topology: bool):\n        pass\n\n    async def get_posible_neighbors(self):\n        \"\"\"Return set of posible neighbors to connect to.\"\"\"\n        return await self.get_nodes_known(neighbors_too=False)\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/starneighborpolicy/#nebula.core.situationalawareness.awareness.sanetwork.neighborpolicies.starneighborpolicy.STARNeighborPolicy.accept_connection","title":"<code>accept_connection(source, joining=False)</code>  <code>async</code>","text":"<p>return true if connection is accepted</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/neighborpolicies/starneighborpolicy.py</code> <pre><code>async def accept_connection(self, source, joining=False):\n    \"\"\"\n    return true if connection is accepted\n    \"\"\"\n    ac = joining\n    return ac\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/starneighborpolicy/#nebula.core.situationalawareness.awareness.sanetwork.neighborpolicies.starneighborpolicy.STARNeighborPolicy.get_actions","title":"<code>get_actions()</code>  <code>async</code>","text":"<p>return list of actions to do in response to connection     - First list represents addrs argument to LinkMessage to connect to     - Second one represents the same but for disconnect from LinkMessage</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/neighborpolicies/starneighborpolicy.py</code> <pre><code>async def get_actions(self):\n    \"\"\"\n    return list of actions to do in response to connection\n        - First list represents addrs argument to LinkMessage to connect to\n        - Second one represents the same but for disconnect from LinkMessage\n    \"\"\"\n    self.neighbors_lock.acquire()\n    ct_actions = []\n    df_actions = []\n    if len(self.neighbors) &lt; self.max_neighbors:\n        ct_actions.append(self.neighbors[0])  # connect to star point\n        df_actions.append(self.addr)  # disconnect from me\n    self.neighbors_lock.release()\n    return [ct_actions, df_actions]\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/starneighborpolicy/#nebula.core.situationalawareness.awareness.sanetwork.neighborpolicies.starneighborpolicy.STARNeighborPolicy.get_posible_neighbors","title":"<code>get_posible_neighbors()</code>  <code>async</code>","text":"<p>Return set of posible neighbors to connect to.</p> Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/neighborpolicies/starneighborpolicy.py</code> <pre><code>async def get_posible_neighbors(self):\n    \"\"\"Return set of posible neighbors to connect to.\"\"\"\n    return await self.get_nodes_known(neighbors_too=False)\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sanetwork/neighborpolicies/starneighborpolicy/#nebula.core.situationalawareness.awareness.sanetwork.neighborpolicies.starneighborpolicy.STARNeighborPolicy.set_config","title":"<code>set_config(config)</code>  <code>async</code>","text":"Source code in <code>nebula/core/situationalawareness/awareness/sanetwork/neighborpolicies/starneighborpolicy.py</code> <pre><code>async def set_config(self, config):\n    \"\"\"\n    Args:\n        config[0] -&gt; list of self neighbors, in this case, the star point\n        config[1] -&gt; list of nodes known on federation\n        config[2] -&gt; self.addr\n        config[3] -&gt; stricted_topology\n    \"\"\"\n    self.neighbors_lock.acquire()\n    self.neighbors = config[0]\n    self.neighbors_lock.release()\n    for addr in config[1]:\n        self.nodes_known.add(addr)\n    self.addr = config[2]\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/satraining/","title":"Documentation for Satraining Module","text":""},{"location":"api/core/situationalawareness/awareness/satraining/satraining/","title":"Documentation for Satraining Module","text":""},{"location":"api/core/situationalawareness/awareness/satraining/satraining/#nebula.core.situationalawareness.awareness.satraining.satraining.SATraining","title":"<code>SATraining</code>","text":"<p>               Bases: <code>SAMComponent</code></p> <p>SATraining is a Situational Awareness (SA) component responsible for enhancing the training process in Distributed Federated Learning (DFL) environments by leveraging context-awareness and environmental knowledge.</p> <p>This component dynamically instantiates a training policy based on the configuration, allowing the system to adapt training strategies depending on the local topology, node behavior, or environmental constraints.</p> <p>Attributes:</p> Name Type Description <code>_config</code> <code>dict</code> <p>Configuration dictionary containing parameters and references.</p> <code>_sar</code> <code>SAReasoner</code> <p>Reference to the shared situational reasoner.</p> <code>_trainning_policy</code> <p>Instantiated training policy strategy.</p> Source code in <code>nebula/core/situationalawareness/awareness/satraining/satraining.py</code> <pre><code>class SATraining(SAMComponent):\n    \"\"\"\n    SATraining is a Situational Awareness (SA) component responsible for enhancing\n    the training process in Distributed Federated Learning (DFL) environments\n    by leveraging context-awareness and environmental knowledge.\n\n    This component dynamically instantiates a training policy based on the configuration,\n    allowing the system to adapt training strategies depending on the local topology,\n    node behavior, or environmental constraints.\n\n    Attributes:\n        _config (dict): Configuration dictionary containing parameters and references.\n        _sar (SAReasoner): Reference to the shared situational reasoner.\n        _trainning_policy: Instantiated training policy strategy.\n    \"\"\"\n\n    def __init__(self, config):\n        \"\"\"\n        Initialize the SATraining component with a given configuration.\n\n        Args:\n            config (dict): Configuration dictionary containing:\n                - 'addr': Node address.\n                - 'verbose': Verbosity flag.\n                - 'sar': Reference to the SAReasoner instance.\n                - 'training_policy': Training policy name to be used.\n        \"\"\"\n        print_msg_box(\n            msg=f\"Starting Training SA\\nTraining policy: {config['training_policy']}\",\n            indent=2,\n            title=\"Training SA module\",\n        )\n        self._config = config\n        self._sar: SAReasoner = self._config[\"sar\"]\n        tp_config = {}\n        tp_config[\"addr\"] = self._config[\"addr\"]\n        tp_config[\"verbose\"] = self._config[\"verbose\"]\n        training_policy = self._config[\"training_policy\"]\n        self._trainning_policy = factory_training_policy(training_policy, tp_config)\n\n    @property\n    def sar(self):\n        \"\"\"\n        Returns the current instance of the SAReasoner.\n        \"\"\"\n        return self._sar\n\n    @property\n    def tp(self):\n        \"\"\"\n        Returns the currently active training policy instance.\n        \"\"\"\n        return self._trainning_policy    \n\n    async def init(self):\n        \"\"\"\n        Initialize the training policy with the current known neighbors from the SAReasoner.\n        This setup enables the policy to make informed decisions based on local topology.\n        \"\"\"\n        config = {}\n        config[\"nodes\"] = set(await self.sar.get_nodes_known(neighbors_only=True)) \n        await self.tp.init(config)\n\n    async def sa_component_actions(self):\n        \"\"\"\n        Periodically called action of the SA component to evaluate the current scenario.\n        This invokes the evaluation logic defined in the training policy to adapt behavior.\n        \"\"\"\n        logging.info(\"SA Trainng evaluating current scenario\")\n        asyncio.create_task(self.tp.get_evaluation_results())\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/satraining/satraining/#nebula.core.situationalawareness.awareness.satraining.satraining.SATraining.sar","title":"<code>sar</code>  <code>property</code>","text":"<p>Returns the current instance of the SAReasoner.</p>"},{"location":"api/core/situationalawareness/awareness/satraining/satraining/#nebula.core.situationalawareness.awareness.satraining.satraining.SATraining.tp","title":"<code>tp</code>  <code>property</code>","text":"<p>Returns the currently active training policy instance.</p>"},{"location":"api/core/situationalawareness/awareness/satraining/satraining/#nebula.core.situationalawareness.awareness.satraining.satraining.SATraining.__init__","title":"<code>__init__(config)</code>","text":"<p>Initialize the SATraining component with a given configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration dictionary containing: - 'addr': Node address. - 'verbose': Verbosity flag. - 'sar': Reference to the SAReasoner instance. - 'training_policy': Training policy name to be used.</p> required Source code in <code>nebula/core/situationalawareness/awareness/satraining/satraining.py</code> <pre><code>def __init__(self, config):\n    \"\"\"\n    Initialize the SATraining component with a given configuration.\n\n    Args:\n        config (dict): Configuration dictionary containing:\n            - 'addr': Node address.\n            - 'verbose': Verbosity flag.\n            - 'sar': Reference to the SAReasoner instance.\n            - 'training_policy': Training policy name to be used.\n    \"\"\"\n    print_msg_box(\n        msg=f\"Starting Training SA\\nTraining policy: {config['training_policy']}\",\n        indent=2,\n        title=\"Training SA module\",\n    )\n    self._config = config\n    self._sar: SAReasoner = self._config[\"sar\"]\n    tp_config = {}\n    tp_config[\"addr\"] = self._config[\"addr\"]\n    tp_config[\"verbose\"] = self._config[\"verbose\"]\n    training_policy = self._config[\"training_policy\"]\n    self._trainning_policy = factory_training_policy(training_policy, tp_config)\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/satraining/satraining/#nebula.core.situationalawareness.awareness.satraining.satraining.SATraining.init","title":"<code>init()</code>  <code>async</code>","text":"<p>Initialize the training policy with the current known neighbors from the SAReasoner. This setup enables the policy to make informed decisions based on local topology.</p> Source code in <code>nebula/core/situationalawareness/awareness/satraining/satraining.py</code> <pre><code>async def init(self):\n    \"\"\"\n    Initialize the training policy with the current known neighbors from the SAReasoner.\n    This setup enables the policy to make informed decisions based on local topology.\n    \"\"\"\n    config = {}\n    config[\"nodes\"] = set(await self.sar.get_nodes_known(neighbors_only=True)) \n    await self.tp.init(config)\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/satraining/satraining/#nebula.core.situationalawareness.awareness.satraining.satraining.SATraining.sa_component_actions","title":"<code>sa_component_actions()</code>  <code>async</code>","text":"<p>Periodically called action of the SA component to evaluate the current scenario. This invokes the evaluation logic defined in the training policy to adapt behavior.</p> Source code in <code>nebula/core/situationalawareness/awareness/satraining/satraining.py</code> <pre><code>async def sa_component_actions(self):\n    \"\"\"\n    Periodically called action of the SA component to evaluate the current scenario.\n    This invokes the evaluation logic defined in the training policy to adapt behavior.\n    \"\"\"\n    logging.info(\"SA Trainng evaluating current scenario\")\n    asyncio.create_task(self.tp.get_evaluation_results())\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/satraining/trainingpolicy/","title":"Documentation for Trainingpolicy Module","text":""},{"location":"api/core/situationalawareness/awareness/satraining/trainingpolicy/bpstrainingpolicy/","title":"Documentation for Bpstrainingpolicy Module","text":""},{"location":"api/core/situationalawareness/awareness/satraining/trainingpolicy/fastreboot/","title":"Documentation for Fastreboot Module","text":""},{"location":"api/core/situationalawareness/awareness/satraining/trainingpolicy/htstrainingpolicy/","title":"Documentation for Htstrainingpolicy Module","text":""},{"location":"api/core/situationalawareness/awareness/satraining/trainingpolicy/htstrainingpolicy/#nebula.core.situationalawareness.awareness.satraining.trainingpolicy.htstrainingpolicy.HTSTrainingPolicy","title":"<code>HTSTrainingPolicy</code>","text":"<p>               Bases: <code>TrainingPolicy</code></p> <p>Implements a Hybrid Training Strategy (HTS) that combines multiple training policies  (e.g., QDS, FRTS) to collaboratively decide on the evaluation and potential pruning  of neighbors in a decentralized federated learning scenario.</p> <p>Attributes:</p> Name Type Description <code>TRAINING_POLICY</code> <code>set</code> <p>Names of training policy classes to instantiate and manage.</p> Source code in <code>nebula/core/situationalawareness/awareness/satraining/trainingpolicy/htstrainingpolicy.py</code> <pre><code>class HTSTrainingPolicy(TrainingPolicy):\n    \"\"\"\n    Implements a Hybrid Training Strategy (HTS) that combines multiple training policies \n    (e.g., QDS, FRTS) to collaboratively decide on the evaluation and potential pruning \n    of neighbors in a decentralized federated learning scenario.\n\n    Attributes:\n        TRAINING_POLICY (set): Names of training policy classes to instantiate and manage.\n    \"\"\"\n\n    TRAINING_POLICY = {\n        \"Quality-Driven Selection\",\n        \"Fast Reboot Training Strategy\",\n    }\n\n    def __init__(self, config):\n        \"\"\"\n        Initializes the HTS policy with the node's address and verbosity level.\n        It creates instances of each sub-policy listed in TRAINING_POLICY.\n\n        Args:\n            config (dict): Configuration dictionary with keys:\n                - 'addr': Node's address\n                - 'verbose': Enable verbose logging\n        \"\"\"\n        self._addr = config[\"addr\"]\n        self._verbose = config[\"verbose\"]\n        self._training_policies : set[TrainingPolicy] = set()\n        self._training_policies.add([factory_training_policy(x, config) for x in self.TRAINING_POLICY])\n\n    def __str__(self):\n        return \"HTS\"    \n\n    @property\n    def tps(self):\n        return self._training_policies  \n\n    async def init(self, config):\n        for tp in self.tps:\n            await tp.init(config)    \n\n    async def update_neighbors(self, node, remove=False):\n        pass\n\n    async def get_evaluation_results(self):\n        \"\"\"\n        Asynchronously calls the `get_evaluation_results` of each policy,\n        and logs the nodes each policy would remove.\n\n        Returns:\n            None (future version may merge all evaluations).\n        \"\"\"\n        nodes_to_remove = dict()\n        for tp in self.tps:\n            nodes_to_remove[tp] = await tp.get_evaluation_results()\n\n        for tp, nodes in nodes_to_remove.items():\n            logging.info(f\"Training Policy: {tp}, nodes to remove: {nodes}\")\n\n        return None\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/satraining/trainingpolicy/htstrainingpolicy/#nebula.core.situationalawareness.awareness.satraining.trainingpolicy.htstrainingpolicy.HTSTrainingPolicy.__init__","title":"<code>__init__(config)</code>","text":"<p>Initializes the HTS policy with the node's address and verbosity level. It creates instances of each sub-policy listed in TRAINING_POLICY.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration dictionary with keys: - 'addr': Node's address - 'verbose': Enable verbose logging</p> required Source code in <code>nebula/core/situationalawareness/awareness/satraining/trainingpolicy/htstrainingpolicy.py</code> <pre><code>def __init__(self, config):\n    \"\"\"\n    Initializes the HTS policy with the node's address and verbosity level.\n    It creates instances of each sub-policy listed in TRAINING_POLICY.\n\n    Args:\n        config (dict): Configuration dictionary with keys:\n            - 'addr': Node's address\n            - 'verbose': Enable verbose logging\n    \"\"\"\n    self._addr = config[\"addr\"]\n    self._verbose = config[\"verbose\"]\n    self._training_policies : set[TrainingPolicy] = set()\n    self._training_policies.add([factory_training_policy(x, config) for x in self.TRAINING_POLICY])\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/satraining/trainingpolicy/htstrainingpolicy/#nebula.core.situationalawareness.awareness.satraining.trainingpolicy.htstrainingpolicy.HTSTrainingPolicy.get_evaluation_results","title":"<code>get_evaluation_results()</code>  <code>async</code>","text":"<p>Asynchronously calls the <code>get_evaluation_results</code> of each policy, and logs the nodes each policy would remove.</p> <p>Returns:</p> Type Description <p>None (future version may merge all evaluations).</p> Source code in <code>nebula/core/situationalawareness/awareness/satraining/trainingpolicy/htstrainingpolicy.py</code> <pre><code>async def get_evaluation_results(self):\n    \"\"\"\n    Asynchronously calls the `get_evaluation_results` of each policy,\n    and logs the nodes each policy would remove.\n\n    Returns:\n        None (future version may merge all evaluations).\n    \"\"\"\n    nodes_to_remove = dict()\n    for tp in self.tps:\n        nodes_to_remove[tp] = await tp.get_evaluation_results()\n\n    for tp, nodes in nodes_to_remove.items():\n        logging.info(f\"Training Policy: {tp}, nodes to remove: {nodes}\")\n\n    return None\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/satraining/trainingpolicy/qdstrainingpolicy/","title":"Documentation for Qdstrainingpolicy Module","text":""},{"location":"api/core/situationalawareness/awareness/satraining/trainingpolicy/qdstrainingpolicy/#nebula.core.situationalawareness.awareness.satraining.trainingpolicy.qdstrainingpolicy.QDSTrainingPolicy","title":"<code>QDSTrainingPolicy</code>","text":"<p>               Bases: <code>TrainingPolicy</code></p> <p>Implements a Quality-Driven Selection (QDS) strategy for training in DFL.</p> <p>This policy tracks the cosine similarity of neighbor model updates over time, and detects nodes that are inactive or provide redundant updates. Based on these evaluations, the policy suggests disconnecting such nodes to promote better model convergence and network efficiency.</p> Source code in <code>nebula/core/situationalawareness/awareness/satraining/trainingpolicy/qdstrainingpolicy.py</code> <pre><code>class QDSTrainingPolicy(TrainingPolicy):\n    \"\"\"\n    Implements a Quality-Driven Selection (QDS) strategy for training in DFL.\n\n    This policy tracks the cosine similarity of neighbor model updates over time,\n    and detects nodes that are inactive or provide redundant updates.\n    Based on these evaluations, the policy suggests disconnecting such nodes\n    to promote better model convergence and network efficiency.\n    \"\"\"\n\n    MAX_HISTORIC_SIZE = 10\n    SIMILARITY_THRESHOLD = 0.73\n    INACTIVE_THRESHOLD = 3\n    GRACE_ROUNDS = 0\n    CHECK_COOLDOWN = 10000\n\n    def __init__(self, config : dict):\n        \"\"\"\n        Initializes the QDS training policy.\n\n        Args:\n            config (dict): Configuration dictionary with keys:\n                - \"addr\": Local node address.\n                - \"verbose\": Boolean flag for logging verbosity.\n        \"\"\"\n        self._addr = config[\"addr\"]\n        self._verbose = config[\"verbose\"]\n        self._nodes : dict[str, tuple[deque, int]] = {}\n        self._nodes_lock = Locker(name=\"nodes_lock\", async_lock=True)\n        self._round_missing_nodes = set()\n        self._grace_rounds = self.GRACE_ROUNDS\n        self._last_check = 0\n        self._check_done = False\n        self._evaluation_results = set()\n\n    def __str__(self):\n        return \"QDS\"\n\n    async def init(self, config):\n        \"\"\"\n        Initializes the internal state and subscribes to necessary events.\n\n        Args:\n            config (dict): Must contain a 'nodes' set representing known neighbors.\n        \"\"\"\n        async with self._nodes_lock:\n            nodes = config[\"nodes\"]\n            self._nodes : dict[str, tuple[deque, int]] = {node_id: (deque(maxlen=self.MAX_HISTORIC_SIZE), 0) for node_id in nodes}\n        await EventManager.get_instance().subscribe_node_event(AggregationEvent, self._process_aggregation_event)\n        await EventManager.get_instance().subscribe_node_event(UpdateNeighborEvent, self._update_neighbors)\n        await self.register_sa_agent()\n\n    async def _update_neighbors(self, une: UpdateNeighborEvent):\n        \"\"\"\n        Updates the internal list of neighbors based on topology changes.\n\n        Args:\n            une (UpdateNeighborEvent): Event containing added/removed neighbor information.\n        \"\"\"\n        node, remove = await une.get_event_data()\n        async with self._nodes_lock:\n            if remove:\n                self._nodes.pop(node, None)\n            else:\n                if not node in self._nodes:\n                    self._nodes.update({node : (deque(maxlen=self.MAX_HISTORIC_SIZE), 0)})\n\n    async def _process_aggregation_event(self, agg_ev : AggregationEvent):\n        \"\"\"\n        Processes an AggregationEvent and updates similarity/inactivity metrics.\n\n        Args:\n            agg_ev (AggregationEvent): Aggregation event with model updates and missing nodes.\n        \"\"\"\n        if self._verbose: logging.info(\"Processing aggregation event\")\n        (updates, expected_nodes, missing_nodes) = await agg_ev.get_event_data()\n        self._round_missing_nodes = missing_nodes\n        self_updt = updates[self._addr]\n        async with self._nodes_lock:\n            for addr, updt in updates.items():\n                if addr == self._addr: continue\n                if not addr in self._nodes.keys(): continue\n\n                deque_history, missed_count = self._nodes[addr]\n                if addr in missing_nodes:\n                    if self._verbose: logging.info(f\"Node inactivity counter increased for: {addr}\")\n                    self._nodes[addr] = (deque_history, missed_count + 1)   # Inactive rounds counter +1\n                else:\n                    self._nodes[addr] = (deque_history, 0)                  # Reset inactive counter\n\n                #TODO Do it for the ones not using the last update received cause they are missing this round                      \n                (model,_) = updt\n                (self_model, _) = self_updt \n                cos_sim = cosine_metric(self_model, model, similarity=True)\n                self._nodes[addr][0].append(cos_sim)\n        self._evaluation_results = await self.evaluate()\n\n    async def _get_nodes(self):\n        \"\"\"\n        Safely returns a copy of the current node tracking dictionary.\n\n        Returns:\n            dict: A copy of the internal node state.\n        \"\"\"\n        async with self._nodes_lock:\n            nodes = self._nodes.copy()\n        return nodes    \n\n    async def evaluate(self):\n        \"\"\"\n        Evaluates the current neighbor set to determine inactive or redundant nodes.\n\n        Returns:\n            set: A set of node addresses suggested for disconnection.\n        \"\"\"\n        if self._grace_rounds:  # Grace rounds\n            self._grace_rounds -= 1\n            if self._verbose: logging.info(\"Grace time hasnt finished...\")\n            return None\n\n        if self._verbose: logging.info(\"Evaluation in process\")\n\n        result = set()     \n        if self._last_check == 0:\n            self._check_done = True\n            nodes = await self._get_nodes()\n            redundant_nodes = set()\n            inactive_nodes = set()\n            for node in nodes:\n                if nodes[node][0]:\n                    last_sim = nodes[node][0][-1]\n                    inactivity_counter =  nodes[node][1]\n                    if inactivity_counter &gt;= self.INACTIVE_THRESHOLD:\n                        inactive_nodes.add(node)\n                        if self._verbose: logging.info(f\"Node: {node} hadn't participated in any of the last {self.INACTIVE_THRESHOLD} rounds\")\n                    else:\n                        if self._verbose: logging.info(f\"Node: {node} inactivity counter: {inactivity_counter}\")\n\n                    if node not in self._round_missing_nodes:\n                        if last_sim &lt; self.SIMILARITY_THRESHOLD:\n                            if self._verbose: logging.info(f\"Node: {node} got a similarity value of: {last_sim} under threshold: {self.SIMILARITY_THRESHOLD}\")\n                        else:\n                            if self._verbose: logging.info(f\"Node: {node} got a redundant model, cossine simmilarity: {last_sim} over threshold: {self.SIMILARITY_THRESHOLD}\")\n                            redundant_nodes.add((node, last_sim))\n\n            if self._verbose: logging.info(f\"Inactive nodes on aggregations: {inactive_nodes}\")\n            if self._verbose: logging.info(f\"Redundant nodes on aggregations: {redundant_nodes}\")\n            if inactive_nodes:\n                result = result.union(inactive_nodes)    \n            if len(redundant_nodes):\n                sorted_redundant_nodes = sorted(redundant_nodes, key=lambda x: x[1])\n                n_discarded = math.ceil((len(redundant_nodes)/2))\n                discard_nodes = sorted_redundant_nodes[:n_discarded]\n                discard_nodes = [node for (node,_) in discard_nodes]\n                if self._verbose: logging.info(f\"Discarded redundant nodes: {discard_nodes}\")\n                result = result.union(discard_nodes)\n        else:\n            if self._verbose: logging.info(f\"Evaluation is on cooldown... | {self.CHECK_COOLDOWN - self._last_check} rounds remaining\")\n            self._check_done = False\n\n        self._last_check = (self._last_check + 1)  % self.CHECK_COOLDOWN\n\n        return result\n\n    async def get_evaluation_results(self):\n        \"\"\"\n        Triggers suggested actions based on last evaluation results.\n\n        Suggests disconnection from nodes marked as inactive or redundant.\n        \"\"\"\n        if self._check_done:\n            for node_discarded in self._evaluation_results:\n                args = (node_discarded, False, True)\n                sac = factory_sa_command(\n                    \"connectivity\",                        \n                    SACommandAction.DISCONNECT,\n                    self,           \n                    node_discarded,                       \n                    SACommandPRIO.MEDIUM,                 \n                    False,                                \n                    CommunicationsManager.get_instance().disconnect,  \n                    *args                                  \n                )\n                await self.suggest_action(sac)\n            await self.notify_all_suggestions_done(RoundEndEvent)\n\n    async def get_agent(self) -&gt; str:\n        return \"SATraining_QDSTP\"\n\n    async def register_sa_agent(self):\n        await SuggestionBuffer.get_instance().register_event_agents(RoundEndEvent, self)\n\n    async def suggest_action(self, sac : SACommand):\n        await SuggestionBuffer.get_instance().register_suggestion(RoundEndEvent, self, sac)\n\n    async def notify_all_suggestions_done(self, event_type):\n        await SuggestionBuffer.get_instance().notify_all_suggestions_done_for_agent(self, event_type)\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/satraining/trainingpolicy/qdstrainingpolicy/#nebula.core.situationalawareness.awareness.satraining.trainingpolicy.qdstrainingpolicy.QDSTrainingPolicy.__init__","title":"<code>__init__(config)</code>","text":"<p>Initializes the QDS training policy.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration dictionary with keys: - \"addr\": Local node address. - \"verbose\": Boolean flag for logging verbosity.</p> required Source code in <code>nebula/core/situationalawareness/awareness/satraining/trainingpolicy/qdstrainingpolicy.py</code> <pre><code>def __init__(self, config : dict):\n    \"\"\"\n    Initializes the QDS training policy.\n\n    Args:\n        config (dict): Configuration dictionary with keys:\n            - \"addr\": Local node address.\n            - \"verbose\": Boolean flag for logging verbosity.\n    \"\"\"\n    self._addr = config[\"addr\"]\n    self._verbose = config[\"verbose\"]\n    self._nodes : dict[str, tuple[deque, int]] = {}\n    self._nodes_lock = Locker(name=\"nodes_lock\", async_lock=True)\n    self._round_missing_nodes = set()\n    self._grace_rounds = self.GRACE_ROUNDS\n    self._last_check = 0\n    self._check_done = False\n    self._evaluation_results = set()\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/satraining/trainingpolicy/qdstrainingpolicy/#nebula.core.situationalawareness.awareness.satraining.trainingpolicy.qdstrainingpolicy.QDSTrainingPolicy.evaluate","title":"<code>evaluate()</code>  <code>async</code>","text":"<p>Evaluates the current neighbor set to determine inactive or redundant nodes.</p> <p>Returns:</p> Name Type Description <code>set</code> <p>A set of node addresses suggested for disconnection.</p> Source code in <code>nebula/core/situationalawareness/awareness/satraining/trainingpolicy/qdstrainingpolicy.py</code> <pre><code>async def evaluate(self):\n    \"\"\"\n    Evaluates the current neighbor set to determine inactive or redundant nodes.\n\n    Returns:\n        set: A set of node addresses suggested for disconnection.\n    \"\"\"\n    if self._grace_rounds:  # Grace rounds\n        self._grace_rounds -= 1\n        if self._verbose: logging.info(\"Grace time hasnt finished...\")\n        return None\n\n    if self._verbose: logging.info(\"Evaluation in process\")\n\n    result = set()     \n    if self._last_check == 0:\n        self._check_done = True\n        nodes = await self._get_nodes()\n        redundant_nodes = set()\n        inactive_nodes = set()\n        for node in nodes:\n            if nodes[node][0]:\n                last_sim = nodes[node][0][-1]\n                inactivity_counter =  nodes[node][1]\n                if inactivity_counter &gt;= self.INACTIVE_THRESHOLD:\n                    inactive_nodes.add(node)\n                    if self._verbose: logging.info(f\"Node: {node} hadn't participated in any of the last {self.INACTIVE_THRESHOLD} rounds\")\n                else:\n                    if self._verbose: logging.info(f\"Node: {node} inactivity counter: {inactivity_counter}\")\n\n                if node not in self._round_missing_nodes:\n                    if last_sim &lt; self.SIMILARITY_THRESHOLD:\n                        if self._verbose: logging.info(f\"Node: {node} got a similarity value of: {last_sim} under threshold: {self.SIMILARITY_THRESHOLD}\")\n                    else:\n                        if self._verbose: logging.info(f\"Node: {node} got a redundant model, cossine simmilarity: {last_sim} over threshold: {self.SIMILARITY_THRESHOLD}\")\n                        redundant_nodes.add((node, last_sim))\n\n        if self._verbose: logging.info(f\"Inactive nodes on aggregations: {inactive_nodes}\")\n        if self._verbose: logging.info(f\"Redundant nodes on aggregations: {redundant_nodes}\")\n        if inactive_nodes:\n            result = result.union(inactive_nodes)    \n        if len(redundant_nodes):\n            sorted_redundant_nodes = sorted(redundant_nodes, key=lambda x: x[1])\n            n_discarded = math.ceil((len(redundant_nodes)/2))\n            discard_nodes = sorted_redundant_nodes[:n_discarded]\n            discard_nodes = [node for (node,_) in discard_nodes]\n            if self._verbose: logging.info(f\"Discarded redundant nodes: {discard_nodes}\")\n            result = result.union(discard_nodes)\n    else:\n        if self._verbose: logging.info(f\"Evaluation is on cooldown... | {self.CHECK_COOLDOWN - self._last_check} rounds remaining\")\n        self._check_done = False\n\n    self._last_check = (self._last_check + 1)  % self.CHECK_COOLDOWN\n\n    return result\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/satraining/trainingpolicy/qdstrainingpolicy/#nebula.core.situationalawareness.awareness.satraining.trainingpolicy.qdstrainingpolicy.QDSTrainingPolicy.get_evaluation_results","title":"<code>get_evaluation_results()</code>  <code>async</code>","text":"<p>Triggers suggested actions based on last evaluation results.</p> <p>Suggests disconnection from nodes marked as inactive or redundant.</p> Source code in <code>nebula/core/situationalawareness/awareness/satraining/trainingpolicy/qdstrainingpolicy.py</code> <pre><code>async def get_evaluation_results(self):\n    \"\"\"\n    Triggers suggested actions based on last evaluation results.\n\n    Suggests disconnection from nodes marked as inactive or redundant.\n    \"\"\"\n    if self._check_done:\n        for node_discarded in self._evaluation_results:\n            args = (node_discarded, False, True)\n            sac = factory_sa_command(\n                \"connectivity\",                        \n                SACommandAction.DISCONNECT,\n                self,           \n                node_discarded,                       \n                SACommandPRIO.MEDIUM,                 \n                False,                                \n                CommunicationsManager.get_instance().disconnect,  \n                *args                                  \n            )\n            await self.suggest_action(sac)\n        await self.notify_all_suggestions_done(RoundEndEvent)\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/satraining/trainingpolicy/qdstrainingpolicy/#nebula.core.situationalawareness.awareness.satraining.trainingpolicy.qdstrainingpolicy.QDSTrainingPolicy.init","title":"<code>init(config)</code>  <code>async</code>","text":"<p>Initializes the internal state and subscribes to necessary events.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Must contain a 'nodes' set representing known neighbors.</p> required Source code in <code>nebula/core/situationalawareness/awareness/satraining/trainingpolicy/qdstrainingpolicy.py</code> <pre><code>async def init(self, config):\n    \"\"\"\n    Initializes the internal state and subscribes to necessary events.\n\n    Args:\n        config (dict): Must contain a 'nodes' set representing known neighbors.\n    \"\"\"\n    async with self._nodes_lock:\n        nodes = config[\"nodes\"]\n        self._nodes : dict[str, tuple[deque, int]] = {node_id: (deque(maxlen=self.MAX_HISTORIC_SIZE), 0) for node_id in nodes}\n    await EventManager.get_instance().subscribe_node_event(AggregationEvent, self._process_aggregation_event)\n    await EventManager.get_instance().subscribe_node_event(UpdateNeighborEvent, self._update_neighbors)\n    await self.register_sa_agent()\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/satraining/trainingpolicy/trainingpolicy/","title":"Documentation for Trainingpolicy Module","text":""},{"location":"api/core/situationalawareness/awareness/sautils/","title":"Documentation for Sautils Module","text":""},{"location":"api/core/situationalawareness/awareness/sautils/sacommand/","title":"Documentation for Sacommand Module","text":""},{"location":"api/core/situationalawareness/awareness/sautils/sacommand/#nebula.core.situationalawareness.awareness.sautils.sacommand.AggregationCommand","title":"<code>AggregationCommand</code>","text":"<p>               Bases: <code>SACommand</code></p> <p>Commands related to data aggregation.</p> Source code in <code>nebula/core/situationalawareness/awareness/sautils/sacommand.py</code> <pre><code>class AggregationCommand(SACommand):\n    \"\"\"Commands related to data aggregation.\"\"\"\n\n    def __init__(\n        self,\n        action: SACommandAction,\n        owner: \"SAModuleAgent\",\n        target: dict,\n        priority: SACommandPRIO = SACommandPRIO.MEDIUM,\n        parallelizable=False,\n    ):\n        super().__init__(SACommandType.CONNECTIVITY, action, owner, target, priority, parallelizable)\n\n    async def execute(self):\n        await self._update_command_state(SACommandState.EXECUTED)\n        return self._target\n\n    async def conflicts_with(self, other: \"AggregationCommand\") -&gt; bool:\n        \"\"\"Determines if two commands conflict with each other.\"\"\"\n        topologic_conflict = False\n        weight_conflict = False\n\n        if set(self._target.keys()) != set(other._target.keys()):\n            topologic_conflict = True\n\n        weight_conflict = any(\n            abs(self._target[node][1] - other._target[node][1]) &gt; 0\n            for node in self._target.keys()\n            if node in other._target.keys()\n        )\n\n        return weight_conflict and topologic_conflict\n\n    \"\"\"                                             ###############################\n                                                    #     SA COMMAND FACTORY      #\n                                                    ###############################\n    \"\"\"\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sautils/sacommand/#nebula.core.situationalawareness.awareness.sautils.sacommand.AggregationCommand.conflicts_with","title":"<code>conflicts_with(other)</code>  <code>async</code>","text":"<p>Determines if two commands conflict with each other.</p> Source code in <code>nebula/core/situationalawareness/awareness/sautils/sacommand.py</code> <pre><code>async def conflicts_with(self, other: \"AggregationCommand\") -&gt; bool:\n    \"\"\"Determines if two commands conflict with each other.\"\"\"\n    topologic_conflict = False\n    weight_conflict = False\n\n    if set(self._target.keys()) != set(other._target.keys()):\n        topologic_conflict = True\n\n    weight_conflict = any(\n        abs(self._target[node][1] - other._target[node][1]) &gt; 0\n        for node in self._target.keys()\n        if node in other._target.keys()\n    )\n\n    return weight_conflict and topologic_conflict\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sautils/sacommand/#nebula.core.situationalawareness.awareness.sautils.sacommand.ConnectivityCommand","title":"<code>ConnectivityCommand</code>","text":"<p>               Bases: <code>SACommand</code></p> <p>Commands related to connectivity.</p> Source code in <code>nebula/core/situationalawareness/awareness/sautils/sacommand.py</code> <pre><code>class ConnectivityCommand(SACommand):\n    \"\"\"Commands related to connectivity.\"\"\"\n\n    def __init__(\n        self,\n        action: SACommandAction,\n        owner: \"SAModuleAgent\",\n        target: str,\n        priority: SACommandPRIO = SACommandPRIO.MEDIUM,\n        parallelizable=False,\n        action_function=None,\n        *args,\n    ):\n        super().__init__(SACommandType.CONNECTIVITY, action, owner, target, priority, parallelizable)\n        self._action_function = action_function\n        self._args = args\n\n    async def execute(self):\n        \"\"\"Executes the assigned action function with the given parameters.\"\"\"\n        await self._update_command_state(SACommandState.EXECUTED)\n        if self._action_function:\n            if asyncio.iscoroutinefunction(self._action_function):\n                await self._action_function(*self._args)\n            else:\n                self._action_function(*self._args)\n\n    async def conflicts_with(self, other: \"ConnectivityCommand\") -&gt; bool:\n        \"\"\"Determines if two commands conflict with each other.\"\"\"\n        if await self._owner.get_agent() == await other._owner.get_agent():\n            return False\n\n        if self._target == other._target:\n            conflict_pairs = [\n                {SACommandAction.DISCONNECT, SACommandAction.DISCONNECT},\n            ]\n            return {self._action, other._action} in conflict_pairs\n        else:\n            conflict_pairs = [\n                {SACommandAction.DISCONNECT, SACommandAction.RECONNECT},\n                {SACommandAction.DISCONNECT, SACommandAction.MAINTAIN_CONNECTIONS},\n                {SACommandAction.DISCONNECT, SACommandAction.SEARCH_CONNECTIONS},\n            ]\n            return {self._action, other._action} in conflict_pairs\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sautils/sacommand/#nebula.core.situationalawareness.awareness.sautils.sacommand.ConnectivityCommand.conflicts_with","title":"<code>conflicts_with(other)</code>  <code>async</code>","text":"<p>Determines if two commands conflict with each other.</p> Source code in <code>nebula/core/situationalawareness/awareness/sautils/sacommand.py</code> <pre><code>async def conflicts_with(self, other: \"ConnectivityCommand\") -&gt; bool:\n    \"\"\"Determines if two commands conflict with each other.\"\"\"\n    if await self._owner.get_agent() == await other._owner.get_agent():\n        return False\n\n    if self._target == other._target:\n        conflict_pairs = [\n            {SACommandAction.DISCONNECT, SACommandAction.DISCONNECT},\n        ]\n        return {self._action, other._action} in conflict_pairs\n    else:\n        conflict_pairs = [\n            {SACommandAction.DISCONNECT, SACommandAction.RECONNECT},\n            {SACommandAction.DISCONNECT, SACommandAction.MAINTAIN_CONNECTIONS},\n            {SACommandAction.DISCONNECT, SACommandAction.SEARCH_CONNECTIONS},\n        ]\n        return {self._action, other._action} in conflict_pairs\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sautils/sacommand/#nebula.core.situationalawareness.awareness.sautils.sacommand.ConnectivityCommand.execute","title":"<code>execute()</code>  <code>async</code>","text":"<p>Executes the assigned action function with the given parameters.</p> Source code in <code>nebula/core/situationalawareness/awareness/sautils/sacommand.py</code> <pre><code>async def execute(self):\n    \"\"\"Executes the assigned action function with the given parameters.\"\"\"\n    await self._update_command_state(SACommandState.EXECUTED)\n    if self._action_function:\n        if asyncio.iscoroutinefunction(self._action_function):\n            await self._action_function(*self._args)\n        else:\n            self._action_function(*self._args)\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sautils/sacommand/#nebula.core.situationalawareness.awareness.sautils.sacommand.SACommand","title":"<code>SACommand</code>","text":"<p>Base class for Situational Awareness (SA) module commands.</p> <p>This class defines the core structure and behavior of commands that can be issued by SA agents. Each command has an associated type, action, target, priority, and execution state. Commands may also declare whether they can be executed in parallel. Subclasses must implement the actual logic for execution and conflict detection.</p> <p>Attributes:</p> Name Type Description <code>command_type</code> <code>SACommandType</code> <p>Type of the command (e.g., parameter update, structural change).</p> <code>action</code> <code>SACommandAction</code> <p>Specific action the command performs.</p> <code>owner</code> <code>SAModuleAgent</code> <p>Reference to the module or agent that issued the command.</p> <code>target</code> <code>Any</code> <p>Target of the command (e.g., node, parameter name).</p> <code>priority</code> <code>SACommandPRIO</code> <p>Priority level of the command.</p> <code>parallelizable</code> <code>bool</code> <p>Indicates whether the command can be run concurrently.</p> <code>_state</code> <code>SACommandState</code> <p>Internal state of the command (e.g., PENDING, DISCARDED).</p> <code>_state_future</code> <code>Future</code> <p>Future that resolves when the command changes state.</p> Source code in <code>nebula/core/situationalawareness/awareness/sautils/sacommand.py</code> <pre><code>class SACommand:\n    \"\"\"\n    Base class for Situational Awareness (SA) module commands.\n\n    This class defines the core structure and behavior of commands that can be\n    issued by SA agents. Each command has an associated type, action, target,\n    priority, and execution state. Commands may also declare whether they can be\n    executed in parallel. Subclasses must implement the actual logic for execution\n    and conflict detection.\n\n    Attributes:\n        command_type (SACommandType): Type of the command (e.g., parameter update, structural change).\n        action (SACommandAction): Specific action the command performs.\n        owner (SAModuleAgent): Reference to the module or agent that issued the command.\n        target (Any): Target of the command (e.g., node, parameter name).\n        priority (SACommandPRIO): Priority level of the command.\n        parallelizable (bool): Indicates whether the command can be run concurrently.\n        _state (SACommandState): Internal state of the command (e.g., PENDING, DISCARDED).\n        _state_future (asyncio.Future): Future that resolves when the command changes state.\n    \"\"\"\n\n    def __init__(\n        self,\n        command_type: SACommandType,\n        action: SACommandAction,\n        owner: \"SAModuleAgent\",\n        target,\n        priority: SACommandPRIO = SACommandPRIO.MEDIUM,\n        parallelizable=False,\n    ):\n        self._command_type = command_type\n        self._action = action\n        self._owner = owner\n        self._target = target  # Could be a node, parameter, etc.\n        self._priority = priority\n        self._parallelizable = parallelizable\n        self._state = SACommandState.PENDING\n        self._state_future = asyncio.get_event_loop().create_future()\n\n    @abstractmethod\n    async def execute(self):\n        \"\"\"\n        Execute the command's action on the specified target.\n\n        This method must be implemented by subclasses to define the actual logic\n        of how the command affects the system. It may involve sending messages,\n        modifying local or global state, or interacting with external components.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    async def conflicts_with(self, other: \"SACommand\") -&gt; bool:\n        \"\"\"\n        Determine whether this command conflicts with another command.\n\n        This method must be implemented by subclasses to define conflict logic,\n        e.g., whether two commands target the same resource in incompatible ways.\n        Used during arbitration to resolve simultaneous command suggestions.\n\n        Parameters:\n            other (SACommand): Another command instance to check for conflicts.\n\n        Returns:\n            bool: True if there is a conflict, False otherwise.\n        \"\"\"\n        raise NotImplementedError\n\n    async def discard_command(self):\n        \"\"\"\n        Mark this command as discarded, updating its internal state accordingly.\n        \"\"\"\n        await self._update_command_state(SACommandState.DISCARDED)\n\n    def got_higher_priority_than(self, other_prio: SACommandPRIO):\n        \"\"\"\n        Compare this command's priority against another.\n\n        Args:\n            other_prio (SACommandPRIO): The priority of the other command.\n\n        Returns:\n            bool: True if this command has higher priority, False otherwise.\n        \"\"\"\n        return self._priority.value &gt; other_prio.value\n\n    def get_prio(self):\n        \"\"\"\n        Retrieve the priority level of this command.\n\n        Returns:\n            SACommandPRIO: The command's priority enum.\n        \"\"\"\n        return self._priority\n\n    async def get_owner(self):\n        \"\"\"\n        Asynchronously obtain the owner agent of this command.\n\n        Returns:\n            Agent: The agent instance that owns this command.\n        \"\"\"\n        return await self._owner.get_agent()\n\n    def get_action(self) -&gt; SACommandAction:\n        \"\"\"\n        Get the action associated with this command.\n\n        Returns:\n            SACommandAction: The command's action enum.\n        \"\"\"\n        return self._action\n\n    async def _update_command_state(self, sacs: SACommandState):\n        \"\"\"\n        Update the command's state and resolve its completion future if pending.\n\n        Args:\n            sacs (SACommandState): The new state to assign to the command.\n        \"\"\"\n        self._state = sacs\n        if not self._state_future.done():\n            self._state_future.set_result(sacs)\n\n    def get_state_future(self) -&gt; asyncio.Future:\n        \"\"\"\n        Get the Future that will be completed when the command's state changes.\n\n        Returns:\n            asyncio.Future: Future that resolves to the command's final state.\n        \"\"\"\n        return self._state_future\n\n    def is_parallelizable(self):\n        \"\"\"\n        Indicates whether this command can be executed in parallel with others.\n\n        Returns:\n            bool: True if parallel execution is allowed, False otherwise.\n        \"\"\"\n        return self._parallelizable\n\n    def __repr__(self):\n        return (\n            f\"{self.__class__.__name__}(Type={self._command_type.value}, \"\n            f\"Action={self._action.value}, Target={self._target}, Priority={self._priority.value})\"\n        )\n\n    \"\"\"                                             ###############################\n                                                    #     SA COMMAND SUBCLASS     #\n                                                    ###############################\n    \"\"\"\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sautils/sacommand/#nebula.core.situationalawareness.awareness.sautils.sacommand.SACommand.conflicts_with","title":"<code>conflicts_with(other)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Determine whether this command conflicts with another command.</p> <p>This method must be implemented by subclasses to define conflict logic, e.g., whether two commands target the same resource in incompatible ways. Used during arbitration to resolve simultaneous command suggestions.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>SACommand</code> <p>Another command instance to check for conflicts.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if there is a conflict, False otherwise.</p> Source code in <code>nebula/core/situationalawareness/awareness/sautils/sacommand.py</code> <pre><code>@abstractmethod\nasync def conflicts_with(self, other: \"SACommand\") -&gt; bool:\n    \"\"\"\n    Determine whether this command conflicts with another command.\n\n    This method must be implemented by subclasses to define conflict logic,\n    e.g., whether two commands target the same resource in incompatible ways.\n    Used during arbitration to resolve simultaneous command suggestions.\n\n    Parameters:\n        other (SACommand): Another command instance to check for conflicts.\n\n    Returns:\n        bool: True if there is a conflict, False otherwise.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sautils/sacommand/#nebula.core.situationalawareness.awareness.sautils.sacommand.SACommand.discard_command","title":"<code>discard_command()</code>  <code>async</code>","text":"<p>Mark this command as discarded, updating its internal state accordingly.</p> Source code in <code>nebula/core/situationalawareness/awareness/sautils/sacommand.py</code> <pre><code>async def discard_command(self):\n    \"\"\"\n    Mark this command as discarded, updating its internal state accordingly.\n    \"\"\"\n    await self._update_command_state(SACommandState.DISCARDED)\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sautils/sacommand/#nebula.core.situationalawareness.awareness.sautils.sacommand.SACommand.execute","title":"<code>execute()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Execute the command's action on the specified target.</p> <p>This method must be implemented by subclasses to define the actual logic of how the command affects the system. It may involve sending messages, modifying local or global state, or interacting with external components.</p> Source code in <code>nebula/core/situationalawareness/awareness/sautils/sacommand.py</code> <pre><code>@abstractmethod\nasync def execute(self):\n    \"\"\"\n    Execute the command's action on the specified target.\n\n    This method must be implemented by subclasses to define the actual logic\n    of how the command affects the system. It may involve sending messages,\n    modifying local or global state, or interacting with external components.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sautils/sacommand/#nebula.core.situationalawareness.awareness.sautils.sacommand.SACommand.get_action","title":"<code>get_action()</code>","text":"<p>Get the action associated with this command.</p> <p>Returns:</p> Name Type Description <code>SACommandAction</code> <code>SACommandAction</code> <p>The command's action enum.</p> Source code in <code>nebula/core/situationalawareness/awareness/sautils/sacommand.py</code> <pre><code>def get_action(self) -&gt; SACommandAction:\n    \"\"\"\n    Get the action associated with this command.\n\n    Returns:\n        SACommandAction: The command's action enum.\n    \"\"\"\n    return self._action\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sautils/sacommand/#nebula.core.situationalawareness.awareness.sautils.sacommand.SACommand.get_owner","title":"<code>get_owner()</code>  <code>async</code>","text":"<p>Asynchronously obtain the owner agent of this command.</p> <p>Returns:</p> Name Type Description <code>Agent</code> <p>The agent instance that owns this command.</p> Source code in <code>nebula/core/situationalawareness/awareness/sautils/sacommand.py</code> <pre><code>async def get_owner(self):\n    \"\"\"\n    Asynchronously obtain the owner agent of this command.\n\n    Returns:\n        Agent: The agent instance that owns this command.\n    \"\"\"\n    return await self._owner.get_agent()\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sautils/sacommand/#nebula.core.situationalawareness.awareness.sautils.sacommand.SACommand.get_prio","title":"<code>get_prio()</code>","text":"<p>Retrieve the priority level of this command.</p> <p>Returns:</p> Name Type Description <code>SACommandPRIO</code> <p>The command's priority enum.</p> Source code in <code>nebula/core/situationalawareness/awareness/sautils/sacommand.py</code> <pre><code>def get_prio(self):\n    \"\"\"\n    Retrieve the priority level of this command.\n\n    Returns:\n        SACommandPRIO: The command's priority enum.\n    \"\"\"\n    return self._priority\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sautils/sacommand/#nebula.core.situationalawareness.awareness.sautils.sacommand.SACommand.get_state_future","title":"<code>get_state_future()</code>","text":"<p>Get the Future that will be completed when the command's state changes.</p> <p>Returns:</p> Type Description <code>Future</code> <p>asyncio.Future: Future that resolves to the command's final state.</p> Source code in <code>nebula/core/situationalawareness/awareness/sautils/sacommand.py</code> <pre><code>def get_state_future(self) -&gt; asyncio.Future:\n    \"\"\"\n    Get the Future that will be completed when the command's state changes.\n\n    Returns:\n        asyncio.Future: Future that resolves to the command's final state.\n    \"\"\"\n    return self._state_future\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sautils/sacommand/#nebula.core.situationalawareness.awareness.sautils.sacommand.SACommand.got_higher_priority_than","title":"<code>got_higher_priority_than(other_prio)</code>","text":"<p>Compare this command's priority against another.</p> <p>Parameters:</p> Name Type Description Default <code>other_prio</code> <code>SACommandPRIO</code> <p>The priority of the other command.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if this command has higher priority, False otherwise.</p> Source code in <code>nebula/core/situationalawareness/awareness/sautils/sacommand.py</code> <pre><code>def got_higher_priority_than(self, other_prio: SACommandPRIO):\n    \"\"\"\n    Compare this command's priority against another.\n\n    Args:\n        other_prio (SACommandPRIO): The priority of the other command.\n\n    Returns:\n        bool: True if this command has higher priority, False otherwise.\n    \"\"\"\n    return self._priority.value &gt; other_prio.value\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sautils/sacommand/#nebula.core.situationalawareness.awareness.sautils.sacommand.SACommand.is_parallelizable","title":"<code>is_parallelizable()</code>","text":"<p>Indicates whether this command can be executed in parallel with others.</p> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if parallel execution is allowed, False otherwise.</p> Source code in <code>nebula/core/situationalawareness/awareness/sautils/sacommand.py</code> <pre><code>def is_parallelizable(self):\n    \"\"\"\n    Indicates whether this command can be executed in parallel with others.\n\n    Returns:\n        bool: True if parallel execution is allowed, False otherwise.\n    \"\"\"\n    return self._parallelizable\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sautils/sacommand/#nebula.core.situationalawareness.awareness.sautils.sacommand.SACommandState","title":"<code>SACommandState</code>","text":"<p>               Bases: <code>Enum</code></p> Source code in <code>nebula/core/situationalawareness/awareness/sautils/sacommand.py</code> <pre><code>class SACommandState(Enum):\n    PENDING = \"pending\"\n    DISCARDED = \"discarded\"\n    EXECUTED = \"executed\"\n\n    \"\"\"                                             ###############################\n                                                    #      SA COMMAND CLASS       #\n                                                    ###############################\n    \"\"\"\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sautils/sacommand/#nebula.core.situationalawareness.awareness.sautils.sacommand.SACommandState.EXECUTED","title":"<code>EXECUTED = 'executed'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/core/situationalawareness/awareness/sautils/sacommand/#nebula.core.situationalawareness.awareness.sautils.sacommand.SACommandState.EXECUTED--_1","title":"sacommand","text":""},{"location":"api/core/situationalawareness/awareness/sautils/sacommand/#nebula.core.situationalawareness.awareness.sautils.sacommand.SACommandState.EXECUTED--sa-command-class","title":"SA COMMAND CLASS","text":""},{"location":"api/core/situationalawareness/awareness/sautils/sacommand/#nebula.core.situationalawareness.awareness.sautils.sacommand.SACommandState.EXECUTED--_2","title":"sacommand","text":""},{"location":"api/core/situationalawareness/awareness/sautils/sacommand/#nebula.core.situationalawareness.awareness.sautils.sacommand.factory_sa_command","title":"<code>factory_sa_command(sacommand_type, *config)</code>","text":"<p>Factory function to create situational awareness command instances.</p> <p>Parameters:</p> Name Type Description Default <code>sacommand_type</code> <code>str</code> <p>Identifier of the command type (e.g., \"connectivity\", \"aggregation\").</p> required <code>*config</code> <p>Positional arguments to initialize the specific command class.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>SACommand</code> <code>SACommand</code> <p>An instance of the requested situational awareness command.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided command type is not recognized.</p> Source code in <code>nebula/core/situationalawareness/awareness/sautils/sacommand.py</code> <pre><code>def factory_sa_command(sacommand_type, *config) -&gt; SACommand:\n    \"\"\"\n    Factory function to create situational awareness command instances.\n\n    Args:\n        sacommand_type (str): Identifier of the command type (e.g., \"connectivity\", \"aggregation\").\n        *config: Positional arguments to initialize the specific command class.\n\n    Returns:\n        SACommand: An instance of the requested situational awareness command.\n\n    Raises:\n        ValueError: If the provided command type is not recognized.\n    \"\"\"\n    options = {\n        \"connectivity\": ConnectivityCommand,\n        \"aggregation\": AggregationCommand,\n    }\n\n    cs = options.get(sacommand_type)\n    if cs is None:\n        raise ValueError(f\"Unknown SACommand type: {sacommand_type}\")\n    return cs(*config)\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sautils/samoduleagent/","title":"Documentation for Samoduleagent Module","text":""},{"location":"api/core/situationalawareness/awareness/sautils/samoduleagent/#nebula.core.situationalawareness.awareness.sautils.samoduleagent.SAModuleAgent","title":"<code>SAModuleAgent</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class representing a Situational Awareness (SA) module agent.</p> <p>This interface defines the essential methods that any SA agent must implement to participate in the suggestion and arbitration pipeline. Agents are responsible for registering themselves, suggesting actions in the form of commands, and notifying when all suggestions related to an event are complete.</p> <p>Methods: - get_agent(): Return a unique identifier or name of the agent. - register_sa_agent(): Perform initialization or registration steps for the agent. - suggest_action(sac): Submit a suggested command (SACommand) for arbitration. - notify_all_suggestions_done(event_type): Indicate that all suggestions for a given event are complete.</p> Source code in <code>nebula/core/situationalawareness/awareness/sautils/samoduleagent.py</code> <pre><code>class SAModuleAgent(ABC):\n    \"\"\"\n    Abstract base class representing a Situational Awareness (SA) module agent.\n\n    This interface defines the essential methods that any SA agent must implement\n    to participate in the suggestion and arbitration pipeline. Agents are responsible\n    for registering themselves, suggesting actions in the form of commands, and\n    notifying when all suggestions related to an event are complete.\n\n    Methods:\n    - get_agent(): Return a unique identifier or name of the agent.\n    - register_sa_agent(): Perform initialization or registration steps for the agent.\n    - suggest_action(sac): Submit a suggested command (SACommand) for arbitration.\n    - notify_all_suggestions_done(event_type): Indicate that all suggestions for a given event are complete.\n    \"\"\"\n\n    @abstractmethod\n    async def get_agent(self) -&gt; str:\n        \"\"\"\n        Return the unique identifier or name of the agent.\n\n        Returns:\n            str: The identifier or label representing this SA agent.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    async def register_sa_agent(self):\n        \"\"\"\n        Perform initialization logic required to register this SA agent\n        within the system (e.g., announcing its presence or preparing state).\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    async def suggest_action(self, sac: SACommand):\n        \"\"\"\n        Submit a suggested action in the form of a SACommand for a given context.\n\n        Parameters:\n            sac (SACommand): The command proposed by the agent for execution.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    async def notify_all_suggestions_done(self, event_type):\n        \"\"\"\n        Notify that this agent has completed all its suggestions for a particular event.\n\n        Parameters:\n            event_type (Type[NodeEvent]): The type of the event for which suggestions are now complete.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sautils/samoduleagent/#nebula.core.situationalawareness.awareness.sautils.samoduleagent.SAModuleAgent.get_agent","title":"<code>get_agent()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Return the unique identifier or name of the agent.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The identifier or label representing this SA agent.</p> Source code in <code>nebula/core/situationalawareness/awareness/sautils/samoduleagent.py</code> <pre><code>@abstractmethod\nasync def get_agent(self) -&gt; str:\n    \"\"\"\n    Return the unique identifier or name of the agent.\n\n    Returns:\n        str: The identifier or label representing this SA agent.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sautils/samoduleagent/#nebula.core.situationalawareness.awareness.sautils.samoduleagent.SAModuleAgent.notify_all_suggestions_done","title":"<code>notify_all_suggestions_done(event_type)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Notify that this agent has completed all its suggestions for a particular event.</p> <p>Parameters:</p> Name Type Description Default <code>event_type</code> <code>Type[NodeEvent]</code> <p>The type of the event for which suggestions are now complete.</p> required Source code in <code>nebula/core/situationalawareness/awareness/sautils/samoduleagent.py</code> <pre><code>@abstractmethod\nasync def notify_all_suggestions_done(self, event_type):\n    \"\"\"\n    Notify that this agent has completed all its suggestions for a particular event.\n\n    Parameters:\n        event_type (Type[NodeEvent]): The type of the event for which suggestions are now complete.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sautils/samoduleagent/#nebula.core.situationalawareness.awareness.sautils.samoduleagent.SAModuleAgent.register_sa_agent","title":"<code>register_sa_agent()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Perform initialization logic required to register this SA agent within the system (e.g., announcing its presence or preparing state).</p> Source code in <code>nebula/core/situationalawareness/awareness/sautils/samoduleagent.py</code> <pre><code>@abstractmethod\nasync def register_sa_agent(self):\n    \"\"\"\n    Perform initialization logic required to register this SA agent\n    within the system (e.g., announcing its presence or preparing state).\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sautils/samoduleagent/#nebula.core.situationalawareness.awareness.sautils.samoduleagent.SAModuleAgent.suggest_action","title":"<code>suggest_action(sac)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Submit a suggested action in the form of a SACommand for a given context.</p> <p>Parameters:</p> Name Type Description Default <code>sac</code> <code>SACommand</code> <p>The command proposed by the agent for execution.</p> required Source code in <code>nebula/core/situationalawareness/awareness/sautils/samoduleagent.py</code> <pre><code>@abstractmethod\nasync def suggest_action(self, sac: SACommand):\n    \"\"\"\n    Submit a suggested action in the form of a SACommand for a given context.\n\n    Parameters:\n        sac (SACommand): The command proposed by the agent for execution.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sautils/sasystemmonitor/","title":"Documentation for Sasystemmonitor Module","text":""},{"location":"api/core/situationalawareness/awareness/sautils/sasystemmonitor/#nebula.core.situationalawareness.awareness.sautils.sasystemmonitor.SystemMonitor","title":"<code>SystemMonitor</code>","text":"Source code in <code>nebula/core/situationalawareness/awareness/sautils/sasystemmonitor.py</code> <pre><code>class SystemMonitor:\n    _instance = None\n    _lock = Locker(\"communications_manager_lock\", async_lock=False)\n\n    def __new__(cls):\n        with cls._lock:\n            if cls._instance is None:\n                cls._instance = super().__new__(cls)\n        return cls._instance\n\n    @classmethod\n    def get_instance(cls):\n        \"\"\"Obtain SystemMonitor instance\"\"\"\n        if cls._instance is None:\n            raise ValueError(\"SystemMonitor has not been initialized yet.\")\n        return cls._instance\n\n    def __init__(self):\n        \"\"\"Initialize the system monitor and check for GPU availability.\"\"\"\n        if not hasattr(self, \"_initialized\"):  # To avoid reinitialization on subsequent calls\n            # Try to initialize NVIDIA library if available\n            try:\n                nvmlInit()\n                self.gpu_available = True  # Flag to check if GPU is available\n            except Exception:\n                self.gpu_available = False  # If not, set GPU availability to False\n            self._initialized = True\n\n    async def get_cpu_usage(self):\n        \"\"\"Returns the CPU usage percentage.\"\"\"\n        return psutil.cpu_percent(interval=1)\n\n    async def get_cpu_per_core_usage(self):\n        \"\"\"Returns the CPU usage percentage per core.\"\"\"\n        return psutil.cpu_percent(interval=1, percpu=True)\n\n    async def get_memory_usage(self):\n        \"\"\"Returns the percentage of used RAM memory.\"\"\"\n        memory_info = psutil.virtual_memory()\n        return memory_info.percent\n\n    async def get_swap_memory_usage(self):\n        \"\"\"Returns the percentage of used swap memory.\"\"\"\n        swap_info = psutil.swap_memory()\n        return swap_info.percent\n\n    async def get_network_usage(self, interval=5):\n        \"\"\"Measures network usage over a time interval and returns bandwidth percentage usage.\"\"\"\n        os_name = platform.system()\n\n        # Get max bandwidth (only implemented for Linux)\n        if os_name == \"Linux\":\n            max_bandwidth = self._get_max_bandwidth_linux()\n        else:\n            max_bandwidth = None\n\n        # Take first measurement\n        net_io_start = psutil.net_io_counters()\n        bytes_sent_start = net_io_start.bytes_sent\n        bytes_recv_start = net_io_start.bytes_recv\n\n        # Wait for the interval\n        await asyncio.sleep(interval)\n\n        # Take second measurement\n        net_io_end = psutil.net_io_counters()\n        bytes_sent_end = net_io_end.bytes_sent\n        bytes_recv_end = net_io_end.bytes_recv\n\n        # Calculate bytes transferred during interval\n        bytes_sent = bytes_sent_end - bytes_sent_start\n        bytes_recv = bytes_recv_end - bytes_recv_start\n        total_bytes = bytes_sent + bytes_recv\n\n        # Calculate bandwidth usage percentage\n        bandwidth_used_percent = self._calculate_bandwidth_usage(total_bytes, max_bandwidth, interval)\n\n        return {\n            \"interval\": interval,\n            \"bytes_sent\": bytes_sent,\n            \"bytes_recv\": bytes_recv,\n            \"bandwidth_used_percent\": bandwidth_used_percent,\n            \"bandwidth_max\": max_bandwidth,\n        }\n\n    # TODO catched speed to avoid reading file\n    def _get_max_bandwidth_linux(self, interface=\"eth0\"):\n        \"\"\"Reads max bandwidth from /sys/class/net/{iface}/speed (Linux only).\"\"\"\n        try:\n            with open(f\"/sys/class/net/{interface}/speed\") as f:\n                speed = int(f.read().strip())  # In Mbps\n                return speed\n        except Exception as e:\n            print(f\"Could not read max bandwidth: {e}\")\n            return None\n\n    def _calculate_bandwidth_usage(self, bytes_transferred, max_bandwidth_mbps, interval):\n        \"\"\"Calculates bandwidth usage percentage over the given interval.\"\"\"\n        if max_bandwidth_mbps is None or interval &lt;= 0:\n            return None\n\n        try:\n            # Convert bytes to megabits\n            megabits_transferred = (bytes_transferred * 8) / (1024 * 1024)\n            # Calculate usage in Mbps\n            current_usage_mbps = megabits_transferred / interval\n            # Percentage of max bandwidth\n            usage_percentage = (current_usage_mbps / max_bandwidth_mbps) * 100\n            return usage_percentage\n        except Exception as e:\n            print(f\"Error calculating bandwidth usage: {e}\")\n            return None\n\n    async def get_gpu_usage(self):\n        \"\"\"Returns GPU usage stats if available, otherwise returns None.\"\"\"\n        if not self.gpu_available:\n            return None  # No GPU available, return None\n\n        # If GPU is available, get the usage using pynvml\n        device_count = nvmlDeviceGetCount()\n        gpu_usage = []\n        for i in range(device_count):\n            handle = nvmlDeviceGetHandleByIndex(i)\n            memory_info = nvmlDeviceGetMemoryInfo(handle)\n            utilization = nvmlDeviceGetUtilizationRates(handle)\n            gpu_usage.append({\n                \"gpu\": i,\n                \"memory_used\": memory_info.used / 1024**2,  # MB\n                \"memory_total\": memory_info.total / 1024**2,  # MB\n                \"gpu_usage\": utilization.gpu,\n            })\n        return gpu_usage\n\n    async def get_system_resources(self):\n        \"\"\"Returns a dictionary with all system resource usage statistics.\"\"\"\n        resources = {\n            \"cpu_usage\": await self.get_cpu_usage(),\n            \"cpu_per_core_usage\": await self.get_cpu_per_core_usage(),\n            \"memory_usage\": await self.get_memory_usage(),\n            \"swap_memory_usage\": await self.get_swap_memory_usage(),\n            \"network_usage\": await self.get_network_usage(),\n            \"gpu_usage\": await self.get_gpu_usage(),  # Includes GPU usage or None if no GPU\n        }\n        return resources\n\n    async def close(self):\n        \"\"\"Closes the initialization of the NVIDIA library (if used).\"\"\"\n        if self.gpu_available:\n            nvmlShutdown()\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sautils/sasystemmonitor/#nebula.core.situationalawareness.awareness.sautils.sasystemmonitor.SystemMonitor.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the system monitor and check for GPU availability.</p> Source code in <code>nebula/core/situationalawareness/awareness/sautils/sasystemmonitor.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the system monitor and check for GPU availability.\"\"\"\n    if not hasattr(self, \"_initialized\"):  # To avoid reinitialization on subsequent calls\n        # Try to initialize NVIDIA library if available\n        try:\n            nvmlInit()\n            self.gpu_available = True  # Flag to check if GPU is available\n        except Exception:\n            self.gpu_available = False  # If not, set GPU availability to False\n        self._initialized = True\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sautils/sasystemmonitor/#nebula.core.situationalawareness.awareness.sautils.sasystemmonitor.SystemMonitor.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Closes the initialization of the NVIDIA library (if used).</p> Source code in <code>nebula/core/situationalawareness/awareness/sautils/sasystemmonitor.py</code> <pre><code>async def close(self):\n    \"\"\"Closes the initialization of the NVIDIA library (if used).\"\"\"\n    if self.gpu_available:\n        nvmlShutdown()\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sautils/sasystemmonitor/#nebula.core.situationalawareness.awareness.sautils.sasystemmonitor.SystemMonitor.get_cpu_per_core_usage","title":"<code>get_cpu_per_core_usage()</code>  <code>async</code>","text":"<p>Returns the CPU usage percentage per core.</p> Source code in <code>nebula/core/situationalawareness/awareness/sautils/sasystemmonitor.py</code> <pre><code>async def get_cpu_per_core_usage(self):\n    \"\"\"Returns the CPU usage percentage per core.\"\"\"\n    return psutil.cpu_percent(interval=1, percpu=True)\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sautils/sasystemmonitor/#nebula.core.situationalawareness.awareness.sautils.sasystemmonitor.SystemMonitor.get_cpu_usage","title":"<code>get_cpu_usage()</code>  <code>async</code>","text":"<p>Returns the CPU usage percentage.</p> Source code in <code>nebula/core/situationalawareness/awareness/sautils/sasystemmonitor.py</code> <pre><code>async def get_cpu_usage(self):\n    \"\"\"Returns the CPU usage percentage.\"\"\"\n    return psutil.cpu_percent(interval=1)\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sautils/sasystemmonitor/#nebula.core.situationalawareness.awareness.sautils.sasystemmonitor.SystemMonitor.get_gpu_usage","title":"<code>get_gpu_usage()</code>  <code>async</code>","text":"<p>Returns GPU usage stats if available, otherwise returns None.</p> Source code in <code>nebula/core/situationalawareness/awareness/sautils/sasystemmonitor.py</code> <pre><code>async def get_gpu_usage(self):\n    \"\"\"Returns GPU usage stats if available, otherwise returns None.\"\"\"\n    if not self.gpu_available:\n        return None  # No GPU available, return None\n\n    # If GPU is available, get the usage using pynvml\n    device_count = nvmlDeviceGetCount()\n    gpu_usage = []\n    for i in range(device_count):\n        handle = nvmlDeviceGetHandleByIndex(i)\n        memory_info = nvmlDeviceGetMemoryInfo(handle)\n        utilization = nvmlDeviceGetUtilizationRates(handle)\n        gpu_usage.append({\n            \"gpu\": i,\n            \"memory_used\": memory_info.used / 1024**2,  # MB\n            \"memory_total\": memory_info.total / 1024**2,  # MB\n            \"gpu_usage\": utilization.gpu,\n        })\n    return gpu_usage\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sautils/sasystemmonitor/#nebula.core.situationalawareness.awareness.sautils.sasystemmonitor.SystemMonitor.get_instance","title":"<code>get_instance()</code>  <code>classmethod</code>","text":"<p>Obtain SystemMonitor instance</p> Source code in <code>nebula/core/situationalawareness/awareness/sautils/sasystemmonitor.py</code> <pre><code>@classmethod\ndef get_instance(cls):\n    \"\"\"Obtain SystemMonitor instance\"\"\"\n    if cls._instance is None:\n        raise ValueError(\"SystemMonitor has not been initialized yet.\")\n    return cls._instance\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sautils/sasystemmonitor/#nebula.core.situationalawareness.awareness.sautils.sasystemmonitor.SystemMonitor.get_memory_usage","title":"<code>get_memory_usage()</code>  <code>async</code>","text":"<p>Returns the percentage of used RAM memory.</p> Source code in <code>nebula/core/situationalawareness/awareness/sautils/sasystemmonitor.py</code> <pre><code>async def get_memory_usage(self):\n    \"\"\"Returns the percentage of used RAM memory.\"\"\"\n    memory_info = psutil.virtual_memory()\n    return memory_info.percent\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sautils/sasystemmonitor/#nebula.core.situationalawareness.awareness.sautils.sasystemmonitor.SystemMonitor.get_network_usage","title":"<code>get_network_usage(interval=5)</code>  <code>async</code>","text":"<p>Measures network usage over a time interval and returns bandwidth percentage usage.</p> Source code in <code>nebula/core/situationalawareness/awareness/sautils/sasystemmonitor.py</code> <pre><code>async def get_network_usage(self, interval=5):\n    \"\"\"Measures network usage over a time interval and returns bandwidth percentage usage.\"\"\"\n    os_name = platform.system()\n\n    # Get max bandwidth (only implemented for Linux)\n    if os_name == \"Linux\":\n        max_bandwidth = self._get_max_bandwidth_linux()\n    else:\n        max_bandwidth = None\n\n    # Take first measurement\n    net_io_start = psutil.net_io_counters()\n    bytes_sent_start = net_io_start.bytes_sent\n    bytes_recv_start = net_io_start.bytes_recv\n\n    # Wait for the interval\n    await asyncio.sleep(interval)\n\n    # Take second measurement\n    net_io_end = psutil.net_io_counters()\n    bytes_sent_end = net_io_end.bytes_sent\n    bytes_recv_end = net_io_end.bytes_recv\n\n    # Calculate bytes transferred during interval\n    bytes_sent = bytes_sent_end - bytes_sent_start\n    bytes_recv = bytes_recv_end - bytes_recv_start\n    total_bytes = bytes_sent + bytes_recv\n\n    # Calculate bandwidth usage percentage\n    bandwidth_used_percent = self._calculate_bandwidth_usage(total_bytes, max_bandwidth, interval)\n\n    return {\n        \"interval\": interval,\n        \"bytes_sent\": bytes_sent,\n        \"bytes_recv\": bytes_recv,\n        \"bandwidth_used_percent\": bandwidth_used_percent,\n        \"bandwidth_max\": max_bandwidth,\n    }\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sautils/sasystemmonitor/#nebula.core.situationalawareness.awareness.sautils.sasystemmonitor.SystemMonitor.get_swap_memory_usage","title":"<code>get_swap_memory_usage()</code>  <code>async</code>","text":"<p>Returns the percentage of used swap memory.</p> Source code in <code>nebula/core/situationalawareness/awareness/sautils/sasystemmonitor.py</code> <pre><code>async def get_swap_memory_usage(self):\n    \"\"\"Returns the percentage of used swap memory.\"\"\"\n    swap_info = psutil.swap_memory()\n    return swap_info.percent\n</code></pre>"},{"location":"api/core/situationalawareness/awareness/sautils/sasystemmonitor/#nebula.core.situationalawareness.awareness.sautils.sasystemmonitor.SystemMonitor.get_system_resources","title":"<code>get_system_resources()</code>  <code>async</code>","text":"<p>Returns a dictionary with all system resource usage statistics.</p> Source code in <code>nebula/core/situationalawareness/awareness/sautils/sasystemmonitor.py</code> <pre><code>async def get_system_resources(self):\n    \"\"\"Returns a dictionary with all system resource usage statistics.\"\"\"\n    resources = {\n        \"cpu_usage\": await self.get_cpu_usage(),\n        \"cpu_per_core_usage\": await self.get_cpu_per_core_usage(),\n        \"memory_usage\": await self.get_memory_usage(),\n        \"swap_memory_usage\": await self.get_swap_memory_usage(),\n        \"network_usage\": await self.get_network_usage(),\n        \"gpu_usage\": await self.get_gpu_usage(),  # Includes GPU usage or None if no GPU\n    }\n    return resources\n</code></pre>"},{"location":"api/core/situationalawareness/discovery/","title":"Documentation for Discovery Module","text":""},{"location":"api/core/situationalawareness/discovery/federationconnector/","title":"Documentation for Federationconnector Module","text":""},{"location":"api/core/situationalawareness/discovery/federationconnector/#nebula.core.situationalawareness.discovery.federationconnector.FederationConnector","title":"<code>FederationConnector</code>","text":"<p>               Bases: <code>ISADiscovery</code></p> <p>Responsible for the discovery and operational management of the federation within the Situational Awareness module.</p> <p>The FederationConnector implements the ISADiscovery interface and coordinates both the discovery of participants in the federation and the operational steps required to integrate them into the Situational Awareness (SA) workflow. Its responsibilities include:</p> <ul> <li>Initiating the discovery process using the configured CandidateSelector and ModelHandler.</li> <li>Managing neighbor evaluation and model exchange.</li> <li>Interfacing with the SAReasoner to accept connections and ask for actions to do in response.</li> <li>Applying neighbor policies and orchestrating topology changes.</li> <li>Acting as the operational core of the SA module by executing workflows and ensuring coordination.</li> </ul> <p>This class bridges the discovery logic with situational response capabilities in decentralized or federated systems.</p> Source code in <code>nebula/core/situationalawareness/discovery/federationconnector.py</code> <pre><code>class FederationConnector(ISADiscovery):\n    \"\"\"\n    Responsible for the discovery and operational management of the federation within the Situational Awareness module.\n\n    The FederationConnector implements the ISADiscovery interface and coordinates both the discovery\n    of participants in the federation and the operational steps required to integrate them into the\n    Situational Awareness (SA) workflow. Its responsibilities include:\n\n    - Initiating the discovery process using the configured CandidateSelector and ModelHandler.\n    - Managing neighbor evaluation and model exchange.\n    - Interfacing with the SAReasoner to accept connections and ask for actions to do in response.\n    - Applying neighbor policies and orchestrating topology changes.\n    - Acting as the operational core of the SA module by executing workflows and ensuring coordination.\n\n    This class bridges the discovery logic with situational response capabilities in decentralized or federated systems.\n    \"\"\"\n\n    def __init__(self, aditional_participant, selector, model_handler, engine: \"Engine\", verbose=False):\n        \"\"\"\n        Initialize the FederationConnector.\n\n        Args:\n            aditional_participant (bool): Whether this is an additional participant.\n            selector: The candidate selector instance.\n            model_handler: The model handler instance.\n            engine (Engine): The main engine instance.\n            verbose (bool): Whether to enable verbose logging.\n        \"\"\"\n        self._aditional_participant = aditional_participant\n        self._selector = selector\n        self._model_handler = model_handler\n        self._engine = engine\n        self._verbose = verbose\n        self._sar = None\n\n        # Locks for thread safety\n        self._update_neighbors_lock = Locker(\"update_neighbors_lock\", async_lock=True)\n        self.pending_confirmation_from_nodes_lock = Locker(\"pending_confirmation_from_nodes_lock\", async_lock=True)\n        self.discarded_offers_addr_lock = Locker(\"discarded_offers_addr_lock\", async_lock=True)\n        self.accept_candidates_lock = Locker(\"accept_candidates_lock\", async_lock=True)\n        self.late_connection_process_lock = Locker(\"late_connection_process_lock\", async_lock=True)\n\n        # Data structures\n        self.pending_confirmation_from_nodes = set()\n        self.discarded_offers_addr = []\n        self._background_tasks = []  # Track background tasks\n\n        print_msg_box(msg=\"Starting FederationConnector module...\", indent=2, title=\"FederationConnector module\")\n        logging.info(\"\ud83c\udf10  Initializing Federation Connector\")\n        self._cm = None\n        self.config = engine.get_config()\n        logging.info(\"Initializing Candidate Selector\")\n        self._candidate_selector = factory_CandidateSelector(self._selector)\n        logging.info(\"Initializing Model Handler\")\n        self._model_handler = factory_ModelHandler(model_handler)\n        self.recieve_offer_timer = OFFER_TIMEOUT\n\n    @property\n    def engine(self):\n        \"\"\"Engine\"\"\"\n        return self._engine\n\n    @cached_property\n    def cm(self):\n        \"\"\"Communication Manager\"\"\"\n        return CommunicationsManager.get_instance()\n\n    @property\n    def candidate_selector(self):\n        \"\"\"Candidate selector strategy\"\"\"\n        return self._candidate_selector\n\n    @property\n    def model_handler(self):\n        \"\"\"Model handler strategy\"\"\"\n        return self._model_handler\n\n    @property\n    def sar(self):\n        \"\"\"Situational Awareness Reasoner\"\"\"\n        return self._sar\n\n    async def init(self, sa_reasoner):\n        \"\"\"\n        Initializes the main components of the federation connector, including the situational awareness reasoner\n        and the necessary configuration for neighbor handling and candidate selection.\n\n        This method performs the following tasks:\n        - Stores the reference to the situational awareness reasoner (`SAReasoner`).\n        - Registers message event callbacks.\n        - Subscribes to relevant events such as neighbor updates and model updates.\n        - Configures the `CandidateSelector` with initial weights for:\n            * Model loss\n            * Weight distance\n            * Data heterogeneity\n        - Configures the `ModelHandler`:\n            * total rounds\n            * current round\n            * epochs\n\n        Args:\n            sa_reasoner (ISAReasoner): An instance of the situational awareness reasoner used for decision-making.\n        \"\"\"\n        logging.info(\"Building Federation Connector configurations...\")\n        self._sar: ISAReasoner = sa_reasoner\n        await self._register_message_events_callbacks()\n        await EventManager.get_instance().subscribe_node_event(UpdateNeighborEvent, self._update_neighbors)\n        await EventManager.get_instance().subscribe((\"model\", \"update\"), self._model_update_callback)\n\n        logging.info(\"Building candidate selector configuration..\")\n        await self.candidate_selector.set_config([0, 0.5, 0.5])\n        # self.engine.trainer.get_loss(), self.config.participant[\"molibity_args\"][\"weight_distance\"], self.config.participant[\"molibity_args\"][\"weight_het\"]\n\n    \"\"\"\n                ##############################\n                #        CONNECTIONS         #\n                ##############################\n    \"\"\"\n\n    async def _accept_connection(self, source, joining=False):\n        \"\"\"\n        Handles the acceptance of a connection request delegating on reasoner.\n\n        Args:\n            source (str): Address of the source node requesting the connection.\n            joining (bool): Indicates whether the source node is joining the federation.\n\n        Returns:\n            Any: The result of the underlying connection acceptance process.\n        \"\"\"\n        return await self.sar.accept_connection(source, joining)\n\n    def _still_waiting_for_candidates(self):\n        \"\"\"\n        Checks whether the system is still waiting for candidate neighbors to complete the late connection process.\n\n        Returns:\n            bool: True if still waiting for candidates, False otherwise.\n        \"\"\"\n        return not self.accept_candidates_lock.locked() and self.late_connection_process_lock.locked()\n\n    async def _add_pending_connection_confirmation(self, addr):\n        \"\"\"\n        Adds a node to the pending confirmation set and schedules a cleanup task.\n\n        Args:\n            addr (str): Address of the node to add to pending confirmations.\n        \"\"\"\n        added = False\n        async with self._update_neighbors_lock:\n            async with self.pending_confirmation_from_nodes_lock:\n                if addr not in await self.sar.get_nodes_known(neighbors_only=True):\n                    if addr not in self.pending_confirmation_from_nodes:\n                        logging.info(f\"Addition | pending connection confirmation from: {addr}\")\n                        self.pending_confirmation_from_nodes.add(addr)\n                        added = True\n        if added:\n            task = asyncio.create_task(\n                self._clear_pending_confirmations(node=addr), name=f\"FederationConnector_clear_pending_{addr}\"\n            )\n            self._background_tasks.append(task)\n\n    async def _remove_pending_confirmation_from(self, addr):\n        \"\"\"\n        Removes a node from the pending confirmation set.\n\n        Args:\n            addr (str): Address of the node to remove.\n        \"\"\"\n        async with self.pending_confirmation_from_nodes_lock:\n            self.pending_confirmation_from_nodes.discard(addr)\n\n    async def _clear_pending_confirmations(self, node):\n        \"\"\"\n        Clears the pending confirmation for a given node after a expired timeout.\n\n        Args:\n            node (str): The node address to clear from the pending set.\n        \"\"\"\n        await asyncio.sleep(PENDING_CONFIRMATION_TTL)\n        async with self.pending_confirmation_from_nodes_lock:\n            if node in self.pending_confirmation_from_nodes:\n                logging.info(f\"Discard pending confirmation from: {node} cause of time to live expired...\")\n                self.pending_confirmation_from_nodes.discard(node)\n\n    async def _waiting_confirmation_from(self, addr):\n        \"\"\"\n        Checks whether a node is still pending confirmation.\n\n        Args:\n            addr (str): Address of the node to check.\n\n        Returns:\n            bool: True if the node is still pending confirmation, False otherwise.\n        \"\"\"\n        async with self.pending_confirmation_from_nodes_lock:\n            found = addr in self.pending_confirmation_from_nodes\n        #     logging.info(f\"pending confirmations:{self.pending_confirmation_from_nodes}\")\n        # logging.info(f\"Waiting confirmation from source: {addr}, status: {found}\")\n        return found\n\n    async def _confirmation_received(self, addr, confirmation=True, joining=False):\n        \"\"\"\n        Handles when a confirmation is received from a node.\n\n        If the confirmation is positive, the node is added to the connected list and the appropriate\n        event is published.\n\n        Args:\n            addr (str): Address of the confirming node.\n            confirmation (bool): Whether the confirmation is positive.\n            joining (bool): Whether the node is joining the federation.\n        \"\"\"\n        logging.info(f\" Update | connection confirmation received from: {addr} | joining federation: {joining}\")\n        await self._remove_pending_confirmation_from(addr)\n        if confirmation:\n            await self.cm.connect(addr, direct=True)\n            une = UpdateNeighborEvent(addr, joining=joining)\n            await EventManager.get_instance().publish_node_event(une)\n\n    async def _add_to_discarded_offers(self, addr_discarded):\n        \"\"\"\n        Adds a given address to the list of discarded offers.\n\n        Args:\n            addr_discarded (str): Address of the node whose offer was discarded.\n        \"\"\"\n        async with self.discarded_offers_addr_lock:\n            self.discarded_offers_addr.append(addr_discarded)\n\n    async def _get_actions(self):\n        \"\"\"\n        Retrieves the list of current SA actions.\n\n        Returns:\n            list: A list of SA actions from the situational awareness reasoner.\n        \"\"\"\n        return await self.sar.get_actions()\n\n    async def _register_late_neighbor(self, addr, joinning_federation=False):\n        \"\"\"\n        Registers a node that joined the federation later than expected.\n\n        Args:\n            addr (str): Address of the late neighbor.\n            joinning_federation (bool): Whether the node is joining the federation.\n        \"\"\"\n        if self._verbose:\n            logging.info(f\"Registering | late neighbor: {addr}, joining: {joinning_federation}\")\n        une = UpdateNeighborEvent(addr, joining=joinning_federation)\n        await EventManager.get_instance().publish_node_event(une)\n\n    async def _update_neighbors(self, une: UpdateNeighborEvent):\n        \"\"\"\n        Handles an update to the neighbor list based on an UpdateNeighborEvent.\n\n        Args:\n            une (UpdateNeighborEvent): The event carrying the node to add or remove.\n        \"\"\"\n        node, remove = await une.get_event_data()\n        await self._update_neighbors_lock.acquire_async()\n        if not remove:\n            await self._meet_node(node)\n        await self._remove_pending_confirmation_from(node)\n        await self._update_neighbors_lock.release_async()\n\n    async def _meet_node(self, node):\n        \"\"\"\n        Publishes a NodeFoundEvent for a newly discovered or confirmed neighbor.\n\n        Args:\n            node (str): Address of the node that has been met.\n        \"\"\"\n        nfe = NodeFoundEvent(node)\n        await EventManager.get_instance().publish_node_event(nfe)\n\n    async def accept_model_offer(self, source, decoded_model, rounds, round, epochs, n_neighbors, loss):\n        \"\"\"\n        Evaluate and possibly accept a model offer from a remote source.\n\n        Parameters:\n            source (str): Identifier of the node sending the model.\n            decoded_model (object): The model received and decoded from the sender.\n            rounds (int): Total number of training rounds in the current session.\n            round (int): Current round.\n            epochs (int): Number of epochs assigned for local training.\n            n_neighbors (int): Number of neighbors of the sender.\n            loss (float): Loss value associated with the proposed model.\n\n        Returns:\n            bool: True if the model is accepted and the sender added as a candidate, False otherwise.\n        \"\"\"\n        if not self.accept_candidates_lock.locked():\n            if self._verbose:\n                logging.info(f\"\ud83d\udd04 Processing offer from {source}...\")\n            model_accepted = self.model_handler.accept_model(decoded_model)\n            self.model_handler.set_config(config=(rounds, round, epochs, self))\n            if model_accepted:\n                await self.candidate_selector.add_candidate((source, n_neighbors, loss))\n                return True\n        else:\n            return False\n\n    async def get_trainning_info(self):\n        \"\"\"\n        Retrieves the current training model information from the model handler.\n\n        Returns:\n            Any: The current model or training-related information.\n        \"\"\"\n        return await self.model_handler.get_model(None)\n\n    async def _add_candidate(self, source, n_neighbors, loss):\n        \"\"\"\n        Adds a candidate node to the candidate selector if candidates are currently being accepted.\n\n        Args:\n            source (str): Address of the candidate node.\n            n_neighbors (int): Number of neighbors the candidate currently has.\n            loss (float): Reported model loss from the candidate.\n        \"\"\"\n        if not self.accept_candidates_lock.locked():\n            await self.candidate_selector.add_candidate((source, n_neighbors, loss))\n\n    async def _stop_not_selected_connections(self, rejected: set = {}):\n        \"\"\"\n        Asynchronously stop connections that were not selected after a waiting period.\n\n        Parameters:\n            rejected (set): A set of node addresses that were explicitly rejected\n                            and should be marked for disconnection.\n        \"\"\"\n        await asyncio.sleep(20)\n        for r in rejected:\n            await self._add_to_discarded_offers(r)\n\n        try:\n            async with self.discarded_offers_addr_lock:\n                if len(self.discarded_offers_addr) &gt; 0:\n                    self.discarded_offers_addr = set(self.discarded_offers_addr).difference_update(\n                        await self.cm.get_addrs_current_connections(only_direct=True, myself=False)\n                    )\n                    if self._verbose:\n                        logging.info(\n                            f\"Interrupting connections | discarded offers | nodes discarded: {self.discarded_offers_addr}\"\n                        )\n                    for addr in self.discarded_offers_addr:\n                        if not self._waiting_confirmation_from(addr):\n                            await self.cm.disconnect(addr, mutual_disconnection=True)\n                            await asyncio.sleep(1)\n                    self.discarded_offers_addr = []\n        except asyncio.CancelledError:\n            pass\n\n    async def start_late_connection_process(self, connected=False, msg_type=\"discover_join\", addrs_known=None):\n        \"\"\"\n        Starts the late connection process to discover and join an existing federation.\n\n        This method initiates the discovery phase by broadcasting a `DISCOVER_JOIN` or `DISCOVER_NODES` message\n        to nearby nodes. Nodes that receive this message respond with an `OFFER_MODEL` or `OFFER_METRIC` message,\n        which contains the necessary information to evaluate and select the most suitable candidates.\n\n        The process is protected by locks to avoid race conditions, and it continues iteratively until at least\n        one valid candidate is found. Once candidates are selected, a connection message is sent to the best nodes.\n\n        Args:\n            connected (bool): Whether the node is already connected to some federation (used to differentiate restructuring).\n            msg_type (str): Type of discovery message to send (\"discover_join\" or \"discover_nodes\").\n            addrs_known (Optional[Iterable[str]]): Optional list of known node addresses to use for discovery.\n\n        Notes:\n            - Uses `late_connection_process_lock` to avoid concurrent executions of the discovery process.\n            - Uses `accept_candidates_lock` to prevent late candidate acceptance after selection.\n            - Logs progress and state transitions for monitoring purposes.\n        \"\"\"\n        logging.info(\"\ud83c\udf10  Initializing late connection process..\")\n\n        await self.late_connection_process_lock.acquire_async()\n        best_candidates = []\n        await self.candidate_selector.remove_candidates()\n\n        # find federation and send discover\n        discovers_sent, connections_stablished = await self.cm.stablish_connection_to_federation(msg_type, addrs_known)\n\n        # wait offer\n        if self._verbose:\n            logging.info(f\"Discover messages sent after finding federation: {discovers_sent}\")\n        if discovers_sent:\n            if self._verbose:\n                logging.info(f\"Waiting: {self.recieve_offer_timer}s to receive offers from federation\")\n            await asyncio.sleep(self.recieve_offer_timer)\n\n        # acquire lock to not accept late candidates\n        await self.accept_candidates_lock.acquire_async()\n\n        if await self.candidate_selector.any_candidate():\n            if self._verbose:\n                logging.info(\"Candidates found to connect to...\")\n            # create message to send to candidates selected\n            if not connected:\n                msg = self.cm.create_message(\"connection\", \"late_connect\")\n            else:\n                msg = self.cm.create_message(\"connection\", \"restructure\")\n\n            best_candidates, rejected_candidates = await self.candidate_selector.select_candidates()\n            if self._verbose:\n                logging.info(f\"Candidates | {[addr for addr, _, _ in best_candidates]}\")\n            try:\n                for addr, _, _ in best_candidates:\n                    await self._add_pending_connection_confirmation(addr)\n                    await self.cm.send_message(addr, msg)\n            except asyncio.CancelledError:\n                if self._verbose:\n                    logging.info(\"Error during stablishment\")\n\n            await self.accept_candidates_lock.release_async()\n            await self.late_connection_process_lock.release_async()\n            await self.candidate_selector.remove_candidates()\n            logging.info(\"\ud83c\udf10  Ending late connection process..\")\n        # if no candidates, repeat process\n        else:\n            if self._verbose:\n                logging.info(\"\u2757\ufe0f  No Candidates found...\")\n            await self.accept_candidates_lock.release_async()\n            await self.late_connection_process_lock.release_async()\n            if not connected:\n                if self._verbose:\n                    logging.info(\"\u2757\ufe0f  repeating process...\")\n                await self.start_late_connection_process(connected, msg_type, addrs_known)\n\n    \"\"\"                                                     ##############################\n                                                            #     Mobility callbacks     #\n                                                            ##############################\n    \"\"\"\n\n    async def _register_message_events_callbacks(self):\n        \"\"\"Dinamyc message callback registration\"\"\"\n        me_dict = self.cm.get_messages_events()\n        message_events = [\n            (message_name, message_action)\n            for (message_name, message_actions) in me_dict.items()\n            for message_action in message_actions\n        ]\n        for event_type, action in message_events:\n            callback_name = f\"_{event_type}_{action}_callback\"\n            method = getattr(self, callback_name, None)\n\n            if callable(method):\n                await EventManager.get_instance().subscribe((event_type, action), method)\n\n    async def _connection_disconnect_callback(self, source, message):\n        \"\"\"Remove if there is any pending confirmation from the disconnected node\"\"\"\n        if await self._waiting_confirmation_from(source):\n            await self._confirmation_received(source, confirmation=False)\n\n    async def _model_update_callback(self, source, message):\n        \"\"\"Update confirmation if a model update is received while there is a pending confirmation\"\"\"\n        if await self._waiting_confirmation_from(source):\n            await self._confirmation_received(source)\n\n    async def _connection_late_connect_callback(self, source, message):\n        logging.info(f\"\ud83d\udd17  handle_connection_message | Trigger | Received late connect message from {source}\")\n        # Verify if it's a confirmation message from a previous late connection message sent to source\n        if await self._waiting_confirmation_from(source):\n            await self._confirmation_received(source, joining=True)\n            return\n\n        if not self.engine.get_initialization_status():\n            logging.info(\"\u2757\ufe0f Connection refused | Device not initialized yet...\")\n            return\n\n        if await self._accept_connection(source, joining=True):\n            logging.info(f\"\ud83d\udd17  handle_connection_message | Late connection accepted | source: {source}\")\n            await self.cm.connect(source, direct=True)\n\n            # Verify conenction is accepted\n            conf_msg = self.cm.create_message(\"connection\", \"late_connect\")\n            await self.cm.send_message(source, conf_msg)\n\n            ct_actions, df_actions = await self._get_actions()\n            if len(ct_actions):\n                # logging.info(f\"{ct_actions}\")\n                cnt_msg = self.cm.create_message(\"link\", \"connect_to\", addrs=ct_actions)\n                await self.cm.send_message(source, cnt_msg)\n\n            if len(df_actions):\n                # logging.info(f\"{df_actions}\")\n                for addr in df_actions.split():\n                    await self.cm.disconnect(addr, mutual_disconnection=False)\n\n            await self._register_late_neighbor(source, joinning_federation=True)\n\n        else:\n            logging.info(f\"\u2757\ufe0f  Late connection NOT accepted | source: {source}\")\n\n    async def _connection_restructure_callback(self, source, message):\n        logging.info(f\"\ud83d\udd17  handle_connection_message | Trigger | Received restructure message from {source}\")\n        # Verify if it's a confirmation message from a previous restructure connection message sent to source\n        if await self._waiting_confirmation_from(source):\n            await self._confirmation_received(source, joining=False)\n            return\n\n        if not self.engine.get_initialization_status():\n            logging.info(\"\u2757\ufe0f Connection refused | Device not initialized yet...\")\n            return\n\n        if await self._accept_connection(source, joining=False):\n            logging.info(f\"\ud83d\udd17  handle_connection_message | Trigger | restructure connection accepted from {source}\")\n            await self.cm.connect(source, direct=True)\n\n            conf_msg = self.cm.create_message(\"connection\", \"restructure\")\n            await self.cm.send_message(source, conf_msg)\n\n            ct_actions, df_actions = await self._get_actions()\n            if len(ct_actions):\n                cnt_msg = self.cm.create_message(\"link\", \"connect_to\", addrs=ct_actions)\n                await self.cm.send_message(source, cnt_msg)\n\n            if len(df_actions):\n                for addr in df_actions.split():\n                    await self.cm.disconnect(addr, mutual_disconnection=False)\n                # df_msg = self.cm.create_message(\"link\", \"disconnect_from\", addrs=df_actions)\n                # await self.cm.send_message(source, df_msg)\n\n            await self._register_late_neighbor(source, joinning_federation=False)\n        else:\n            logging.info(f\"\u2757\ufe0f  handle_connection_message | Trigger | restructure connection denied from {source}\")\n\n    async def _discover_discover_join_callback(self, source, message):\n        logging.info(f\"\ud83d\udd0d  handle_discover_message | Trigger | Received discover_join message from {source} \")\n        if len(await self.engine.get_federation_nodes()) &gt; 0:\n            await self.engine.trainning_in_progress_lock.acquire_async()\n            model, rounds, round = (\n                await self.cm.propagator.get_model_information(source, \"stable\")\n                if await self.engine.get_round() &gt; 0\n                else await self.cm.propagator.get_model_information(source, \"initialization\")\n            )\n            await self.engine.trainning_in_progress_lock.release_async()\n            if round != -1:\n                epochs = self.config.participant[\"training_args\"][\"epochs\"]\n                msg = self.cm.create_message(\n                    \"offer\",\n                    \"offer_model\",\n                    len(await self.engine.get_federation_nodes()),\n                    0,\n                    parameters=model,\n                    rounds=rounds,\n                    round=round,\n                    epochs=epochs,\n                )\n                logging.info(f\"Sending offer model to {source}\")\n                await self.cm.send_message(source, msg, message_type=\"offer_model\")\n            else:\n                logging.info(\"Discover join received before federation is running..\")\n                # starter node is going to send info to the new node\n        else:\n            logging.info(f\"\ud83d\udd17  Dissmissing discover join from {source} | no active connections at the moment\")\n\n    async def _discover_discover_nodes_callback(self, source, message):\n        logging.info(f\"\ud83d\udd0d  handle_discover_message | Trigger | Received discover_node message from {source} \")\n        if len(await self.engine.get_federation_nodes()) &gt; 0:\n            if await self._accept_connection(source, joining=False):\n                msg = self.cm.create_message(\n                    \"offer\",\n                    \"offer_metric\",\n                    n_neighbors=len(await self.engine.get_federation_nodes()),\n                    loss=0,  # self.engine.trainer.get_current_loss()\n                )\n                logging.info(f\"Sending offer metric to {source}\")\n                await self.cm.send_message(source, msg)\n        else:\n            logging.info(f\"\ud83d\udd17  Dissmissing discover nodes from {source} | no active connections at the moment\")\n\n    async def _offer_offer_model_callback(self, source, message):\n        logging.info(f\"\ud83d\udd0d  handle_offer_message | Trigger | Received offer_model message from {source}\")\n        await self._meet_node(source)\n        if self._still_waiting_for_candidates():\n            try:\n                model_compressed = message.parameters\n                if await self.accept_model_offer(\n                    source,\n                    model_compressed,\n                    message.rounds,\n                    message.round,\n                    message.epochs,\n                    message.n_neighbors,\n                    message.loss,\n                ):\n                    logging.info(f\"\ud83d\udd27 Model accepted from offer | source: {source}\")\n                else:\n                    logging.info(f\"\u2757\ufe0f Model offer discarded | source: {source}\")\n                    await self._add_to_discarded_offers(source)\n            except RuntimeError:\n                logging.info(f\"\u2757\ufe0f Error proccesing offer model from {source}\")\n        else:\n            logging.info(\n                f\"\u2757\ufe0f handfle_offer_message | NOT accepting offers | waiting candidates: {self._still_waiting_for_candidates()}\"\n            )\n            await self._add_to_discarded_offers(source)\n\n    async def _offer_offer_metric_callback(self, source, message):\n        logging.info(f\"\ud83d\udd0d  handle_offer_message | Trigger | Received offer_metric message from {source}\")\n        await self._meet_node(source)\n        if self._still_waiting_for_candidates():\n            n_neighbors = message.n_neighbors\n            loss = message.loss\n            await self._add_candidate(source, n_neighbors, loss)\n\n    async def _link_connect_to_callback(self, source, message):\n        logging.info(f\"\ud83d\udd17  handle_link_message | Trigger | Received connect_to message from {source}\")\n        addrs = message.addrs\n        for addr in addrs.split():\n            asyncio.create_task(self._meet_node(addr))\n\n    async def _link_disconnect_from_callback(self, source, message):\n        logging.info(f\"\ud83d\udd17  handle_link_message | Trigger | Received disconnect_from message from {source}\")\n        for addr in message.addrs.split():\n            asyncio.create_task(self.cm.disconnect(addr, mutual_disconnection=False))\n\n    async def stop(self):\n        \"\"\"\n        Stop the FederationConnector by clearing pending confirmations and stopping background tasks.\n        \"\"\"\n        logging.info(\"\ud83d\uded1  Stopping FederationConnector...\")\n\n        # Cancel all background tasks\n        if self._background_tasks:\n            logging.info(f\"\ud83d\uded1  Cancelling {len(self._background_tasks)} background tasks...\")\n            for task in self._background_tasks:\n                if not task.done():\n                    task.cancel()\n                    try:\n                        await task\n                    except asyncio.CancelledError:\n                        pass\n            self._background_tasks.clear()\n            logging.info(\"\ud83d\uded1  All background tasks cancelled\")\n\n        # Clear any pending confirmations\n        try:\n            async with self.pending_confirmation_from_nodes_lock:\n                self.pending_confirmation_from_nodes.clear()\n        except Exception as e:\n            logging.warning(f\"Error clearing pending confirmations: {e}\")\n\n        # Clear discarded offers\n        try:\n            async with self.discarded_offers_addr_lock:\n                self.discarded_offers_addr.clear()\n        except Exception as e:\n            logging.warning(f\"Error clearing discarded offers: {e}\")\n\n        logging.info(\"\u2705  FederationConnector stopped successfully\")\n</code></pre>"},{"location":"api/core/situationalawareness/discovery/federationconnector/#nebula.core.situationalawareness.discovery.federationconnector.FederationConnector.candidate_selector","title":"<code>candidate_selector</code>  <code>property</code>","text":"<p>Candidate selector strategy</p>"},{"location":"api/core/situationalawareness/discovery/federationconnector/#nebula.core.situationalawareness.discovery.federationconnector.FederationConnector.cm","title":"<code>cm</code>  <code>cached</code> <code>property</code>","text":"<p>Communication Manager</p>"},{"location":"api/core/situationalawareness/discovery/federationconnector/#nebula.core.situationalawareness.discovery.federationconnector.FederationConnector.engine","title":"<code>engine</code>  <code>property</code>","text":"<p>Engine</p>"},{"location":"api/core/situationalawareness/discovery/federationconnector/#nebula.core.situationalawareness.discovery.federationconnector.FederationConnector.model_handler","title":"<code>model_handler</code>  <code>property</code>","text":"<p>Model handler strategy</p>"},{"location":"api/core/situationalawareness/discovery/federationconnector/#nebula.core.situationalawareness.discovery.federationconnector.FederationConnector.sar","title":"<code>sar</code>  <code>property</code>","text":"<p>Situational Awareness Reasoner</p>"},{"location":"api/core/situationalawareness/discovery/federationconnector/#nebula.core.situationalawareness.discovery.federationconnector.FederationConnector.__init__","title":"<code>__init__(aditional_participant, selector, model_handler, engine, verbose=False)</code>","text":"<p>Initialize the FederationConnector.</p> <p>Parameters:</p> Name Type Description Default <code>aditional_participant</code> <code>bool</code> <p>Whether this is an additional participant.</p> required <code>selector</code> <p>The candidate selector instance.</p> required <code>model_handler</code> <p>The model handler instance.</p> required <code>engine</code> <code>Engine</code> <p>The main engine instance.</p> required <code>verbose</code> <code>bool</code> <p>Whether to enable verbose logging.</p> <code>False</code> Source code in <code>nebula/core/situationalawareness/discovery/federationconnector.py</code> <pre><code>def __init__(self, aditional_participant, selector, model_handler, engine: \"Engine\", verbose=False):\n    \"\"\"\n    Initialize the FederationConnector.\n\n    Args:\n        aditional_participant (bool): Whether this is an additional participant.\n        selector: The candidate selector instance.\n        model_handler: The model handler instance.\n        engine (Engine): The main engine instance.\n        verbose (bool): Whether to enable verbose logging.\n    \"\"\"\n    self._aditional_participant = aditional_participant\n    self._selector = selector\n    self._model_handler = model_handler\n    self._engine = engine\n    self._verbose = verbose\n    self._sar = None\n\n    # Locks for thread safety\n    self._update_neighbors_lock = Locker(\"update_neighbors_lock\", async_lock=True)\n    self.pending_confirmation_from_nodes_lock = Locker(\"pending_confirmation_from_nodes_lock\", async_lock=True)\n    self.discarded_offers_addr_lock = Locker(\"discarded_offers_addr_lock\", async_lock=True)\n    self.accept_candidates_lock = Locker(\"accept_candidates_lock\", async_lock=True)\n    self.late_connection_process_lock = Locker(\"late_connection_process_lock\", async_lock=True)\n\n    # Data structures\n    self.pending_confirmation_from_nodes = set()\n    self.discarded_offers_addr = []\n    self._background_tasks = []  # Track background tasks\n\n    print_msg_box(msg=\"Starting FederationConnector module...\", indent=2, title=\"FederationConnector module\")\n    logging.info(\"\ud83c\udf10  Initializing Federation Connector\")\n    self._cm = None\n    self.config = engine.get_config()\n    logging.info(\"Initializing Candidate Selector\")\n    self._candidate_selector = factory_CandidateSelector(self._selector)\n    logging.info(\"Initializing Model Handler\")\n    self._model_handler = factory_ModelHandler(model_handler)\n    self.recieve_offer_timer = OFFER_TIMEOUT\n</code></pre>"},{"location":"api/core/situationalawareness/discovery/federationconnector/#nebula.core.situationalawareness.discovery.federationconnector.FederationConnector.accept_model_offer","title":"<code>accept_model_offer(source, decoded_model, rounds, round, epochs, n_neighbors, loss)</code>  <code>async</code>","text":"<p>Evaluate and possibly accept a model offer from a remote source.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>Identifier of the node sending the model.</p> required <code>decoded_model</code> <code>object</code> <p>The model received and decoded from the sender.</p> required <code>rounds</code> <code>int</code> <p>Total number of training rounds in the current session.</p> required <code>round</code> <code>int</code> <p>Current round.</p> required <code>epochs</code> <code>int</code> <p>Number of epochs assigned for local training.</p> required <code>n_neighbors</code> <code>int</code> <p>Number of neighbors of the sender.</p> required <code>loss</code> <code>float</code> <p>Loss value associated with the proposed model.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the model is accepted and the sender added as a candidate, False otherwise.</p> Source code in <code>nebula/core/situationalawareness/discovery/federationconnector.py</code> <pre><code>async def accept_model_offer(self, source, decoded_model, rounds, round, epochs, n_neighbors, loss):\n    \"\"\"\n    Evaluate and possibly accept a model offer from a remote source.\n\n    Parameters:\n        source (str): Identifier of the node sending the model.\n        decoded_model (object): The model received and decoded from the sender.\n        rounds (int): Total number of training rounds in the current session.\n        round (int): Current round.\n        epochs (int): Number of epochs assigned for local training.\n        n_neighbors (int): Number of neighbors of the sender.\n        loss (float): Loss value associated with the proposed model.\n\n    Returns:\n        bool: True if the model is accepted and the sender added as a candidate, False otherwise.\n    \"\"\"\n    if not self.accept_candidates_lock.locked():\n        if self._verbose:\n            logging.info(f\"\ud83d\udd04 Processing offer from {source}...\")\n        model_accepted = self.model_handler.accept_model(decoded_model)\n        self.model_handler.set_config(config=(rounds, round, epochs, self))\n        if model_accepted:\n            await self.candidate_selector.add_candidate((source, n_neighbors, loss))\n            return True\n    else:\n        return False\n</code></pre>"},{"location":"api/core/situationalawareness/discovery/federationconnector/#nebula.core.situationalawareness.discovery.federationconnector.FederationConnector.get_trainning_info","title":"<code>get_trainning_info()</code>  <code>async</code>","text":"<p>Retrieves the current training model information from the model handler.</p> <p>Returns:</p> Name Type Description <code>Any</code> <p>The current model or training-related information.</p> Source code in <code>nebula/core/situationalawareness/discovery/federationconnector.py</code> <pre><code>async def get_trainning_info(self):\n    \"\"\"\n    Retrieves the current training model information from the model handler.\n\n    Returns:\n        Any: The current model or training-related information.\n    \"\"\"\n    return await self.model_handler.get_model(None)\n</code></pre>"},{"location":"api/core/situationalawareness/discovery/federationconnector/#nebula.core.situationalawareness.discovery.federationconnector.FederationConnector.init","title":"<code>init(sa_reasoner)</code>  <code>async</code>","text":"<p>Initializes the main components of the federation connector, including the situational awareness reasoner and the necessary configuration for neighbor handling and candidate selection.</p> <p>This method performs the following tasks: - Stores the reference to the situational awareness reasoner (<code>SAReasoner</code>). - Registers message event callbacks. - Subscribes to relevant events such as neighbor updates and model updates. - Configures the <code>CandidateSelector</code> with initial weights for:     * Model loss     * Weight distance     * Data heterogeneity - Configures the <code>ModelHandler</code>:     * total rounds     * current round     * epochs</p> <p>Parameters:</p> Name Type Description Default <code>sa_reasoner</code> <code>ISAReasoner</code> <p>An instance of the situational awareness reasoner used for decision-making.</p> required Source code in <code>nebula/core/situationalawareness/discovery/federationconnector.py</code> <pre><code>async def init(self, sa_reasoner):\n    \"\"\"\n    Initializes the main components of the federation connector, including the situational awareness reasoner\n    and the necessary configuration for neighbor handling and candidate selection.\n\n    This method performs the following tasks:\n    - Stores the reference to the situational awareness reasoner (`SAReasoner`).\n    - Registers message event callbacks.\n    - Subscribes to relevant events such as neighbor updates and model updates.\n    - Configures the `CandidateSelector` with initial weights for:\n        * Model loss\n        * Weight distance\n        * Data heterogeneity\n    - Configures the `ModelHandler`:\n        * total rounds\n        * current round\n        * epochs\n\n    Args:\n        sa_reasoner (ISAReasoner): An instance of the situational awareness reasoner used for decision-making.\n    \"\"\"\n    logging.info(\"Building Federation Connector configurations...\")\n    self._sar: ISAReasoner = sa_reasoner\n    await self._register_message_events_callbacks()\n    await EventManager.get_instance().subscribe_node_event(UpdateNeighborEvent, self._update_neighbors)\n    await EventManager.get_instance().subscribe((\"model\", \"update\"), self._model_update_callback)\n\n    logging.info(\"Building candidate selector configuration..\")\n    await self.candidate_selector.set_config([0, 0.5, 0.5])\n</code></pre>"},{"location":"api/core/situationalawareness/discovery/federationconnector/#nebula.core.situationalawareness.discovery.federationconnector.FederationConnector.start_late_connection_process","title":"<code>start_late_connection_process(connected=False, msg_type='discover_join', addrs_known=None)</code>  <code>async</code>","text":"<p>Starts the late connection process to discover and join an existing federation.</p> <p>This method initiates the discovery phase by broadcasting a <code>DISCOVER_JOIN</code> or <code>DISCOVER_NODES</code> message to nearby nodes. Nodes that receive this message respond with an <code>OFFER_MODEL</code> or <code>OFFER_METRIC</code> message, which contains the necessary information to evaluate and select the most suitable candidates.</p> <p>The process is protected by locks to avoid race conditions, and it continues iteratively until at least one valid candidate is found. Once candidates are selected, a connection message is sent to the best nodes.</p> <p>Parameters:</p> Name Type Description Default <code>connected</code> <code>bool</code> <p>Whether the node is already connected to some federation (used to differentiate restructuring).</p> <code>False</code> <code>msg_type</code> <code>str</code> <p>Type of discovery message to send (\"discover_join\" or \"discover_nodes\").</p> <code>'discover_join'</code> <code>addrs_known</code> <code>Optional[Iterable[str]]</code> <p>Optional list of known node addresses to use for discovery.</p> <code>None</code> Notes <ul> <li>Uses <code>late_connection_process_lock</code> to avoid concurrent executions of the discovery process.</li> <li>Uses <code>accept_candidates_lock</code> to prevent late candidate acceptance after selection.</li> <li>Logs progress and state transitions for monitoring purposes.</li> </ul> Source code in <code>nebula/core/situationalawareness/discovery/federationconnector.py</code> <pre><code>async def start_late_connection_process(self, connected=False, msg_type=\"discover_join\", addrs_known=None):\n    \"\"\"\n    Starts the late connection process to discover and join an existing federation.\n\n    This method initiates the discovery phase by broadcasting a `DISCOVER_JOIN` or `DISCOVER_NODES` message\n    to nearby nodes. Nodes that receive this message respond with an `OFFER_MODEL` or `OFFER_METRIC` message,\n    which contains the necessary information to evaluate and select the most suitable candidates.\n\n    The process is protected by locks to avoid race conditions, and it continues iteratively until at least\n    one valid candidate is found. Once candidates are selected, a connection message is sent to the best nodes.\n\n    Args:\n        connected (bool): Whether the node is already connected to some federation (used to differentiate restructuring).\n        msg_type (str): Type of discovery message to send (\"discover_join\" or \"discover_nodes\").\n        addrs_known (Optional[Iterable[str]]): Optional list of known node addresses to use for discovery.\n\n    Notes:\n        - Uses `late_connection_process_lock` to avoid concurrent executions of the discovery process.\n        - Uses `accept_candidates_lock` to prevent late candidate acceptance after selection.\n        - Logs progress and state transitions for monitoring purposes.\n    \"\"\"\n    logging.info(\"\ud83c\udf10  Initializing late connection process..\")\n\n    await self.late_connection_process_lock.acquire_async()\n    best_candidates = []\n    await self.candidate_selector.remove_candidates()\n\n    # find federation and send discover\n    discovers_sent, connections_stablished = await self.cm.stablish_connection_to_federation(msg_type, addrs_known)\n\n    # wait offer\n    if self._verbose:\n        logging.info(f\"Discover messages sent after finding federation: {discovers_sent}\")\n    if discovers_sent:\n        if self._verbose:\n            logging.info(f\"Waiting: {self.recieve_offer_timer}s to receive offers from federation\")\n        await asyncio.sleep(self.recieve_offer_timer)\n\n    # acquire lock to not accept late candidates\n    await self.accept_candidates_lock.acquire_async()\n\n    if await self.candidate_selector.any_candidate():\n        if self._verbose:\n            logging.info(\"Candidates found to connect to...\")\n        # create message to send to candidates selected\n        if not connected:\n            msg = self.cm.create_message(\"connection\", \"late_connect\")\n        else:\n            msg = self.cm.create_message(\"connection\", \"restructure\")\n\n        best_candidates, rejected_candidates = await self.candidate_selector.select_candidates()\n        if self._verbose:\n            logging.info(f\"Candidates | {[addr for addr, _, _ in best_candidates]}\")\n        try:\n            for addr, _, _ in best_candidates:\n                await self._add_pending_connection_confirmation(addr)\n                await self.cm.send_message(addr, msg)\n        except asyncio.CancelledError:\n            if self._verbose:\n                logging.info(\"Error during stablishment\")\n\n        await self.accept_candidates_lock.release_async()\n        await self.late_connection_process_lock.release_async()\n        await self.candidate_selector.remove_candidates()\n        logging.info(\"\ud83c\udf10  Ending late connection process..\")\n    # if no candidates, repeat process\n    else:\n        if self._verbose:\n            logging.info(\"\u2757\ufe0f  No Candidates found...\")\n        await self.accept_candidates_lock.release_async()\n        await self.late_connection_process_lock.release_async()\n        if not connected:\n            if self._verbose:\n                logging.info(\"\u2757\ufe0f  repeating process...\")\n            await self.start_late_connection_process(connected, msg_type, addrs_known)\n</code></pre>"},{"location":"api/core/situationalawareness/discovery/federationconnector/#nebula.core.situationalawareness.discovery.federationconnector.FederationConnector.stop","title":"<code>stop()</code>  <code>async</code>","text":"<p>Stop the FederationConnector by clearing pending confirmations and stopping background tasks.</p> Source code in <code>nebula/core/situationalawareness/discovery/federationconnector.py</code> <pre><code>async def stop(self):\n    \"\"\"\n    Stop the FederationConnector by clearing pending confirmations and stopping background tasks.\n    \"\"\"\n    logging.info(\"\ud83d\uded1  Stopping FederationConnector...\")\n\n    # Cancel all background tasks\n    if self._background_tasks:\n        logging.info(f\"\ud83d\uded1  Cancelling {len(self._background_tasks)} background tasks...\")\n        for task in self._background_tasks:\n            if not task.done():\n                task.cancel()\n                try:\n                    await task\n                except asyncio.CancelledError:\n                    pass\n        self._background_tasks.clear()\n        logging.info(\"\ud83d\uded1  All background tasks cancelled\")\n\n    # Clear any pending confirmations\n    try:\n        async with self.pending_confirmation_from_nodes_lock:\n            self.pending_confirmation_from_nodes.clear()\n    except Exception as e:\n        logging.warning(f\"Error clearing pending confirmations: {e}\")\n\n    # Clear discarded offers\n    try:\n        async with self.discarded_offers_addr_lock:\n            self.discarded_offers_addr.clear()\n    except Exception as e:\n        logging.warning(f\"Error clearing discarded offers: {e}\")\n\n    logging.info(\"\u2705  FederationConnector stopped successfully\")\n</code></pre>"},{"location":"api/core/situationalawareness/discovery/candidateselection/","title":"Documentation for Candidateselection Module","text":""},{"location":"api/core/situationalawareness/discovery/candidateselection/candidateselector/","title":"Documentation for Candidateselector Module","text":""},{"location":"api/core/situationalawareness/discovery/candidateselection/candidateselector/#nebula.core.situationalawareness.discovery.candidateselection.candidateselector.CandidateSelector","title":"<code>CandidateSelector</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>nebula/core/situationalawareness/discovery/candidateselection/candidateselector.py</code> <pre><code>class CandidateSelector(ABC):\n    @abstractmethod\n    async def set_config(self, config):\n        \"\"\"\n        Configure internal parameters for the candidate selection strategy.\n\n        Parameters:\n            config: A configuration object or dictionary with necessary parameters.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def add_candidate(self, candidate):\n        \"\"\"\n        Add a new candidate to the internal pool of potential selections.\n\n        Parameters:\n            candidate: The candidate node or object to be considered for selection.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def select_candidates(self):\n        \"\"\"\n        Apply the selection logic to choose the best candidates from the internal pool.\n\n        Returns:\n            list: A list of selected candidates based on the implemented strategy.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def remove_candidates(self):\n        \"\"\"\n        Remove one or more candidates from the pool based on internal rules or external decisions.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def any_candidate(self):\n        \"\"\"\n        Check whether there are any candidates currently available in the internal pool.\n\n        Returns:\n            bool: True if at least one candidate is available, False otherwise.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/core/situationalawareness/discovery/candidateselection/candidateselector/#nebula.core.situationalawareness.discovery.candidateselection.candidateselector.CandidateSelector.add_candidate","title":"<code>add_candidate(candidate)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Add a new candidate to the internal pool of potential selections.</p> <p>Parameters:</p> Name Type Description Default <code>candidate</code> <p>The candidate node or object to be considered for selection.</p> required Source code in <code>nebula/core/situationalawareness/discovery/candidateselection/candidateselector.py</code> <pre><code>@abstractmethod\nasync def add_candidate(self, candidate):\n    \"\"\"\n    Add a new candidate to the internal pool of potential selections.\n\n    Parameters:\n        candidate: The candidate node or object to be considered for selection.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/situationalawareness/discovery/candidateselection/candidateselector/#nebula.core.situationalawareness.discovery.candidateselection.candidateselector.CandidateSelector.any_candidate","title":"<code>any_candidate()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Check whether there are any candidates currently available in the internal pool.</p> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if at least one candidate is available, False otherwise.</p> Source code in <code>nebula/core/situationalawareness/discovery/candidateselection/candidateselector.py</code> <pre><code>@abstractmethod\nasync def any_candidate(self):\n    \"\"\"\n    Check whether there are any candidates currently available in the internal pool.\n\n    Returns:\n        bool: True if at least one candidate is available, False otherwise.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/situationalawareness/discovery/candidateselection/candidateselector/#nebula.core.situationalawareness.discovery.candidateselection.candidateselector.CandidateSelector.remove_candidates","title":"<code>remove_candidates()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Remove one or more candidates from the pool based on internal rules or external decisions.</p> Source code in <code>nebula/core/situationalawareness/discovery/candidateselection/candidateselector.py</code> <pre><code>@abstractmethod\nasync def remove_candidates(self):\n    \"\"\"\n    Remove one or more candidates from the pool based on internal rules or external decisions.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/situationalawareness/discovery/candidateselection/candidateselector/#nebula.core.situationalawareness.discovery.candidateselection.candidateselector.CandidateSelector.select_candidates","title":"<code>select_candidates()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Apply the selection logic to choose the best candidates from the internal pool.</p> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of selected candidates based on the implemented strategy.</p> Source code in <code>nebula/core/situationalawareness/discovery/candidateselection/candidateselector.py</code> <pre><code>@abstractmethod\nasync def select_candidates(self):\n    \"\"\"\n    Apply the selection logic to choose the best candidates from the internal pool.\n\n    Returns:\n        list: A list of selected candidates based on the implemented strategy.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/situationalawareness/discovery/candidateselection/candidateselector/#nebula.core.situationalawareness.discovery.candidateselection.candidateselector.CandidateSelector.set_config","title":"<code>set_config(config)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Configure internal parameters for the candidate selection strategy.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>A configuration object or dictionary with necessary parameters.</p> required Source code in <code>nebula/core/situationalawareness/discovery/candidateselection/candidateselector.py</code> <pre><code>@abstractmethod\nasync def set_config(self, config):\n    \"\"\"\n    Configure internal parameters for the candidate selection strategy.\n\n    Parameters:\n        config: A configuration object or dictionary with necessary parameters.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/situationalawareness/discovery/candidateselection/distcandidateselector/","title":"Documentation for Distcandidateselector Module","text":""},{"location":"api/core/situationalawareness/discovery/candidateselection/distcandidateselector/#nebula.core.situationalawareness.discovery.candidateselection.distcandidateselector.DistanceCandidateSelector","title":"<code>DistanceCandidateSelector</code>","text":"<p>               Bases: <code>CandidateSelector</code></p> <p>Selects candidate nodes based on their physical proximity.</p> <p>This selector uses geolocation data to filter candidates within a  maximum distance threshold. It listens for GPS updates and maintains  a mapping of node identifiers to their distances and coordinates.</p> <p>Attributes:</p> Name Type Description <code>MAX_DISTANCE_THRESHOLD</code> <code>int</code> <p>Maximum distance (in meters) allowed  for a node to be considered a valid candidate.</p> <code>candidates</code> <code>list</code> <p>List of candidate nodes to be evaluated.</p> <code>candidates_lock</code> <code>Locker</code> <p>Async lock for managing concurrent access  to the candidate list.</p> <code>nodes_distances</code> <code>dict</code> <p>Maps node IDs to a tuple containing the  distance and GPS coordinates.</p> <code>nodes_distances_lock</code> <code>Locker</code> <p>Async lock for the distance mapping.</p> <code>_verbose</code> <code>bool</code> <p>Flag to enable verbose logging for debugging.</p> <p>Methods:</p> Name Description <code>set_config</code> <p>Subscribes to GPS events for distance updates.</p> <code>add_candidate</code> <p>Adds a new candidate to the list.</p> <code>select_candidates</code> <p>Returns candidates within the allowed distance.</p> <code>remove_candidates</code> <p>Clears the candidate list.</p> <code>any_candidate</code> <p>Returns True if there is at least one candidate.</p> Inherits from <p>CandidateSelector: Base class interface for candidate selection logic.</p> Source code in <code>nebula/core/situationalawareness/discovery/candidateselection/distcandidateselector.py</code> <pre><code>class DistanceCandidateSelector(CandidateSelector):\n    \"\"\"\n    Selects candidate nodes based on their physical proximity.\n\n    This selector uses geolocation data to filter candidates within a \n    maximum distance threshold. It listens for GPS updates and maintains \n    a mapping of node identifiers to their distances and coordinates.\n\n    Attributes:\n        MAX_DISTANCE_THRESHOLD (int): Maximum distance (in meters) allowed \n            for a node to be considered a valid candidate.\n        candidates (list): List of candidate nodes to be evaluated.\n        candidates_lock (Locker): Async lock for managing concurrent access \n            to the candidate list.\n        nodes_distances (dict): Maps node IDs to a tuple containing the \n            distance and GPS coordinates.\n        nodes_distances_lock (Locker): Async lock for the distance mapping.\n        _verbose (bool): Flag to enable verbose logging for debugging.\n\n    Methods:\n        set_config(config): Subscribes to GPS events for distance updates.\n        add_candidate(candidate): Adds a new candidate to the list.\n        select_candidates(): Returns candidates within the allowed distance.\n        remove_candidates(): Clears the candidate list.\n        any_candidate(): Returns True if there is at least one candidate.\n\n    Inherits from:\n        CandidateSelector: Base class interface for candidate selection logic.\n    \"\"\"\n    # INFO: This value may change according to the needs of the federation\n    MAX_DISTANCE_THRESHOLD = 200\n\n    def __init__(self):\n        self.candidates = []\n        self.candidates_lock = Locker(name=\"candidates_lock\", async_lock=True)\n        self.nodes_distances: dict[str, tuple[float, tuple[float, float]]] = None\n        self.nodes_distances_lock = Locker(\"nodes_distances_lock\", async_lock=True)\n        self._verbose = False\n\n    async def set_config(self, config):\n        await EventManager.get_instance().subscribe_addonevent(GPSEvent, self._udpate_distances)\n\n    async def _udpate_distances(self, gpsevent: GPSEvent):\n        async with self.nodes_distances_lock:\n            distances = await gpsevent.get_event_data()\n            self.nodes_distances = distances\n\n    async def add_candidate(self, candidate):\n        async with self.candidates_lock:\n            self.candidates.append(candidate)\n\n    async def select_candidates(self):\n        async with self.candidates_lock:\n            async with self.nodes_distances_lock:\n                nodes_available = [\n                    candidate\n                    for candidate in self.candidates\n                    if candidate[0] in self.nodes_distances\n                    and self.nodes_distances[candidate[0]][0] &lt; self.MAX_DISTANCE_THRESHOLD\n                ]\n                if self._verbose:\n                    logging.info(f\"Nodes availables: {nodes_available}\")\n        return (nodes_available, [])\n\n    async def remove_candidates(self):\n        async with self.candidates_lock:\n            self.candidates = []\n\n    async def any_candidate(self):\n        async with self.candidates_lock:\n            any = True if len(self.candidates) &gt; 0 else False\n        return any\n</code></pre>"},{"location":"api/core/situationalawareness/discovery/candidateselection/fccandidateselector/","title":"Documentation for Fccandidateselector Module","text":""},{"location":"api/core/situationalawareness/discovery/candidateselection/fccandidateselector/#nebula.core.situationalawareness.discovery.candidateselection.fccandidateselector.FCCandidateSelector","title":"<code>FCCandidateSelector</code>","text":"<p>               Bases: <code>CandidateSelector</code></p> <p>Candidate selector for fully-connected (FC) topologies.</p> <p>In a fully-connected network, all available candidates are accepted without applying any filtering criteria. This selector simply returns all collected candidates.</p> <p>Attributes:</p> Name Type Description <code>candidates</code> <code>list</code> <p>List of all discovered candidate nodes.</p> <code>candidates_lock</code> <code>Locker</code> <p>Lock to ensure thread-safe access to the candidate list.</p> <p>Methods:</p> Name Description <code>set_config</code> <p>No-op for fully-connected mode.</p> <code>add_candidate</code> <p>Adds a new candidate to the list.</p> <code>select_candidates</code> <p>Returns all currently stored candidates.</p> <code>remove_candidates</code> <p>Clears the candidate list.</p> <code>any_candidate</code> <p>Returns True if there is at least one candidate.</p> Inherits from <p>CandidateSelector: Base class interface for candidate selection logic.</p> Source code in <code>nebula/core/situationalawareness/discovery/candidateselection/fccandidateselector.py</code> <pre><code>class FCCandidateSelector(CandidateSelector):\n    \"\"\"\n    Candidate selector for fully-connected (FC) topologies.\n\n    In a fully-connected network, all available candidates are accepted\n    without applying any filtering criteria. This selector simply returns\n    all collected candidates.\n\n    Attributes:\n        candidates (list): List of all discovered candidate nodes.\n        candidates_lock (Locker): Lock to ensure thread-safe access to the candidate list.\n\n    Methods:\n        set_config(config): No-op for fully-connected mode.\n        add_candidate(candidate): Adds a new candidate to the list.\n        select_candidates(): Returns all currently stored candidates.\n        remove_candidates(): Clears the candidate list.\n        any_candidate(): Returns True if there is at least one candidate.\n\n    Inherits from:\n        CandidateSelector: Base class interface for candidate selection logic.\n    \"\"\"\n\n    def __init__(self):\n        self.candidates = []\n        self.candidates_lock = Locker(name=\"candidates_lock\")\n\n    async def set_config(self, config):\n        pass\n\n    async def add_candidate(self, candidate):\n        self.candidates_lock.acquire()\n        self.candidates.append(candidate)\n        self.candidates_lock.release()\n\n    async def select_candidates(self):\n        \"\"\"\n        In Fully-Connected topology all candidates should be selected\n        \"\"\"\n        self.candidates_lock.acquire()\n        cdts = self.candidates.copy()\n        self.candidates_lock.release()\n        return (cdts, [])\n\n    async def remove_candidates(self):\n        self.candidates_lock.acquire()\n        self.candidates = []\n        self.candidates_lock.release()\n\n    async def any_candidate(self):\n        self.candidates_lock.acquire()\n        any = True if len(self.candidates) &gt; 0 else False\n        self.candidates_lock.release()\n        return any\n</code></pre>"},{"location":"api/core/situationalawareness/discovery/candidateselection/fccandidateselector/#nebula.core.situationalawareness.discovery.candidateselection.fccandidateselector.FCCandidateSelector.select_candidates","title":"<code>select_candidates()</code>  <code>async</code>","text":"<p>In Fully-Connected topology all candidates should be selected</p> Source code in <code>nebula/core/situationalawareness/discovery/candidateselection/fccandidateselector.py</code> <pre><code>async def select_candidates(self):\n    \"\"\"\n    In Fully-Connected topology all candidates should be selected\n    \"\"\"\n    self.candidates_lock.acquire()\n    cdts = self.candidates.copy()\n    self.candidates_lock.release()\n    return (cdts, [])\n</code></pre>"},{"location":"api/core/situationalawareness/discovery/candidateselection/ringcandidateselector/","title":"Documentation for Ringcandidateselector Module","text":""},{"location":"api/core/situationalawareness/discovery/candidateselection/ringcandidateselector/#nebula.core.situationalawareness.discovery.candidateselection.ringcandidateselector.RINGCandidateSelector","title":"<code>RINGCandidateSelector</code>","text":"<p>               Bases: <code>CandidateSelector</code></p> <p>Candidate selector for ring topology.</p> <p>In a ring topology, each node connects to a limited set of neighbors forming a closed loop. This selector chooses exactly one candidate from the pool of candidates that has the fewest neighbors, aiming to maintain a balanced ring by connecting nodes with fewer existing connections, avoiding overcharging as possible.</p> <p>Attributes:</p> Name Type Description <code>candidates</code> <code>list</code> <p>List of candidate nodes available for selection.</p> <code>candidates_lock</code> <code>Locker</code> <p>Async lock to ensure thread-safe access to candidates.</p> <p>Methods:</p> Name Description <code>set_config</code> <p>Optional configuration, currently unused.</p> <code>add_candidate</code> <p>Adds a candidate node to the candidate list.</p> <code>select_candidates</code> <p>Selects and returns a single candidate with the minimum number of neighbors.</p> <code>remove_candidates</code> <p>Clears the candidates list.</p> <code>any_candidate</code> <p>Returns True if there is at least one candidate available.</p> Inherits from <p>CandidateSelector: Base interface for candidate selection strategies.</p> Source code in <code>nebula/core/situationalawareness/discovery/candidateselection/ringcandidateselector.py</code> <pre><code>class RINGCandidateSelector(CandidateSelector):\n    \"\"\"\n    Candidate selector for ring topology.\n\n    In a ring topology, each node connects to a limited set of neighbors forming a closed loop.\n    This selector chooses exactly one candidate from the pool of candidates that has the fewest neighbors,\n    aiming to maintain a balanced ring by connecting nodes with fewer existing connections, avoiding overcharging\n    as possible.\n\n    Attributes:\n        candidates (list): List of candidate nodes available for selection.\n        candidates_lock (Locker): Async lock to ensure thread-safe access to candidates.\n\n    Methods:\n        set_config(config): Optional configuration, currently unused.\n        add_candidate(candidate): Adds a candidate node to the candidate list.\n        select_candidates(): Selects and returns a single candidate with the minimum number of neighbors.\n        remove_candidates(): Clears the candidates list.\n        any_candidate(): Returns True if there is at least one candidate available.\n\n    Inherits from:\n        CandidateSelector: Base interface for candidate selection strategies.\n    \"\"\"\n\n    def __init__(self):\n        self._candidates = []\n        self._rejected_candidates = []\n        self.candidates_lock = Locker(name=\"candidates_lock\")\n\n    async def set_config(self, config):\n        pass\n\n    async def add_candidate(self, candidate):\n        \"\"\"\n        To avoid topology problems select 1st candidate found\n        \"\"\"\n        self.candidates_lock.acquire()\n        self._candidates.append(candidate)\n        self.candidates_lock.release()\n\n    async def select_candidates(self):\n        self.candidates_lock.acquire()\n        cdts = []\n\n        if self._candidates:\n            min_neighbors = min(self._candidates, key=lambda x: x[1])[1]\n            tied_candidates = [c for c in self._candidates if c[1] == min_neighbors]\n\n            selected = random.choice(tied_candidates)\n            cdts.append(selected)\n\n        for cdt in self._candidates:\n            if cdt not in cdts:\n                self._rejected_candidates.append(cdt)\n\n        not_cdts = self._rejected_candidates.copy()\n        self.candidates_lock.release()\n        return (cdts, not_cdts)\n\n    async def remove_candidates(self):\n        self.candidates_lock.acquire()\n        self._candidates = []\n        self.candidates_lock.release()\n\n    async def any_candidate(self):\n        self.candidates_lock.acquire()\n        any = True if len(self._candidates) &gt; 0 else False\n        self.candidates_lock.release()\n        return any\n</code></pre>"},{"location":"api/core/situationalawareness/discovery/candidateselection/ringcandidateselector/#nebula.core.situationalawareness.discovery.candidateselection.ringcandidateselector.RINGCandidateSelector.add_candidate","title":"<code>add_candidate(candidate)</code>  <code>async</code>","text":"<p>To avoid topology problems select 1st candidate found</p> Source code in <code>nebula/core/situationalawareness/discovery/candidateselection/ringcandidateselector.py</code> <pre><code>async def add_candidate(self, candidate):\n    \"\"\"\n    To avoid topology problems select 1st candidate found\n    \"\"\"\n    self.candidates_lock.acquire()\n    self._candidates.append(candidate)\n    self.candidates_lock.release()\n</code></pre>"},{"location":"api/core/situationalawareness/discovery/candidateselection/stdcandidateselector/","title":"Documentation for Stdcandidateselector Module","text":""},{"location":"api/core/situationalawareness/discovery/candidateselection/stdcandidateselector/#nebula.core.situationalawareness.discovery.candidateselection.stdcandidateselector.STDandidateSelector","title":"<code>STDandidateSelector</code>","text":"<p>               Bases: <code>CandidateSelector</code></p> <p>Candidate selector for scenarios without a predefined structural topology.</p> <p>In cases where the federation topology is not explicitly structured, this selector chooses candidates based on the average number of neighbors  indicated in their offers. It selects approximately as many candidates as the  average neighbor count, aiming to balance connectivity dynamically.</p> <p>Attributes:</p> Name Type Description <code>candidates</code> <code>list</code> <p>List of candidate nodes available for selection.</p> <code>candidates_lock</code> <code>Locker</code> <p>Async lock to ensure thread-safe access to candidates.</p> <p>Methods:</p> Name Description <code>set_config</code> <p>Optional configuration method.</p> <code>add_candidate</code> <p>Adds a candidate node to the candidate list.</p> <code>select_candidates</code> <p>Selects candidates based on the average neighbor count from offers.</p> <code>remove_candidates</code> <p>Clears the candidates list.</p> <code>any_candidate</code> <p>Returns True if there is at least one candidate available.</p> Inherits from <p>CandidateSelector: Base interface for candidate selection strategies.</p> Source code in <code>nebula/core/situationalawareness/discovery/candidateselection/stdcandidateselector.py</code> <pre><code>class STDandidateSelector(CandidateSelector):\n    \"\"\"\n    Candidate selector for scenarios without a predefined structural topology.\n\n    In cases where the federation topology is not explicitly structured,\n    this selector chooses candidates based on the average number of neighbors \n    indicated in their offers. It selects approximately as many candidates as the \n    average neighbor count, aiming to balance connectivity dynamically.\n\n    Attributes:\n        candidates (list): List of candidate nodes available for selection.\n        candidates_lock (Locker): Async lock to ensure thread-safe access to candidates.\n\n    Methods:\n        set_config(config): Optional configuration method.\n        add_candidate(candidate): Adds a candidate node to the candidate list.\n        select_candidates(): Selects candidates based on the average neighbor count from offers.\n        remove_candidates(): Clears the candidates list.\n        any_candidate(): Returns True if there is at least one candidate available.\n\n    Inherits from:\n        CandidateSelector: Base interface for candidate selection strategies.\n    \"\"\"\n\n    def __init__(self):\n        self.candidates = []\n        self.candidates_lock = Locker(name=\"candidates_lock\")\n\n    async def set_config(self, config):\n        pass\n\n    async def add_candidate(self, candidate):\n        self.candidates_lock.acquire()\n        self.candidates.append(candidate)\n        self.candidates_lock.release()\n\n    async def select_candidates(self):\n        \"\"\"\n        Select mean number of neighbors\n        \"\"\"\n        self.candidates_lock.acquire()\n        mean_neighbors = round(sum(n for _, n, _ in self.candidates) / len(self.candidates) if self.candidates else 0)\n        logging.info(f\"mean number of neighbors: {mean_neighbors}\")\n        cdts = self.candidates[:mean_neighbors]\n        not_selected = set(self.candidates) - set(cdts)\n        self.candidates_lock.release()\n        return (cdts, not_selected)\n\n    async def remove_candidates(self):\n        self.candidates_lock.acquire()\n        self.candidates = []\n        self.candidates_lock.release()\n\n    async def any_candidate(self):\n        self.candidates_lock.acquire()\n        any = True if len(self.candidates) &gt; 0 else False\n        self.candidates_lock.release()\n        return any\n</code></pre>"},{"location":"api/core/situationalawareness/discovery/candidateselection/stdcandidateselector/#nebula.core.situationalawareness.discovery.candidateselection.stdcandidateselector.STDandidateSelector.select_candidates","title":"<code>select_candidates()</code>  <code>async</code>","text":"<p>Select mean number of neighbors</p> Source code in <code>nebula/core/situationalawareness/discovery/candidateselection/stdcandidateselector.py</code> <pre><code>async def select_candidates(self):\n    \"\"\"\n    Select mean number of neighbors\n    \"\"\"\n    self.candidates_lock.acquire()\n    mean_neighbors = round(sum(n for _, n, _ in self.candidates) / len(self.candidates) if self.candidates else 0)\n    logging.info(f\"mean number of neighbors: {mean_neighbors}\")\n    cdts = self.candidates[:mean_neighbors]\n    not_selected = set(self.candidates) - set(cdts)\n    self.candidates_lock.release()\n    return (cdts, not_selected)\n</code></pre>"},{"location":"api/core/situationalawareness/discovery/modelhandlers/","title":"Documentation for Modelhandlers Module","text":""},{"location":"api/core/situationalawareness/discovery/modelhandlers/aggmodelhandler/","title":"Documentation for Aggmodelhandler Module","text":""},{"location":"api/core/situationalawareness/discovery/modelhandlers/aggmodelhandler/#nebula.core.situationalawareness.discovery.modelhandlers.aggmodelhandler.AGGModelHandler","title":"<code>AGGModelHandler</code>","text":"<p>               Bases: <code>ModelHandler</code></p> Source code in <code>nebula/core/situationalawareness/discovery/modelhandlers/aggmodelhandler.py</code> <pre><code>class AGGModelHandler(ModelHandler):\n    def __init__(self):\n        self.model = None\n        self.rounds = 0\n        self.round = 0\n        self.epochs = 1\n        self.model_list = []\n        self.models_lock = Locker(name=\"model_lock\")\n        self.params_lock = Locker(name=\"param_lock\")\n\n    def set_config(self, config):\n        \"\"\"\n        Args:\n            config[0] -&gt; total rounds\n            config[1] -&gt; current round\n            config[2] -&gt; epochs\n        \"\"\"\n        self.params_lock.acquire()\n        self.rounds = config[0]\n        if config[1] &gt; self.round:\n            self.round = config[0]\n        self.epochs = config[2]\n        self.params_lock.release()\n\n    def accept_model(self, model):\n        \"\"\"\n        Save first model receive and collect the rest for pre-processing\n        \"\"\"\n        self.models_lock.acquire()\n        if self.model is None:\n            self.model = model\n        else:\n            self.model_list.append(model)\n        self.models_lock.release()\n\n    def get_model(self, model):\n        \"\"\"\n        Returns:\n            neccesary data to create trainer after pre-processing\n        \"\"\"\n        self.models_lock.acquire()\n        self.pre_process_model()\n        self.models_lock.release()\n        return (self.model, self.rounds, self.round, self.epochs)\n\n    def pre_process_model(self):\n        # define pre-processing strategy\n        pass\n</code></pre>"},{"location":"api/core/situationalawareness/discovery/modelhandlers/aggmodelhandler/#nebula.core.situationalawareness.discovery.modelhandlers.aggmodelhandler.AGGModelHandler.accept_model","title":"<code>accept_model(model)</code>","text":"<p>Save first model receive and collect the rest for pre-processing</p> Source code in <code>nebula/core/situationalawareness/discovery/modelhandlers/aggmodelhandler.py</code> <pre><code>def accept_model(self, model):\n    \"\"\"\n    Save first model receive and collect the rest for pre-processing\n    \"\"\"\n    self.models_lock.acquire()\n    if self.model is None:\n        self.model = model\n    else:\n        self.model_list.append(model)\n    self.models_lock.release()\n</code></pre>"},{"location":"api/core/situationalawareness/discovery/modelhandlers/aggmodelhandler/#nebula.core.situationalawareness.discovery.modelhandlers.aggmodelhandler.AGGModelHandler.get_model","title":"<code>get_model(model)</code>","text":"<p>Returns:</p> Type Description <p>neccesary data to create trainer after pre-processing</p> Source code in <code>nebula/core/situationalawareness/discovery/modelhandlers/aggmodelhandler.py</code> <pre><code>def get_model(self, model):\n    \"\"\"\n    Returns:\n        neccesary data to create trainer after pre-processing\n    \"\"\"\n    self.models_lock.acquire()\n    self.pre_process_model()\n    self.models_lock.release()\n    return (self.model, self.rounds, self.round, self.epochs)\n</code></pre>"},{"location":"api/core/situationalawareness/discovery/modelhandlers/aggmodelhandler/#nebula.core.situationalawareness.discovery.modelhandlers.aggmodelhandler.AGGModelHandler.set_config","title":"<code>set_config(config)</code>","text":"Source code in <code>nebula/core/situationalawareness/discovery/modelhandlers/aggmodelhandler.py</code> <pre><code>def set_config(self, config):\n    \"\"\"\n    Args:\n        config[0] -&gt; total rounds\n        config[1] -&gt; current round\n        config[2] -&gt; epochs\n    \"\"\"\n    self.params_lock.acquire()\n    self.rounds = config[0]\n    if config[1] &gt; self.round:\n        self.round = config[0]\n    self.epochs = config[2]\n    self.params_lock.release()\n</code></pre>"},{"location":"api/core/situationalawareness/discovery/modelhandlers/defaultmodelhandler/","title":"Documentation for Defaultmodelhandler Module","text":""},{"location":"api/core/situationalawareness/discovery/modelhandlers/defaultmodelhandler/#nebula.core.situationalawareness.discovery.modelhandlers.defaultmodelhandler.DefaultModelHandler","title":"<code>DefaultModelHandler</code>","text":"<p>               Bases: <code>ModelHandler</code></p> <p>Provides the initial default model.</p> <p>This handler returns the baseline model with default weights,  typically used at the start of the federation or when no suitable  model offers have been received from peers.</p> Inherits from <p>ModelHandler: Provides the base interface for model operations.</p> Source code in <code>nebula/core/situationalawareness/discovery/modelhandlers/defaultmodelhandler.py</code> <pre><code>class DefaultModelHandler(ModelHandler):\n    \"\"\"\n    Provides the initial default model.\n\n    This handler returns the baseline model with default weights, \n    typically used at the start of the federation or when no suitable \n    model offers have been received from peers.\n\n    Inherits from:\n        ModelHandler: Provides the base interface for model operations.\n    \"\"\"\n\n    def __init__(self):\n        self.model = None\n        self.rounds = 0\n        self.round = 0\n        self.epochs = 0\n        self.model_lock = Locker(name=\"model_lock\")\n        self.params_lock = Locker(name=\"param_lock\")\n        self._nm: FederationConnector = None\n\n    def set_config(self, config):\n        \"\"\"\n        Args:\n            config[0] -&gt; total rounds\n            config[1] -&gt; current round\n            config[2] -&gt; epochs\n            config[3] -&gt; FederationConnector\n        \"\"\"\n        self.params_lock.acquire()\n        self.rounds = config[0]\n        if config[1] &gt; self.round:\n            self.round = config[1]\n        self.epochs = config[2]\n        if not self._nm:\n            self._nm = config[3]\n        self.params_lock.release()\n\n    def accept_model(self, model):\n        return True\n\n    async def get_model(self, model):\n        \"\"\"\n        Returns:\n            model with default weights\n        \"\"\"\n        (sm, _, _) = await self._nm.engine.cm.propagator.get_model_information(None, \"initialization\", init=True)\n        return (sm, self.rounds, self.round, self.epochs)\n\n    def pre_process_model(self):\n        \"\"\"\n        no pre-processing defined\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/core/situationalawareness/discovery/modelhandlers/defaultmodelhandler/#nebula.core.situationalawareness.discovery.modelhandlers.defaultmodelhandler.DefaultModelHandler.get_model","title":"<code>get_model(model)</code>  <code>async</code>","text":"<p>Returns:</p> Type Description <p>model with default weights</p> Source code in <code>nebula/core/situationalawareness/discovery/modelhandlers/defaultmodelhandler.py</code> <pre><code>async def get_model(self, model):\n    \"\"\"\n    Returns:\n        model with default weights\n    \"\"\"\n    (sm, _, _) = await self._nm.engine.cm.propagator.get_model_information(None, \"initialization\", init=True)\n    return (sm, self.rounds, self.round, self.epochs)\n</code></pre>"},{"location":"api/core/situationalawareness/discovery/modelhandlers/defaultmodelhandler/#nebula.core.situationalawareness.discovery.modelhandlers.defaultmodelhandler.DefaultModelHandler.pre_process_model","title":"<code>pre_process_model()</code>","text":"<p>no pre-processing defined</p> Source code in <code>nebula/core/situationalawareness/discovery/modelhandlers/defaultmodelhandler.py</code> <pre><code>def pre_process_model(self):\n    \"\"\"\n    no pre-processing defined\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/situationalawareness/discovery/modelhandlers/defaultmodelhandler/#nebula.core.situationalawareness.discovery.modelhandlers.defaultmodelhandler.DefaultModelHandler.set_config","title":"<code>set_config(config)</code>","text":"Source code in <code>nebula/core/situationalawareness/discovery/modelhandlers/defaultmodelhandler.py</code> <pre><code>def set_config(self, config):\n    \"\"\"\n    Args:\n        config[0] -&gt; total rounds\n        config[1] -&gt; current round\n        config[2] -&gt; epochs\n        config[3] -&gt; FederationConnector\n    \"\"\"\n    self.params_lock.acquire()\n    self.rounds = config[0]\n    if config[1] &gt; self.round:\n        self.round = config[1]\n    self.epochs = config[2]\n    if not self._nm:\n        self._nm = config[3]\n    self.params_lock.release()\n</code></pre>"},{"location":"api/core/situationalawareness/discovery/modelhandlers/modelhandler/","title":"Documentation for Modelhandler Module","text":""},{"location":"api/core/situationalawareness/discovery/modelhandlers/modelhandler/#nebula.core.situationalawareness.discovery.modelhandlers.modelhandler.ModelHandler","title":"<code>ModelHandler</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>nebula/core/situationalawareness/discovery/modelhandlers/modelhandler.py</code> <pre><code>class ModelHandler(ABC):\n    @abstractmethod\n    def set_config(self, config):\n        \"\"\"\n        Configure internal settings for the model handler using the provided configuration.\n\n        Parameters:\n            config: A configuration object or dictionary with parameters relevant to model handling.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def accept_model(self, model):\n        \"\"\"\n        Evaluate and store a received model if it satisfies the required criteria.\n\n        Parameters:\n            model: The model object to be processed or stored.\n\n        Returns:\n            bool: True if the model is accepted, False otherwise.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def get_model(self, model):\n        \"\"\"\n        Asynchronously retrieve or generate the model to be used.\n\n        Parameters:\n            model: A reference to the kind of model to be used.\n\n        Returns:\n            object: The model instance requested.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def pre_process_model(self):\n        \"\"\"\n        Perform any necessary preprocessing steps on the model before it is used.\n\n        Returns:\n            object: The preprocessed model, ready for further operations.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/core/situationalawareness/discovery/modelhandlers/modelhandler/#nebula.core.situationalawareness.discovery.modelhandlers.modelhandler.ModelHandler.accept_model","title":"<code>accept_model(model)</code>  <code>abstractmethod</code>","text":"<p>Evaluate and store a received model if it satisfies the required criteria.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The model object to be processed or stored.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the model is accepted, False otherwise.</p> Source code in <code>nebula/core/situationalawareness/discovery/modelhandlers/modelhandler.py</code> <pre><code>@abstractmethod\ndef accept_model(self, model):\n    \"\"\"\n    Evaluate and store a received model if it satisfies the required criteria.\n\n    Parameters:\n        model: The model object to be processed or stored.\n\n    Returns:\n        bool: True if the model is accepted, False otherwise.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/situationalawareness/discovery/modelhandlers/modelhandler/#nebula.core.situationalawareness.discovery.modelhandlers.modelhandler.ModelHandler.get_model","title":"<code>get_model(model)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Asynchronously retrieve or generate the model to be used.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>A reference to the kind of model to be used.</p> required <p>Returns:</p> Name Type Description <code>object</code> <p>The model instance requested.</p> Source code in <code>nebula/core/situationalawareness/discovery/modelhandlers/modelhandler.py</code> <pre><code>@abstractmethod\nasync def get_model(self, model):\n    \"\"\"\n    Asynchronously retrieve or generate the model to be used.\n\n    Parameters:\n        model: A reference to the kind of model to be used.\n\n    Returns:\n        object: The model instance requested.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/situationalawareness/discovery/modelhandlers/modelhandler/#nebula.core.situationalawareness.discovery.modelhandlers.modelhandler.ModelHandler.pre_process_model","title":"<code>pre_process_model()</code>  <code>abstractmethod</code>","text":"<p>Perform any necessary preprocessing steps on the model before it is used.</p> <p>Returns:</p> Name Type Description <code>object</code> <p>The preprocessed model, ready for further operations.</p> Source code in <code>nebula/core/situationalawareness/discovery/modelhandlers/modelhandler.py</code> <pre><code>@abstractmethod\ndef pre_process_model(self):\n    \"\"\"\n    Perform any necessary preprocessing steps on the model before it is used.\n\n    Returns:\n        object: The preprocessed model, ready for further operations.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/situationalawareness/discovery/modelhandlers/modelhandler/#nebula.core.situationalawareness.discovery.modelhandlers.modelhandler.ModelHandler.set_config","title":"<code>set_config(config)</code>  <code>abstractmethod</code>","text":"<p>Configure internal settings for the model handler using the provided configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>A configuration object or dictionary with parameters relevant to model handling.</p> required Source code in <code>nebula/core/situationalawareness/discovery/modelhandlers/modelhandler.py</code> <pre><code>@abstractmethod\ndef set_config(self, config):\n    \"\"\"\n    Configure internal settings for the model handler using the provided configuration.\n\n    Parameters:\n        config: A configuration object or dictionary with parameters relevant to model handling.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/situationalawareness/discovery/modelhandlers/stdmodelhandler/","title":"Documentation for Stdmodelhandler Module","text":""},{"location":"api/core/situationalawareness/discovery/modelhandlers/stdmodelhandler/#nebula.core.situationalawareness.discovery.modelhandlers.stdmodelhandler.STDModelHandler","title":"<code>STDModelHandler</code>","text":"<p>               Bases: <code>ModelHandler</code></p> <p>Handles the selection and acquisition of the most up-to-date model  during the discovery phase of the federation process.</p> <p>This handler choose the first model received.</p> Inherits from <p>ModelHandler: Provides the base interface for model operations.</p> Intended Use <p>Used during the initial, when a node discovers others and must  align itself with the most recent global model state.</p> Source code in <code>nebula/core/situationalawareness/discovery/modelhandlers/stdmodelhandler.py</code> <pre><code>class STDModelHandler(ModelHandler):\n    \"\"\"\n    Handles the selection and acquisition of the most up-to-date model \n    during the discovery phase of the federation process.\n\n    This handler choose the first model received.\n\n    Inherits from:\n        ModelHandler: Provides the base interface for model operations.\n\n    Intended Use:\n        Used during the initial, when a node discovers others and must \n        align itself with the most recent global model state.\n    \"\"\"\n\n    def __init__(self):\n        self.model = None\n        self.rounds = 0\n        self.round = 0\n        self.epochs = 0\n        self.model_lock = Locker(name=\"model_lock\")\n        self.params_lock = Locker(name=\"param_lock\")\n\n    def set_config(self, config):\n        \"\"\"\n        Args:\n            config[0] -&gt; total rounds\n            config[1] -&gt; current round\n            config[2] -&gt; epochs\n        \"\"\"\n        self.params_lock.acquire()\n        self.rounds = config[0]\n        if config[1] &gt; self.round:\n            self.round = config[1]\n        self.epochs = config[2]\n        self.params_lock.release()\n\n    def accept_model(self, model):\n        \"\"\"\n        save only first model received to set up own model later\n        \"\"\"\n        if not self.model_lock.locked():\n            self.model_lock.acquire()\n            self.model = model\n        return True\n\n    async def get_model(self, model):\n        \"\"\"\n        Returns:\n            neccesary data to create trainer\n        \"\"\"\n        if self.model is not None:\n            return (self.model, self.rounds, self.round, self.epochs)\n        else:\n            return (None, 0, 0, 0)\n\n    def pre_process_model(self):\n        \"\"\"\n        no pre-processing defined\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/core/situationalawareness/discovery/modelhandlers/stdmodelhandler/#nebula.core.situationalawareness.discovery.modelhandlers.stdmodelhandler.STDModelHandler.accept_model","title":"<code>accept_model(model)</code>","text":"<p>save only first model received to set up own model later</p> Source code in <code>nebula/core/situationalawareness/discovery/modelhandlers/stdmodelhandler.py</code> <pre><code>def accept_model(self, model):\n    \"\"\"\n    save only first model received to set up own model later\n    \"\"\"\n    if not self.model_lock.locked():\n        self.model_lock.acquire()\n        self.model = model\n    return True\n</code></pre>"},{"location":"api/core/situationalawareness/discovery/modelhandlers/stdmodelhandler/#nebula.core.situationalawareness.discovery.modelhandlers.stdmodelhandler.STDModelHandler.get_model","title":"<code>get_model(model)</code>  <code>async</code>","text":"<p>Returns:</p> Type Description <p>neccesary data to create trainer</p> Source code in <code>nebula/core/situationalawareness/discovery/modelhandlers/stdmodelhandler.py</code> <pre><code>async def get_model(self, model):\n    \"\"\"\n    Returns:\n        neccesary data to create trainer\n    \"\"\"\n    if self.model is not None:\n        return (self.model, self.rounds, self.round, self.epochs)\n    else:\n        return (None, 0, 0, 0)\n</code></pre>"},{"location":"api/core/situationalawareness/discovery/modelhandlers/stdmodelhandler/#nebula.core.situationalawareness.discovery.modelhandlers.stdmodelhandler.STDModelHandler.pre_process_model","title":"<code>pre_process_model()</code>","text":"<p>no pre-processing defined</p> Source code in <code>nebula/core/situationalawareness/discovery/modelhandlers/stdmodelhandler.py</code> <pre><code>def pre_process_model(self):\n    \"\"\"\n    no pre-processing defined\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/situationalawareness/discovery/modelhandlers/stdmodelhandler/#nebula.core.situationalawareness.discovery.modelhandlers.stdmodelhandler.STDModelHandler.set_config","title":"<code>set_config(config)</code>","text":"Source code in <code>nebula/core/situationalawareness/discovery/modelhandlers/stdmodelhandler.py</code> <pre><code>def set_config(self, config):\n    \"\"\"\n    Args:\n        config[0] -&gt; total rounds\n        config[1] -&gt; current round\n        config[2] -&gt; epochs\n    \"\"\"\n    self.params_lock.acquire()\n    self.rounds = config[0]\n    if config[1] &gt; self.round:\n        self.round = config[1]\n    self.epochs = config[2]\n    self.params_lock.release()\n</code></pre>"},{"location":"api/core/training/","title":"Documentation for Training Module","text":""},{"location":"api/core/training/lightning/","title":"Documentation for Lightning Module","text":""},{"location":"api/core/training/lightning/#nebula.core.training.lightning.Lightning","title":"<code>Lightning</code>","text":"Source code in <code>nebula/core/training/lightning.py</code> <pre><code>class Lightning:\n    DEFAULT_MODEL_WEIGHT = 1\n    BYPASS_MODEL_WEIGHT = 0\n\n    def __init__(self, model, datamodule, config=None):\n        # self.model = torch.compile(model, mode=\"reduce-overhead\")\n        self.model = model\n        self.datamodule = datamodule\n        self.config = config\n        self._trainer = None\n        self.epochs = 1\n        self.round = 0\n        self.experiment_name = self.config.participant[\"scenario_args\"][\"name\"]\n        self.idx = self.config.participant[\"device_args\"][\"idx\"]\n        self.log_dir = os.path.join(self.config.participant[\"tracking_args\"][\"log_dir\"], self.experiment_name)\n        self._logger = None\n        self.create_logger()\n        enable_deterministic(seed=self.config.participant[\"scenario_args\"][\"random_seed\"])\n\n    @property\n    def logger(self):\n        return self._logger\n\n    def get_round(self):\n        return self.round\n\n    def set_model(self, model):\n        self.model = model\n\n    def set_datamodule(self, datamodule):\n        self.datamodule = datamodule\n\n    def create_logger(self):\n        if self.config.participant[\"tracking_args\"][\"local_tracking\"] == \"csv\":\n            nebulalogger = CSVLogger(f\"{self.log_dir}\", name=\"metrics\", version=f\"participant_{self.idx}\")\n        elif self.config.participant[\"tracking_args\"][\"local_tracking\"] == \"basic\":\n            logger_config = None\n            if self._logger is not None:\n                logger_config = self._logger.get_logger_config()\n            nebulalogger = NebulaTensorBoardLogger(\n                self.config.participant[\"scenario_args\"][\"start_time\"],\n                f\"{self.log_dir}\",\n                name=\"metrics\",\n                version=f\"participant_{self.idx}\",\n                log_graph=False,\n            )\n            # Restore logger configuration\n            nebulalogger.set_logger_config(logger_config)\n        else:\n            nebulalogger = None\n\n        self._logger = nebulalogger\n\n    def create_trainer(self):\n        # Create a new trainer and logger for each round\n        self.create_logger()\n        num_gpus = len(self.config.participant[\"device_args\"][\"gpu_id\"])\n        if self.config.participant[\"device_args\"][\"accelerator\"] == \"gpu\" and num_gpus &gt; 0:\n            # Use all available GPUs\n            if num_gpus &gt; 1:\n                gpu_index = [self.config.participant[\"device_args\"][\"idx\"] % num_gpus]\n            # Use the selected GPU\n            else:\n                gpu_index = self.config.participant[\"device_args\"][\"gpu_id\"]\n            logging_training.info(f\"Creating trainer with accelerator GPU ({gpu_index})\")\n            self._trainer = Trainer(\n                callbacks=[ModelSummary(max_depth=1), NebulaProgressBar()],\n                max_epochs=self.epochs,\n                accelerator=\"gpu\",\n                devices=gpu_index,\n                logger=self._logger,\n                enable_checkpointing=False,\n                enable_model_summary=False,\n                # deterministic=True\n            )\n        else:\n            logging_training.info(\"Creating trainer with accelerator CPU\")\n            self._trainer = Trainer(\n                callbacks=[ModelSummary(max_depth=1), NebulaProgressBar()],\n                max_epochs=self.epochs,\n                accelerator=\"cpu\",\n                devices=\"auto\",\n                logger=self._logger,\n                enable_checkpointing=False,\n                enable_model_summary=False,\n                # deterministic=True\n            )\n        logging_training.info(f\"Trainer strategy: {self._trainer.strategy}\")\n\n    def validate_neighbour_model(self, neighbour_model_param):\n        avg_loss = 0\n        running_loss = 0\n        bootstrap_dataloader = self.datamodule.bootstrap_dataloader()\n        num_samples = 0\n        neighbour_model = copy.deepcopy(self.model)\n        neighbour_model.load_state_dict(neighbour_model_param)\n\n        # enable evaluation mode, prevent memory leaks.\n        # no need to switch back to training since model is not further used.\n        if torch.cuda.is_available():\n            neighbour_model = neighbour_model.to(\"cuda\")\n        neighbour_model.eval()\n\n        # bootstrap_dataloader = bootstrap_dataloader.to('cuda')\n        with torch.no_grad():\n            for inputs, labels in bootstrap_dataloader:\n                if torch.cuda.is_available():\n                    inputs = inputs.to(\"cuda\")\n                    labels = labels.to(\"cuda\")\n                outputs = neighbour_model(inputs)\n                loss = F.cross_entropy(outputs, labels)\n                running_loss += loss.item()\n                num_samples += inputs.size(0)\n\n        avg_loss = running_loss / len(bootstrap_dataloader)\n        logging_training.info(f\"Computed neighbor loss over {num_samples} data samples\")\n        return avg_loss\n\n    def get_hash_model(self):\n        \"\"\"\n        Returns:\n            str: SHA256 hash of model parameters\n        \"\"\"\n        return hashlib.sha256(self.serialize_model(self.model)).hexdigest()\n\n    def set_epochs(self, epochs):\n        self.epochs = epochs\n\n    def set_current_round(self, round):\n        logging.info(f\"Update | current round = {round}\")\n        self.round = round\n        self.model.set_updated_round(round)\n\n    def get_current_loss(self):\n        return self.model.get_loss()\n\n    def serialize_model(self, model):\n        # From https://pytorch.org/docs/stable/notes/serialization.html\n        try:\n            buffer = io.BytesIO()\n            with gzip.GzipFile(fileobj=buffer, mode=\"wb\") as f:\n                torch.save(model, f, pickle_protocol=pickle.HIGHEST_PROTOCOL)\n            serialized_data = buffer.getvalue()\n            buffer.close()\n            del buffer\n            return serialized_data\n        except Exception as e:\n            raise ParameterSerializeError(\"Error serializing model\") from e\n\n    def deserialize_model(self, data):\n        # From https://pytorch.org/docs/stable/notes/serialization.html\n        try:\n            buffer = io.BytesIO(data)\n            with gzip.GzipFile(fileobj=buffer, mode=\"rb\") as f:\n                params_dict = torch.load(f)\n            buffer.close()\n            del buffer\n            return OrderedDict(params_dict)\n        except Exception as e:\n            raise ParameterDeserializeError(\"Error decoding parameters\") from e\n\n    def set_model_parameters(self, params, initialize=False):\n        try:\n            self.model.load_state_dict(params)\n        except Exception as e:\n            raise ParameterSettingError(\"Error setting parameters\") from e\n\n    def get_model_parameters(self, bytes=False, initialize=False):\n        if bytes:\n            return self.serialize_model(self.model.state_dict())\n        return self.model.state_dict()\n\n    async def train(self):\n        try:\n            self.create_trainer()\n            logging.info(f\"{'=' * 10} [Training] Started (check training logs for progress) {'=' * 10}\")\n            await asyncio.to_thread(self._train_sync)\n            logging.info(f\"{'=' * 10} [Training] Finished (check training logs for progress) {'=' * 10}\")\n        except Exception as e:\n            logging_training.error(f\"Error training model: {e}\")\n            logging_training.error(traceback.format_exc())\n\n    def _train_sync(self):\n        try:\n            self._trainer.fit(self.model, self.datamodule)\n        except Exception as e:\n            logging_training.error(f\"Error in _train_sync: {e}\")\n            tb = traceback.format_exc()\n            logging_training.error(f\"Traceback: {tb}\")\n            # If \"raise\", the exception will be managed by the main thread\n\n    async def test(self):\n        try:\n            self.create_trainer()\n            logging.info(f\"{'=' * 10} [Testing] Started (check training logs for progress) {'=' * 10}\")\n            loss, accuracy = await asyncio.to_thread(self._test_sync)\n            logging.info(f\"{'=' * 10} [Testing] Finished (check training logs for progress) {'=' * 10}\")\n            tme = TestMetricsEvent(loss, accuracy)\n            await EventManager.get_instance().publish_addonevent(tme)\n        except Exception as e:\n            logging_training.error(f\"Error testing model: {e}\")\n            logging_training.error(traceback.format_exc())\n\n    def _test_sync(self):\n        try:\n            self._trainer.test(self.model, self.datamodule, verbose=True)\n            metrics = self._trainer.callback_metrics\n            loss = metrics.get('val_loss/dataloader_idx_0', None).item()\n            accuracy = metrics.get('val_accuracy/dataloader_idx_0', None).item()\n            return loss, accuracy\n        except Exception as e:\n            logging_training.error(f\"Error in _test_sync: {e}\")\n            tb = traceback.format_exc()\n            logging_training.error(f\"Traceback: {tb}\")\n            # If \"raise\", the exception will be managed by the main thread\n            return None, None\n\n    def cleanup(self):\n        if self._trainer is not None:\n            self._trainer._teardown()\n            del self._trainer\n        if self.datamodule is not None:\n            self.datamodule.teardown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def get_model_weight(self):\n        weight = self.datamodule.model_weight\n        if weight is None:\n            raise ValueError(\"Model weight not set. Please call setup('fit') before requesting model weight.\")\n        return weight\n\n    def on_round_start(self):\n        self.datamodule.setup()\n        self._logger.log_data({\"A-Round\": self.round})\n        # self.reporter.enqueue_data(\"Round\", self.round)\n\n    def on_round_end(self):\n        self._logger.global_step = self._logger.global_step + self._logger.local_step\n        self._logger.local_step = 0\n        self.round += 1\n        self.model.on_round_end()\n        logging.info(\"Flushing memory cache at the end of round...\")\n        self.cleanup()\n\n    def on_learning_cycle_end(self):\n        self._logger.log_data({\"A-Round\": self.round})\n        # self.reporter.enqueue_data(\"Round\", self.round)\n\n    def update_model_learning_rate(self, new_lr):\n        self.model.modify_learning_rate(new_lr)\n\n    def show_current_learning_rate(self):\n        self.model.show_current_learning_rate()\n</code></pre>"},{"location":"api/core/training/lightning/#nebula.core.training.lightning.Lightning.get_hash_model","title":"<code>get_hash_model()</code>","text":"<p>Returns:</p> Name Type Description <code>str</code> <p>SHA256 hash of model parameters</p> Source code in <code>nebula/core/training/lightning.py</code> <pre><code>def get_hash_model(self):\n    \"\"\"\n    Returns:\n        str: SHA256 hash of model parameters\n    \"\"\"\n    return hashlib.sha256(self.serialize_model(self.model)).hexdigest()\n</code></pre>"},{"location":"api/core/training/lightning/#nebula.core.training.lightning.NebulaProgressBar","title":"<code>NebulaProgressBar</code>","text":"<p>               Bases: <code>ProgressBar</code></p> <p>Nebula progress bar for training. Logs the percentage of completion of the training process using logging.</p> Source code in <code>nebula/core/training/lightning.py</code> <pre><code>class NebulaProgressBar(ProgressBar):\n    \"\"\"Nebula progress bar for training.\n    Logs the percentage of completion of the training process using logging.\n    \"\"\"\n\n    def __init__(self, log_every_n_steps=100):\n        super().__init__()\n        self.enable = True\n        self.log_every_n_steps = log_every_n_steps\n\n    def enable(self):\n        \"\"\"Enable progress bar logging.\"\"\"\n        self.enable = True\n\n    def disable(self):\n        \"\"\"Disable the progress bar logging.\"\"\"\n        self.enable = False\n\n    def on_train_epoch_start(self, trainer, pl_module):\n        \"\"\"Called when the training epoch starts.\"\"\"\n        super().on_train_epoch_start(trainer, pl_module)\n        if self.enable:\n            logging_training.info(f\"Starting Epoch {trainer.current_epoch}\")\n\n    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n        \"\"\"Called at the end of each training batch.\"\"\"\n        super().on_train_batch_end(trainer, pl_module, outputs, batch, batch_idx)\n        if self.enable:\n            if (batch_idx + 1) % self.log_every_n_steps == 0 or (batch_idx + 1) == self.total_train_batches:\n                # Calculate percentage complete for the current epoch\n                percent = ((batch_idx + 1) / self.total_train_batches) * 100  # +1 to count current batch\n                logging_training.info(f\"Epoch {trainer.current_epoch} - {percent:.01f}% complete\")\n\n    def on_train_epoch_end(self, trainer, pl_module):\n        \"\"\"Called at the end of the training epoch.\"\"\"\n        super().on_train_epoch_end(trainer, pl_module)\n        if self.enable:\n            logging_training.info(f\"Epoch {trainer.current_epoch} finished\")\n\n    def on_validation_epoch_start(self, trainer, pl_module):\n        super().on_validation_epoch_start(trainer, pl_module)\n        if self.enable:\n            logging_training.info(f\"Starting validation for Epoch {trainer.current_epoch}\")\n\n    def on_validation_epoch_end(self, trainer, pl_module):\n        super().on_validation_epoch_end(trainer, pl_module)\n        if self.enable:\n            logging_training.info(f\"Validation for Epoch {trainer.current_epoch} finished\")\n\n    def on_test_batch_start(self, trainer, pl_module, batch, batch_idx, dataloader_idx):\n        super().on_test_batch_start(trainer, pl_module, batch, batch_idx, dataloader_idx)\n        if not self.has_dataloader_changed(dataloader_idx):\n            return\n\n    def on_test_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n        \"\"\"Called at the end of each test batch.\"\"\"\n        super().on_test_batch_end(trainer, pl_module, outputs, batch, batch_idx, dataloader_idx)\n        if self.enable:\n            total_batches = self.total_test_batches_current_dataloader\n            if total_batches == 0:\n                logging_training.warning(\n                    f\"Total test batches is 0 for dataloader {dataloader_idx}, cannot compute progress.\"\n                )\n                return\n\n            if (batch_idx + 1) % self.log_every_n_steps == 0 or (batch_idx + 1) == total_batches:\n                percent = ((batch_idx + 1) / total_batches) * 100  # +1 to count the current batch\n                logging_training.info(\n                    f\"Test Epoch {trainer.current_epoch}, Dataloader {dataloader_idx} - {percent:.01f}% complete\"\n                )\n\n    def on_test_epoch_start(self, trainer, pl_module):\n        super().on_test_epoch_start(trainer, pl_module)\n        if self.enable:\n            logging_training.info(f\"Starting testing for Epoch {trainer.current_epoch}\")\n\n    def on_test_epoch_end(self, trainer, pl_module):\n        super().on_test_epoch_end(trainer, pl_module)\n        if self.enable:\n            logging_training.info(f\"Testing for Epoch {trainer.current_epoch} finished\")\n</code></pre>"},{"location":"api/core/training/lightning/#nebula.core.training.lightning.NebulaProgressBar.disable","title":"<code>disable()</code>","text":"<p>Disable the progress bar logging.</p> Source code in <code>nebula/core/training/lightning.py</code> <pre><code>def disable(self):\n    \"\"\"Disable the progress bar logging.\"\"\"\n    self.enable = False\n</code></pre>"},{"location":"api/core/training/lightning/#nebula.core.training.lightning.NebulaProgressBar.enable","title":"<code>enable()</code>","text":"<p>Enable progress bar logging.</p> Source code in <code>nebula/core/training/lightning.py</code> <pre><code>def enable(self):\n    \"\"\"Enable progress bar logging.\"\"\"\n    self.enable = True\n</code></pre>"},{"location":"api/core/training/lightning/#nebula.core.training.lightning.NebulaProgressBar.on_test_batch_end","title":"<code>on_test_batch_end(trainer, pl_module, outputs, batch, batch_idx, dataloader_idx)</code>","text":"<p>Called at the end of each test batch.</p> Source code in <code>nebula/core/training/lightning.py</code> <pre><code>def on_test_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n    \"\"\"Called at the end of each test batch.\"\"\"\n    super().on_test_batch_end(trainer, pl_module, outputs, batch, batch_idx, dataloader_idx)\n    if self.enable:\n        total_batches = self.total_test_batches_current_dataloader\n        if total_batches == 0:\n            logging_training.warning(\n                f\"Total test batches is 0 for dataloader {dataloader_idx}, cannot compute progress.\"\n            )\n            return\n\n        if (batch_idx + 1) % self.log_every_n_steps == 0 or (batch_idx + 1) == total_batches:\n            percent = ((batch_idx + 1) / total_batches) * 100  # +1 to count the current batch\n            logging_training.info(\n                f\"Test Epoch {trainer.current_epoch}, Dataloader {dataloader_idx} - {percent:.01f}% complete\"\n            )\n</code></pre>"},{"location":"api/core/training/lightning/#nebula.core.training.lightning.NebulaProgressBar.on_train_batch_end","title":"<code>on_train_batch_end(trainer, pl_module, outputs, batch, batch_idx)</code>","text":"<p>Called at the end of each training batch.</p> Source code in <code>nebula/core/training/lightning.py</code> <pre><code>def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n    \"\"\"Called at the end of each training batch.\"\"\"\n    super().on_train_batch_end(trainer, pl_module, outputs, batch, batch_idx)\n    if self.enable:\n        if (batch_idx + 1) % self.log_every_n_steps == 0 or (batch_idx + 1) == self.total_train_batches:\n            # Calculate percentage complete for the current epoch\n            percent = ((batch_idx + 1) / self.total_train_batches) * 100  # +1 to count current batch\n            logging_training.info(f\"Epoch {trainer.current_epoch} - {percent:.01f}% complete\")\n</code></pre>"},{"location":"api/core/training/lightning/#nebula.core.training.lightning.NebulaProgressBar.on_train_epoch_end","title":"<code>on_train_epoch_end(trainer, pl_module)</code>","text":"<p>Called at the end of the training epoch.</p> Source code in <code>nebula/core/training/lightning.py</code> <pre><code>def on_train_epoch_end(self, trainer, pl_module):\n    \"\"\"Called at the end of the training epoch.\"\"\"\n    super().on_train_epoch_end(trainer, pl_module)\n    if self.enable:\n        logging_training.info(f\"Epoch {trainer.current_epoch} finished\")\n</code></pre>"},{"location":"api/core/training/lightning/#nebula.core.training.lightning.NebulaProgressBar.on_train_epoch_start","title":"<code>on_train_epoch_start(trainer, pl_module)</code>","text":"<p>Called when the training epoch starts.</p> Source code in <code>nebula/core/training/lightning.py</code> <pre><code>def on_train_epoch_start(self, trainer, pl_module):\n    \"\"\"Called when the training epoch starts.\"\"\"\n    super().on_train_epoch_start(trainer, pl_module)\n    if self.enable:\n        logging_training.info(f\"Starting Epoch {trainer.current_epoch}\")\n</code></pre>"},{"location":"api/core/training/lightning/#nebula.core.training.lightning.ParameterDeserializeError","title":"<code>ParameterDeserializeError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception for errors setting model parameters.</p> Source code in <code>nebula/core/training/lightning.py</code> <pre><code>class ParameterDeserializeError(Exception):\n    \"\"\"Custom exception for errors setting model parameters.\"\"\"\n</code></pre>"},{"location":"api/core/training/lightning/#nebula.core.training.lightning.ParameterSerializeError","title":"<code>ParameterSerializeError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception for errors setting model parameters.</p> Source code in <code>nebula/core/training/lightning.py</code> <pre><code>class ParameterSerializeError(Exception):\n    \"\"\"Custom exception for errors setting model parameters.\"\"\"\n</code></pre>"},{"location":"api/core/training/lightning/#nebula.core.training.lightning.ParameterSettingError","title":"<code>ParameterSettingError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception for errors setting model parameters.</p> Source code in <code>nebula/core/training/lightning.py</code> <pre><code>class ParameterSettingError(Exception):\n    \"\"\"Custom exception for errors setting model parameters.\"\"\"\n</code></pre>"},{"location":"api/core/training/scikit/","title":"Documentation for Scikit Module","text":""},{"location":"api/core/training/siamese/","title":"Documentation for Siamese Module","text":""},{"location":"api/core/training/siamese/#nebula.core.training.siamese.Siamese","title":"<code>Siamese</code>","text":"Source code in <code>nebula/core/training/siamese.py</code> <pre><code>class Siamese:\n    def __init__(self, model, data, config=None, logger=None):\n        # self.model = torch.compile(model, mode=\"reduce-overhead\")\n        self.model = model\n        self.data = data\n        self.config = config\n        self.logger = logger\n        self.__trainer = None\n        self.epochs = 1\n        logging.getLogger(\"lightning.pytorch\").setLevel(logging.INFO)\n        self.round = 0\n        enable_deterministic(seed=self.config.participant[\"scenario_args\"][\"random_seed\"])\n        self.logger.log_data({\"Round\": self.round}, step=self.logger.global_step)\n\n    @property\n    def logger(self):\n        return self._logger\n\n    def get_round(self):\n        return self.round\n\n    def set_model(self, model):\n        self.model = model\n\n    def set_data(self, data):\n        self.data = data\n\n    def create_trainer(self):\n        logging.info(\n            \"[Trainer] Creating trainer with accelerator: {}\".format(\n                self.config.participant[\"device_args\"][\"accelerator\"]\n            )\n        )\n        progress_bar = RichProgressBar(\n            theme=RichProgressBarTheme(\n                description=\"green_yellow\",\n                progress_bar=\"green1\",\n                progress_bar_finished=\"green1\",\n                progress_bar_pulse=\"#6206E0\",\n                batch_progress=\"green_yellow\",\n                time=\"grey82\",\n                processing_speed=\"grey82\",\n                metrics=\"grey82\",\n            ),\n            leave=True,\n        )\n        if self.config.participant[\"device_args\"][\"accelerator\"] == \"gpu\":\n            # NEBULA uses 2 GPUs (max) to distribute the nodes.\n            if self.config.participant[\"device_args\"][\"devices\"] &gt; 1:\n                # If you have more than 2 GPUs, you should specify which ones to use.\n                gpu_id = ([1] if self.config.participant[\"device_args\"][\"idx\"] % 2 == 0 else [2],)\n            else:\n                # If there is only one GPU, it will be used.\n                gpu_id = [1]\n\n            self.__trainer = Trainer(\n                callbacks=[RichModelSummary(max_depth=1), progress_bar],\n                max_epochs=self.epochs,\n                accelerator=self.config.participant[\"device_args\"][\"accelerator\"],\n                devices=gpu_id,\n                logger=self.logger,\n                log_every_n_steps=50,\n                enable_checkpointing=False,\n                enable_model_summary=False,\n                enable_progress_bar=True,\n                # deterministic=True\n            )\n        else:\n            # NEBULA uses only CPU to distribute the nodes\n            self.__trainer = Trainer(\n                callbacks=[RichModelSummary(max_depth=1), progress_bar],\n                max_epochs=self.epochs,\n                accelerator=self.config.participant[\"device_args\"][\"accelerator\"],\n                devices=\"auto\",\n                logger=self.logger,\n                log_every_n_steps=50,\n                enable_checkpointing=False,\n                enable_model_summary=False,\n                enable_progress_bar=True,\n                # deterministic=True\n            )\n\n    def get_global_model_parameters(self):\n        return self.model.get_global_model_parameters()\n\n    def set_parameter_second_aggregation(self, params):\n        try:\n            logging.info(\"Setting parameters in second aggregation...\")\n            self.model.load_state_dict(params)\n        except:\n            raise Exception(\"Error setting parameters\")\n\n    def get_model_parameters(self, bytes=False):\n        if bytes:\n            return self.serialize_model(self.model.state_dict())\n        else:\n            return self.model.state_dict()\n\n    def get_hash_model(self):\n        \"\"\"\n        Returns:\n            str: SHA256 hash of model parameters\n        \"\"\"\n        return hashlib.sha256(self.serialize_model()).hexdigest()\n\n    def set_epochs(self, epochs):\n        self.epochs = epochs\n\n    ####\n    # Model parameters serialization/deserialization\n    # From https://pytorch.org/docs/stable/notes/serialization.html\n    ####\n    def serialize_model(self, model):\n        try:\n            buffer = io.BytesIO()\n            # with gzip.GzipFile(fileobj=buffer, mode='wb') as f:\n            #    torch.save(params, f)\n            torch.save(model, buffer)\n            return buffer.getvalue()\n        except:\n            raise Exception(\"Error serializing model\")\n\n    def deserialize_model(self, data):\n        try:\n            buffer = io.BytesIO(data)\n            # with gzip.GzipFile(fileobj=buffer, mode='rb') as f:\n            #    params_dict = torch.load(f, map_location='cpu')\n            params_dict = torch.load(buffer, map_location=\"cpu\")\n            return OrderedDict(params_dict)\n        except:\n            raise Exception(\"Error decoding parameters\")\n\n    def set_model_parameters(self, params, initialize=False):\n        try:\n            if initialize:\n                self.model.load_state_dict(params)\n                self.model.global_load_state_dict(params)\n                self.model.historical_load_state_dict(params)\n            else:\n                # First aggregation\n                self.model.global_load_state_dict(params)\n        except:\n            raise Exception(\"Error setting parameters\")\n\n    def train(self):\n        try:\n            self.create_trainer()\n            # torch.autograd.set_detect_anomaly(True)\n            # TODO: It is necessary to train only the local model, save the history of the previous model and then load it, the global model is the aggregation of all the models.\n            self.__trainer.fit(self.model, self.data)\n            # Save local model as historical model (previous round)\n            # It will be compared the next round during training local model (constrantive loss)\n            # When aggregation in global model (first) and aggregation with similarities and weights (second), the historical model keeps inmutable\n            logging.info(\"Saving historical model...\")\n            self.model.save_historical_model()\n        except Exception as e:\n            logging.exception(f\"Error training model: {e}\")\n            logging.exception(traceback.format_exc())\n\n    def test(self):\n        try:\n            self.create_trainer()\n            self.__trainer.test(self.model, self.data, verbose=True)\n        except Exception as e:\n            logging.exception(f\"Error testing model: {e}\")\n            logging.exception(traceback.format_exc())\n\n    def get_model_weight(self):\n        return (\n            len(self.data.train_dataloader().dataset),\n            len(self.data.test_dataloader().dataset),\n        )\n\n    def finalize_round(self):\n        self.logger.global_step = self.logger.global_step + self.logger.local_step\n        self.logger.local_step = 0\n        self.round += 1\n        self.logger.log_data({\"Round\": self.round}, step=self.logger.global_step)\n        pass\n</code></pre>"},{"location":"api/core/training/siamese/#nebula.core.training.siamese.Siamese.get_hash_model","title":"<code>get_hash_model()</code>","text":"<p>Returns:</p> Name Type Description <code>str</code> <p>SHA256 hash of model parameters</p> Source code in <code>nebula/core/training/siamese.py</code> <pre><code>def get_hash_model(self):\n    \"\"\"\n    Returns:\n        str: SHA256 hash of model parameters\n    \"\"\"\n    return hashlib.sha256(self.serialize_model()).hexdigest()\n</code></pre>"},{"location":"api/frontend/","title":"Documentation for Frontend Module","text":""},{"location":"api/frontend/app/","title":"Documentation for App Module","text":""},{"location":"api/frontend/app/#nebula.frontend.app.ConnectionManager","title":"<code>ConnectionManager</code>","text":"<p>Manages WebSocket client connections, broadcasts messages to all connected clients, and retains a history of exchanged messages.</p> <p>Attributes:</p> Name Type Description <code>historic_messages</code> <code>dict[str, dict]</code> <p>Stores each received or broadcast message keyed by timestamp (formatted as \"%Y-%m-%d %H:%M:%S\").</p> <code>active_connections</code> <code>list[WebSocket]</code> <p>List of currently open WebSocket connections.</p> <p>Methods:</p> Name Description <code>async connect</code> <p>WebSocket): Accepts a new WebSocket connection, registers it, and broadcasts a control message indicating the new client count.</p> <code>disconnect</code> <p>WebSocket): Removes the specified WebSocket from the active connections list if present.</p> <code>add_message</code> <p>str): Parses the incoming JSON-formatted message string, timestamps it, and adds it to historic_messages.</p> <code>async send_personal_message</code> <p>str, websocket: WebSocket): Sends a text message to a single WebSocket; on connection closure, cleans up the connection.</p> <code>async broadcast</code> <p>str): Logs the message via add_message, then iterates through active_connections to send the message to all clients; collects and removes any connections that have been closed or error out, logging exceptions as needed.</p> <code>get_historic</code> <p>Returns the full history of timestamped messages.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>class ConnectionManager:\n    \"\"\"\n    Manages WebSocket client connections, broadcasts messages to all connected clients, and retains a history of exchanged messages.\n\n    Attributes:\n        historic_messages (dict[str, dict]): Stores each received or broadcast message keyed by timestamp (formatted as \"%Y-%m-%d %H:%M:%S\").\n        active_connections (list[WebSocket]): List of currently open WebSocket connections.\n\n    Methods:\n        async connect(websocket: WebSocket):\n            Accepts a new WebSocket connection, registers it, and broadcasts a control message indicating the new client count.\n        disconnect(websocket: WebSocket):\n            Removes the specified WebSocket from the active connections list if present.\n        add_message(message: str):\n            Parses the incoming JSON-formatted message string, timestamps it, and adds it to historic_messages.\n        async send_personal_message(message: str, websocket: WebSocket):\n            Sends a text message to a single WebSocket; on connection closure, cleans up the connection.\n        async broadcast(message: str):\n            Logs the message via add_message, then iterates through active_connections to send the message to all clients;\n            collects and removes any connections that have been closed or error out, logging exceptions as needed.\n        get_historic() -&gt; dict[str, dict]:\n            Returns the full history of timestamped messages.\n    \"\"\"\n\n    def __init__(self):\n        self.historic_messages = {}\n        self.active_connections: list[WebSocket] = []\n\n    async def connect(self, websocket: WebSocket):\n        await websocket.accept()\n        self.active_connections.append(websocket)\n        message = {\n            \"type\": \"control\",\n            \"message\": f\"Client #{len(self.active_connections)} connected\",\n        }\n        try:\n            await self.broadcast(json.dumps(message))\n        except:\n            pass\n\n    def disconnect(self, websocket: WebSocket):\n        if websocket in self.active_connections:\n            self.active_connections.remove(websocket)\n\n    def add_message(self, message):\n        current_timestamp = datetime.datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d %H:%M:%S\")\n        self.historic_messages.update({current_timestamp: json.loads(message)})\n\n    async def send_personal_message(self, message: str, websocket: WebSocket):\n        try:\n            await websocket.send_text(message)\n        except RuntimeError:\n            # Connection was closed, remove it from active connections\n            self.disconnect(websocket)\n\n    async def broadcast(self, message: str):\n        self.add_message(message)\n        disconnected_websockets = []\n\n        for connection in self.active_connections:\n            try:\n                await connection.send_text(message)\n            except RuntimeError:\n                # Mark connection for removal\n                disconnected_websockets.append(connection)\n            except Exception as e:\n                logging.exception(f\"Error broadcasting message: {e}\")\n                disconnected_websockets.append(connection)\n\n        # Remove disconnected websockets\n        for websocket in disconnected_websockets:\n            self.disconnect(websocket)\n\n    def get_historic(self):\n        return self.historic_messages\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.Settings","title":"<code>Settings</code>","text":"<p>Configuration settings for the Nebula application, loaded from environment variables with sensible defaults.</p> <p>Attributes:</p> Name Type Description <code>controller_host</code> <code>str</code> <p>Hostname or IP address of the Nebula controller service.</p> <code>controller_port</code> <code>int</code> <p>Port on which the Nebula controller listens (default: 5050).</p> <code>resources_threshold</code> <code>float</code> <p>Threshold for resource usage alerts (default: 0.0).</p> <code>port</code> <code>int</code> <p>Port for the Nebula frontend service (default: 6060).</p> <code>production</code> <code>bool</code> <p>Whether the application is running in production mode.</p> <code>advanced_analytics</code> <code>bool</code> <p>Whether advanced analytics features are enabled.</p> <code>host_platform</code> <code>str</code> <p>Underlying host operating platform (e.g., 'unix').</p> <code>log_dir</code> <code>str</code> <p>Directory path where application logs are stored.</p> <code>config_dir</code> <code>str</code> <p>Directory path for general configuration files.</p> <code>cert_dir</code> <code>str</code> <p>Directory path for SSL/TLS certificates.</p> <code>root_host_path</code> <code>str</code> <p>Root path on the host for volume mounting.</p> <code>config_frontend_dir</code> <code>str</code> <p>Subdirectory for frontend-specific configuration (default: 'config').</p> <code>env_file</code> <code>str</code> <p>Path to the environment file to load additional variables (default: '.env').</p> <code>statistics_port</code> <code>int</code> <p>Port for the statistics/metrics endpoint (default: 8080).</p> <code>PERMANENT_SESSION_LIFETIME</code> <code>timedelta</code> <p>Duration for session permanence (default: 60 minutes).</p> <code>templates_dir</code> <code>str</code> <p>Directory name containing template files (default: 'templates').</p> <code>frontend_log</code> <code>str</code> <p>File path for the frontend log output.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>class Settings:\n    \"\"\"\n    Configuration settings for the Nebula application, loaded from environment variables with sensible defaults.\n\n    Attributes:\n        controller_host (str): Hostname or IP address of the Nebula controller service.\n        controller_port (int): Port on which the Nebula controller listens (default: 5050).\n        resources_threshold (float): Threshold for resource usage alerts (default: 0.0).\n        port (int): Port for the Nebula frontend service (default: 6060).\n        production (bool): Whether the application is running in production mode.\n        advanced_analytics (bool): Whether advanced analytics features are enabled.\n        host_platform (str): Underlying host operating platform (e.g., 'unix').\n        log_dir (str): Directory path where application logs are stored.\n        config_dir (str): Directory path for general configuration files.\n        cert_dir (str): Directory path for SSL/TLS certificates.\n        root_host_path (str): Root path on the host for volume mounting.\n        config_frontend_dir (str): Subdirectory for frontend-specific configuration (default: 'config').\n        env_file (str): Path to the environment file to load additional variables (default: '.env').\n        statistics_port (int): Port for the statistics/metrics endpoint (default: 8080).\n        PERMANENT_SESSION_LIFETIME (datetime.timedelta): Duration for session permanence (default: 60 minutes).\n        templates_dir (str): Directory name containing template files (default: 'templates').\n        frontend_log (str): File path for the frontend log output.\n    \"\"\"\n\n    controller_host: str = os.environ.get(\"NEBULA_CONTROLLER_HOST\")\n    controller_port: int = os.environ.get(\"NEBULA_CONTROLLER_PORT\", 5050)\n    resources_threshold: float = 0.0\n    port: int = os.environ.get(\"NEBULA_FRONTEND_PORT\", 6060)\n    production: bool = os.environ.get(\"NEBULA_PRODUCTION\", \"False\") == \"True\"\n    advanced_analytics: bool = os.environ.get(\"NEBULA_ADVANCED_ANALYTICS\", \"False\") == \"True\"\n    host_platform: str = os.environ.get(\"NEBULA_HOST_PLATFORM\", \"unix\")\n    log_dir: str = os.environ.get(\"NEBULA_LOGS_DIR\")\n    config_dir: str = os.environ.get(\"NEBULA_CONFIG_DIR\")\n    cert_dir: str = os.environ.get(\"NEBULA_CERTS_DIR\")\n    root_host_path: str = os.environ.get(\"NEBULA_ROOT_HOST\")\n    config_frontend_dir: str = os.environ.get(\"NEBULA_CONFIG_FRONTEND_DIR\", \"config\")\n    env_file: str = os.environ.get(\"NEBULA_ENV_PATH\", \".env\")\n    statistics_port: int = os.environ.get(\"NEBULA_STATISTICS_PORT\", 8080)\n    PERMANENT_SESSION_LIFETIME: datetime.timedelta = datetime.timedelta(minutes=60)\n    templates_dir: str = \"templates\"\n    frontend_log: str = os.environ.get(\"NEBULA_FRONTEND_LOG\", \"/nebula/app/logs/frontend.log\")\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.UserData","title":"<code>UserData</code>","text":"<p>Holds runtime state and synchronization events for user-specific scenario execution.</p> <p>Attributes:</p> Name Type Description <code>nodes_registration</code> <code>dict</code> <p>Mapping of node identifiers to their registration data.</p> <code>scenarios_list</code> <code>list</code> <p>Ordered list of scenario identifiers or objects to be executed.</p> <code>scenarios_list_length</code> <code>int</code> <p>Total number of scenarios in scenarios_list.</p> <code>scenarios_finished</code> <code>int</code> <p>Count of scenarios that have completed execution.</p> <code>nodes_finished</code> <code>list</code> <p>List of node identifiers that have finished processing.</p> <code>stop_all_scenarios_event</code> <code>Event</code> <p>Event used to signal all scenarios should be halted.</p> <code>finish_scenario_event</code> <code>Event</code> <p>Event used to signal a single scenario has finished.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>class UserData:\n    \"\"\"\n    Holds runtime state and synchronization events for user-specific scenario execution.\n\n    Attributes:\n        nodes_registration (dict): Mapping of node identifiers to their registration data.\n        scenarios_list (list): Ordered list of scenario identifiers or objects to be executed.\n        scenarios_list_length (int): Total number of scenarios in scenarios_list.\n        scenarios_finished (int): Count of scenarios that have completed execution.\n        nodes_finished (list): List of node identifiers that have finished processing.\n        stop_all_scenarios_event (asyncio.Event): Event used to signal all scenarios should be halted.\n        finish_scenario_event (asyncio.Event): Event used to signal a single scenario has finished.\n    \"\"\"\n\n    def __init__(self):\n        self.nodes_registration = {}\n        self.scenarios_list = []\n        self.scenarios_list_length = 0\n        self.scenarios_finished = 0\n        self.nodes_finished = []\n        self.stop_all_scenarios_event = asyncio.Event()\n        self.finish_scenario_event = asyncio.Event()\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.add_global_context","title":"<code>add_global_context(request)</code>","text":"<p>Add global context variables for template rendering.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>The incoming request object.</p> required <p>Returns:</p> Type Description <p>dict[str, bool]: is_production: Flag indicating if the application is running in production mode.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>def add_global_context(request: Request):\n    \"\"\"\n    Add global context variables for template rendering.\n\n    Parameters:\n        request (Request): The incoming request object.\n\n    Returns:\n        dict[str, bool]:\n            is_production: Flag indicating if the application is running in production mode.\n    \"\"\"\n    return {\n        \"is_production\": settings.production,\n    }\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.add_user","title":"<code>add_user(user, password, role)</code>  <code>async</code>","text":"<p>Create a new user via the controller endpoint.</p> <p>Parameters: - user (str): The username for the new user. - password (str): The password for the new user. - role (str): The role assigned to the new user.</p> <p>Returns: - None</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>async def add_user(user, password, role):\n    \"\"\"\n    Create a new user via the controller endpoint.\n\n    Parameters:\n    - user (str): The username for the new user.\n    - password (str): The password for the new user.\n    - role (str): The role assigned to the new user.\n\n    Returns:\n    - None\n    \"\"\"\n    url = f\"http://{settings.controller_host}:{settings.controller_port}/user/add\"\n    data = {\"user\": user, \"password\": password, \"role\": role}\n    await controller_post(url, data)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.assign_available_gpu","title":"<code>assign_available_gpu(scenario_data, role)</code>  <code>async</code>","text":"<p>Assign available GPU(s) or default to CPU for a scenario based on system resources and user role.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_data</code> <code>dict</code> <p>Scenario configuration dict to be updated with accelerator settings.</p> required <code>role</code> <code>str</code> <p>User role ('user', 'admin', or other).</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Updated scenario_data including 'accelerator' and 'gpu_id' fields.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>async def assign_available_gpu(scenario_data, role):\n    \"\"\"\n    Assign available GPU(s) or default to CPU for a scenario based on system resources and user role.\n\n    Parameters:\n        scenario_data (dict): Scenario configuration dict to be updated with accelerator settings.\n        role (str): User role ('user', 'admin', or other).\n\n    Returns:\n        dict: Updated scenario_data including 'accelerator' and 'gpu_id' fields.\n    \"\"\"\n    available_gpus = []\n\n    response = await get_available_gpus()\n    # Obtain available system_gpus\n    available_system_gpus = response.get(\"available_gpus\", None) if response is not None else None\n\n    if available_system_gpus:\n        running_scenarios = await get_running_scenarios(get_all=True)\n        # Obtain currently used gpus\n        if running_scenarios:\n            running_gpus = []\n            # Obtain associated gpus of the running scenarios\n            for scenario in running_scenarios:\n                scenario_gpus = json.loads(scenario[\"gpu_id\"])\n                # Obtain the list of gpus in use without duplicates\n                for gpu in scenario_gpus:\n                    if gpu not in running_gpus:\n                        running_gpus.append(gpu)\n\n            # Add available system gpus if they are not in use\n            for gpu in available_system_gpus:\n                if gpu not in running_gpus:\n                    available_gpus.append(gpu)\n        else:\n            available_gpus = available_system_gpus\n\n    # Assign gpus based in user role\n    if len(available_gpus) &gt; 0:\n        if role == \"user\":\n            scenario_data[\"accelerator\"] = \"gpu\"\n            scenario_data[\"gpu_id\"] = [available_gpus.pop()]\n        elif role == \"admin\":\n            scenario_data[\"accelerator\"] = \"gpu\"\n            scenario_data[\"gpu_id\"] = available_gpus\n        else:\n            scenario_data[\"accelerator\"] = \"cpu\"\n            scenario_data[\"gpu_id\"] = []\n    else:\n        scenario_data[\"accelerator\"] = \"cpu\"\n        scenario_data[\"gpu_id\"] = []\n\n    return scenario_data\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.check_enough_resources","title":"<code>check_enough_resources()</code>  <code>async</code>","text":"<p>Check if the host's memory usage is below the configured threshold.</p> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if sufficient resources are available (or threshold is 0.0), False otherwise.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>async def check_enough_resources():\n    \"\"\"\n    Check if the host's memory usage is below the configured threshold.\n\n    Returns:\n        bool: True if sufficient resources are available (or threshold is 0.0), False otherwise.\n    \"\"\"\n    resources = await get_host_resources()\n\n    mem_percent = resources.get(\"memory_percent\")\n\n    if settings.resources_threshold == 0.0:\n        return True\n\n    if mem_percent &gt;= settings.resources_threshold:\n        return False\n\n    return True\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.check_scenario_with_role","title":"<code>check_scenario_with_role(role, scenario_name)</code>  <code>async</code>","text":"<p>Check if a specific scenario is allowed for the session's role.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>dict</code> <p>Session data containing at least a 'role' key.</p> required <code>scenario_name</code> <code>str</code> <p>Name of the scenario to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the scenario is allowed for the role, False otherwise.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>If the underlying HTTP GET request fails.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>async def check_scenario_with_role(role, scenario_name):\n    \"\"\"\n    Check if a specific scenario is allowed for the session's role.\n\n    Parameters:\n        session (dict): Session data containing at least a 'role' key.\n        scenario_name (str): Name of the scenario to check.\n\n    Returns:\n        bool: True if the scenario is allowed for the role, False otherwise.\n\n    Raises:\n        HTTPException: If the underlying HTTP GET request fails.\n    \"\"\"\n    url = f\"http://{settings.controller_host}:{settings.controller_port}/scenarios/check/{role}/{scenario_name}\"\n    check_data = await controller_get(url)\n    return check_data.get(\"allowed\", False)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.controller_get","title":"<code>controller_get(url)</code>  <code>async</code>","text":"<p>Fetch JSON data from a remote controller endpoint via asynchronous HTTP GET.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The full URL of the controller API endpoint.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <p>Parsed JSON response when the HTTP status code is 200.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>If the response status is not 200, raises with the response status code and an error detail.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>async def controller_get(url):\n    \"\"\"\n    Fetch JSON data from a remote controller endpoint via asynchronous HTTP GET.\n\n    Parameters:\n        url (str): The full URL of the controller API endpoint.\n\n    Returns:\n        Any: Parsed JSON response when the HTTP status code is 200.\n\n    Raises:\n        HTTPException: If the response status is not 200, raises with the response status code and an error detail.\n    \"\"\"\n\n    async def _get():\n        async with aiohttp.ClientSession() as session:\n            async with session.get(url) as response:\n                if response.status == 200:\n                    return await response.json()\n                else:\n                    raise HTTPException(status_code=response.status, detail=\"Error fetching data\")\n\n    return await retry_with_backoff(_get)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.controller_post","title":"<code>controller_post(url, data=None)</code>  <code>async</code>","text":"<p>Asynchronously send a JSON payload via HTTP POST to a controller endpoint and parse the response.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The full URL of the controller API endpoint.</p> required <code>data</code> <code>Any</code> <p>JSON-serializable payload to include in the POST request (default: None).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Any</code> <p>Parsed JSON response when the HTTP status code is 200.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>If the response status is not 200, with the status code and an error detail.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>async def controller_post(url, data=None):\n    \"\"\"\n    Asynchronously send a JSON payload via HTTP POST to a controller endpoint and parse the response.\n\n    Parameters:\n        url (str): The full URL of the controller API endpoint.\n        data (Any, optional): JSON-serializable payload to include in the POST request (default: None).\n\n    Returns:\n        Any: Parsed JSON response when the HTTP status code is 200.\n\n    Raises:\n        HTTPException: If the response status is not 200, with the status code and an error detail.\n    \"\"\"\n\n    async def _post():\n        async with aiohttp.ClientSession() as session:\n            async with session.post(url, json=data) as response:\n                if response.status == 200:\n                    return await response.json()\n                else:\n                    detail = await response.text()\n                    raise HTTPException(status_code=response.status, detail=detail)\n\n    return await retry_with_backoff(_post)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.custom_http_exception_handler","title":"<code>custom_http_exception_handler(request, exc)</code>  <code>async</code>","text":"<p>Custom HTTP exception handler for Starlette applications.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>The incoming HTTP request object.</p> required <code>exc</code> <code>HTTPException</code> <p>The HTTP exception instance containing the status code to handle.</p> required Functionality <ul> <li>Builds a context dict with the request and its session.</li> <li>For specific HTTP status codes (401, 403, 404, 405, 413), returns a TemplateResponse rendering the corresponding error page and status.</li> <li>For all other status codes, returns a JSON response with the error details.</li> </ul> <p>Returns:</p> Name Type Description <code>Response</code> <p>Either a TemplateResponse for the matched error code or a JSON response with error details.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>@app.exception_handler(StarletteHTTPException)\nasync def custom_http_exception_handler(request: Request, exc: StarletteHTTPException):\n    \"\"\"\n    Custom HTTP exception handler for Starlette applications.\n\n    Parameters:\n        request (Request): The incoming HTTP request object.\n        exc (StarletteHTTPException): The HTTP exception instance containing the status code to handle.\n\n    Functionality:\n        - Builds a context dict with the request and its session.\n        - For specific HTTP status codes (401, 403, 404, 405, 413), returns a TemplateResponse rendering the corresponding error page and status.\n        - For all other status codes, returns a JSON response with the error details.\n\n    Returns:\n        Response: Either a TemplateResponse for the matched error code or a JSON response with error details.\n    \"\"\"\n    context = {\"request\": request, \"session\": request.session}\n    if exc.status_code == status.HTTP_401_UNAUTHORIZED:\n        return templates.TemplateResponse(\"401.html\", context, status_code=exc.status_code)\n    elif exc.status_code == status.HTTP_403_FORBIDDEN:\n        return templates.TemplateResponse(\"403.html\", context, status_code=exc.status_code)\n    elif exc.status_code == status.HTTP_404_NOT_FOUND:\n        return templates.TemplateResponse(\"404.html\", context, status_code=exc.status_code)\n    elif exc.status_code == status.HTTP_405_METHOD_NOT_ALLOWED:\n        return templates.TemplateResponse(\"405.html\", context, status_code=exc.status_code)\n    elif exc.status_code == status.HTTP_413_REQUEST_ENTITY_TOO_LARGE:\n        return templates.TemplateResponse(\"413.html\", context, status_code=exc.status_code)\n    return JSONResponse({\"detail\": exc.detail, \"status_code\": exc.status_code}, status_code=exc.status_code)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.datetimeformat","title":"<code>datetimeformat(value, format='%B %d, %Y %H:%M')</code>","text":"<p>Formats a datetime string into a specified output format.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>Input datetime string in \"%Y-%m-%d %H:%M:%S\" format.</p> required <code>format</code> <code>str</code> <p>Desired output datetime format (default: \"%B %d, %Y %H:%M\").</p> <code>'%B %d, %Y %H:%M'</code> <p>Returns:</p> Name Type Description <code>str</code> <p>The datetime string formatted according to the provided format.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>def datetimeformat(value, format=\"%B %d, %Y %H:%M\"):\n    \"\"\"\n    Formats a datetime string into a specified output format.\n\n    Parameters:\n        value (str): Input datetime string in \"%Y-%m-%d %H:%M:%S\" format.\n        format (str): Desired output datetime format (default: \"%B %d, %Y %H:%M\").\n\n    Returns:\n        str: The datetime string formatted according to the provided format.\n    \"\"\"\n    return datetime.datetime.strptime(value, \"%Y-%m-%d %H:%M:%S\").strftime(format)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.delete_user","title":"<code>delete_user(user)</code>  <code>async</code>","text":"<p>Delete an existing user via the controller endpoint.</p> <p>Parameters: - user (str): The username of the user to delete.</p> <p>Returns: - None</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>async def delete_user(user):\n    \"\"\"\n    Delete an existing user via the controller endpoint.\n\n    Parameters:\n    - user (str): The username of the user to delete.\n\n    Returns:\n    - None\n    \"\"\"\n    url = f\"http://{settings.controller_host}:{settings.controller_port}/user/delete\"\n    data = {\"user\": user}\n    await controller_post(url, data)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.deploy_scenario","title":"<code>deploy_scenario(scenario_data, role, user)</code>  <code>async</code>","text":"<p>Deploy a new scenario on the controller with the given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_data</code> <code>Any</code> <p>Data payload describing the scenario to run.</p> required <code>role</code> <code>str</code> <p>Role identifier for the scenario execution.</p> required <code>user</code> <code>str</code> <p>Username initiating the deployment.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <p>Parsed JSON response confirming scenario deployment.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>If the underlying HTTP POST request fails.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>async def deploy_scenario(scenario_data, role, user):\n    \"\"\"\n    Deploy a new scenario on the controller with the given parameters.\n\n    Parameters:\n        scenario_data (Any): Data payload describing the scenario to run.\n        role (str): Role identifier for the scenario execution.\n        user (str): Username initiating the deployment.\n\n    Returns:\n        Any: Parsed JSON response confirming scenario deployment.\n\n    Raises:\n        HTTPException: If the underlying HTTP POST request fails.\n    \"\"\"\n    url = f\"http://{settings.controller_host}:{settings.controller_port}/scenarios/run\"\n    data = {\"scenario_data\": scenario_data, \"role\": role, \"user\": user}\n    return await controller_post(url, data)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.frontend_discover_vpn","title":"<code>frontend_discover_vpn(session=Depends(get_session))</code>  <code>async</code>","text":"<p>Proxy endpoint that forwards a VPN-device discovery request from the frontend to the internal controller, then returns the JSON result back to the client. Requires the user to be logged in.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>@app.get(\"/platform/api/discover-vpn\", response_class=JSONResponse)\nasync def frontend_discover_vpn(session: dict = Depends(get_session)):\n    \"\"\"\n    Proxy endpoint that forwards a VPN-device discovery request from the frontend\n    to the internal controller, then returns the JSON result back to the client.\n    Requires the user to be logged in.\n    \"\"\"\n\n    # 1) Enforce authentication\n    if \"user\" not in session:\n        # If there's no user in session, return HTTP 401 Unauthorized\n        raise HTTPException(status_code=401, detail=\"Login required\")\n\n    # 2) Build the controller URL (using host/port from settings)\n    url = f\"http://{settings.controller_host}:{settings.controller_port}/discover-vpn\"\n\n    try:\n        # 3) Call the controller's /discover-vpn endpoint\n        data = await controller_get(url)\n\n        # 4) Return whatever JSON the controller gave us\n        return JSONResponse(content=data)\n\n    except HTTPException as e:\n        # 5) If the controller itself raised an HTTPException, propagate it as-is\n        raise e\n\n    except Exception as e:\n        # 6) For any other error, log it and return a generic 500 response\n        logging.exception(f\"Error proxying discover-vpn: {e}\")\n        raise HTTPException(status_code=500, detail=\"Error discovering VPN devices\")\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.get_available_gpus","title":"<code>get_available_gpus()</code>  <code>async</code>","text":"<p>Fetch the list of available GPUs from the controller service.</p> <p>Returns:</p> Name Type Description <code>Any</code> <p>Parsed JSON response containing available GPU information.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>If the underlying HTTP request fails.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>async def get_available_gpus():\n    \"\"\"\n    Fetch the list of available GPUs from the controller service.\n\n    Returns:\n        Any: Parsed JSON response containing available GPU information.\n\n    Raises:\n        HTTPException: If the underlying HTTP request fails.\n    \"\"\"\n    url = f\"http://{settings.controller_host}:{settings.controller_port}/available_gpus\"\n    return await controller_get(url)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.get_config_for_scenario","title":"<code>get_config_for_scenario(scenario_name)</code>  <code>async</code>","text":"<p>Load configuration for a specific scenario from the filesystem.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_name</code> <code>str</code> <p>Name of the scenario to load configuration for.</p> required <p>Returns:</p> Name Type Description <code>JSONResponse</code> <p>{\"status\": \"success\", \"config\": } if successful, or error message if file not found or invalid JSON. Source code in <code>nebula/frontend/app.py</code> <pre><code>@app.get(\"/platform/dashboard/{scenario_name}/config\")\nasync def get_config_for_scenario(scenario_name: str):\n    \"\"\"\n    Load configuration for a specific scenario from the filesystem.\n\n    Parameters:\n        scenario_name (str): Name of the scenario to load configuration for.\n\n    Returns:\n        JSONResponse: {\"status\": \"success\", \"config\": &lt;data&gt;} if successful, or error message if file not found or invalid JSON.\n    \"\"\"\n    json_path = os.path.join(os.environ.get(\"NEBULA_CONFIG_DIR\"), scenario_name, \"scenario.json\")\n\n    try:\n        with open(json_path) as file:\n            scenarios_data = json.load(file)\n\n        if scenarios_data:\n            return JSONResponse({\"status\": \"success\", \"config\": scenarios_data})\n        else:\n            return JSONResponse({\"status\": \"error\", \"message\": \"Configuration not found for the specified scenario\"})\n\n    except FileNotFoundError:\n        return JSONResponse({\"status\": \"error\", \"message\": \"scenario.json file not found\"})\n    except json.JSONDecodeError:\n        return JSONResponse({\"status\": \"error\", \"message\": \"Error decoding JSON file\"})\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.get_host_resources","title":"<code>get_host_resources()</code>  <code>async</code>","text":"<p>Retrieve host resource usage data from the controller endpoint.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>Parsed JSON resource metrics on success, or {'error': } on parse failure. <code>None</code> <p>If the HTTP response status is not 200.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>async def get_host_resources():\n    \"\"\"\n    Retrieve host resource usage data from the controller endpoint.\n\n    Returns:\n        dict: Parsed JSON resource metrics on success, or {'error': &lt;message&gt;} on parse failure.\n        None: If the HTTP response status is not 200.\n    \"\"\"\n    url = f\"http://{settings.controller_host}:{settings.controller_port}/resources\"\n\n    async def _get_resources():\n        async with aiohttp.ClientSession() as session:\n            async with session.get(url) as response:\n                if response.status == 200:\n                    try:\n                        return await response.json()\n                    except Exception as e:\n                        return {\"error\": f\"Failed to parse JSON: {e}\"}\n                else:\n                    return None\n\n    return await retry_with_backoff(_get_resources)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.get_least_memory_gpu","title":"<code>get_least_memory_gpu()</code>  <code>async</code>","text":"<p>Fetch the GPU with the least memory usage from the controller service.</p> <p>Returns:</p> Name Type Description <code>Any</code> <p>Parsed JSON response with details of the GPU having the least memory usage.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>If the underlying HTTP request fails.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>async def get_least_memory_gpu():\n    \"\"\"\n    Fetch the GPU with the least memory usage from the controller service.\n\n    Returns:\n        Any: Parsed JSON response with details of the GPU having the least memory usage.\n\n    Raises:\n        HTTPException: If the underlying HTTP request fails.\n    \"\"\"\n    url = f\"http://{settings.controller_host}:{settings.controller_port}/least_memory_gpu\"\n    return await controller_get(url)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.get_notes","title":"<code>get_notes(scenario_name)</code>  <code>async</code>","text":"<p>Fetch saved notes for a specific scenario.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_name</code> <code>str</code> <p>Name of the scenario to retrieve notes for.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <p>Parsed JSON response containing the notes.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>If the underlying HTTP GET request fails.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>async def get_notes(scenario_name):\n    \"\"\"\n    Fetch saved notes for a specific scenario.\n\n    Parameters:\n        scenario_name (str): Name of the scenario to retrieve notes for.\n\n    Returns:\n        Any: Parsed JSON response containing the notes.\n\n    Raises:\n        HTTPException: If the underlying HTTP GET request fails.\n    \"\"\"\n    url = f\"http://{settings.controller_host}:{settings.controller_port}/notes/{scenario_name}\"\n    return await controller_get(url)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.get_notes_for_scenario","title":"<code>get_notes_for_scenario(scenario_name)</code>  <code>async</code>","text":"<p>Retrieve saved notes for a specific scenario.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_name</code> <code>str</code> <p>Name of the scenario to retrieve notes for.</p> required <p>Returns:</p> Name Type Description <code>JSONResponse</code> <p>{\"status\": \"success\", \"notes\": } if found, otherwise an error message. Source code in <code>nebula/frontend/app.py</code> <pre><code>@app.get(\"/platform/dashboard/{scenario_name}/notes\")\nasync def get_notes_for_scenario(scenario_name: str):\n    \"\"\"\n    Retrieve saved notes for a specific scenario.\n\n    Parameters:\n        scenario_name (str): Name of the scenario to retrieve notes for.\n\n    Returns:\n        JSONResponse: {\"status\": \"success\", \"notes\": &lt;notes&gt;} if found, otherwise an error message.\n    \"\"\"\n    notes_record = await get_notes(scenario_name)\n    if notes_record:\n        notes_data = dict(notes_record)\n        return JSONResponse({\"status\": \"success\", \"notes\": notes_data[\"scenario_notes\"]})\n    else:\n        return JSONResponse({\"status\": \"error\", \"message\": \"Notes not found for the specified scenario\"})\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.get_running_scenarios","title":"<code>get_running_scenarios(get_all=False)</code>  <code>async</code>","text":"<p>Retrieve a list of currently running scenarios.</p> <p>Parameters:</p> Name Type Description Default <code>get_all</code> <code>bool</code> <p>If True, include all running scenarios; if False, apply default filtering.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Any</code> <p>Parsed JSON response listing running scenarios.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>If the underlying HTTP GET request fails.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>async def get_running_scenarios(get_all=False):\n    \"\"\"\n    Retrieve a list of currently running scenarios.\n\n    Parameters:\n        get_all (bool): If True, include all running scenarios; if False, apply default filtering.\n\n    Returns:\n        Any: Parsed JSON response listing running scenarios.\n\n    Raises:\n        HTTPException: If the underlying HTTP GET request fails.\n    \"\"\"\n    url = f\"http://{settings.controller_host}:{settings.controller_port}/scenarios/running?get_all={get_all}\"\n    return await controller_get(url)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.get_scenario_by_name","title":"<code>get_scenario_by_name(scenario_name)</code>  <code>async</code>","text":"<p>Fetch the details of a scenario by name from the controller.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_name</code> <code>str</code> <p>Name of the scenario to retrieve.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <p>Parsed JSON response with scenario details.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>If the underlying HTTP GET request fails.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>async def get_scenario_by_name(scenario_name):\n    \"\"\"\n    Fetch the details of a scenario by name from the controller.\n\n    Parameters:\n        scenario_name (str): Name of the scenario to retrieve.\n\n    Returns:\n        Any: Parsed JSON response with scenario details.\n\n    Raises:\n        HTTPException: If the underlying HTTP GET request fails.\n    \"\"\"\n    url = f\"http://{settings.controller_host}:{settings.controller_port}/scenarios/{scenario_name}\"\n    return await controller_get(url)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.get_scenarios","title":"<code>get_scenarios(user, role)</code>  <code>async</code>","text":"<p>Retrieve all scenarios available for a specific user and role.</p> <p>Parameters:</p> Name Type Description Default <code>user</code> <code>str</code> <p>Username to query scenarios for.</p> required <code>role</code> <code>str</code> <p>Role identifier to filter scenarios.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <p>Parsed JSON response listing available scenarios.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>If the underlying HTTP GET request fails.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>async def get_scenarios(user, role):\n    \"\"\"\n    Retrieve all scenarios available for a specific user and role.\n\n    Parameters:\n        user (str): Username to query scenarios for.\n        role (str): Role identifier to filter scenarios.\n\n    Returns:\n        Any: Parsed JSON response listing available scenarios.\n\n    Raises:\n        HTTPException: If the underlying HTTP GET request fails.\n    \"\"\"\n    url = f\"http://{settings.controller_host}:{settings.controller_port}/scenarios/{user}/{role}\"\n    return await controller_get(url)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.get_session","title":"<code>get_session(request)</code>","text":"<p>Retrieve the session data associated with the incoming request.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>The HTTP request object containing session information.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The session data dictionary stored in the request.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>def get_session(request: Request) -&gt; dict:\n    \"\"\"\n    Retrieve the session data associated with the incoming request.\n\n    Parameters:\n        request (Request): The HTTP request object containing session information.\n\n    Returns:\n        dict: The session data dictionary stored in the request.\n    \"\"\"\n    return request.session\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.get_user_by_scenario_name","title":"<code>get_user_by_scenario_name(scenario_name)</code>  <code>async</code>","text":"<p>Fetch user data for a given scenario from the controller.</p> <p>Parameters: - scenario_name (str): The name of the scenario whose user data to retrieve.</p> <p>Returns: - dict: The user data associated with the specified scenario.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>async def get_user_by_scenario_name(scenario_name):\n    \"\"\"\n    Fetch user data for a given scenario from the controller.\n\n    Parameters:\n    - scenario_name (str): The name of the scenario whose user data to retrieve.\n\n    Returns:\n    - dict: The user data associated with the specified scenario.\n    \"\"\"\n    url = f\"http://{settings.controller_host}:{settings.controller_port}/user/{scenario_name}\"\n    return await controller_get(url)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.index","title":"<code>index()</code>  <code>async</code>","text":"<p>Handle root path by redirecting to the platform home page.</p> <p>Returns:</p> Name Type Description <code>RedirectResponse</code> <p>Redirects client to the '/platform' endpoint.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>@app.get(\"/\", response_class=HTMLResponse)\nasync def index():\n    \"\"\"\n    Handle root path by redirecting to the platform home page.\n\n    Returns:\n        RedirectResponse: Redirects client to the '/platform' endpoint.\n    \"\"\"\n    return RedirectResponse(url=\"/platform\")\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.list_nodes_by_scenario_name","title":"<code>list_nodes_by_scenario_name(scenario_name)</code>  <code>async</code>","text":"<p>List all nodes associated with a given scenario.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_name</code> <code>str</code> <p>Name of the scenario to list nodes for.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <p>Parsed JSON response containing node details.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>If the underlying HTTP GET request fails.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>async def list_nodes_by_scenario_name(scenario_name):\n    \"\"\"\n    List all nodes associated with a given scenario.\n\n    Parameters:\n        scenario_name (str): Name of the scenario to list nodes for.\n\n    Returns:\n        Any: Parsed JSON response containing node details.\n\n    Raises:\n        HTTPException: If the underlying HTTP GET request fails.\n    \"\"\"\n    url = f\"http://{settings.controller_host}:{settings.controller_port}/nodes/{scenario_name}\"\n    return await controller_get(url)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.list_users","title":"<code>list_users(allinfo=True)</code>  <code>async</code>","text":"<p>Retrieves the list of users by calling the controller endpoint.</p> <p>Parameters: - all_info (bool): If True, retrieves detailed information for each user.</p> <p>Returns: - A list of users, as provided by the controller.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>async def list_users(allinfo=True):\n    \"\"\"\n    Retrieves the list of users by calling the controller endpoint.\n\n    Parameters:\n    - all_info (bool): If True, retrieves detailed information for each user.\n\n    Returns:\n    - A list of users, as provided by the controller.\n    \"\"\"\n    url = f\"http://{settings.controller_host}:{settings.controller_port}/user/list?all_info={allinfo}\"\n    data = await controller_get(url)\n    user_list = data[\"users\"]\n\n    return user_list\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.metrics_proxy","title":"<code>metrics_proxy(path=None, request=None)</code>  <code>async</code>","text":"<p>Proxy experiment metric requests to the platform statistics endpoint.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The dynamic path segment to append to the statistics URL.</p> <code>None</code> <code>request</code> <code>Request</code> <p>FastAPI request object containing query parameters.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>RedirectResponse</code> <p>Redirects the client to the corresponding platform statistics experiment URL.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>@app.get(\"/experiment/{path:path}\")\n@app.post(\"/experiment/{path:path}\")\nasync def metrics_proxy(path: str = None, request: Request = None):\n    \"\"\"\n    Proxy experiment metric requests to the platform statistics endpoint.\n\n    Parameters:\n        path (str): The dynamic path segment to append to the statistics URL.\n        request (Request): FastAPI request object containing query parameters.\n\n    Returns:\n        RedirectResponse: Redirects the client to the corresponding platform statistics experiment URL.\n    \"\"\"\n    query_params = request.query_params\n    new_url = \"/platform/statistics/experiment/\" + path\n    if query_params:\n        new_url += \"?\" + urlencode(query_params)\n\n    return RedirectResponse(url=new_url)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.monitor_resources","title":"<code>monitor_resources()</code>  <code>async</code>","text":"<p>Continuously monitor host resources and, if usage exceeds the threshold, stop the last running scenario after broadcasting a message, then wait for resources to recover.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>async def monitor_resources():\n    \"\"\"\n    Continuously monitor host resources and, if usage exceeds the threshold, stop the last running scenario\n    after broadcasting a message, then wait for resources to recover.\n    \"\"\"\n    while True:\n        try:\n            enough_resources = await check_enough_resources()\n            if not enough_resources:\n                running_scenarios = await get_running_scenarios(get_all=True)\n                if running_scenarios:\n                    last_running_scenario = running_scenarios.pop()\n                    running_scenario_as_dict = dict(last_running_scenario)\n                    scenario_name = running_scenario_as_dict[\"name\"]\n                    user = running_scenario_as_dict[\"username\"]\n\n                    # Notify that the scenario has been stopped due to excessive resource usage\n                    scenario_exceed_resources = {\n                        \"type\": \"exceed_resources\",\n                        \"user\": user,\n                    }\n                    try:\n                        await manager.broadcast(json.dumps(scenario_exceed_resources))\n                    except Exception as e:\n                        print(f\"[monitor_resources] Error while broadcasting message: {e}\")\n\n                    await stop_scenario_by_name(scenario_name, user)\n\n                    user_data = user_data_store[user]\n                    user_data.scenarios_list_length -= 1\n\n                    await wait_for_enough_ram()\n                    user_data.finish_scenario_event.set()\n\n        except Exception as e:\n            print(f\"[monitor_resources] Error while checking resources or handling scenario: {e}\")\n            await asyncio.sleep(5)\n            continue\n\n        await asyncio.sleep(20)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.nebula_add_user","title":"<code>nebula_add_user(request, session=Depends(get_session), user=Form(...), password=Form(...), role=Form(...))</code>  <code>async</code>","text":"<p>Add a new user to the system via form submission, available only to admin users, with basic username validation.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>FastAPI request object.</p> required <code>session</code> <code>dict</code> <p>Session data extracted via dependency.</p> <code>Depends(get_session)</code> <code>user</code> <code>str</code> <p>Username provided in form data.</p> <code>Form(...)</code> <code>password</code> <code>str</code> <p>Password provided in form data.</p> <code>Form(...)</code> <code>role</code> <code>str</code> <p>Role provided in form data.</p> <code>Form(...)</code> <p>Returns:</p> Name Type Description <code>RedirectResponse</code> <p>Redirects client to '/platform/admin' with status 303 on success.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>401 Unauthorized if the current user is not an admin.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>@app.post(\"/platform/user/add\")\nasync def nebula_add_user(\n    request: Request,\n    session: dict = Depends(get_session),\n    user: str = Form(...),\n    password: str = Form(...),\n    role: str = Form(...),\n):\n    \"\"\"\n    Add a new user to the system via form submission, available only to admin users, with basic username validation.\n\n    Parameters:\n        request (Request): FastAPI request object.\n        session (dict): Session data extracted via dependency.\n        user (str): Username provided in form data.\n        password (str): Password provided in form data.\n        role (str): Role provided in form data.\n\n    Returns:\n        RedirectResponse: Redirects client to '/platform/admin' with status 303 on success.\n\n    Raises:\n        HTTPException: 401 Unauthorized if the current user is not an admin.\n    \"\"\"\n    # Only admin users can add new users.\n    if session.get(\"role\") != \"admin\":\n        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED)\n\n    # Basic validation on the user value before calling the controller.\n    user_list = await list_users()\n\n    if user.upper() in user_list or \" \" in user or \"'\" in user or '\"' in user:\n        return RedirectResponse(url=\"/platform/admin\", status_code=status.HTTP_303_SEE_OTHER)\n\n    # Call the controller's endpoint to add the user.\n    await add_user(user, password, role)\n    return RedirectResponse(url=\"/platform/admin\", status_code=status.HTTP_303_SEE_OTHER)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.nebula_admin","title":"<code>nebula_admin(request, session=Depends(get_session))</code>  <code>async</code>","text":"<p>Render the admin interface showing a list of users for admin role.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>FastAPI request object.</p> required <code>session</code> <code>dict</code> <p>Session data extracted via dependency.</p> <code>Depends(get_session)</code> <p>Returns:</p> Name Type Description <code>HTMLResponse</code> <p>Rendered 'admin.html' template with user table context.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>401 Unauthorized if the user is not an admin.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>@app.get(\"/platform/admin\", response_class=HTMLResponse)\nasync def nebula_admin(request: Request, session: dict = Depends(get_session)):\n    \"\"\"\n    Render the admin interface showing a list of users for admin role.\n\n    Parameters:\n        request (Request): FastAPI request object.\n        session (dict): Session data extracted via dependency.\n\n    Returns:\n        HTMLResponse: Rendered 'admin.html' template with user table context.\n\n    Raises:\n        HTTPException: 401 Unauthorized if the user is not an admin.\n    \"\"\"\n    if session.get(\"role\") == \"admin\":\n        user_list = await list_users()\n\n        user_table = zip(\n            range(1, len(user_list) + 1),\n            [user[\"user\"] for user in user_list],\n            [user[\"role\"] for user in user_list],\n            strict=False,\n        )\n        return templates.TemplateResponse(\"admin.html\", {\"request\": request, \"users\": user_table})\n    else:\n        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.nebula_change_password","title":"<code>nebula_change_password(request, session=Depends(get_session), current_password=Form(...), new_password=Form(...))</code>  <code>async</code>","text":"<p>Allow an authenticated user to change their own password by verifying the current password first.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>@app.post(\"/platform/user/change_password\")\nasync def nebula_change_password(\n    request: Request,\n    session: dict = Depends(get_session),\n    current_password: str = Form(...),\n    new_password: str = Form(...),\n):\n    \"\"\"\n    Allow an authenticated user to change their own password by verifying the current password first.\n    \"\"\"\n    if \"user\" not in session:\n        return RedirectResponse(url=\"/platform/login\", status_code=status.HTTP_302_FOUND)\n\n    username = session[\"user\"]\n    # Verify current password\n    try:\n        user_data = await verify_user(username, current_password)\n    except HTTPException as e:\n        if e.status_code == 401:\n            return templates.TemplateResponse(\n                \"settings.html\",\n                {\"request\": request, \"error\": \"Current password is incorrect.\"},\n            )\n        else:\n            return templates.TemplateResponse(\n                \"settings.html\",\n                {\"request\": request, \"error\": f\"Error: {e.detail}\"},\n            )\n    # Update password (keep role unchanged)\n    await update_user(username, new_password, user_data[\"role\"])\n    return templates.TemplateResponse(\n        \"settings.html\",\n        {\"request\": request, \"success\": \"Password changed successfully.\"},\n    )\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.nebula_dashboard","title":"<code>nebula_dashboard(request, session=Depends(get_session))</code>  <code>async</code>","text":"<p>Render or return the dashboard view or API data for the current user.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>FastAPI request object.</p> required <code>session</code> <code>dict</code> <p>Session data extracted via dependency.</p> <code>Depends(get_session)</code> <p>Returns:</p> Name Type Description <code>TemplateResponse</code> <p>Rendered 'dashboard.html' for HTML endpoint.</p> <code>JSONResponse</code> <p>List of scenario dictionaries or status message for API endpoint.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>401 Unauthorized for invalid path access.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>@app.get(\"/platform/api/dashboard\", response_class=JSONResponse)\n@app.get(\"/platform/dashboard\", response_class=HTMLResponse)\nasync def nebula_dashboard(request: Request, session: dict = Depends(get_session)):\n    \"\"\"\n    Render or return the dashboard view or API data for the current user.\n\n    Parameters:\n        request (Request): FastAPI request object.\n        session (dict): Session data extracted via dependency.\n\n    Returns:\n        TemplateResponse: Rendered 'dashboard.html' for HTML endpoint.\n        JSONResponse: List of scenario dictionaries or status message for API endpoint.\n\n    Raises:\n        HTTPException: 401 Unauthorized for invalid path access.\n    \"\"\"\n    if \"user\" in session:\n        response = await get_scenarios(session[\"user\"], session[\"role\"])\n        scenarios = response.get(\"scenarios\")\n        scenario_running = response.get(\"scenario_running\")\n\n        if session[\"user\"] not in user_data_store:\n            user_data_store[session[\"user\"]] = UserData()\n\n        user_data = user_data_store[session[\"user\"]]\n    else:\n        scenarios = None\n        scenario_running = None\n\n    bool_completed = False\n    if scenario_running:\n        bool_completed = scenario_running[\"status\"] == \"completed\"\n    if scenarios:\n        if request.url.path == \"/platform/dashboard\":\n            return templates.TemplateResponse(\n                \"dashboard.html\",\n                {\n                    \"request\": request,\n                    \"scenarios\": scenarios,\n                    \"scenarios_list_length\": user_data.scenarios_list_length,\n                    \"scenarios_finished\": user_data.scenarios_finished,\n                    \"scenario_running\": scenario_running,\n                    \"scenario_completed\": bool_completed,\n                    \"user_logged_in\": session.get(\"user\"),\n                    \"user_role\": session.get(\"role\"),\n                    \"user_data_store\": user_data_store,\n                },\n            )\n        elif request.url.path == \"/platform/api/dashboard\":\n            scenarios_as_dict = [dict(row) for row in scenarios]\n            return JSONResponse(scenarios_as_dict)\n        else:\n            raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED)\n    else:\n        if request.url.path == \"/platform/dashboard\":\n            return templates.TemplateResponse(\n                \"dashboard.html\",\n                {\n                    \"request\": request,\n                    \"user_logged_in\": session.get(\"user\"),\n                },\n            )\n        elif request.url.path == \"/platform/api/dashboard\":\n            return JSONResponse({\"scenarios_status\": \"not found in database\"})\n        else:\n            raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.nebula_dashboard_deployment","title":"<code>nebula_dashboard_deployment(request, session=Depends(get_session))</code>  <code>async</code>","text":"<p>Render the deployment dashboard with running scenarios and GPU availability.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>FastAPI request object.</p> required <code>session</code> <code>dict</code> <p>Session data extracted via dependency.</p> <code>Depends(get_session)</code> <p>Returns:</p> Name Type Description <code>HTMLResponse</code> <p>Rendered 'deployment.html' template with scenario and GPU context.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>@app.get(\"/platform/dashboard/deployment/\", response_class=HTMLResponse)\nasync def nebula_dashboard_deployment(request: Request, session: dict = Depends(get_session)):\n    \"\"\"\n    Render the deployment dashboard with running scenarios and GPU availability.\n\n    Parameters:\n        request (Request): FastAPI request object.\n        session (dict): Session data extracted via dependency.\n\n    Returns:\n        HTMLResponse: Rendered 'deployment.html' template with scenario and GPU context.\n    \"\"\"\n    scenario_running = await get_running_scenarios()\n    return templates.TemplateResponse(\n        \"deployment.html\",\n        {\n            \"request\": request,\n            \"scenario_running\": scenario_running,\n            \"user_logged_in\": session.get(\"user\"),\n        },\n    )\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.nebula_dashboard_deployment_run","title":"<code>nebula_dashboard_deployment_run(request, background_tasks, session=Depends(get_session))</code>  <code>async</code>","text":"<p>Handle incoming deployment requests to run one or more scenarios, enqueue them, and trigger background execution.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>FastAPI request object containing a JSON list of scenarios to run.</p> required <code>background_tasks</code> <code>BackgroundTasks</code> <p>Instance for scheduling tasks.</p> required <code>session</code> <code>dict</code> <p>Session data extracted via dependency.</p> <code>Depends(get_session)</code> <p>Returns:</p> Name Type Description <code>RedirectResponse</code> <p>Redirects to '/platform/dashboard' on successful enqueue.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>401 Unauthorized if the user is not logged in or content type is invalid.</p> <code>HTTPException</code> <p>503 Service Unavailable if resources are insufficient.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>@app.post(\"/platform/dashboard/deployment/run\")\nasync def nebula_dashboard_deployment_run(\n    request: Request,\n    background_tasks: BackgroundTasks,\n    session: dict = Depends(get_session),\n):\n    \"\"\"\n    Handle incoming deployment requests to run one or more scenarios, enqueue them,\n    and trigger background execution.\n\n    Parameters:\n        request (Request): FastAPI request object containing a JSON list of scenarios to run.\n        background_tasks (BackgroundTasks): Instance for scheduling tasks.\n        session (dict): Session data extracted via dependency.\n\n    Returns:\n        RedirectResponse: Redirects to '/platform/dashboard' on successful enqueue.\n\n    Raises:\n        HTTPException: 401 Unauthorized if the user is not logged in or content type is invalid.\n        HTTPException: 503 Service Unavailable if resources are insufficient.\n    \"\"\"\n    enough_resources = await check_enough_resources()\n\n    if \"user\" not in session:\n        raise HTTPException(status_code=401, detail=\"Login in to deploy scenarios\")\n    elif not enough_resources:\n        raise HTTPException(status_code=503, detail=\"Not enough resources to run a scenario\")\n\n    if request.headers.get(\"content-type\") != \"application/json\":\n        raise HTTPException(status_code=401)\n\n    data = await request.json()\n    user_data = user_data_store[session[\"user\"]]\n\n    if user_data.scenarios_list_length &lt; 1:\n        user_data.scenarios_finished = 0\n        user_data.scenarios_list_length = len(data)\n        user_data.scenarios_list = data\n        background_tasks.add_task(run_scenarios, session[\"role\"], session[\"user\"])\n    else:\n        user_data.scenarios_list_length += len(data)\n        user_data.scenarios_list.extend(data)\n        await asyncio.sleep(3)\n    logging.info(\n        f\"Running deployment with {len(data)} scenarios_list_length: {user_data.scenarios_list_length} scenarios\"\n    )\n    return RedirectResponse(url=\"/platform/dashboard\", status_code=303)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.nebula_dashboard_download_logs_metrics","title":"<code>nebula_dashboard_download_logs_metrics(scenario_name, request, session=Depends(get_session))</code>  <code>async</code>","text":"<p>Package scenario logs and configuration into a zip archive and stream it to the client.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_name</code> <code>str</code> <p>Name of the scenario whose files are to be downloaded.</p> required <code>request</code> <code>Request</code> <p>FastAPI request object.</p> required <code>session</code> <code>dict</code> <p>Session data extracted via dependency.</p> <code>Depends(get_session)</code> <p>Returns:</p> Name Type Description <code>StreamingResponse</code> <p>A zip file containing the scenario's logs and configuration.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>401 Unauthorized if the user is not logged in.</p> <code>HTTPException</code> <p>404 Not Found if the log or config folder does not exist.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>@app.get(\"/platform/dashboard/{scenario_name}/download/logs\")\nasync def nebula_dashboard_download_logs_metrics(\n    scenario_name: str, request: Request, session: dict = Depends(get_session)\n):\n    \"\"\"\n    Package scenario logs and configuration into a zip archive and stream it to the client.\n\n    Parameters:\n        scenario_name (str): Name of the scenario whose files are to be downloaded.\n        request (Request): FastAPI request object.\n        session (dict): Session data extracted via dependency.\n\n    Returns:\n        StreamingResponse: A zip file containing the scenario's logs and configuration.\n\n    Raises:\n        HTTPException: 401 Unauthorized if the user is not logged in.\n        HTTPException: 404 Not Found if the log or config folder does not exist.\n    \"\"\"\n    if \"user\" in session:\n        log_folder = FileUtils.check_path(settings.log_dir, scenario_name)\n        config_folder = FileUtils.check_path(settings.config_dir, scenario_name)\n        if os.path.exists(log_folder) and os.path.exists(config_folder):\n            # Crear un archivo zip con los logs y los archivos de configuraci\u00f3n, enviarlo al usuario\n            memory_file = io.BytesIO()\n            with zipfile.ZipFile(memory_file, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n                zipdir(log_folder, zipf)\n                zipdir(config_folder, zipf)\n\n            memory_file.seek(0)\n\n            return StreamingResponse(\n                memory_file,\n                media_type=\"application/zip\",\n                headers={\"Content-Disposition\": f\"attachment; filename={scenario_name}.zip\"},\n            )\n        else:\n            raise HTTPException(status_code=404, detail=\"Log or config folder not found\")\n    else:\n        raise HTTPException(status_code=401)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.nebula_dashboard_monitor","title":"<code>nebula_dashboard_monitor(scenario_name, request, session=Depends(get_session))</code>  <code>async</code>","text":"<p>Display or return monitoring information for a specific scenario, including node statuses.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_name</code> <code>str</code> <p>Name of the scenario to monitor.</p> required <code>request</code> <code>Request</code> <p>FastAPI request object.</p> required <code>session</code> <code>dict</code> <p>Session data extracted via dependency.</p> <code>Depends(get_session)</code> <p>Returns:</p> Name Type Description <code>TemplateResponse</code> <p>Rendered 'monitor.html' for HTML endpoint.</p> <code>JSONResponse</code> <p>JSON object containing scenario status, node list, and scenario metadata.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>401 Unauthorized for invalid path access.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>@app.get(\"/platform/api/dashboard/{scenario_name}/monitor\", response_class=JSONResponse)\n@app.get(\"/platform/dashboard/{scenario_name}/monitor\", response_class=HTMLResponse)\nasync def nebula_dashboard_monitor(scenario_name: str, request: Request, session: dict = Depends(get_session)):\n    \"\"\"\n    Display or return monitoring information for a specific scenario, including node statuses.\n\n    Parameters:\n        scenario_name (str): Name of the scenario to monitor.\n        request (Request): FastAPI request object.\n        session (dict): Session data extracted via dependency.\n\n    Returns:\n        TemplateResponse: Rendered 'monitor.html' for HTML endpoint.\n        JSONResponse: JSON object containing scenario status, node list, and scenario metadata.\n\n    Raises:\n        HTTPException: 401 Unauthorized for invalid path access.\n    \"\"\"\n    scenario = await get_scenario_by_name(scenario_name)\n    if scenario:\n        nodes_list = await list_nodes_by_scenario_name(scenario_name)\n        if nodes_list:\n            formatted_nodes = []\n            for node in nodes_list:\n                # Calculate initial status based on timestamp\n                timestamp = datetime.datetime.strptime(node[\"timestamp\"], \"%Y-%m-%d %H:%M:%S.%f\")\n                is_online = (datetime.datetime.now() - timestamp) &lt;= datetime.timedelta(seconds=25)\n\n                formatted_nodes.append({\n                    \"uid\": node[\"uid\"],\n                    \"idx\": node[\"idx\"],\n                    \"ip\": node[\"ip\"],\n                    \"port\": node[\"port\"],\n                    \"role\": node[\"role\"],\n                    \"neighbors\": node[\"neighbors\"],\n                    \"latitude\": node[\"latitude\"],\n                    \"longitude\": node[\"longitude\"],\n                    \"timestamp\": node[\"timestamp\"],\n                    \"federation\": node[\"federation\"],\n                    \"round\": str(node[\"round\"]),\n                    \"scenario_name\": node[\"scenario\"],\n                    \"hash\": node[\"hash\"],\n                    \"malicious\": node[\"malicious\"],\n                    \"status\": is_online,\n                })\n\n            # For HTML response, return the template with basic data\n            if request.url.path == f\"/platform/dashboard/{scenario_name}/monitor\":\n                return templates.TemplateResponse(\n                    \"monitor.html\",\n                    {\n                        \"request\": request,\n                        \"scenario_name\": scenario_name,\n                        \"scenario\": scenario,\n                        \"nodes\": [list(node.values()) for node in formatted_nodes],\n                        \"user_logged_in\": session.get(\"user\"),\n                    },\n                )\n            # For API response, return the formatted node data\n            elif request.url.path == f\"/platform/api/dashboard/{scenario_name}/monitor\":\n                return JSONResponse({\n                    \"scenario_status\": scenario[\"status\"],\n                    \"nodes\": formatted_nodes,\n                    \"scenario_name\": scenario[\"name\"],\n                    \"title\": scenario[\"title\"],\n                    \"description\": scenario[\"description\"],\n                })\n            else:\n                raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED)\n        else:\n            # No nodes found\n            if request.url.path == f\"/platform/dashboard/{scenario_name}/monitor\":\n                return templates.TemplateResponse(\n                    \"monitor.html\",\n                    {\n                        \"request\": request,\n                        \"scenario_name\": scenario_name,\n                        \"scenario\": scenario,\n                        \"nodes\": [],\n                        \"user_logged_in\": session.get(\"user\"),\n                    },\n                )\n            elif request.url.path == f\"/platform/api/dashboard/{scenario_name}/monitor\":\n                return JSONResponse({\n                    \"scenario_status\": scenario[\"status\"],\n                    \"nodes\": [],\n                    \"scenario_name\": scenario[\"name\"],\n                    \"title\": scenario[\"title\"],\n                    \"description\": scenario[\"description\"],\n                })\n            else:\n                raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED)\n    else:\n        # Scenario not found\n        if request.url.path == f\"/platform/dashboard/{scenario_name}/monitor\":\n            return templates.TemplateResponse(\n                \"monitor.html\",\n                {\n                    \"request\": request,\n                    \"scenario_name\": scenario_name,\n                    \"scenario\": None,\n                    \"nodes\": [],\n                    \"user_logged_in\": session.get(\"user\"),\n                },\n            )\n        elif request.url.path == f\"/platform/api/dashboard/{scenario_name}/monitor\":\n            return JSONResponse({\"scenario_status\": \"not exists\"})\n        else:\n            raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.nebula_dashboard_private","title":"<code>nebula_dashboard_private(request, scenario_name, session=Depends(get_session))</code>  <code>async</code>","text":"<p>Render the private scenario dashboard for authenticated users.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>FastAPI request object.</p> required <code>scenario_name</code> <code>str</code> <p>Name of the scenario to display.</p> required <code>session</code> <code>dict</code> <p>Session data extracted via dependency.</p> <code>Depends(get_session)</code> <p>Returns:</p> Name Type Description <code>HTMLResponse</code> <p>Rendered 'private.html' template with scenario context.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>401 Unauthorized if the user is not authenticated.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>@app.get(\"/platform/dashboard/{scenario_name}/private\", response_class=HTMLResponse)\nasync def nebula_dashboard_private(request: Request, scenario_name: str, session: dict = Depends(get_session)):\n    \"\"\"\n    Render the private scenario dashboard for authenticated users.\n\n    Parameters:\n        request (Request): FastAPI request object.\n        scenario_name (str): Name of the scenario to display.\n        session (dict): Session data extracted via dependency.\n\n    Returns:\n        HTMLResponse: Rendered 'private.html' template with scenario context.\n\n    Raises:\n        HTTPException: 401 Unauthorized if the user is not authenticated.\n    \"\"\"\n    if \"user\" in session:\n        return templates.TemplateResponse(\"private.html\", {\"request\": request, \"scenario_name\": scenario_name})\n    else:\n        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.nebula_dashboard_runningscenario","title":"<code>nebula_dashboard_runningscenario(session=Depends(get_session))</code>  <code>async</code>","text":"<p>Get information about currently running scenario(s) for the authenticated user or admin.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>dict</code> <p>Session data extracted via dependency.</p> <code>Depends(get_session)</code> <p>Returns:</p> Name Type Description <code>JSONResponse</code> <p>JSON object containing running scenario details and status, or {\"scenario_status\": \"not running\"}.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>@app.get(\"/platform/api/dashboard/runningscenario\", response_class=JSONResponse)\nasync def nebula_dashboard_runningscenario(session: dict = Depends(get_session)):\n    \"\"\"\n    Get information about currently running scenario(s) for the authenticated user or admin.\n\n    Parameters:\n        session (dict): Session data extracted via dependency.\n\n    Returns:\n        JSONResponse: JSON object containing running scenario details and status, or {\"scenario_status\": \"not running\"}.\n    \"\"\"\n    if session.get(\"role\") == \"admin\":\n        scenario_running = await get_running_scenarios()\n    elif \"user\" in session:\n        scenario_running = await get_running_scenarios(session[\"user\"])\n    if scenario_running:\n        scenario_running_as_dict = dict(scenario_running)\n        scenario_running_as_dict[\"scenario_status\"] = \"running\"\n        return JSONResponse(scenario_running_as_dict)\n    else:\n        return JSONResponse({\"scenario_status\": \"not running\"})\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.nebula_dashboard_statistics","title":"<code>nebula_dashboard_statistics(request, scenario_name=None)</code>  <code>async</code>","text":"<p>Render the TensorBoard statistics page for all experiments or filter by scenario.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>FastAPI request object.</p> required <code>scenario_name</code> <code>str</code> <p>Scenario name to filter statistics by; defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>TemplateResponse</code> <p>Rendered 'statistics.html' with the appropriate URL parameter for TensorBoard.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>@app.get(\"/platform/dashboard/statistics/\", response_class=HTMLResponse)\n@app.get(\"/platform/dashboard/{scenario_name}/statistics/\", response_class=HTMLResponse)\nasync def nebula_dashboard_statistics(request: Request, scenario_name: str = None):\n    \"\"\"\n    Render the TensorBoard statistics page for all experiments or filter by scenario.\n\n    Parameters:\n        request (Request): FastAPI request object.\n        scenario_name (str, optional): Scenario name to filter statistics by; defaults to None.\n\n    Returns:\n        TemplateResponse: Rendered 'statistics.html' with the appropriate URL parameter for TensorBoard.\n    \"\"\"\n    statistics_url = \"/platform/statistics/\"\n    if scenario_name is not None:\n        statistics_url += f\"?smoothing=0&amp;runFilter={scenario_name}\"\n\n    return templates.TemplateResponse(\"statistics.html\", {\"request\": request, \"statistics_url\": statistics_url})\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.nebula_delete_user","title":"<code>nebula_delete_user(user, request, session=Depends(get_session))</code>  <code>async</code>","text":"<p>Delete a specified user account via admin privileges, preventing deletion of 'ADMIN' or self.</p> <p>Parameters:</p> Name Type Description Default <code>user</code> <code>str</code> <p>Username of the account to delete.</p> required <code>request</code> <code>Request</code> <p>FastAPI request object.</p> required <code>session</code> <code>dict</code> <p>Session data extracted via dependency.</p> <code>Depends(get_session)</code> <p>Returns:</p> Name Type Description <code>RedirectResponse</code> <p>Redirects client to '/platform/admin' on success.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>403 Forbidden if attempting to delete 'ADMIN' or the current user.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>@app.get(\"/platform/user/delete/{user}/\")\nasync def nebula_delete_user(user: str, request: Request, session: dict = Depends(get_session)):\n    \"\"\"\n    Delete a specified user account via admin privileges, preventing deletion of 'ADMIN' or self.\n\n    Parameters:\n        user (str): Username of the account to delete.\n        request (Request): FastAPI request object.\n        session (dict): Session data extracted via dependency.\n\n    Returns:\n        RedirectResponse: Redirects client to '/platform/admin' on success.\n\n    Raises:\n        HTTPException: 403 Forbidden if attempting to delete 'ADMIN' or the current user.\n    \"\"\"\n    if session.get(\"role\") == \"admin\":\n        if user == \"ADMIN\":  # ADMIN account can't be deleted.\n            raise HTTPException(status_code=status.HTTP_403_FORBIDDEN)\n        if user == session[\"user\"]:  # Current user can't delete himself.\n            raise HTTPException(status_code=status.HTTP_403_FORBIDDEN)\n\n        await delete_user(user)\n        return RedirectResponse(url=\"/platform/admin\")\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.nebula_home","title":"<code>nebula_home(request)</code>  <code>async</code>","text":"<p>Render the Nebula platform home page.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>FastAPI request object.</p> required <p>Returns:</p> Name Type Description <code>HTMLResponse</code> <p>Rendered 'index.html' template with alerts context.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>@app.get(\"/platform\", response_class=HTMLResponse)\n@app.get(\"/platform/\", response_class=HTMLResponse)\nasync def nebula_home(request: Request):\n    \"\"\"\n    Render the Nebula platform home page.\n\n    Parameters:\n        request (Request): FastAPI request object.\n\n    Returns:\n        HTMLResponse: Rendered 'index.html' template with alerts context.\n    \"\"\"\n    alerts = []\n    return templates.TemplateResponse(\"index.html\", {\"request\": request, \"alerts\": alerts})\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.nebula_login","title":"<code>nebula_login(request, session=Depends(get_session), user=Form(...), password=Form(...))</code>  <code>async</code>","text":"<p>Authenticate a user and initialize session data.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>FastAPI request object.</p> required <code>session</code> <code>dict</code> <p>Session data extracted via dependency.</p> <code>Depends(get_session)</code> <code>user</code> <code>str</code> <p>Username provided in form data.</p> <code>Form(...)</code> <code>password</code> <code>str</code> <p>Password provided in form data.</p> <code>Form(...)</code> <p>Returns:</p> Name Type Description <code>JSONResponse</code> <p>{\"message\": \"Login successful\"} with HTTP 200 status on success.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>@app.post(\"/platform/login\")\nasync def nebula_login(\n    request: Request,\n    session: dict = Depends(get_session),\n    user: str = Form(...),\n    password: str = Form(...),\n):\n    \"\"\"\n    Authenticate a user and initialize session data.\n\n    Parameters:\n        request (Request): FastAPI request object.\n        session (dict): Session data extracted via dependency.\n        user (str): Username provided in form data.\n        password (str): Password provided in form data.\n\n    Returns:\n        JSONResponse: {\"message\": \"Login successful\"} with HTTP 200 status on success.\n    \"\"\"\n    logging.info(f\"Login attempt for user: {user}\")\n    try:\n        user_data = await verify_user(user, password)\n        if not user_data:\n            logging.error(f\"Login failed: Invalid credentials for user {user}\")\n            return JSONResponse({\"message\": \"Invalid credentials\"}, status_code=401)\n\n        session[\"user\"] = user_data.get(\"user\")\n        session[\"role\"] = user_data.get(\"role\")\n        logging.info(f\"Login successful for user: {user} with role: {user_data.get('role')}\")\n        return JSONResponse({\"message\": \"Login successful\"}, status_code=200)\n    except Exception as e:\n        logging.exception(f\"Login error for user {user}: {str(e)}\")\n        return JSONResponse({\"message\": \"Login failed\", \"error\": str(e)}, status_code=401)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.nebula_logout","title":"<code>nebula_logout(request, session=Depends(get_session))</code>  <code>async</code>","text":"<p>Log out the authenticated user and redirect to the platform home.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>FastAPI request object.</p> required <code>session</code> <code>dict</code> <p>Session data extracted via dependency.</p> <code>Depends(get_session)</code> <p>Returns:</p> Name Type Description <code>RedirectResponse</code> <p>Redirects client to the '/platform' endpoint.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>@app.get(\"/platform/logout\")\nasync def nebula_logout(request: Request, session: dict = Depends(get_session)):\n    \"\"\"\n    Log out the authenticated user and redirect to the platform home.\n\n    Parameters:\n        request (Request): FastAPI request object.\n        session (dict): Session data extracted via dependency.\n\n    Returns:\n        RedirectResponse: Redirects client to the '/platform' endpoint.\n    \"\"\"\n    session.pop(\"user\", None)\n    return RedirectResponse(url=\"/platform\")\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.nebula_monitor_image","title":"<code>nebula_monitor_image(scenario_name)</code>  <code>async</code>","text":"<p>Serve the topology image for a given scenario if available.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_name</code> <code>str</code> <p>Name of the scenario.</p> required <p>Returns:</p> Name Type Description <code>FileResponse</code> <p>The topology.png image for the scenario.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>404 Not Found if the topology image does not exist.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>@app.get(\"/platform/dashboard/{scenario_name}/topology/image/\")\nasync def nebula_monitor_image(scenario_name: str):\n    \"\"\"\n    Serve the topology image for a given scenario if available.\n\n    Parameters:\n        scenario_name (str): Name of the scenario.\n\n    Returns:\n        FileResponse: The topology.png image for the scenario.\n\n    Raises:\n        HTTPException: 404 Not Found if the topology image does not exist.\n    \"\"\"\n    topology_image = FileUtils.check_path(settings.config_dir, os.path.join(scenario_name, \"topology.png\"))\n    if os.path.exists(topology_image):\n        return FileResponse(topology_image, media_type=\"image/png\", filename=f\"{scenario_name}_topology.png\")\n    else:\n        raise HTTPException(status_code=404, detail=\"Topology image not found\")\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.nebula_relaunch_scenario","title":"<code>nebula_relaunch_scenario(scenario_name, background_tasks, session=Depends(get_session))</code>  <code>async</code>","text":"<p>Relaunch a previously run scenario by loading its configuration, enqueuing it, and starting execution in the background.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_name</code> <code>str</code> <p>Name of the scenario to relaunch.</p> required <code>background_tasks</code> <code>BackgroundTasks</code> <p>FastAPI BackgroundTasks instance for deferred execution.</p> required <code>session</code> <code>dict</code> <p>Session data extracted via dependency.</p> <code>Depends(get_session)</code> <p>Returns:</p> Name Type Description <code>RedirectResponse</code> <p>Redirects to the '/platform/dashboard' endpoint.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>401 Unauthorized if the user is not authenticated or lacks permission.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>@app.get(\"/platform/dashboard/{scenario_name}/relaunch\")\nasync def nebula_relaunch_scenario(\n    scenario_name: str, background_tasks: BackgroundTasks, session: dict = Depends(get_session)\n):\n    \"\"\"\n    Relaunch a previously run scenario by loading its configuration, enqueuing it,\n    and starting execution in the background.\n\n    Parameters:\n        scenario_name (str): Name of the scenario to relaunch.\n        background_tasks (BackgroundTasks): FastAPI BackgroundTasks instance for deferred execution.\n        session (dict): Session data extracted via dependency.\n\n    Returns:\n        RedirectResponse: Redirects to the '/platform/dashboard' endpoint.\n\n    Raises:\n        HTTPException: 401 Unauthorized if the user is not authenticated or lacks permission.\n    \"\"\"\n    user_data = user_data_store[session[\"user\"]]\n\n    if \"user\" in session:\n        if session[\"role\"] == \"demo\":\n            raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED)\n        elif session[\"role\"] == \"user\":\n            if not await check_scenario_with_role(session[\"role\"], scenario_name):\n                raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED)\n\n        scenario_path = FileUtils.check_path(settings.config_dir, os.path.join(scenario_name, \"scenario.json\"))\n        with open(scenario_path) as scenario_file:\n            scenario = json.load(scenario_file)\n\n        user_data.scenarios_list_length = user_data.scenarios_list_length + 1\n\n        if user_data.scenarios_list_length == 1:\n            user_data.scenarios_finished = 0\n            user_data.scenarios_list.clear()\n            user_data.scenarios_list.append(scenario)\n            background_tasks.add_task(run_scenarios, session[\"role\"], session[\"user\"])\n        else:\n            user_data.scenarios_list.append(scenario)\n\n        return RedirectResponse(url=\"/platform/dashboard\", status_code=303)\n    else:\n        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.nebula_remove_scenario","title":"<code>nebula_remove_scenario(scenario_name, session=Depends(get_session))</code>  <code>async</code>","text":"<p>Remove a scenario for the authenticated user and redirect back to the dashboard.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_name</code> <code>str</code> <p>Name of the scenario to remove.</p> required <code>session</code> <code>dict</code> <p>Session data extracted via dependency.</p> <code>Depends(get_session)</code> <p>Returns:</p> Name Type Description <code>RedirectResponse</code> <p>Redirects to the '/platform/dashboard' endpoint.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>401 Unauthorized if the user is not authenticated or lacks permission.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>@app.get(\"/platform/dashboard/{scenario_name}/remove\")\nasync def nebula_remove_scenario(scenario_name: str, session: dict = Depends(get_session)):\n    \"\"\"\n    Remove a scenario for the authenticated user and redirect back to the dashboard.\n\n    Parameters:\n        scenario_name (str): Name of the scenario to remove.\n        session (dict): Session data extracted via dependency.\n\n    Returns:\n        RedirectResponse: Redirects to the '/platform/dashboard' endpoint.\n\n    Raises:\n        HTTPException: 401 Unauthorized if the user is not authenticated or lacks permission.\n    \"\"\"\n    if \"user\" in session:\n        if session[\"role\"] == \"demo\":\n            raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED)\n        elif session[\"role\"] == \"user\":\n            if not await check_scenario_with_role(session[\"role\"], scenario_name):\n                raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED)\n        await remove_scenario(scenario_name, session[\"user\"])\n        return RedirectResponse(url=\"/platform/dashboard\")\n    else:\n        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.nebula_settings","title":"<code>nebula_settings(request, session=Depends(get_session))</code>  <code>async</code>","text":"<p>Render the settings interface for authenticated users. Enable to change the password of the user.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>@app.get(\"/platform/settings\", response_class=HTMLResponse)\nasync def nebula_settings(request: Request, session: dict = Depends(get_session)):\n    \"\"\"\n    Render the settings interface for authenticated users.\n    Enable to change the password of the user.\n    \"\"\"\n    if \"user\" in session:\n        return templates.TemplateResponse(\"settings.html\", {\"request\": request})\n    else:\n        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.nebula_stop_scenario","title":"<code>nebula_stop_scenario(scenario_name, stop_all, request, session=Depends(get_session))</code>  <code>async</code>","text":"<p>Stop one or all scenarios for the current user and redirect to the dashboard.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_name</code> <code>str</code> <p>Name of the scenario to stop.</p> required <code>stop_all</code> <code>bool</code> <p>If True, stop all scenarios; otherwise stop only the specified one.</p> required <code>request</code> <code>Request</code> <p>FastAPI request object.</p> required <code>session</code> <code>dict</code> <p>Session data extracted via dependency.</p> <code>Depends(get_session)</code> <p>Returns:</p> Name Type Description <code>RedirectResponse</code> <p>Redirects to the '/platform/dashboard' endpoint.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>401 Unauthorized if the user is not authenticated or lacks permission.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>@app.get(\"/platform/dashboard/{scenario_name}/stop/{stop_all}\")\nasync def nebula_stop_scenario(\n    scenario_name: str,\n    stop_all: bool,\n    request: Request,\n    session: dict = Depends(get_session),\n):\n    \"\"\"\n    Stop one or all scenarios for the current user and redirect to the dashboard.\n\n    Parameters:\n        scenario_name (str): Name of the scenario to stop.\n        stop_all (bool): If True, stop all scenarios; otherwise stop only the specified one.\n        request (Request): FastAPI request object.\n        session (dict): Session data extracted via dependency.\n\n    Returns:\n        RedirectResponse: Redirects to the '/platform/dashboard' endpoint.\n\n    Raises:\n        HTTPException: 401 Unauthorized if the user is not authenticated or lacks permission.\n    \"\"\"\n    if \"user\" in session:\n        user = await get_user_by_scenario_name(scenario_name)\n        user_data = user_data_store[user]\n\n        if session[\"role\"] == \"demo\":\n            raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED)\n        if stop_all:\n            user_data.stop_all_scenarios_event.set()\n            user_data.scenarios_list_length = 0\n            user_data.scenarios_finished = 0\n            await stop_scenario_by_name(scenario_name, user)\n        else:\n            user_data.finish_scenario_event.set()\n            user_data.scenarios_list_length -= 1\n            await stop_scenario_by_name(scenario_name, user)\n        return RedirectResponse(url=\"/platform/dashboard\")\n    else:\n        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.nebula_update_node","title":"<code>nebula_update_node(scenario_name, request)</code>  <code>async</code>","text":"<p>Process a node update request for a scenario and broadcast the updated node information.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_name</code> <code>str</code> <p>Name of the scenario.</p> required <code>request</code> <code>Request</code> <p>FastAPI request object containing a JSON payload with node update data.</p> required <p>Returns:</p> Name Type Description <code>JSONResponse</code> <p>{\"message\": \"Node updated\", \"status\": \"success\"} on success.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>400 Bad Request if the content type is not application/json.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>@app.post(\"/platform/dashboard/{scenario_name}/node/update\")\nasync def nebula_update_node(scenario_name: str, request: Request):\n    \"\"\"\n    Process a node update request for a scenario and broadcast the updated node information.\n\n    Parameters:\n        scenario_name (str): Name of the scenario.\n        request (Request): FastAPI request object containing a JSON payload with node update data.\n\n    Returns:\n        JSONResponse: {\"message\": \"Node updated\", \"status\": \"success\"} on success.\n\n    Raises:\n        HTTPException: 400 Bad Request if the content type is not application/json.\n    \"\"\"\n    if request.method == \"POST\":\n        if request.headers.get(\"content-type\") == \"application/json\":\n            config = await request.json()\n\n            node_update = {\n                \"type\": \"node_update\",\n                \"scenario_name\": scenario_name,\n                \"uid\": config[\"device_args\"][\"uid\"],\n                \"idx\": config[\"device_args\"][\"idx\"],\n                \"ip\": config[\"network_args\"][\"ip\"],\n                \"port\": str(config[\"network_args\"][\"port\"]),\n                \"role\": config[\"device_args\"][\"role\"],\n                \"malicious\": config[\"device_args\"][\"malicious\"],\n                \"neighbors\": config[\"network_args\"][\"neighbors\"],\n                \"latitude\": config[\"mobility_args\"][\"latitude\"],\n                \"longitude\": config[\"mobility_args\"][\"longitude\"],\n                \"timestamp\": config[\"timestamp\"],\n                \"federation\": config[\"scenario_args\"][\"federation\"],\n                \"round\": config[\"federation_args\"][\"round\"],\n                \"name\": config[\"scenario_args\"][\"name\"],\n                \"status\": True,\n                \"neighbors_distance\": config[\"mobility_args\"][\"neighbors_distance\"],\n                \"malicious\": str(config[\"device_args\"][\"malicious\"]),\n            }\n\n            try:\n                await manager.broadcast(json.dumps(node_update))\n            except Exception:\n                pass\n\n            return JSONResponse({\"message\": \"Node updated\", \"status\": \"success\"}, status_code=200)\n        else:\n            raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.nebula_update_user","title":"<code>nebula_update_user(request, session=Depends(get_session), user=Form(...), password=Form(...), role=Form(...))</code>  <code>async</code>","text":"<p>Update an existing user's credentials and role via form submission, accessible only to admin users.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>FastAPI request object.</p> required <code>session</code> <code>dict</code> <p>Session data extracted via dependency.</p> <code>Depends(get_session)</code> <code>user</code> <code>str</code> <p>Username provided in form data.</p> <code>Form(...)</code> <code>password</code> <code>str</code> <p>New password provided in form data.</p> <code>Form(...)</code> <code>role</code> <code>str</code> <p>New role provided in form data.</p> <code>Form(...)</code> <p>Returns:</p> Name Type Description <code>RedirectResponse</code> <p>Redirects client to '/platform/admin' on success, or to '/platform' if unauthorized.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>@app.post(\"/platform/user/update\")\nasync def nebula_update_user(\n    request: Request,\n    session: dict = Depends(get_session),\n    user: str = Form(...),\n    password: str = Form(...),\n    role: str = Form(...),\n):\n    \"\"\"\n    Update an existing user's credentials and role via form submission, accessible only to admin users.\n\n    Parameters:\n        request (Request): FastAPI request object.\n        session (dict): Session data extracted via dependency.\n        user (str): Username provided in form data.\n        password (str): New password provided in form data.\n        role (str): New role provided in form data.\n\n    Returns:\n        RedirectResponse: Redirects client to '/platform/admin' on success, or to '/platform' if unauthorized.\n    \"\"\"\n    if \"user\" not in session or session[\"role\"] != \"admin\":\n        return RedirectResponse(url=\"/platform\", status_code=status.HTTP_302_FOUND)\n\n    await update_user(user, password, role)\n    return RedirectResponse(url=\"/platform/admin\", status_code=status.HTTP_302_FOUND)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.nebula_ws_historic","title":"<code>nebula_ws_historic(session=Depends(get_session))</code>  <code>async</code>","text":"<p>Retrieve historical data for admin users.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>dict</code> <p>Session data extracted via dependency.</p> <code>Depends(get_session)</code> <p>Returns:</p> Name Type Description <code>JSONResponse</code> <p>Historical data if available, otherwise an error message.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>@app.get(\"/platform/historic\")\nasync def nebula_ws_historic(session: dict = Depends(get_session)):\n    \"\"\"\n    Retrieve historical data for admin users.\n\n    Parameters:\n        session (dict): Session data extracted via dependency.\n\n    Returns:\n        JSONResponse: Historical data if available, otherwise an error message.\n    \"\"\"\n    if session.get(\"role\") == \"admin\":\n        historic = manager.get_historic()\n        if historic:\n            pretty_historic = historic\n            return JSONResponse(content=pretty_historic)\n        else:\n            return JSONResponse({\"status\": \"error\", \"message\": \"Historic not found\"})\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.node_stopped","title":"<code>node_stopped(scenario_name, request)</code>  <code>async</code>","text":"<p>Handle notification that a node has finished its task; mark the node as finished and signal scenario completion when all nodes are done.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_name</code> <code>str</code> <p>Name of the scenario.</p> required <code>request</code> <code>Request</code> <p>FastAPI request object containing a JSON payload with the finished node index.</p> required <p>Returns:</p> Name Type Description <code>JSONResponse</code> <p>Message indicating node completion status or scenario completion.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>400 Bad Request if the content type is not application/json.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>@app.post(\"/platform/dashboard/{scenario_name}/node/done\")\nasync def node_stopped(scenario_name: str, request: Request):\n    \"\"\"\n    Handle notification that a node has finished its task; mark the node as finished\n    and signal scenario completion when all nodes are done.\n\n    Parameters:\n        scenario_name (str): Name of the scenario.\n        request (Request): FastAPI request object containing a JSON payload with the finished node index.\n\n    Returns:\n        JSONResponse: Message indicating node completion status or scenario completion.\n\n    Raises:\n        HTTPException: 400 Bad Request if the content type is not application/json.\n    \"\"\"\n    user = await get_user_by_scenario_name(scenario_name)\n    user_data = user_data_store[user]\n\n    if request.headers.get(\"content-type\") == \"application/json\":\n        data = await request.json()\n        user_data.nodes_finished.append(data[\"idx\"])\n        nodes_list = await list_nodes_by_scenario_name(scenario_name)\n\n        # Check if all nodes have finished by comparing sets of node IDs\n        finished_node_ids = set(map(str, user_data.nodes_finished))\n        all_node_ids = {str(node[\"idx\"]) for node in nodes_list}\n        all_nodes_finished = finished_node_ids &gt;= all_node_ids\n\n        if all_nodes_finished:\n            user_data.nodes_finished.clear()\n            user_data.finish_scenario_event.set()\n            return JSONResponse(\n                status_code=200,\n                content={\"message\": \"All nodes finished, scenario marked as completed.\"},\n            )\n        else:\n            return JSONResponse(\n                status_code=200,\n                content={\"message\": \"Node marked as finished, waiting for other nodes.\"},\n            )\n    else:\n        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.physical_nodes_available","title":"<code>physical_nodes_available(ips)</code>  <code>async</code>","text":"<p>Return True only if every ip answered with {\"running\": false}. Any error or timeout counts as not available.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>async def physical_nodes_available(ips: list[str]) -&gt; bool:\n    \"\"\"\n    Return True only if *every* ip answered with {\"running\": false}.\n    Any error or timeout counts as *not available*.\n    \"\"\"\n    tasks = [\n        controller_get(f\"http://{settings.controller_host}:{settings.controller_port}/physical/state/{ip}\")\n        for ip in ips\n    ]\n    states = await asyncio.gather(*tasks, return_exceptions=True)\n\n    for st in states:\n        if not isinstance(st, dict):\n            return False\n        if st.get(\"running\") is True:\n            return False\n    return True\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.proxy_physical_state","title":"<code>proxy_physical_state(ip)</code>  <code>async</code>","text":"<p>Forward the request to the controller so the frontend can query a node state without exposing controller host/port to the browser.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>@app.get(\"/platform/api/physical/state/{ip}\", response_class=JSONResponse)\nasync def proxy_physical_state(ip: str):\n    \"\"\"\n    Forward the request to the controller so the frontend\n    can query a node state without exposing controller host/port\n    to the browser.\n    \"\"\"\n    url = f\"http://{settings.controller_host}:{settings.controller_port}/physical/state/{ip}\"\n    return await controller_get(url)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.remove_nodes_by_scenario_name","title":"<code>remove_nodes_by_scenario_name(scenario_name)</code>  <code>async</code>","text":"<p>Remove all nodes associated with a scenario from the controller records.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_name</code> <code>str</code> <p>Name of the scenario whose nodes should be removed.</p> required <p>Raises:</p> Type Description <code>HTTPException</code> <p>If the underlying HTTP POST request fails.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>async def remove_nodes_by_scenario_name(scenario_name):\n    \"\"\"\n    Remove all nodes associated with a scenario from the controller records.\n\n    Parameters:\n        scenario_name (str): Name of the scenario whose nodes should be removed.\n\n    Raises:\n        HTTPException: If the underlying HTTP POST request fails.\n    \"\"\"\n    url = f\"http://{settings.controller_host}:{settings.controller_port}/nodes/remove\"\n    data = {\"scenario_name\": scenario_name}\n    await controller_post(url, data)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.remove_note","title":"<code>remove_note(scenario_name)</code>  <code>async</code>","text":"<p>Remove notes for a specific scenario from the controller.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_name</code> <code>str</code> <p>Name of the scenario whose notes should be removed.</p> required <p>Raises:</p> Type Description <code>HTTPException</code> <p>If the underlying HTTP POST request fails.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>async def remove_note(scenario_name):\n    \"\"\"\n    Remove notes for a specific scenario from the controller.\n\n    Parameters:\n        scenario_name (str): Name of the scenario whose notes should be removed.\n\n    Raises:\n        HTTPException: If the underlying HTTP POST request fails.\n    \"\"\"\n    url = f\"http://{settings.controller_host}:{settings.controller_port}/notes/remove\"\n    data = {\"scenario_name\": scenario_name}\n    await controller_post(url, data)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.remove_scenario","title":"<code>remove_scenario(scenario_name=None, user=None)</code>  <code>async</code>","text":"<p>Remove all data and resources associated with a scenario, including nodes, notes, and files.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_name</code> <code>str</code> <p>Name of the scenario to remove.</p> <code>None</code> <code>user</code> <code>str</code> <p>Username associated with the scenario.</p> <code>None</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>async def remove_scenario(scenario_name=None, user=None):\n    \"\"\"\n    Remove all data and resources associated with a scenario, including nodes, notes, and files.\n\n    Parameters:\n        scenario_name (str, optional): Name of the scenario to remove.\n        user (str, optional): Username associated with the scenario.\n\n    Returns:\n        None\n    \"\"\"\n\n    user_data = user_data_store[user]\n\n    if settings.advanced_analytics:\n        logging.info(\"Advanced analytics enabled\")\n    # Remove registered nodes and conditions\n    user_data.nodes_registration.pop(scenario_name, None)\n    await remove_nodes_by_scenario_name(scenario_name)\n    await remove_scenario_by_name(scenario_name)\n    await remove_note(scenario_name)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.remove_scenario_by_name","title":"<code>remove_scenario_by_name(scenario_name)</code>  <code>async</code>","text":"<p>Remove a scenario by name from the controller's records.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_name</code> <code>str</code> <p>Name of the scenario to remove.</p> required <p>Raises:</p> Type Description <code>HTTPException</code> <p>If the underlying HTTP POST request fails.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>async def remove_scenario_by_name(scenario_name):\n    \"\"\"\n    Remove a scenario by name from the controller's records.\n\n    Parameters:\n        scenario_name (str): Name of the scenario to remove.\n\n    Raises:\n        HTTPException: If the underlying HTTP POST request fails.\n    \"\"\"\n    url = f\"http://{settings.controller_host}:{settings.controller_port}/scenarios/remove\"\n    data = {\"scenario_name\": scenario_name}\n    await controller_post(url, data)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.retry_with_backoff","title":"<code>retry_with_backoff(func, *args, max_retries=5, initial_delay=1)</code>  <code>async</code>","text":"<p>Retry a function with exponential backoff.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <p>The async function to retry</p> required <code>*args</code> <p>Arguments to pass to the function</p> <code>()</code> <code>max_retries</code> <p>Maximum number of retry attempts</p> <code>5</code> <code>initial_delay</code> <p>Initial delay between retries in seconds</p> <code>1</code> <p>Returns:</p> Type Description <p>The result of the function if successful</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>async def retry_with_backoff(func, *args, max_retries=5, initial_delay=1):\n    \"\"\"\n    Retry a function with exponential backoff.\n\n    Args:\n        func: The async function to retry\n        *args: Arguments to pass to the function\n        max_retries: Maximum number of retry attempts\n        initial_delay: Initial delay between retries in seconds\n\n    Returns:\n        The result of the function if successful\n\n    Raises:\n        The last exception if all retries fail\n    \"\"\"\n    delay = initial_delay\n    last_exception = None\n\n    for attempt in range(max_retries):\n        try:\n            return await func(*args)\n        except (ClientConnectorError, ClientError) as e:\n            last_exception = e\n            if attempt &lt; max_retries - 1:\n                logging.warning(f\"Connection attempt {attempt + 1} failed: {str(e)}. Retrying in {delay} seconds...\")\n                await asyncio.sleep(delay)\n                delay *= 2  # Exponential backoff\n            else:\n                logging.error(f\"All {max_retries} connection attempts failed\")\n                raise last_exception\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.run_scenario","title":"<code>run_scenario(scenario_data, role, user)</code>  <code>async</code>","text":"<p>Deploy a single scenario (physical / docker / process).</p> <p>\u2022 If it's physical \u2192 wait for all Raspberry nodes to be free. \u2022 Then reserve GPU/CPU based on availability and role. \u2022 Call the controller to launch the scenario. \u2022 Record the necessary info to track node completion.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>async def run_scenario(scenario_data: dict, role: str, user: str) -&gt; None:\n    \"\"\"\n    Deploy a single scenario (physical / docker / process).\n\n    \u2022 If it's physical \u2192 wait for all Raspberry nodes to be free.\n    \u2022 Then reserve GPU/CPU based on availability and role.\n    \u2022 Call the controller to launch the scenario.\n    \u2022 Record the necessary info to track node completion.\n    \"\"\"\n    user_data = user_data_store[user]\n\n    # PHYSICAL  \u279c wait until all nodes are idle\n    is_physical = scenario_data.get(\"deployment\") == \"physical\"\n    phys_ips = scenario_data.get(\"physical_ips\", [])\n\n    if is_physical:\n        wait_start = asyncio.get_event_loop().time()\n        while not await physical_nodes_available(phys_ips):\n            elapsed = int(asyncio.get_event_loop().time() - wait_start)\n            mins, secs = divmod(elapsed, 60)\n            logging.info(\n                f\"[{scenario_data.get('scenario_title', 'Unnamed')}] \"\n                f\"Raspberry nodes busy \u2013 waited {mins:02d}:{secs:02d}; retrying in 10 s\u2026\"\n            )\n            await asyncio.sleep(10)\n\n    # Reserve CPU/GPU based on availability and role\n    scenario_data = await assign_available_gpu(scenario_data, role)\n\n    # Launch on the controller\n    scenario_name = await deploy_scenario(scenario_data, role, user)\n\n    # Register for synchronization\n    user_data.nodes_registration[scenario_name] = {\n        \"n_nodes\": scenario_data[\"n_nodes\"],\n        \"nodes\": set(),\n        \"condition\": asyncio.Condition(),\n    }\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.run_scenarios","title":"<code>run_scenarios(role, user)</code>  <code>async</code>","text":"<p>Sequentially execute all enqueued scenarios for a user, waiting for each to complete and for sufficient resources before starting the next.</p> <p>Parameters:</p> Name Type Description Default <code>role</code> <code>str</code> <p>The role of the user initiating the scenarios.</p> required <code>user</code> <code>str</code> <p>Username associated with the scenarios.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>async def run_scenarios(role, user):\n    \"\"\"\n    Sequentially execute all enqueued scenarios for a user, waiting for each to complete\n    and for sufficient resources before starting the next.\n\n    Parameters:\n        role (str): The role of the user initiating the scenarios.\n        user (str): Username associated with the scenarios.\n\n    Returns:\n        None\n    \"\"\"\n    try:\n        user_data = user_data_store[user]\n\n        for scenario_data in user_data.scenarios_list:\n            user_data.finish_scenario_event.clear()\n            logging.info(f\"Running scenario {scenario_data['scenario_title']}\")\n            await run_scenario(scenario_data, role, user)\n            # Waits till the scenario is completed\n            while not user_data.finish_scenario_event.is_set() and not user_data.stop_all_scenarios_event.is_set():\n                await asyncio.sleep(1)\n\n            # Wait until theres enough resources to launch the next scenario\n            while not await check_enough_resources():\n                await asyncio.sleep(1)\n\n            if user_data.stop_all_scenarios_event.is_set():\n                user_data.stop_all_scenarios_event.clear()\n                user_data.scenarios_list_length = 0\n                return\n            user_data.scenarios_finished += 1\n            await asyncio.sleep(5)\n    finally:\n        user_data.scenarios_list_length = 0\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.save_note_for_scenario","title":"<code>save_note_for_scenario(scenario_name, request, session=Depends(get_session))</code>  <code>async</code>","text":"<p>Save notes for a specific scenario for authenticated users.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_name</code> <code>str</code> <p>Name of the scenario.</p> required <code>request</code> <code>Request</code> <p>FastAPI request object containing JSON payload.</p> required <code>session</code> <code>dict</code> <p>Session data extracted via dependency.</p> <code>Depends(get_session)</code> <p>Returns:</p> Name Type Description <code>JSONResponse</code> <p>{\"status\": \"success\"} on success, or error message on failure.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>@app.post(\"/platform/dashboard/{scenario_name}/save_note\")\nasync def save_note_for_scenario(scenario_name: str, request: Request, session: dict = Depends(get_session)):\n    \"\"\"\n    Save notes for a specific scenario for authenticated users.\n\n    Parameters:\n        scenario_name (str): Name of the scenario.\n        request (Request): FastAPI request object containing JSON payload.\n        session (dict): Session data extracted via dependency.\n\n    Returns:\n        JSONResponse: {\"status\": \"success\"} on success, or error message on failure.\n    \"\"\"\n    if \"user\" in session:\n        data = await request.json()\n        notes = data[\"notes\"]\n        try:\n            await save_notes(scenario_name, notes)\n            return JSONResponse({\"status\": \"success\"})\n        except Exception as e:\n            logging.exception(e)\n            return JSONResponse(\n                {\"status\": \"error\", \"message\": \"Could not save the notes\"},\n                status_code=500,\n            )\n    else:\n        return JSONResponse({\"status\": \"error\", \"message\": \"User not logged in\"}, status_code=401)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.save_notes","title":"<code>save_notes(scenario_name, notes)</code>  <code>async</code>","text":"<p>Save or update notes for a specific scenario on the controller.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_name</code> <code>str</code> <p>Name of the scenario to attach notes to.</p> required <code>notes</code> <code>Any</code> <p>Content of the notes to be saved.</p> required <p>Raises:</p> Type Description <code>HTTPException</code> <p>If the underlying HTTP POST request fails.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>async def save_notes(scenario_name, notes):\n    \"\"\"\n    Save or update notes for a specific scenario on the controller.\n\n    Parameters:\n        scenario_name (str): Name of the scenario to attach notes to.\n        notes (Any): Content of the notes to be saved.\n\n    Raises:\n        HTTPException: If the underlying HTTP POST request fails.\n    \"\"\"\n    url = f\"http://{settings.controller_host}:{settings.controller_port}/notes/update\"\n    data = {\"scenario_name\": scenario_name, \"notes\": notes}\n    await controller_post(url, data)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.scenario_set_status_to_finished","title":"<code>scenario_set_status_to_finished(scenario_name, all=False)</code>  <code>async</code>","text":"<p>Mark one or all scenarios as finished on the controller.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_name</code> <code>str</code> <p>Name of the scenario to update.</p> required <code>all</code> <code>bool</code> <p>If True, mark all scenarios as finished; otherwise only the named one.</p> <code>False</code> <p>Raises:</p> Type Description <code>HTTPException</code> <p>If the underlying HTTP POST request fails.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>async def scenario_set_status_to_finished(scenario_name, all=False):\n    \"\"\"\n    Mark one or all scenarios as finished on the controller.\n\n    Parameters:\n        scenario_name (str): Name of the scenario to update.\n        all (bool): If True, mark all scenarios as finished; otherwise only the named one.\n\n    Raises:\n        HTTPException: If the underlying HTTP POST request fails.\n    \"\"\"\n    url = f\"http://{settings.controller_host}:{settings.controller_port}/scenarios/set_status_to_finished\"\n    data = {\"scenario_name\": scenario_name, \"all\": all}\n    await controller_post(url, data)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.scenario_update_record","title":"<code>scenario_update_record(scenario_name, start_time, end_time, scenario, status, role, username)</code>  <code>async</code>","text":"<p>Update the record of a scenario's execution status on the controller.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_name</code> <code>str</code> <p>Unique name of the scenario.</p> required <code>start_time</code> <code>str</code> <p>ISO-formatted start timestamp.</p> required <code>end_time</code> <code>str</code> <p>ISO-formatted end timestamp.</p> required <code>scenario</code> <code>Any</code> <p>Scenario payload or identifier.</p> required <code>status</code> <code>str</code> <p>New status value (e.g., 'running', 'finished').</p> required <code>role</code> <code>str</code> <p>Role associated with the scenario.</p> required <code>username</code> <code>str</code> <p>User who ran or updated the scenario.</p> required <p>Raises:</p> Type Description <code>HTTPException</code> <p>If the underlying HTTP POST request fails.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>async def scenario_update_record(scenario_name, start_time, end_time, scenario, status, role, username):\n    \"\"\"\n    Update the record of a scenario's execution status on the controller.\n\n    Parameters:\n        scenario_name (str): Unique name of the scenario.\n        start_time (str): ISO-formatted start timestamp.\n        end_time (str): ISO-formatted end timestamp.\n        scenario (Any): Scenario payload or identifier.\n        status (str): New status value (e.g., 'running', 'finished').\n        role (str): Role associated with the scenario.\n        username (str): User who ran or updated the scenario.\n\n    Raises:\n        HTTPException: If the underlying HTTP POST request fails.\n    \"\"\"\n    url = f\"http://{settings.controller_host}:{settings.controller_port}/scenarios/update\"\n    data = {\n        \"scenario_name\": scenario_name,\n        \"start_time\": start_time,\n        \"end_time\": end_time,\n        \"scenario\": scenario,\n        \"status\": status,\n        \"role\": role,\n        \"username\": username,\n    }\n    await controller_post(url, data)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.signal_handler","title":"<code>signal_handler(signal, frame)</code>  <code>async</code>","text":"<p>Asynchronous signal handler for Ctrl+C (SIGINT) in the frontend.</p> <p>Logs the interrupt event, schedules all scenarios to be marked as finished by creating an asyncio task for <code>scenario_set_status_to_finished(all=True)</code>, and then exits the process.</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>int</code> <p>The signal number received (e.g., signal.SIGINT).</p> required <code>frame</code> <code>FrameType</code> <p>The current stack frame at the moment the signal was handled.</p> required Source code in <code>nebula/frontend/app.py</code> <pre><code>async def signal_handler(signal, frame):\n    \"\"\"\n    Asynchronous signal handler for Ctrl+C (SIGINT) in the frontend.\n\n    Logs the interrupt event, schedules all scenarios to be marked as finished by creating an asyncio task for `scenario_set_status_to_finished(all=True)`, and then exits the process.\n\n    Parameters:\n        signal (int): The signal number received (e.g., signal.SIGINT).\n        frame (types.FrameType): The current stack frame at the moment the signal was handled.\n    \"\"\"\n    logging.info(\"You pressed Ctrl+C [frontend]!\")\n    asyncio.get_event_loop().create_task(scenario_set_status_to_finished(all=True))\n    sys.exit(0)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.statistics_proxy","title":"<code>statistics_proxy(request, path=None, session=Depends(get_session))</code>  <code>async</code>","text":"<p>Proxy requests to the TensorBoard backend to fetch experiment statistics, rewriting URLs and filtering headers as needed.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>FastAPI request object with original headers, cookies, and body.</p> required <code>path</code> <code>str</code> <p>Specific TensorBoard sub-path to proxy; defaults to None.</p> <code>None</code> <code>session</code> <code>dict</code> <p>Session data extracted via dependency.</p> <code>Depends(get_session)</code> <p>Returns:</p> Name Type Description <code>Response</code> <p>The proxied TensorBoard response with adjusted headers and content.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>401 Unauthorized if the user is not authenticated.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>@app.api_route(\"/platform/statistics/\", methods=[\"GET\", \"POST\"])\n@app.api_route(\"/platform/statistics/{path:path}\", methods=[\"GET\", \"POST\"])\nasync def statistics_proxy(request: Request, path: str = None, session: dict = Depends(get_session)):\n    \"\"\"\n    Proxy requests to the TensorBoard backend to fetch experiment statistics,\n    rewriting URLs and filtering headers as needed.\n\n    Parameters:\n        request (Request): FastAPI request object with original headers, cookies, and body.\n        path (str, optional): Specific TensorBoard sub-path to proxy; defaults to None.\n        session (dict): Session data extracted via dependency.\n\n    Returns:\n        Response: The proxied TensorBoard response with adjusted headers and content.\n\n    Raises:\n        HTTPException: 401 Unauthorized if the user is not authenticated.\n    \"\"\"\n    if \"user\" in session:\n        query_string = urlencode(request.query_params)\n\n        url = \"http://localhost:8080\"\n        tensorboard_url = f\"{url}{('/' + path) if path else ''}\" + (\"?\" + query_string if query_string else \"\")\n\n        headers = {key: value for key, value in request.headers.items() if key.lower() != \"host\"}\n\n        response = requests.request(\n            method=request.method,\n            url=tensorboard_url,\n            headers=headers,\n            data=await request.body(),\n            cookies=request.cookies,\n            allow_redirects=False,\n        )\n\n        excluded_headers = [\n            \"content-encoding\",\n            \"content-length\",\n            \"transfer-encoding\",\n            \"connection\",\n        ]\n\n        filtered_headers = [\n            (name, value) for name, value in response.raw.headers.items() if name.lower() not in excluded_headers\n        ]\n\n        if \"text/html\" in response.headers[\"Content-Type\"]:\n            content = response.text\n            content = content.replace(\"url(/\", \"url(/platform/statistics/\")\n            content = content.replace('src=\"/', 'src=\"/platform/statistics/')\n            content = content.replace('href=\"/', 'href=\"/platform/statistics/')\n            response = Response(content, response.status_code, dict(filtered_headers))\n            return response\n\n        if path and path.endswith(\".js\"):\n            content = response.text\n            content = content.replace(\n                \"experiment/${s}/data/plugin\",\n                \"nebula/statistics/experiment/${s}/data/plugin\",\n            )\n            response = Response(content, response.status_code, dict(filtered_headers))\n            return response\n\n        return Response(response.content, response.status_code, dict(filtered_headers))\n\n    else:\n        raise HTTPException(status_code=401)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.stop_scenario_by_name","title":"<code>stop_scenario_by_name(scenario_name, username, all=False)</code>  <code>async</code>","text":"<p>Stops a running scenario via the NEBULA controller.</p> <p>This function sends an HTTP POST request to the controller to stop a specific scenario. It can stop only the nodes associated with a particular user, or all nodes in the scenario if specified.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_name</code> <code>str</code> <p>The name of the scenario to be stopped.</p> required <code>username</code> <code>str</code> <p>The username requesting the scenario to be stopped.</p> required <code>all</code> <code>bool</code> <p>If True, stops all nodes in the scenario regardless of the user. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>Response from the controller indicating the result of the operation.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>If the request to the controller fails or returns an error.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>async def stop_scenario_by_name(scenario_name, username, all=False):\n    \"\"\"\n    Stops a running scenario via the NEBULA controller.\n\n    This function sends an HTTP POST request to the controller to stop a specific scenario.\n    It can stop only the nodes associated with a particular user, or all nodes in the scenario\n    if specified.\n\n    Args:\n        scenario_name (str): The name of the scenario to be stopped.\n        username (str): The username requesting the scenario to be stopped.\n        all (bool, optional): If True, stops all nodes in the scenario regardless of the user.\n            Defaults to False.\n\n    Returns:\n        dict: Response from the controller indicating the result of the operation.\n\n    Raises:\n        HTTPException: If the request to the controller fails or returns an error.\n    \"\"\"\n    url = f\"http://{settings.controller_host}:{settings.controller_port}/scenarios/stop\"\n    data = {\"scenario_name\": scenario_name, \"username\": username, \"all\": all}\n    return await controller_post(url, data)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.update_node_record","title":"<code>update_node_record(uid, idx, ip, port, role, neighbors, latitude, longitude, timestamp, federation, round_number, scenario_name, run_hash, malicious)</code>  <code>async</code>","text":"<p>Update the record of a node's state on the controller.</p> <p>Parameters:</p> Name Type Description Default <code>uid</code> <code>str</code> <p>Unique node identifier.</p> required <code>idx</code> <code>int</code> <p>Node index within the scenario.</p> required <code>ip</code> <code>str</code> <p>Node IP address.</p> required <code>port</code> <code>int</code> <p>Node port number.</p> required <code>role</code> <code>str</code> <p>Node role in the scenario.</p> required <code>neighbors</code> <code>Any</code> <p>Neighboring node references.</p> required <code>latitude</code> <code>float</code> <p>Node's latitude coordinate.</p> required <code>longitude</code> <code>float</code> <p>Node's longitude coordinate.</p> required <code>timestamp</code> <code>str</code> <p>ISO-formatted timestamp of the update.</p> required <code>federation</code> <code>str</code> <p>Federation identifier.</p> required <code>round_number</code> <code>int</code> <p>Current round number in the scenario.</p> required <code>scenario_name</code> <code>str</code> <p>Name of the scenario.</p> required <code>run_hash</code> <code>str</code> <p>Unique hash for this scenario run.</p> required <code>malicious</code> <code>bool</code> <p>Flag indicating if the node is malicious.</p> required <p>Raises:</p> Type Description <code>HTTPException</code> <p>If the underlying HTTP POST request fails.</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>async def update_node_record(\n    uid,\n    idx,\n    ip,\n    port,\n    role,\n    neighbors,\n    latitude,\n    longitude,\n    timestamp,\n    federation,\n    round_number,\n    scenario_name,\n    run_hash,\n    malicious,\n):\n    \"\"\"\n    Update the record of a node's state on the controller.\n\n    Parameters:\n        uid (str): Unique node identifier.\n        idx (int): Node index within the scenario.\n        ip (str): Node IP address.\n        port (int): Node port number.\n        role (str): Node role in the scenario.\n        neighbors (Any): Neighboring node references.\n        latitude (float): Node's latitude coordinate.\n        longitude (float): Node's longitude coordinate.\n        timestamp (str): ISO-formatted timestamp of the update.\n        federation (str): Federation identifier.\n        round_number (int): Current round number in the scenario.\n        scenario_name (str): Name of the scenario.\n        run_hash (str): Unique hash for this scenario run.\n        malicious (bool): Flag indicating if the node is malicious.\n\n    Raises:\n        HTTPException: If the underlying HTTP POST request fails.\n    \"\"\"\n    url = f\"http://{settings.controller_host}:{settings.controller_port}/nodes/update\"\n    data = {\n        \"node_uid\": uid,\n        \"node_idx\": idx,\n        \"node_ip\": ip,\n        \"node_port\": port,\n        \"node_role\": role,\n        \"node_neighbors\": neighbors,\n        \"node_latitude\": latitude,\n        \"node_longitude\": longitude,\n        \"node_timestamp\": timestamp,\n        \"node_federation\": federation,\n        \"node_round\": round_number,\n        \"node_scenario_name\": scenario_name,\n        \"node_run_hash\": run_hash,\n        \"malicious\": malicious,\n    }\n    await controller_post(url, data)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.update_user","title":"<code>update_user(user, password, role)</code>  <code>async</code>","text":"<p>Update an existing user's credentials and role via the controller endpoint.</p> <p>Parameters: - user (str): The username of the user to update. - password (str): The new password for the user. - role (str): The new role to assign to the user.</p> <p>Returns: - None</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>async def update_user(user, password, role):\n    \"\"\"\n    Update an existing user's credentials and role via the controller endpoint.\n\n    Parameters:\n    - user (str): The username of the user to update.\n    - password (str): The new password for the user.\n    - role (str): The new role to assign to the user.\n\n    Returns:\n    - None\n    \"\"\"\n    url = f\"http://{settings.controller_host}:{settings.controller_port}/user/update\"\n    data = {\"user\": user, \"password\": password, \"role\": role}\n    await controller_post(url, data)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.verify_user","title":"<code>verify_user(username, password)</code>  <code>async</code>","text":"<p>Verify user credentials with the controller.</p> <p>Parameters:</p> Name Type Description Default <code>username</code> <code>str</code> <p>The username to verify</p> required <code>password</code> <code>str</code> <p>The password to verify</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>bool</code> <p>User data if credentials are valid, False otherwise</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>If there's an error communicating with the controller</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>async def verify_user(username: str, password: str) -&gt; bool:\n    \"\"\"\n    Verify user credentials with the controller.\n\n    Args:\n        username (str): The username to verify\n        password (str): The password to verify\n\n    Returns:\n        dict: User data if credentials are valid, False otherwise\n\n    Raises:\n        HTTPException: If there's an error communicating with the controller\n    \"\"\"\n    try:\n        url = f\"http://{settings.controller_host}:{settings.controller_port}/user/verify\"\n        logging.info(f\"Verifying user {username} with controller at {url}\")\n\n        response = await controller_post(url, {\"user\": username, \"password\": password})\n        logging.info(f\"Controller response for user {username}: {response}\")\n\n        if not isinstance(response, dict):\n            logging.error(f\"Invalid response format from controller: {response}\")\n            return False\n\n        # Return the user data if verification was successful\n        if response.get(\"user\") and response.get(\"role\"):\n            return response\n        return False\n\n    except HTTPException as e:\n        logging.error(f\"HTTP error verifying user {username}: {str(e)}\")\n        raise\n    except Exception as e:\n        logging.error(f\"Unexpected error verifying user {username}: {str(e)}\")\n        raise HTTPException(status_code=500, detail=f\"Error verifying credentials: {str(e)}\")\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.wait_for_enough_ram","title":"<code>wait_for_enough_ram()</code>  <code>async</code>","text":"<p>Asynchronously wait until the host's memory usage falls below 80% of its initial measurement.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>async def wait_for_enough_ram():\n    \"\"\"\n    Asynchronously wait until the host's memory usage falls below 80% of its initial measurement.\n\n    Returns:\n        None\n    \"\"\"\n    resources = await get_host_resources()\n    initial_ram = resources.get(\"memory_percent\")\n\n    desired_ram = initial_ram * 0.8\n\n    while True:\n        resources = await get_host_resources()\n        actual_ram = resources.get(\"memory_percent\")\n\n        if actual_ram &lt;= desired_ram:\n            break\n\n        await asyncio.sleep(1)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.websocket_endpoint","title":"<code>websocket_endpoint(websocket, client_id)</code>  <code>async</code>","text":"<p>WebSocket endpoint for real-time chat at /platform/ws/{client_id}.</p> <p>Parameters:</p> Name Type Description Default <code>websocket</code> <code>WebSocket</code> <p>The client's WebSocket connection instance.</p> required <code>client_id</code> <code>int</code> <p>Unique identifier for the connecting client.</p> required Functionality <ul> <li>On connection: registers the client via manager.connect(websocket).</li> <li>Message loop: awaits incoming text frames, wraps each in a control message including the client_id, and broadcasts to all active clients using manager.broadcast().</li> <li>On WebSocketDisconnect: deregisters the client via manager.disconnect(websocket) and broadcasts a \"client left\" control message.</li> <li>Error handling: logs exceptions during broadcast or any unexpected WebSocket errors, ensuring the connection is cleaned up on failure.</li> </ul> Source code in <code>nebula/frontend/app.py</code> <pre><code>@app.websocket(\"/platform/ws/{client_id}\")\nasync def websocket_endpoint(websocket: WebSocket, client_id: int):\n    \"\"\"\n    WebSocket endpoint for real-time chat at /platform/ws/{client_id}.\n\n    Parameters:\n        websocket (WebSocket): The client's WebSocket connection instance.\n        client_id (int): Unique identifier for the connecting client.\n\n    Functionality:\n        - On connection: registers the client via manager.connect(websocket).\n        - Message loop: awaits incoming text frames, wraps each in a control message including the client_id, and broadcasts to all active clients using manager.broadcast().\n        - On WebSocketDisconnect: deregisters the client via manager.disconnect(websocket) and broadcasts a \"client left\" control message.\n        - Error handling: logs exceptions during broadcast or any unexpected WebSocket errors, ensuring the connection is cleaned up on failure.\n    \"\"\"\n    await manager.connect(websocket)\n    try:\n        while True:\n            data = await websocket.receive_text()\n            message = {\n                \"type\": \"control\",\n                \"message\": f\"Client #{client_id} says: {data}\",\n            }\n            try:\n                await manager.broadcast(json.dumps(message))\n            except Exception as e:\n                logging.exception(f\"Error broadcasting message: {e}\")\n    except WebSocketDisconnect:\n        manager.disconnect(websocket)\n        try:\n            message = {\"type\": \"control\", \"message\": f\"Client #{client_id} left the chat\"}\n            await manager.broadcast(json.dumps(message))\n        except Exception as e:\n            logging.exception(f\"Error broadcasting disconnect message: {e}\")\n    except Exception as e:\n        logging.exception(f\"WebSocket error: {e}\")\n        manager.disconnect(websocket)\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.zipdir","title":"<code>zipdir(path, ziph)</code>","text":"<p>Recursively add all files from a directory into a zip archive.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The root directory whose contents will be zipped.</p> required <code>ziph</code> <code>ZipFile</code> <p>An open ZipFile handle to which files will be written.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>def zipdir(path, ziph):\n    \"\"\"\n    Recursively add all files from a directory into a zip archive.\n\n    Parameters:\n        path (str): The root directory whose contents will be zipped.\n        ziph (zipfile.ZipFile): An open ZipFile handle to which files will be written.\n\n    Returns:\n        None\n    \"\"\"\n    # ziph is zipfile handle\n    for root, _, files in os.walk(path):\n        for file in files:\n            ziph.write(\n                os.path.join(root, file),\n                os.path.relpath(os.path.join(root, file), os.path.join(path, \"..\")),\n            )\n</code></pre>"},{"location":"api/physical/","title":"Documentation for Physical Module","text":""},{"location":"api/physical/api/","title":"Documentation for Api Module","text":""},{"location":"api/physical/api/#nebula.physical.api.delete_certs","title":"<code>delete_certs()</code>","text":"<p>Remove all certificate files from the system.</p> Source code in <code>nebula/physical/api.py</code> <pre><code>@app.route(\"/certs/\", methods=[\"DELETE\"])\ndef delete_certs():\n    \"\"\"Remove **all** certificate files from the system.\"\"\"\n    certs_files = _find_x_files(CERTS_FOLDER, \".cert\")\n    removed: Dict[str, str] = {}\n    for fn in certs_files:\n        os.remove(fn)\n        removed[fn] = \"deleted\"\n    return jsonify(removed)\n</code></pre>"},{"location":"api/physical/api/#nebula.physical.api.delete_config","title":"<code>delete_config()</code>","text":"<p>Remove the config .json from the given run directory.</p> Source code in <code>nebula/physical/api.py</code> <pre><code>@app.route(\"/config/\", methods=[\"DELETE\"])\ndef delete_config():\n    \"\"\"Remove the config *.json* from the given run directory.\"\"\"\n    path = request.args.get(\"path\", \"\")\n    if _LFI_sentry(path):\n        _json_abort(404, \"Item not found\")\n\n    json_files = _find_x_files(os.path.join(CONFIG_FOLDER, path))\n    if len(json_files) != CONFIG_FILE_COUNT:\n        _json_abort(404, \"Item not found\")\n\n    fn = json_files.pop()\n    os.remove(fn)\n    return jsonify(filename=fn)\n</code></pre>"},{"location":"api/physical/api/#nebula.physical.api.delete_dataset","title":"<code>delete_dataset()</code>","text":"<p>Delete both dataset .h5 files from the specified run directory.</p> Source code in <code>nebula/physical/api.py</code> <pre><code>@app.route(\"/dataset/\", methods=[\"DELETE\"])\ndef delete_dataset():\n    \"\"\"Delete both dataset *.h5* files from the specified run directory.\"\"\"\n    path = request.args.get(\"path\", \"\")\n    if _LFI_sentry(path):\n        _json_abort(404, \"Item not found\")\n\n    data_files = _find_x_files(os.path.join(CONFIG_FOLDER, path), \".h5\")\n    if len(data_files) != DATASET_FILE_COUNT:\n        _json_abort(404, \"Item not found\")\n\n    removed: Dict[str, str] = {}\n    for fn in data_files:\n        os.remove(fn)\n        removed[fn] = \"deleted\"\n    return jsonify(removed)\n</code></pre>"},{"location":"api/physical/api/#nebula.physical.api.delete_logs","title":"<code>delete_logs()</code>","text":"<p>Delete the main .log for the requested run.</p> Source code in <code>nebula/physical/api.py</code> <pre><code>@app.route(\"/get_logs/\", methods=[\"DELETE\"])\ndef delete_logs():\n    \"\"\"Delete the main *.log* for the requested run.\"\"\"\n    path = request.args.get(\"path\", \"\")\n    if _LFI_sentry(path):\n        _json_abort(404, \"Item not found\")\n\n    log_files = _find_x_files(os.path.join(LOGS_FOLDER, path), \".log\")\n    if not log_files:\n        _json_abort(404, \"Log file not found\")\n\n    target = min(log_files, key=lambda x: len(os.path.basename(x)))\n    os.remove(target)\n    return jsonify(filename=target)\n</code></pre>"},{"location":"api/physical/api/#nebula.physical.api.get_certs","title":"<code>get_certs()</code>","text":"<p>Download every .cert file in a ZIP archive.</p> Source code in <code>nebula/physical/api.py</code> <pre><code>@app.route(\"/certs/\", methods=[\"GET\"])\ndef get_certs():\n    \"\"\"Download every *.cert* file in a ZIP archive.\"\"\"\n    certs_files = _find_x_files(CERTS_FOLDER, \".cert\")\n    if not certs_files:\n        _json_abort(404, \"No cert files found\")\n\n    buf = io.BytesIO()\n    with zipfile.ZipFile(buf, \"w\", zipfile.ZIP_DEFLATED) as zf:\n        for f in certs_files:\n            zf.write(f, arcname=os.path.basename(f))\n    buf.seek(0)\n\n    return send_file(buf, mimetype=\"application/zip\",\n                     download_name=\"certs.zip\", as_attachment=True)\n</code></pre>"},{"location":"api/physical/api/#nebula.physical.api.get_config","title":"<code>get_config()</code>","text":"<p>Return the single .json config file for the requested run.</p> Source code in <code>nebula/physical/api.py</code> <pre><code>@app.route(\"/config/\", methods=[\"GET\"])\ndef get_config():\n    \"\"\"Return the single *.json* config file for the requested run.\"\"\"\n    path = request.args.get(\"path\", \"\")\n    if _LFI_sentry(path):\n        _json_abort(404, \"Item not found\")\n\n    json_files = _find_x_files(os.path.join(CONFIG_FOLDER, path))\n    if len(json_files) != CONFIG_FILE_COUNT:\n        _json_abort(404, \"Item not found\")\n\n    return send_file(json_files.pop(), mimetype=\"application/json\",\n                     as_attachment=True)\n</code></pre>"},{"location":"api/physical/api/#nebula.physical.api.get_dataset","title":"<code>get_dataset()</code>","text":"<p>Deliver both .h5 datasets as a single ZIP archive.</p> <p>Returning a single payload simplifies transfer, cache-control and client code compared to sending two independent responses.</p> Source code in <code>nebula/physical/api.py</code> <pre><code>@app.route(\"/dataset/\", methods=[\"GET\"])\ndef get_dataset():\n    \"\"\"\n    Deliver both *.h5* datasets as a single ZIP archive.\n\n    Returning a single payload simplifies transfer, cache-control and client\n    code compared to sending two independent responses.\n    \"\"\"\n    path = request.args.get(\"path\", \"\")\n    if _LFI_sentry(path):\n        _json_abort(404, \"Item not found\")\n\n    h5_files = _find_x_files(os.path.join(CONFIG_FOLDER, path), \".h5\")\n    if len(h5_files) != DATASET_FILE_COUNT:\n        _json_abort(404, \"Item not found\")\n\n    buf = io.BytesIO()\n    with zipfile.ZipFile(buf, \"w\", zipfile.ZIP_DEFLATED) as zf:\n        for f in h5_files:\n            zf.write(f, arcname=os.path.basename(f))\n    buf.seek(0)\n\n    return send_file(buf, mimetype=\"application/zip\",\n                     download_name=\"dataset.zip\", as_attachment=True)\n</code></pre>"},{"location":"api/physical/api/#nebula.physical.api.get_logs","title":"<code>get_logs()</code>","text":"<p>Download the main .log produced during training.</p> Source code in <code>nebula/physical/api.py</code> <pre><code>@app.route(\"/get_logs/\", methods=[\"GET\"])\ndef get_logs():\n    \"\"\"Download the main *.log* produced during training.\"\"\"\n    path = request.args.get(\"path\", \"\")\n    if _LFI_sentry(path):\n        _json_abort(404, \"Item not found\")\n    return _send_single_log(os.path.join(LOGS_FOLDER, path), \".log\")\n</code></pre>"},{"location":"api/physical/api/#nebula.physical.api.get_metrics","title":"<code>get_metrics()</code>","text":"<p>Bundle every file under METRICS_FOLDER into a ZIP archive.</p> Source code in <code>nebula/physical/api.py</code> <pre><code>@app.route(\"/metrics/\", methods=[\"GET\"])\ndef get_metrics():\n    \"\"\"Bundle every file under *METRICS_FOLDER* into a ZIP archive.\"\"\"\n    log_files = _find_x_files(METRICS_FOLDER, \"\")\n    if not log_files:\n        _json_abort(404, \"Log file not found\")\n\n    buf = io.BytesIO()\n    with zipfile.ZipFile(buf, \"w\", zipfile.ZIP_DEFLATED) as zf:\n        for f in log_files:\n            zf.write(f, arcname=os.path.basename(f))\n    buf.seek(0)\n\n    return send_file(buf, mimetype=\"application/zip\",\n                     download_name=\"metrics.zip\", as_attachment=True)\n</code></pre>"},{"location":"api/physical/api/#nebula.physical.api.run","title":"<code>run()</code>","text":"<p>Spawn the federated training process (once).</p>"},{"location":"api/physical/api/#nebula.physical.api.run--returns","title":"Returns","text":"<p>JSON {pid, state}</p> Source code in <code>nebula/physical/api.py</code> <pre><code>@app.route(\"/run/\", methods=[\"GET\"])\ndef run():\n    \"\"\"\n    Spawn the federated training process (once).\n\n    Returns\n    -------\n    JSON {pid, state}\n    \"\"\"\n    json_files = _find_x_files(CONFIG_FOLDER)\n    if len(json_files) != CONFIG_FILE_COUNT:\n        _json_abort(404, \"Config file not found\")\n\n    global TRAINING_PROC\n    if TRAINING_PROC and TRAINING_PROC.poll() is None:\n        _json_abort(409, \"Training already running\")\n\n    cmd = [\"python\", \"/home/dietpi/prueba/nebula/nebula/node.py\", json_files[0]]\n    TRAINING_PROC = subprocess.Popen(cmd)\n\n    return jsonify(pid=TRAINING_PROC.pid, state=\"running\")\n</code></pre>"},{"location":"api/physical/api/#nebula.physical.api.set_cert","title":"<code>set_cert()</code>","text":"<p>Upload one .cert file to the global certificates folder.</p> Source code in <code>nebula/physical/api.py</code> <pre><code>@app.route(\"/certs/\", methods=[\"PUT\"])\ndef set_cert():\n    \"\"\"Upload one *.cert* file to the global certificates folder.\"\"\"\n    if \"cert\" not in request.files:\n        _json_abort(400, \"Missing file field 'cert'\")\n\n    uploaded = request.files[\"cert\"]\n    dst = os.path.join(CERTS_FOLDER, uploaded.filename)\n    uploaded.save(dst)\n\n    return jsonify(filename=uploaded.filename), 201\n</code></pre>"},{"location":"api/physical/api/#nebula.physical.api.set_config","title":"<code>set_config()</code>","text":"<p>Upload a config .json for the provided run directory.</p> Source code in <code>nebula/physical/api.py</code> <pre><code>@app.route(\"/config/\", methods=[\"PUT\"])\ndef set_config():\n    \"\"\"Upload a config *.json* for the provided run directory.\"\"\"\n    if \"config\" not in request.files:\n        _json_abort(400, \"Missing file field 'config'\")\n\n    path = request.args.get(\"path\", \"\")\n    if _LFI_sentry(path):\n        _json_abort(404, \"Item not found\")\n\n    os.makedirs(os.path.join(CONFIG_FOLDER, path), exist_ok=True)\n\n    uploaded = request.files[\"config\"]\n    dst = os.path.join(CONFIG_FOLDER, path, uploaded.filename)\n    uploaded.save(dst)\n\n    return jsonify(filename=uploaded.filename), 201\n</code></pre>"},{"location":"api/physical/api/#nebula.physical.api.set_dataset","title":"<code>set_dataset()</code>","text":"<p>Upload the pair of train/test .h5 files for a run.</p> Source code in <code>nebula/physical/api.py</code> <pre><code>@app.route(\"/dataset/\", methods=[\"PUT\"])\ndef set_dataset():\n    \"\"\"Upload the pair of train/test *.h5* files for a run.\"\"\"\n    missing = [fld for fld in (\"dataset\", \"dataset_p\") if fld not in request.files]\n    if missing:\n        _json_abort(400, f\"Missing file field(s): {', '.join(missing)}\")\n\n    path = request.args.get(\"path\", \"\")\n    if _LFI_sentry(path):\n        _json_abort(404, \"Item not found\")\n\n    os.makedirs(os.path.join(CONFIG_FOLDER, path), exist_ok=True)\n    stored: List[str] = []\n\n    for fld in (\"dataset\", \"dataset_p\"):\n        up = request.files[fld]\n        dst = os.path.join(CONFIG_FOLDER, path, up.filename)\n        up.save(dst)\n        stored.append(up.filename)\n\n    return jsonify(stored), 201\n</code></pre>"},{"location":"api/physical/api/#nebula.physical.api.setup_new_run","title":"<code>setup_new_run()</code>","text":"<p>Prepare a new federated-learning round.</p>"},{"location":"api/physical/api/#nebula.physical.api.setup_new_run--expected-multipart-form-fields","title":"Expected multipart-form fields","text":"<ul> <li>config     \u2013 JSON with scenario, network and security arguments  </li> <li>global_test \u2013 shared evaluation dataset (<code>*.h5</code>)  </li> <li>train_set   \u2013 participant-specific training dataset (<code>*.h5</code>)</li> </ul> <p>The function rewrites paths inside config, validates neighbour IPs through Tailscale, deletes previous artefacts and finally stores the new trio of files.</p> Source code in <code>nebula/physical/api.py</code> <pre><code>@app.route(\"/setup/\", methods=[\"PUT\"])\ndef setup_new_run():\n    \"\"\"\n    Prepare a **new** federated-learning round.\n\n    Expected multipart-form fields\n    -------------------------------\n    * **config**     \u2013 JSON with scenario, network and security arguments  \n    * **global_test** \u2013 shared evaluation dataset (`*.h5`)  \n    * **train_set**   \u2013 participant-specific training dataset (`*.h5`)\n\n    The function rewrites paths inside *config*, validates neighbour IPs\n    through Tailscale, deletes previous artefacts and finally stores the new\n    trio of files.\n    \"\"\"\n    # 1 \u00b7 Refuse while a training task is still running\n    global TRAINING_PROC\n    if TRAINING_PROC and TRAINING_PROC.poll() is None:\n        _json_abort(409, \"Training already running; pause or stop it first.\")\n\n    # 2 \u00b7 Check field presence\n    missing = [x for x in (\"config\", \"global_test\", \"train_set\")\n               if x not in request.files]\n    if missing:\n        _json_abort(400, f\"Missing file field(s): {', '.join(missing)}\")\n\n    config_up   = request.files[\"config\"]\n    global_test = request.files[\"global_test\"]\n    train_set   = request.files[\"train_set\"]\n\n    # 3 \u00b7 Extension sanity\n    if not config_up.filename.endswith(\".json\"):\n        _json_abort(400, f\"`{config_up.filename}` must have a .json extension.\")\n    for ds in (global_test, train_set):\n        if not ds.filename.endswith(\".h5\"):\n            _json_abort(400, f\"`{ds.filename}` must have a .h5 extension.\")\n\n    # 4 \u00b7 Parse + patch JSON\n    try:\n        original_cfg = json.load(config_up)\n    except Exception as exc:  # broad \u2013 any parsing failure should abort\n        _json_abort(400, f\"Invalid JSON file: {exc}\")\n\n    # Update tracking / security paths to local folders\n    tracking = original_cfg.get(\"tracking_args\", {})\n    tracking[\"log_dir\"]    = LOGS_FOLDER.rstrip(\"/\")\n    tracking[\"config_dir\"] = CONFIG_FOLDER.rstrip(\"/\")\n    original_cfg[\"tracking_args\"] = tracking\n\n    sec = original_cfg.get(\"security_args\", {})\n    for key in (\"certfile\", \"keyfile\", \"cafile\"):\n        if key in sec and sec[key]:\n            sec[key] = os.path.join(CERTS_FOLDER.rstrip(\"/\"),\n                                    os.path.basename(sec[key]))\n    original_cfg[\"security_args\"] = sec\n\n    # 5 \u00b7 (May be removed) Check neighbour reachability via Tailscale\n    neigh_str = original_cfg.get(\"network_args\", {}).get(\"neighbors\", \"\").strip()\n    requested_ips: Set[str] = {re.split(r\":\", n)[0] for n in neigh_str.split() if n}\n\n    if requested_ips:\n        try:\n            ts_out = subprocess.run(\n                [\"tailscale\", \"status\", \"--json\"],\n                capture_output=True, text=True, check=True,\n            )\n            ts_status = json.loads(ts_out.stdout)\n            reachable: Set[str] = set(ts_status.get(\"Self\", {}).get(\"TailscaleIPs\", []))\n            for peer in ts_status.get(\"Peer\", {}).values():\n                reachable.update(peer.get(\"TailscaleIPs\", []))\n        except Exception as exc:\n            _json_abort(400, f\"Could not verify neighbours via Tailscale: {exc}\")\n\n        missing = sorted(ip for ip in requested_ips if ip not in reachable)\n        if missing:\n            _json_abort(400, f\"Neighbour IP(s) not reachable: {', '.join(missing)}\")\n\n    # 6 \u00b7 Clean previous JSON/H5 artefacts\n    for fn in os.listdir(CONFIG_FOLDER):\n        if fn.endswith((\".json\", \".h5\")):\n            try:\n                os.remove(os.path.join(CONFIG_FOLDER, fn))\n            except OSError:\n                pass\n    if any(fn.endswith((\".json\", \".h5\")) for fn in os.listdir(CONFIG_FOLDER)):\n        _json_abort(400, \"Could not delete old JSON/H5 files.\")\n\n    # 7 \u00b7 Persist patched JSON\n    json_dest = os.path.join(CONFIG_FOLDER, config_up.filename)\n    with open(json_dest, \"wb\") as dst:\n        dst.write(json.dumps(original_cfg, indent=2).encode())\n\n    # 8 \u00b7 Persist datasets\n    saved = [config_up.filename]\n    for up in (global_test, train_set):\n        dst = os.path.join(CONFIG_FOLDER, up.filename)\n        up.save(dst)\n        saved.append(up.filename)\n\n    # 9 \u00b7 Purge previous log files\n    for root, _, files in os.walk(LOGS_FOLDER):\n        for fn in files:\n            if fn.endswith(\".log\"):\n                try:\n                    os.remove(os.path.join(root, fn))\n                except OSError:\n                    pass\n    if any(fn.endswith(\".log\") for _, _, fns in os.walk(LOGS_FOLDER) for fn in fns):\n        _json_abort(400, \"Could not delete old log files.\")\n\n    return jsonify(saved), 201\n</code></pre>"},{"location":"api/physical/api/#nebula.physical.api.stop","title":"<code>stop()</code>","text":"<p>Terminate the running training process (SIGTERM) and wait for it.</p> Source code in <code>nebula/physical/api.py</code> <pre><code>@app.route(\"/stop/\", methods=[\"GET\"])\ndef stop():\n    \"\"\"Terminate the running training process (SIGTERM) and wait for it.\"\"\"\n    global TRAINING_PROC\n    if not TRAINING_PROC or TRAINING_PROC.poll() is not None:\n        _json_abort(404, \"No training running\")\n\n    TRAINING_PROC.send_signal(signal.SIGTERM)\n    TRAINING_PROC.wait()\n    pid = TRAINING_PROC.pid\n    TRAINING_PROC = None\n\n    return jsonify(pid=pid, state=\"stopped\")\n</code></pre>"}]}