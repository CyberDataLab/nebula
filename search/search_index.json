{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p> NEBULA: A Platform for Decentralized Federated Learning <p> nebula-dfl.com |      nebula-dfl.eu |     federatedlearning.inf.um.es </p> </p>"},{"location":"#about-nebula","title":"\ud83c\udf0c About NEBULA","text":"<p>NEBULA (previously known as Fedstellar<sup>1</sup>) is a cutting-edge platform designed to facilitate the training of federated models within both centralized and decentralized architectures. It streamlines the development, deployment, and management of federated applications across physical and virtualized devices.</p> <p>NEBULA is developed by Enrique Tom\u00e1s Mart\u00ednez Beltr\u00e1n in collaboration with the University of Murcia, armasuisse, and the University of Zurich.</p> <p> </p>"},{"location":"#key-components","title":"\ud83d\ude80 Key Components","text":"<p>NEBULA boasts a modular architecture that consists of three core elements:</p> <ul> <li>Frontend: A user-friendly interface for setting up experiments and monitoring progress.</li> <li>Controller: An orchestrator that ensures efficient operation management.</li> <li>Core: The fundamental component deployed on each device to handle federated learning processes.</li> </ul>"},{"location":"#main-features","title":"\ud83c\udf1f Main Features","text":"<ul> <li>Decentralized: Train models without a central server, leveraging decentralized federated learning.</li> <li>Privacy-preserving: Maintain data privacy by training on-device and only sharing model updates.</li> <li>Topology-agnostic: Support for various network topologies including star, ring, and mesh.</li> <li>Model-agnostic: Compatible with a wide range of machine learning algorithms, from deep learning to traditional methods.</li> <li>Network communication: Secure and efficient device communication with features like compression, network failure tolerance, and condition simulation.</li> <li>Trustworthiness: Ensure the integrity of the learning process by verifying the reliability of the federation.</li> <li>Blockchain integration: Support for blockchain technologies to enhance security and transparency.</li> <li>Security: Implement security mechanisms to protect the learning process from adversarial attacks.</li> <li>Real-time monitoring: Provides live performance metrics and visualizations during the learning process.</li> </ul>"},{"location":"#scenario-applications","title":"\ud83c\udf0d Scenario Applications","text":"<ul> <li>\ud83c\udfe5 Healthcare: Train models on medical devices such as wearables, smartphones, and sensors.</li> <li>\ud83c\udfed Industry 4.0: Implement on industrial devices like robots, drones, and constrained devices.</li> <li>\ud83d\udcf1 Mobile services: Optimize for mobile devices including smartphones, tablets, and laptops.</li> <li>\ud83d\udee1\ufe0f Military: Apply to military equipment such as drones, robots, and sensors.</li> <li>\ud83d\ude97 Vehicular scenarios: Utilize in vehicles including cars, trucks, and drones.</li> </ul>"},{"location":"#get-started","title":"\ud83c\udfaf Get Started","text":"<p>To start using NEBULA, follow our detailed Installation Guide and User Manual. For any queries or contributions, check out our Contribution Guide.</p>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>We welcome contributions from the community to enhance NEBULA. If you are interested in contributing, please follow the next steps:</p> <ol> <li>Fork the repository</li> <li>Create a new branch with your feature or bug fix (<code>git checkout -b feature/your-feature</code>).</li> <li>Commit your changes (<code>git commit -am 'Add new feature'</code>).</li> <li>Push to the branch (<code>git push origin feature/your-feature</code>).</li> <li>Create a new Pull Request.</li> </ol>"},{"location":"#citation","title":"\ud83d\udcda Citation","text":"<p>If you use NEBULA (or Fedstellar) in a scientific publication, we would appreciate using the following citations:</p> <pre><code>@article{MartinezBeltran:DFL:2023,\n    title        = {{Decentralized Federated Learning: Fundamentals, State of the Art, Frameworks, Trends, and Challenges}},\n    author       = {Mart{\\'i}nez Beltr{\\'a}n, Enrique Tom{\\'a}s and Quiles P{\\'e}rez, Mario and S{\\'a}nchez S{\\'a}nchez, Pedro Miguel and L{\\'o}pez Bernal, Sergio and Bovet, G{\\'e}r{\\^o}me and Gil P{\\'e}rez, Manuel and Mart{\\'i}nez P{\\'e}rez, Gregorio and Huertas Celdr{\\'a}n, Alberto},\n    year         = 2023,\n    volume       = {25},\n    number       = {4},\n    pages        = {2983-3013},\n    journal      = {IEEE Communications Surveys &amp; Tutorials},\n    doi          = {10.1109/COMST.2023.3315746},\n    preprint     = {https://arxiv.org/abs/2211.08413}\n}\n</code></pre> <pre><code>@article{MartinezBeltran:fedstellar:2024,\n    title        = {{Fedstellar: A Platform for Decentralized Federated Learning}},\n    author       = {Mart{\\'i}nez Beltr{\\'a}n, Enrique Tom{\\'a}s and Perales G{\\'o}mez, {\\'A}ngel Luis and Feng, Chao and S{\\'a}nchez S{\\'a}nchez, Pedro Miguel and L{\\'o}pez Bernal, Sergio and Bovet, G{\\'e}r{\\^o}me and Gil P{\\'e}rez, Manuel and Mart{\\'i}nez P{\\'e}rez, Gregorio and Huertas Celdr{\\'a}n, Alberto},\n    year         = 2024,\n    volume       = {242},\n    issn         = {0957-4174},\n    pages        = {122861},\n    journal      = {Expert Systems with Applications},\n    doi          = {10.1016/j.eswa.2023.122861},\n    preprint     = {https://arxiv.org/abs/2306.09750}\n}\n</code></pre> <pre><code>@inproceedings{MartinezBeltran:fedstellar_demo:2023,\n    title        = {{Fedstellar: A Platform for Training Models in a Privacy-preserving and Decentralized Fashion}},\n    author       = {Mart{\\'i}nez Beltr{\\'a}n, Enrique Tom{\\'a}s and S{\\'a}nchez S{\\'a}nchez, Pedro Miguel and L{\\'o}pez Bernal, Sergio and Bovet, G{\\'e}r{\\^o}me and Gil P{\\'e}rez, Manuel and Mart{\\'i}nez P{\\'e}rez, Gregorio and Huertas Celdr{\\'a}n, Alberto},\n    year         = 2023,\n    month        = aug,\n    booktitle    = {Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, {IJCAI-23}},\n    publisher    = {International Joint Conferences on Artificial Intelligence Organization},\n    pages        = {7154--7157},\n    doi          = {10.24963/ijcai.2023/838},\n    note         = {Demo Track},\n    editor       = {Edith Elkind}\n}\n</code></pre> <pre><code>@article{MartinezBeltran:DFL_mitigating_threats:2023,\n    title        = {{Mitigating Communications Threats in Decentralized Federated Learning through Moving Target Defense}},\n    author       = {Mart{\\'i}nez Beltr{\\'a}n, Enrique Tom{\\'a}s and S{\\'a}nchez S{\\'a}nchez, Pedro Miguel and L{\\'o}pez Bernal, Sergio and Bovet, G{\\'e}r{\\^o}me and Gil P{\\'e}rez, Manuel and Mart{\\'i}nez P{\\'e}rez, Gregorio and Huertas Celdr{\\'a}n, Alberto},\n    year         = 2024,\n    journal      = {Wireless Networks},\n    doi          = {10.1007/s11276-024-03667-8}\n    preprint     = {https://arxiv.org/abs/2307.11730}\n}\n</code></pre>"},{"location":"#license","title":"\ud83d\udcdd License","text":"<p>Distributed under the GNU GPLv3 License. See <code>LICENSE</code> for more information.</p>"},{"location":"#acknowledgements","title":"\ud83d\ude4f Acknowledgements","text":"<p>We would like to thank the following projects for their contributions which have helped shape NEBULA:</p> <ul> <li>PyTorch Lightning for the training loop and model management</li> <li>Tensorboard for the visualization tools and monitoring capabilities</li> <li>Different datasets (nebula/core/datasets) and models (nebula/core/models) for testing and validation purposes</li> <li>FastAPI for the RESTful API</li> <li>Web3 for the blockchain integration</li> <li>Fedstellar platform and p2pfl library</li> <li>Adversarial Robustness Toolbox (ART) for the implementation of adversarial attacks</li> <li>D3.js for the network visualizations</li> </ul> <ol> <li> <p>Fedstellar was our first version of the platform. We have redesigned the previous functionalities and added new capabilities based on our research. The platform is now called NEBULA and is available as an open-source project.\u00a0\u21a9</p> </li> </ol>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>We welcome contributions to this project. Please read the following guidelines.</p>"},{"location":"developerguide/","title":"Developer Guide","text":"<p>This guide is designed to help developers understand and contribute to the project. It provides detailed instructions on navigating the codebase, and implementing new features. Whether you're looking to fix bugs, add enhancements, or better understand the architecture, this guide will walk you through the essential processes and best practices for development.</p>"},{"location":"developerguide/#nebula-frontend","title":"NEBULA Frontend","text":"<p>This section explains the structure of the frontend and provides instructions on how to add new parameters or sections.</p>"},{"location":"developerguide/#frontend-structure","title":"Frontend Structure","text":"Structure <pre><code>/nebula/\n  addons/\n  config/\n  core/\n  frontend/\n    config/\n      nebula\n      participant.json.example\n    databases/\n      participants.db\n      notes.db\n      scenarios.db\n      users.db\n    static/\n    templates/\n      401.html\n      403.html\n      404.html\n      405.html\n      413.html\n      admin.html\n      dashboard.html\n      deployment.html\n      index.html\n      layout.html\n      monitor.html\n      private.html\n      statistics.html\n    app.py\n    database.py\n    Dockerfile\n    start_services.sh\n</code></pre> <p>The frontend is organized within the <code>frontend/</code> directory. Key files and folders include:</p> <ul> <li><code>config/</code> \u2192 Contains the participant.json.example, the default structure for the paramteres passed to each participant.</li> <li><code>databases/</code> \u2192 Contains the different databases for NEBULA</li> <li><code>static/</code> \u2192 Holds static assets (CSS, images, JS, etc.).</li> <li><code>templates/</code> \u2192 Contains HTML templates. Focus on deployment.html</li> </ul>"},{"location":"developerguide/#adding-a-new-parameter","title":"Adding a New Parameter","text":"<p>Define the new parameter in the participant.json.example file. Only create a new field if necessary</p> participant.json.example <pre><code>{\n    \"scenario_args\": {\n      \"name\": \"\",\n      \"start_time\": \"\",\n      \"federation\": \"DFL\",\n      \"rounds\": 10,\n      \"deployment\": \"process\",\n      \"controller\": \"127.0.0.1:5000\",\n      \"random_seed\": 42,\n      \"n_participants\": 0,\n      /* New parameter in each setting */\n      \"new_parameter_key\" : \"new_parameter_value\",\n      \"config_version\": \"development\"\n    },\n    /* Add a new_field if necessary */\n    \"new_field\": {\n        \"new_parameter_key\" : \"new_parameter_value\"\n    }\n}\n</code></pre> <p>To implement a new attack type, first locate the section where attacks are defined. Then, add the new attack option along with its corresponding parameter. Below is an example of how to integrate the attack and its associated parameter.</p> deployment.html <pre><code>&lt;div class=\"form-group row container-shadow tiny grey\"&gt;\n    &lt;h5 class=\"step-number\"&gt;Robustness &lt;i class=\"fa fa-shield\"&gt;&lt;/i&gt;\n        &lt;input type=\"checkbox\" tyle=\"display: none;\"&gt;\n        &lt;label for=\"robustness-lock\" class=\"icon-container\" style=\"float: right;\"&gt;\n            &lt;i class=\"fa fa-lock\"&gt;&lt;/i&gt;\n        &lt;/label&gt;\n    &lt;/h5&gt;\n    &lt;h5 class=\"step-title\"&gt;Attack Type&lt;/h5&gt;\n    &lt;div class=\"form-check form-check-inline\"&gt;\n        &lt;select class=\"form-control\" id=\"poisoning-attack-select\" name=\"poisoning-attack\"&gt;\n            &lt;option selected&gt;No Attack&lt;/option&gt;\n            &lt;option&gt;New Attack&lt;/option&gt; &lt;!-- Add this --&gt;\n        &lt;/select&gt;\n        &lt;h5 id=\"poisoned-participant-title\" class=\"step-title\"&gt;\n            % Malicious participants\n        &lt;/h5&gt;\n        &lt;div class=\"form-check form-check-inline\" style=\"display: none;\" id=\"poisoned-participant-percent-container\"&gt;\n            &lt;input type=\"number\" class=\"form-control\" id=\"poisoned-participant-percent\"\n                placeholder=\"% malicious participants\" min=\"0\" value=\"0\"&gt;\n                &lt;select class=\"form-control\" id=\"malicious-participants-select\" name=\"malicious-participants-select\"&gt;\n                &lt;option selected&gt;Percentage&lt;/option&gt;\n                &lt;option&gt;Manual&lt;/option&gt;\n            &lt;/select&gt;\n        &lt;/div&gt;\n        &lt;h5 id=\"poisoned-participant-title\" class=\"step-title\"&gt;\n            % Malicious participants\n        &lt;/h5&gt;\n        &lt;div class=\"form-check form-check-inline\" style=\"display: none;\" id=\"poisoned-participant-percent-container\"&gt;\n            &lt;input type=\"number\" class=\"form-control\" id=\"poisoned-participant-percent\"\n                placeholder=\"% malicious participants\" min=\"0\" value=\"0\"&gt;\n        &lt;/div&gt;\n        &lt;h5 id=\"new-parameter-title\" class=\"step-title\"&gt; &lt;!-- Add this --&gt;\n            New parameter\n        &lt;/h5&gt;\n        &lt;div class=\"form-check form-check-inline\" style=\"display: none;\" id=\"new-parameter-container\"&gt;\n            &lt;input type=\"number\" class=\"form-control\" id=\"new-parameter-value\"\n                placeholder=\"new parameter value\" min=\"0\" value=\"0\"&gt;\n        &lt;/div&gt;\n    &lt;/div&gt;\n&lt;/div&gt;\n</code></pre> <p>To receive the parameter in nebula/scenarios.py, you need to modify the Scenario class to accept the new parameter. This involves updating the Scenario class constructor and possibly the relevant methods to handle the new parameter accordingly.</p> Class Scenario <pre><code>class Scenario:\n    def __init__(\n        self,\n        scenario_title,\n        scenario_description,\n        new_paramater, # &lt;--- Add this\n    ):\n        self.scenario_title = scenario_title\n        self.scenario_description = scenario_description\n        self.new_parameter = new_parameter # &lt;--- Add this\n</code></pre> <p>Now you must save the parameter in the participant configuration.</p> <p>The participant configuration files are located in the /app/config/ directory. Ensure that the new parameter is added to the participant's JSON file, so it can be accessed later when the configuration is loaded.</p> Class ScenarioManagement <pre><code>    class ScenarioManagement:\n    def __init__(self, scenario, user=None):\n        # Save participant settings\n        for participant in self.scenario.participants:\n            participant_config = self.scenario.participants[participant]\n            participant_file = os.path.join(self.config_dir, f\"participant_{participant_config['id']}.json\")\n            os.makedirs(os.path.dirname(participant_file), exist_ok=True)\n            shutil.copy(\n                os.path.join(\n                    os.path.dirname(__file__),\n                    \"./frontend/config/participant.json.example\",\n                ),\n                participant_file,\n            )\n            os.chmod(participant_file, 0o777)\n            with open(participant_file) as f:\n                participant_config = json.load(f)\n\n            participant_config[\"network_args\"][\"ip\"] = participant_config[\"ip\"]\n            participant_config[\"network_args\"][\"port\"] = int(participant_config[\"port\"])\n            # In case you are adding a parameter to a previously defined functionality\n            participant_config[\"data_args\"][\"new_parameter\"] = self.scenario.new_parameter\n            # In case you are creating a new functionality\n            participant_config[\"new_field\"][\"new_parameter\"] = self.scenario.new_parameter\n</code></pre>"},{"location":"developerguide/#nebula-backend","title":"NEBULA Backend","text":"<p>To view the documentation of functions in more detail, you must go to the NEBULA API Reference. This reference will provide you with comprehensive details about the available functions, their parameters, return types, and usage examples, allowing you to understand how to properly implement and interact with them.</p>"},{"location":"developerguide/#backend-structure","title":"Backend Structure","text":"Structure <pre><code>/nebula/\n  addons/\n    attacks/\n    blockchain/\n    trustworthiness/\n    waf/\n  core/\n    aggregation/\n    datasets/\n    models/\n    network/\n    pb/\n    training/\n    utils/\n    engine.py\n    eventmanager.py\n    role.py\n  controller.py\n  participant.py\n  scenarios.py\n  utils.py\n</code></pre> <p>The backend is organized within the <code>/nebula/</code> directory. Key files and folders include:</p> <p>Addons/</p> <p>The <code>addons/</code> directory contains extended functionalities that can be integrated into the core system.</p> <ul> <li><code>attacks/</code> \u2192 Simulates attacks, primarily for security purposes, including adversarial attacks in machine learning.</li> <li><code>blockchain/</code> \u2192 Integrates blockchain technology, potentially for decentralized storage or security enhancements.</li> <li><code>trustworthiness/</code> \u2192 Evaluates the trustworthiness and reliability of participants, focusing on security and ethical considerations.</li> <li><code>waf/</code> \u2192 Implements a Web Application Firewall (WAF) to filter and monitor HTTP traffic for potential threats.</li> </ul> <p>Core/</p> <p>The <code>core/</code> directory contains the essential components for the backend operation.</p> <ul> <li><code>aggregation/</code> \u2192 Manages the aggregation of data from different nodes.</li> <li><code>datasets/</code> \u2192 Handles dataset management, including loading and preprocessing data.</li> <li><code>models/</code> \u2192 Defines machine learning model architectures and related functionalities, such as training and evaluation.</li> <li><code>network/</code> \u2192 Manages communication between participants in a distributed system.</li> <li><code>pb/</code> \u2192 Implements Protocol Buffers (PB) for efficient data serialization and communication.</li> <li><code>training/</code> \u2192 Contains the logic for model training, optimization, and evaluation.</li> <li><code>utils/</code> \u2192 Provides utility functions for file handling, logging, and common tasks.</li> </ul> <p>Files</p> <ul> <li><code>engine.py</code> \u2192 The main engine orchestrating participant communications, training, and overall behavior.</li> <li><code>eventmanager.py</code> \u2192 Handles event management, logging, and notifications within the system.</li> <li><code>role.py</code> \u2192 Defines participant roles and their interactions.</li> </ul> <p>Standalone Scripts</p> <p>These scripts act as entry points or controllers for various backend functionalities.</p> <ul> <li><code>controller.py</code> \u2192 Manages the flow of operations, coordinating tasks and interactions.</li> <li><code>participant.py</code> \u2192 Represents a participant in the decentralized network, handling computations and communication.</li> <li><code>scenarios.py</code> \u2192 Defines different simulation scenarios for testing and running participants under specific conditions.</li> <li><code>utils.py</code> \u2192 Contains helper functions that simplify development and maintenance.</li> </ul>"},{"location":"developerguide/#adding-new-datasets","title":"Adding new Datasets","text":""},{"location":"developerguide/#add-the-dataset-option-in-the-front","title":"Add the Dataset option in the front","text":"<p>First, you must add the Dataset option in the frontend. Adding the Dataset option to the scenario generated by the frontend requires a slightly different approach.</p> Datasets in Deployment.html <pre><code>&lt;script&gt;\n    // Add the dataset with each model here\n    var datasets = {\n        \"MNIST\": [\"MLP\", \"CNN\"],\n        \"FashionMNIST\": [\"MLP\", \"CNN\"],\n        \"EMNIST\": [\"MLP\", \"CNN\"],\n        \"CIFAR10\": [\"CNN\", \"CNNv2\", \"CNNv3\", \"ResNet9\", \"fastermobilenet\", \"simplemobilenet\"],\n        \"CIFAR100\": [\"CNN\"],\n    }\n    var datasetSelect = document.getElementById(\"datasetSelect\");\n    var modelSelect = document.getElementById(\"modelSelect\");\n&lt;/script&gt;\n</code></pre> <p>If you want to add a new Dataset you can implement this in two ways on the folder /nebula/core/datasets/new_dataset/new_dataset.py</p>"},{"location":"developerguide/#import-the-dataset-from-torchvision","title":"Import the Dataset from Torchvision","text":"<p>You can use the MNIST Dataset as a code example to demonstrate how to import the dataset from torchvision, initialize it, and load its configuration.</p> MNIST Code example <pre><code>import os\n\nfrom torchvision import transforms\nfrom torchvision.datasets import MNIST\n\nfrom nebula.core.datasets.nebuladataset import NebulaDataset\n\n\nclass MNISTDataset(NebulaDataset):\n    def __init__(\n        self,\n        num_classes=10,\n        partition_id=0,\n        partitions_number=1,\n        batch_size=32,\n        num_workers=4,\n        iid=True,\n        partition=\"dirichlet\",\n        partition_parameter=0.5,\n        seed=42,\n        config=None,\n    ):\n        super().__init__(\n            num_classes=num_classes,\n            partition_id=partition_id,\n            partitions_number=partitions_number,\n            batch_size=batch_size,\n            num_workers=num_workers,\n            iid=iid,\n            partition=partition,\n            partition_parameter=partition_parameter,\n            seed=seed,\n            config=config,\n        )\n        if partition_id &lt; 0 or partition_id &gt;= partitions_number:\n            raise ValueError(f\"partition_id {partition_id} is out of range for partitions_number {partitions_number}\")\n\n    def initialize_dataset(self):\n        if self.train_set is None:\n            self.train_set = self.load_mnist_dataset(train=True)\n        if self.test_set is None:\n            self.test_set = self.load_mnist_dataset(train=False)\n\n        self.test_indices_map = list(range(len(self.test_set)))\n\n        # Depending on the iid flag, generate a non-iid or iid map of the train set\n        if self.iid:\n            self.train_indices_map = self.generate_iid_map(self.train_set, self.partition, self.partition_parameter)\n            self.local_test_indices_map = self.generate_iid_map(self.test_set, self.partition, self.partition_parameter)\n        else:\n            self.train_indices_map = self.generate_non_iid_map(self.train_set, self.partition, self.partition_parameter)\n            self.local_test_indices_map = self.generate_non_iid_map(\n                self.test_set, self.partition, self.partition_parameter\n            )\n\n        print(f\"Length of train indices map: {len(self.train_indices_map)}\")\n        print(f\"Lenght of test indices map (global): {len(self.test_indices_map)}\")\n        print(f\"Length of test indices map (local): {len(self.local_test_indices_map)}\")\n\n    def load_mnist_dataset(self, train=True):\n        apply_transforms = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.5,), (0.5,), inplace=True),\n        ])\n        data_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"data\")\n        os.makedirs(data_dir, exist_ok=True)\n        return MNIST(\n            data_dir,\n            train=train,\n            download=True,\n            transform=apply_transforms,\n        )\n</code></pre>"},{"location":"developerguide/#import-the-dataset-from-your-own","title":"Import the Dataset from your own","text":"<p>If you want to import a dataset, you must first create a folder named data where you will store the image_list. Then, create a Dataset class similar to the one in the MilitarySAR code example.</p> MilitarySAR Code Example <pre><code>class MilitarySAR(Dataset):\ndef __init__(self, name=\"soc\", is_train=False, transform=None):\n    self.is_train = is_train\n    self.name = name\n\n    self.data = []\n    self.targets = []\n    self.serial_numbers = []\n\n    # Path to data is \"data\" folder in the same directory as this file\n    self.path_to_data = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"data\")\n\n    self.transform = transform\n\n    # self._load_data(self.path_to_data)\n\n    mode = \"train\" if self.is_train else \"test\"\n    self.image_list = glob.glob(os.path.join(self.path_to_data, f\"{self.name}/{mode}/*/*.npy\"))\n    self.label_list = glob.glob(os.path.join(self.path_to_data, f\"{self.name}/{mode}/*/*.json\"))\n    self.image_list = sorted(self.image_list, key=os.path.basename)\n    self.label_list = sorted(self.label_list, key=os.path.basename)\n    assert len(self.image_list) == len(self.label_list)\n\ndef __len__(self):\n\ndef __getitem__(self, idx):\n\ndef _load_metadata(self):\n\ndef get_targets(self):\n</code></pre> <p>Then you must create a MilitarySARDataset class in order to use it, as shown in the example below</p> MilitarySARDataset Code example <pre><code>class MilitarySARDataset(NebulaDataset):\n    def __init__(\n        self,\n        num_classes=10,\n        partition_id=0,\n        partitions_number=1,\n        batch_size=32,\n        num_workers=4,\n        iid=True,\n        partition=\"dirichlet\",\n        partition_parameter=0.5,\n        seed=42,\n        config=None,\n    ):\n        super().__init__(\n            num_classes=num_classes,\n            partition_id=partition_id,\n            partitions_number=partitions_number,\n            batch_size=batch_size,\n            num_workers=num_workers,\n            iid=iid,\n            partition=partition,\n            partition_parameter=partition_parameter,\n            seed=seed,\n            config=config,\n        )\n\n    def initialize_dataset(self):\n\n    def load_militarysar_dataset(self, train=True):\n</code></pre>"},{"location":"developerguide/#define-transforms","title":"Define transforms","text":"<p>You can apply transformations like cropping and normalization using <code>torchvision.transforms</code>.</p> <p>For example, the MilitarySAR dataset uses RandomCrop for training and CenterCrop for testing.</p> MilitarySAR <pre><code>class RandomCrop:\n    def __init__(self, size):\n        if isinstance(size, int):\n            self.size = (size, size)\n        else:\n            assert len(size) == 2\n            self.size = size\n\n    def __call__(self, sample):\n        _input = sample\n\n        if len(_input.shape) &lt; 3:\n            _input = np.expand_dims(_input, axis=2)\n\n        h, w, _ = _input.shape\n        oh, ow = self.size\n\n        dh = h - oh\n        dw = w - ow\n        y = np.random.randint(0, dh) if dh &gt; 0 else 0\n        x = np.random.randint(0, dw) if dw &gt; 0 else 0\n        oh = oh if dh &gt; 0 else h\n        ow = ow if dw &gt; 0 else w\n\n        return _input[y : y + oh, x : x + ow, :]\n\n\nclass CenterCrop:\n    def __init__(self, size):\n        if isinstance(size, int):\n            self.size = (size, size)\n        else:\n            assert len(size) == 2\n            self.size = size\n\n    def __call__(self, sample):\n        _input = sample\n\n        if len(_input.shape) &lt; 3:\n            _input = np.expand_dims(_input, axis=2)\n\n        h, w, _ = _input.shape\n        oh, ow = self.size\n        y = (h - oh) // 2\n        x = (w - ow) // 2\n\n        return _input[y : y + oh, x : x + ow, :]\n\nclass MilitarySARDataset(NebulaDataset):\n    def load_militarysar_dataset(self, train=True):\n        apply_transforms = [CenterCrop(88), transforms.ToTensor()]\n        if train:\n            apply_transforms = [RandomCrop(88), transforms.ToTensor()]\n\n        return MilitarySAR(name=\"soc\", is_train=train, transform=transforms.Compose(apply_transforms))\n</code></pre>"},{"location":"developerguide/#associate-the-model-with-the-new-dataset","title":"Associate the Model with the new Dataset","text":"<p>Now, you need to add the model you want to use with the dataset in the /nebula/core/models/ folder, by creating a file named new_dataset/new_model.py</p> <p>The model must inherit from the NebulaModel class</p> MLP Code example <pre><code>import torch\n\nfrom nebula.core.models.nebulamodel import NebulaModel\n\n\nclass MNISTModelMLP(NebulaModel):\n    def __init__(\n        self,\n        input_channels=1,\n        num_classes=10,\n        learning_rate=1e-3,\n        metrics=None,\n        confusion_matrix=None,\n        seed=None,\n    ):\n        super().__init__(input_channels, num_classes, learning_rate, metrics, confusion_matrix, seed)\n\n        self.example_input_array = torch.zeros(1, 1, 28, 28)\n        self.learning_rate = learning_rate\n        self.criterion = torch.nn.CrossEntropyLoss()\n        self.l1 = torch.nn.Linear(28 * 28, 256)\n        self.l2 = torch.nn.Linear(256, 128)\n        self.l3 = torch.nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        batch_size, channels, width, height = x.size()\n        x = x.view(batch_size, -1)\n        x = self.l1(x)\n        x = torch.relu(x)\n        x = self.l2(x)\n        x = torch.relu(x)\n        x = self.l3(x)\n        return x\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n        return optimizer\n</code></pre>"},{"location":"developerguide/#adding-new-aggregators","title":"Adding new Aggregators","text":""},{"location":"developerguide/#adding-the-aggregator-in-the-frontend","title":"Adding the Aggregator in the frontend","text":"<p>You must add the new aggregator in the deployment.html file and ensure that it is correctly included in the JSON files generated within the /app/config folder. After making the necessary updates in the HTML, verify that the new aggregator is properly reflected in the corresponding configuration files by checking the JSON structure and values.</p> deployment.html <pre><code>    &lt;h5 class=\"step-title\"&gt;Aggregation algorithm&lt;/h5&gt;\n    &lt;div class=\"form-check form-check-inline\"&gt;\n        &lt;select class=\"form-control\" id=\"aggregationSelect\" name=\"aggregation\"\n            style=\"display: inline; width: 50%\"&gt;\n            &lt;option selected&gt;FedAvg&lt;/option&gt;\n            &lt;option&gt;Krum&lt;/option&gt;\n            &lt;option&gt;TrimmedMean&lt;/option&gt;\n            &lt;option&gt;Median&lt;/option&gt;\n            &lt;option&gt;BlockchainReputation&lt;/option&gt;\n            &lt;!--Add this--&gt;\n            &lt;option&gt;new_aggregation&lt;/option&gt;\n        &lt;/select&gt;\n    &lt;/div&gt;\n</code></pre>"},{"location":"developerguide/#adding-the-aggregator-file","title":"Adding the Aggregator file","text":"<p>You need to add the aggregator you want to use into /nebula/core/aggregation/ by creating a file named new_aggregator.py</p> <p>The new aggregator must inherit from the Aggregator class. You can use FedAvg as an example to guide your implementation</p> Aggregator class <pre><code>class Aggregator(ABC):\n    def __init__(self, config=None, engine=None):\n        self.config = config\n        self.engine = engine\n        self._addr = config.participant[\"network_args\"][\"addr\"]\n        logging.info(f\"[{self.__class__.__name__}] Starting Aggregator\")\n        self._federation_nodes = set()\n        self._waiting_global_update = False\n        self._pending_models_to_aggregate = {}\n        self._future_models_to_aggregate = {}\n        self._add_model_lock = Locker(name=\"add_model_lock\", async_lock=True)\n        self._aggregation_done_lock = Locker(name=\"aggregation_done_lock\", async_lock=True)\n\n    def __str__(self):\n        return self.__class__.__name__\n\n    def __repr__(self):\n        return self.__str__()\n\n    @property\n    def cm(self):\n        return self.engine.cm\n\n    @abstractmethod\n    def run_aggregation(self, models):\n        if len(models) == 0:\n            logging.error(\"Trying to aggregate models when there are no models\")\n            return None\n\n    async def update_federation_nodes(self, federation_nodes):\n        if not self._aggregation_done_lock.locked():\n            self._federation_nodes = federation_nodes\n            self._pending_models_to_aggregate.clear()\n            await self._aggregation_done_lock.acquire_async(\n                timeout=self.config.participant[\"aggregator_args\"][\"aggregation_timeout\"]\n            )\n        else:\n            raise Exception(\"It is not possible to set nodes to aggregate when the aggregation is running.\")\n\n    def set_waiting_global_update(self):\n        self._waiting_global_update = True\n\n    async def reset(self):\n        await self._add_model_lock.acquire_async()\n        self._federation_nodes.clear()\n        self._pending_models_to_aggregate.clear()\n        try:\n            await self._aggregation_done_lock.release_async()\n        except:\n            pass\n        await self._add_model_lock.release_async()\n\n    def get_nodes_pending_models_to_aggregate(self):\n        return {node for key in self._pending_models_to_aggregate.keys() for node in key.split()}\n\n    async def _handle_global_update(self, model, source):\n        logging.info(f\"\ud83d\udd04  _handle_global_update | source={source}\")\n        logging.info(\n            f\"\ud83d\udd04  _handle_global_update | Received a model from {source}. Overwriting __models with the aggregated model.\"\n        )\n        self._pending_models_to_aggregate.clear()\n        self._pending_models_to_aggregate = {source: (model, 1)}\n        self._waiting_global_update = False\n        await self._add_model_lock.release_async()\n        await self._aggregation_done_lock.release_async()\n\n    async def _add_pending_model(self, model, weight, source):\n        if len(self._federation_nodes) &lt;= len(self.get_nodes_pending_models_to_aggregate()):\n            logging.info(\"\ud83d\udd04  _add_pending_model | Ignoring model...\")\n            await self._add_model_lock.release_async()\n            return None\n\n        if source not in self._federation_nodes:\n            logging.info(f\"\ud83d\udd04  _add_pending_model | Can't add a model from ({source}), which is not in the federation.\")\n            await self._add_model_lock.release_async()\n            return None\n\n        elif source not in self.get_nodes_pending_models_to_aggregate():\n            logging.info(\n                \"\ud83d\udd04  _add_pending_model | Node is not in the aggregation buffer --&gt; Include model in the aggregation buffer.\"\n            )\n            self._pending_models_to_aggregate.update({source: (model, weight)})\n\n        logging.info(\n            f\"\ud83d\udd04  _add_pending_model | Model added in aggregation buffer ({len(self.get_nodes_pending_models_to_aggregate())!s}/{len(self._federation_nodes)!s}) | Pending nodes: {self._federation_nodes - self.get_nodes_pending_models_to_aggregate()}\"\n        )\n\n        # Check if _future_models_to_aggregate has models in the current round to include in the aggregation buffer\n        if self.engine.get_round() in self._future_models_to_aggregate:\n            logging.info(\n                f\"\ud83d\udd04  _add_pending_model | Including next models in the aggregation buffer for round {self.engine.get_round()}\"\n            )\n            for future_model in self._future_models_to_aggregate[self.engine.get_round()]:\n                if future_model is None:\n                    continue\n                future_model, future_weight, future_source = future_model\n                if (\n                    future_source in self._federation_nodes\n                    and future_source not in self.get_nodes_pending_models_to_aggregate()\n                ):\n                    self._pending_models_to_aggregate.update({future_source: (future_model, future_weight)})\n                    logging.info(\n                        f\"\ud83d\udd04  _add_pending_model | Next model added in aggregation buffer ({len(self.get_nodes_pending_models_to_aggregate())!s}/{len(self._federation_nodes)!s}) | Pending nodes: {self._federation_nodes - self.get_nodes_pending_models_to_aggregate()}\"\n                    )\n            del self._future_models_to_aggregate[self.engine.get_round()]\n\n            for future_round in list(self._future_models_to_aggregate.keys()):\n                if future_round &lt; self.engine.get_round():\n                    del self._future_models_to_aggregate[future_round]\n\n        if len(self.get_nodes_pending_models_to_aggregate()) &gt;= len(self._federation_nodes):\n            logging.info(\"\ud83d\udd04  _add_pending_model | All models were added in the aggregation buffer. Run aggregation...\")\n            await self._aggregation_done_lock.release_async()\n        await self._add_model_lock.release_async()\n        return self.get_nodes_pending_models_to_aggregate()\n\n    async def include_model_in_buffer(self, model, weight, source=None, round=None, local=False):\n        await self._add_model_lock.acquire_async()\n        logging.info(\n            f\"\ud83d\udd04  include_model_in_buffer | source={source} | round={round} | weight={weight} |--| __models={self._pending_models_to_aggregate.keys()} | federation_nodes={self._federation_nodes} | pending_models_to_aggregate={self.get_nodes_pending_models_to_aggregate()}\"\n        )\n        if model is None:\n            logging.info(\"\ud83d\udd04  include_model_in_buffer | Ignoring model bad formed...\")\n            await self._add_model_lock.release_async()\n            return\n\n        if round == -1:\n            # Be sure that the model message is not from the initialization round (round = -1)\n            logging.info(\"\ud83d\udd04  include_model_in_buffer | Ignoring model with round -1\")\n            await self._add_model_lock.release_async()\n            return\n\n        if self._waiting_global_update and not local:\n            await self._handle_global_update(model, source)\n            return\n\n        await self._add_pending_model(model, weight, source)\n\n        if len(self.get_nodes_pending_models_to_aggregate()) &gt;= len(self._federation_nodes):\n            logging.info(\n                f\"\ud83d\udd04  include_model_in_buffer | Broadcasting MODELS_INCLUDED for round {self.engine.get_round()}\"\n            )\n            message = self.cm.create_message(\"federation\", \"federation_models_included\", [str(arg) for arg in [self.engine.get_round()]])\n            await self.cm.send_message_to_neighbors(message)\n\n        return\n\n    async def get_aggregation(self):\n        try:\n            timeout = self.config.participant[\"aggregator_args\"][\"aggregation_timeout\"]\n            await self._aggregation_done_lock.acquire_async(timeout=timeout)\n        except TimeoutError:\n            logging.exception(\"\ud83d\udd04  get_aggregation | Timeout reached for aggregation\")\n        except asyncio.CancelledError:\n            logging.exception(\"\ud83d\udd04  get_aggregation | Lock acquisition was cancelled\")\n        except Exception as e:\n            logging.exception(f\"\ud83d\udd04  get_aggregation | Error acquiring lock: {e}\")\n        finally:\n            await self._aggregation_done_lock.release_async()\n\n        if self._waiting_global_update and len(self._pending_models_to_aggregate) == 1:\n            logging.info(\n                \"\ud83d\udd04  get_aggregation | Received an global model. Overwriting my model with the aggregated model.\"\n            )\n            aggregated_model = next(iter(self._pending_models_to_aggregate.values()))[0]\n            self._pending_models_to_aggregate.clear()\n            return aggregated_model\n\n        unique_nodes_involved = set(node for key in self._pending_models_to_aggregate for node in key.split())\n\n        if len(unique_nodes_involved) != len(self._federation_nodes):\n            missing_nodes = self._federation_nodes - unique_nodes_involved\n            logging.info(f\"\ud83d\udd04  get_aggregation | Aggregation incomplete, missing models from: {missing_nodes}\")\n        else:\n            logging.info(\"\ud83d\udd04  get_aggregation | All models accounted for, proceeding with aggregation.\")\n\n        aggregated_result = self.run_aggregation(self._pending_models_to_aggregate)\n        self._pending_models_to_aggregate.clear()\n        return aggregated_result\n\n    async def include_next_model_in_buffer(self, model, weight, source=None, round=None):\n        logging.info(f\"\ud83d\udd04  include_next_model_in_buffer | source={source} | round={round} | weight={weight}\")\n        if round not in self._future_models_to_aggregate:\n            self._future_models_to_aggregate[round] = []\n        decoded_model = self.engine.trainer.deserialize_model(model)\n        self._future_models_to_aggregate[round].append((decoded_model, weight, source))\n\n    def print_model_size(self, model):\n        total_params = 0\n        total_memory = 0\n\n        for _, param in model.items():\n            num_params = param.numel()\n            total_params += num_params\n\n            memory_usage = param.element_size() * num_params\n            total_memory += memory_usage\n\n        total_memory_in_mb = total_memory / (1024**2)\n        logging.info(f\"print_model_size | Model size: {total_memory_in_mb} MB\")\n</code></pre> FedAvg.py <pre><code>import gc\n\nimport torch\n\nfrom nebula.core.aggregation.aggregator import Aggregator\n\n\nclass FedAvg(Aggregator):\n    \"\"\"\n    Aggregator: Federated Averaging (FedAvg)\n    Authors: McMahan et al.\n    Year: 2016\n    \"\"\"\n\n    def __init__(self, config=None, **kwargs):\n        super().__init__(config, **kwargs)\n\n    def run_aggregation(self, models):\n        super().run_aggregation(models)\n\n        models = list(models.values())\n\n        total_samples = float(sum(weight for _, weight in models))\n\n        if total_samples == 0:\n            raise ValueError(\"Total number of samples must be greater than zero.\")\n\n        last_model_params = models[-1][0]\n        accum = {layer: torch.zeros_like(param, dtype=torch.float32) for layer, param in last_model_params.items()}\n\n        with torch.no_grad():\n            for model_parameters, weight in models:\n                normalized_weight = weight / total_samples\n                for layer in accum:\n                    accum[layer].add_(\n                        model_parameters[layer].to(accum[layer].dtype),\n                        alpha=normalized_weight,\n                    )\n\n        del models\n        gc.collect()\n\n        # self.print_model_size(accum)\n        return accum\n</code></pre>"},{"location":"installation/","title":"Installation","text":"<p>Welcome to the NEBULA platform installation guide. This document explains how to obtain, install, run, and troubleshoot NEBULA.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<p>For the best experience, ensure the following prerequisites are met:</p> <ul> <li>Linux (Ubuntu 20.04 LTS recommended) or macOS (10.15 Catalina or later). Currently, we do not maintain an up-to-date version for Windows.</li> <li>Minimum 8 GB RAM (+32 GB recommended for virtualized devices).</li> <li>Minimum 20 GB disk space for Docker images and containers. Additional space is required for datasets, models, and results.</li> <li>Docker Engine 24.0.4 or higher (24.0.7 recommended, https://docs.docker.com/engine/install/)</li> <li>Docker Compose 2.19.0 or higher (2.19.1 recommended, https://docs.docker.com/compose/install/)</li> </ul>"},{"location":"installation/#obtaining-nebula","title":"Obtaining NEBULA","text":"<p>You can obtain the source code from https://github.com/CyberDataLab/nebula</p> <p>Or clone the repository using git:</p> <pre><code>user@host:~$ git clone https://github.com/CyberDataLab/nebula.git</code></pre> <p>Now, you can move to the source directory:</p> <pre><code>user@host:~$ cd nebula</code></pre>"},{"location":"installation/#installing-nebula","title":"Installing NEBULA","text":"<p>Install required dependencies and set up Docker containers by running:</p> <pre><code>user@host:~$ make install</code></pre> <p>Next, activate the virtual environment:</p> <pre><code>user@host:~$ source .venv/bin/activate</code></pre> <p>If you forget this command, you can type:</p> <pre><code>user@host:~$ make shell</code></pre> <p>Your shell prompt should look similar to:</p> <pre><code>(nebula-dfl) user@host:~$</code></pre>"},{"location":"installation/#using-nvidia-gpu-on-nodes-optional","title":"Using NVIDIA GPU on Nodes (Optional)","text":"<p>For nodes equipped with NVIDIA GPUs, ensure the following prerequisites:</p> <ul> <li>NVIDIA Driver: Version 525.60.13 or later.</li> <li>CUDA: Version 12.1 is required. After installation, verify with <code>nvidia-smi</code>.</li> <li>NVIDIA Container Toolkit: Install to enable GPU access within Docker containers.</li> </ul> <p>Follow these guides for proper installation:</p> <ul> <li>CUDA Installation Guide for Linux</li> <li>NVIDIA Container Toolkit Installation Guide</li> </ul> <p>Note: Ensure that the CUDA toolkit version is compatible with your driver and, if needed, update the Docker runtime to support GPU integration.</p>"},{"location":"installation/#running-nebula","title":"Running NEBULA","text":"<p>Once the installation is finished, you can check if NEBULA is installed properly using:</p> <pre><code>(nebula-dfl) user@host:~$ python app/main.py --version</code></pre> <p>To run NEBULA, you can use the following command line:</p> <pre><code>(nebula-dfl) user@host:~$ python app/main.py</code></pre> <p>Note: The first run may build the nebula-frontend Docker image, which can take a few minutes.</p> <p>Display available parameters:</p> <pre><code>(nebula-dfl) user@host:~$ python app/main.py --help</code></pre> <p>By default, the frontend is available at http://127.0.0.1:6060. If the 6060 port is unavailable, a random port will be assigned automatically and prompted in the console.</p> <p>Also, you can define the specific port using the following command line:</p> <pre><code>(nebula-dfl) user@host:~$ python app/main.py --webport [PORT]</code></pre> <p>and the default port of the statistics endpoint:</p> <pre><code>(nebula-dfl) user@host:~$ python app/main.py --statsport [PORT]</code></pre>"},{"location":"installation/#nebula-frontend-credentials","title":"NEBULA Frontend Credentials","text":"<p>You can log in with the default credentials:</p> <pre><code>- User: admin\n- Password: admin\n</code></pre> <p>If these do not work, please contact Enrique Tom\u00e1s Mart\u00ednez Beltr\u00e1n at enriquetomas@um.es.</p>"},{"location":"installation/#stopping-nebula","title":"Stopping NEBULA","text":"<p>To stop NEBULA, you can use the following command line:</p> <pre><code>(nebula-dfl) user@host:~$ python app/main.py --stop</code></pre> <p>Be careful! This command will stop all the containers related to NEBULA: Frontend, Controller, and Nodes.</p>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":"<p>If frontend is not working, check the logs in app/logs/frontend.log</p> <p>If any of the following errors appear, take a look at the docker logs of the nebula-frontend container:</p> <pre><code>user@host:~$ docker logs user_nebula-frontend</code></pre> <p>Network nebula_X Error failed to create network nebula_X: Error response from daemon: Pool overlaps with other one on this address space</p> <p>Solution: Delete the docker network nebula_X</p> <pre><code>user@host:~$ docker network rm nebula_X</code></pre> <p>Error: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?</p> <p>Solution: Start the docker daemon</p> <pre><code>user@host:~$ sudo dockerdX</code></pre> <p>Solution: Enable the following option in Docker Desktop</p> <p>Settings -&gt; Advanced -&gt; Allow the default Docker socket to be used</p> <p></p> <p>Error: Cannot connect to the Docker daemon at tcp://X.X.X.X:2375. Is the docker daemon running?</p> <p>Solution: Start the docker daemon</p> <pre><code>user@host:~$ sudo dockerd -H tcp://X.X.X.X:2375</code></pre> <p>If frontend is not working, restart docker daemon</p> <pre><code>user@host:~$ sudo systemctl restart docker</code></pre> <p>Error: Too many open files</p> <p>Solution: Increase the number of open files</p> <p>ulimit -n 65536</p> <p>Also, you can add the following lines to the file /etc/security/limits.conf</p> <ul> <li>soft nofile 65536</li> <li>hard nofile 65536</li> </ul>"},{"location":"userguide/","title":"User Guide","text":"<p>In this section, we will explain how to use the NEBULA Platform</p>"},{"location":"userguide/#running-nebula","title":"Running NEBULA","text":"<p>To run NEBULA, you can use the following command line:</p> <pre><code>(nebula-dfl)user@host:~$ python app/main.py [PARAMS]</code></pre> <p>The first time you run the platform, the nebula-frontend docker image will be built. This process can take a few minutes.</p> <p>You can show the PARAMS using:</p> <pre><code>(nebula-dfl)user@host:~$ python app/main.py --help</code></pre> <p>The frontend will be available at http://127.0.0.1:6060 (by default) if the port is available</p> <p>To change the default port of the frontend, you can use the following command line:</p> <pre><code>(nebula-dfl)user@host:~$ python app/main.py --webport [PORT]</code></pre> <p>To change the default port of the statistics endpoint, you can use the following command line:</p> <pre><code>(nebula-dfl)user@host:~$ python app/main.py --statsport [PORT]</code></pre>"},{"location":"userguide/#nebula-frontend","title":"NEBULA Frontend","text":""},{"location":"userguide/#top-navigation-bar-buttons","title":"Top Navigation Bar Buttons","text":"<ul> <li>HOME \u2013 Returns to the main landing page of NEBULA.</li> <li>SOURCE CODE \u2013 Redirects to the NEBULA's repository.</li> <li>DOCUMENTATION \u2013 Provides access to NEBULA's official documentation, including installation, usage, and API references.</li> <li>WEBSITE \u2013 Redirects to NEBULA's official website.</li> <li>ACCOUNT \u2013 Contains user-related options such as login, logout, and account settings.</li> <li>DASHBOARD \u2013 Takes users to the main dashboard where they can interact with NEBULA's features.</li> </ul>"},{"location":"userguide/#feedback-section","title":"Feedback Section","text":"<ul> <li>Send Feedback: You can send us feedback through this button, sharing how you use NEBULA and helping improve the platform through community engagement.</li> </ul>"},{"location":"userguide/#deployment-of-scenarios","title":"Deployment of Scenarios","text":"<p>Here you can define the different parameters used to deploy the federation of nodes</p>"},{"location":"userguide/#nodes-graph","title":"Nodes graph","text":"<p>It represents the topology of the nodes. In this graph, nodes can be selected and modified based on their role. The available options include:</p> <p>Modify node type, specific nodes can be selected and assigned roles such as:</p> <ul> <li>Malicious nodes</li> <li>Aggregator nodes</li> </ul> <p>Modify topology, the connections between nodes can be altered to change the structure of the network. The actions available are:</p> <ul> <li>Add links: New connections can be created between selected nodes by right-clicking.</li> <li>Remove links: Existing connections between nodes can be deleted by clicking on the link.</li> </ul>"},{"location":"userguide/#buttons-and-actions","title":"Buttons and Actions","text":"<p>Add Scenario</p> <p>Allows users to create a new scenario.</p> <p>Functionality:</p> <ul> <li>Opens a form to configure a new scenario.</li> <li>Users can define settings such as title, description, and deployment type.</li> </ul> <p>Advanced Mode</p> <p>Expands additional configuration options for nodes (from section 8 onwards).</p> <p>Functionality:   When enabled, displays advanced settings:</p> <ul> <li>Participants</li> <li>Advanced Topology</li> <li>Advanced Training</li> <li>Robustness</li> <li>Defense</li> <li>Mobility</li> </ul> <p>Generate Scenario List</p> <p>Displays a list of all created scenarios.</p> <p>Functionality:</p> <ul> <li>Retrieves and shows existing scenarios.</li> <li>Allows users to select or modify a scenario.</li> </ul> <p>Load and Save Configuration (JSON) Provides options to import/export scenario configurations using JSON.</p> <p>Load JSON:</p> <ul> <li>Allows users to upload a JSON file to load scenario configurations.</li> </ul> <p>Download Configuration:</p> <ul> <li>Exports the current scenario settings into a JSON file.</li> </ul> <p>Run Federation Starts the federation process with the selected configuration.</p>"},{"location":"userguide/#scenario-configuration-steps","title":"Scenario configuration steps","text":"<p>1. Scenario Information Define general information about the scenario.</p> <ul> <li>Scenario title: Set a name for the scenario.</li> <li>Scenario description: Provide a description for the scenario.</li> </ul> <p>2. Deployment Select how the scenario will be deployed.</p> <ul> <li>Processes</li> <li>Docker containers</li> <li>Physical devices</li> </ul> <p>3. Federation Architecture Configure the architecture of the federation.</p> <ul> <li>Federation approach: Select the approach (e.g., DFL).</li> <li>Number of rounds: Define the number of training rounds.</li> </ul> <p>4. Network Topology Set up the network topology for participants.</p> <ul> <li> <p>Topology generation:</p> <ul> <li>Custom topology</li> <li>Predefined topology (e.g., Fully)</li> </ul> </li> <li> <p>Number of nodes: Specify the number of participants.</p> </li> </ul> <p>5. Dataset Configure dataset-related settings.</p> <ul> <li>Federated dataset: Select the dataset (e.g., MNIST).</li> <li>Dataset type: Choose between IID or Non-IID distribution.</li> <li>Partition methods: Set the partition strategy (e.g., Dirichlet).</li> <li>Parameter setting: Adjust parameters for data partitioning.</li> </ul> <p>6. Training Define the training model.</p> <ul> <li>Model: Choose the model architecture (e.g., MLP).</li> </ul> <p>7. Aggregation Configure the aggregation method.</p> <ul> <li>Aggregation algorithm: Select an aggregation algorithm (e.g., FedAvg).</li> </ul> <p>8. Participants Configure the participants involved in the federation.</p> <ul> <li>Logging: Choose the type of logs to be recorded (e.g., alerts and logs).</li> <li>Reporting: Enable or disable reporting for the participants.</li> <li>Individual participants: View details or start specific participants manually.</li> </ul> <p>9. Advanced Topology Define the spatial distribution of participants.</p> <ul> <li>Distance between participants: Adjust the distance between nodes in the topology.</li> </ul> <p>10. Advanced Training Set additional training parameters.</p> <ul> <li>Number of Epochs: Define the number of training epochs for each participant.</li> </ul> <p>11. Robustness Configure the robustness of the federation by specifying potential attacks.</p> <ul> <li>Attack Type: Choose from different types of attacks or select \"No Attack\" for a standard setup.</li> </ul> <p>12. Defense Enable or disable security mechanisms for the federation.</p> <ul> <li>Reputation System: Choose whether to enable or disable reputation-based security.</li> </ul> <p>13. Mobility Manage the mobility settings for participants.</p> <ul> <li>Default location selection: Set participant locations as random or custom.</li> <li>Mobility configuration: Enable or disable participant mobility.</li> </ul>"},{"location":"userguide/#dashboard","title":"Dashboard","text":"<p>The NEBULA Dashboard provides an overview of the current federation scenarios and allows users to manage and monitor them effectively. Below is an explanation of the key components and buttons visible on the dashboard.</p> <p>Current Scenario Status</p> <ul> <li>Scenario name: Displays the name of the currently running scenario.</li> <li>Scenario title and description: Shows the title and description provided during the scenario creation.</li> <li>Scenario start time: Indicates when the scenario was initiated.</li> </ul> <p>Buttons</p> <ul> <li>Deploy new scenario:   Use this button to create and deploy a new federation scenario. It redirects you to the scenario configuration interface.</li> <li>Compare scenarios:   Allows you to compare the results of completed scenarios. Useful for analyzing performance differences.</li> </ul> <p>Scenarios in the Database This section provides a table summarizing all scenarios in the database.</p> <p>Columns</p> <ul> <li>User: Shows the user who created the scenario.</li> <li>Title: Displays the scenario title.</li> <li>Start time: Indicates when the scenario was started.</li> <li>Model: The model being used for training (e.g., MLP).</li> <li>Dataset: The dataset used in the scenario (e.g., MNIST).</li> <li>Rounds: The number of training rounds configured.</li> <li>Status: Indicates whether the scenario is running, stopped, or completed.</li> </ul> <p>Buttons in the \"Action\" Column</p> <ul> <li>Monitor:   Opens the monitoring interface for the selected scenario, showing metrics like accuracy and loss over time.</li> <li>Real-time metrics:   Displays live updates of training metrics while the scenario is running.</li> <li>Save Note:   Allows you to save custom notes or observations related to the scenario for future reference.</li> <li>Scenario Config:   Opens the configuration details of the selected scenario, allowing you to review the parameters used during its creation.</li> <li>Stop scenario:   Immediately halts the execution of the selected scenario.</li> </ul>"},{"location":"userguide/#monitor","title":"Monitor","text":"<p>Scenario Information This section provides a summary of the scenario's metadata and controls for managing it.</p> <ul> <li>Title: Displays the name of the scenario.</li> <li>Description: A brief description of the scenario's purpose or configuration.</li> <li>Status: Indicates whether the scenario is Running, Completed or Stopped.</li> </ul> <p>Buttons</p> <ul> <li>Stop Scenario: Halts the execution of the currently running scenario.</li> <li>Real-time Metrics: Redirects you to the real-time metrics dashboard to monitor the scenario's performance.</li> <li>Download Logs/Metrics: Downloads logs and metrics data for further analysis.</li> <li>Download Metrics: Allows you to save metric data separately in your local environment.</li> </ul> <p>Nodes in the Database This table summarizes all nodes participating in the scenario.</p> <p>Columns</p> <ul> <li>UID: A unique identifier for each node.</li> <li>IDX: The index of the node in the scenario.</li> <li>IP: The IP address of the node.</li> <li>Role: Indicates the role of the node (e.g., aggregator).</li> <li>Round: Specifies the current round of operation for the node.</li> <li>Status: Displays whether the node is Online or Offline.</li> <li>Logs: A button to access detailed logs for each node.</li> </ul> <p>Map A real-time visualization of the nodes and their interactions displayed on a map.</p> <ul> <li>Purpose: Provides a geographical representation of node distributions and connections.</li> <li>Features:</li> <li>Interactive map with zoom and pan functionality.</li> <li>Visualizes active nodes and their interactions with connecting lines.</li> </ul> <p>Topology This section illustrates the network topology of the scenario.</p> <ul> <li>Purpose: Shows the relationships and interactions between nodes in a structured topology.</li> <li>Download: You can click the \"here\" link to download a detailed representation of the topology.</li> </ul>"},{"location":"userguide/#realtime-metrics","title":"Realtime Metrics","text":"<p>Displays graphs of metrics recorded during the execution.   Each graph is interactive, enabling comparison of metrics across participants or scenarios.</p> <ul> <li> <p>Main options:</p> <ul> <li>Filters: Use regex to select the runs to display.</li> <li>Axes: Configure the horizontal axis (step, relative, wall).</li> <li>Smoothing: Adjust the curve using a slider.</li> </ul> </li> <li> <p>Example metrics:</p> <ul> <li>Test (Global)/Accuracy</li> <li>Test (Global)/F1Score</li> </ul> </li> </ul> <p></p> <p>Similar to the Time Series tab but focused on global scalar values, such as accuracy and loss over iterations.</p> <p>Images (Confusion Matrix): </p> <p>Displays the confusion matrix generated during the execution.   Used to evaluate the classification performance of the models.   Each matrix is specific to a participant or global metrics.</p> <p>Configuration Options:</p> <ul> <li>Smoothing: Adjusts the visualization of the curves.</li> <li>Ignore outliers: Excludes outlier values in the graphs for a clearer representation.</li> <li>Card size: Modifies the size of the graphs.</li> </ul>"},{"location":"api/","title":"Documentation for Nebula Module","text":"<p>This API Reference is designed to help developers understand every part of the code, providing detailed information about functions, parameters, data structures, and interactions within the platform.</p> <p>On the left, you'll find the directory tree of the platform, including folders, functions, code, and documentation.</p>"},{"location":"api/SUMMARY/","title":"SUMMARY","text":"<ul> <li>nebula<ul> <li>addons<ul> <li>attacks<ul> <li>attacks</li> <li>communications<ul> <li>communicationattack</li> <li>delayerattack</li> <li>floodingattack</li> </ul> </li> <li>dataset<ul> <li>datapoison</li> <li>datasetattack</li> <li>labelflipping</li> </ul> </li> <li>model<ul> <li>gllneuroninversion</li> <li>modelattack</li> <li>modelpoison</li> <li>swappingweights</li> </ul> </li> </ul> </li> <li>blockchain<ul> <li>blockchain_deployer</li> <li>chaincode</li> <li>geth</li> <li>oracle<ul> <li>app</li> </ul> </li> </ul> </li> <li>env</li> <li>functions</li> <li>gps<ul> <li>gpsmodule</li> <li>nebulagps</li> </ul> </li> <li>mobility</li> <li>networksimulation<ul> <li>nebulanetworksimulator</li> <li>networksimulator</li> </ul> </li> <li>reporter</li> <li>reputation<ul> <li>reputation</li> </ul> </li> <li>topologymanager</li> <li>trustworthiness<ul> <li>calculation</li> <li>factsheet</li> <li>metric</li> <li>pillar</li> <li>utils</li> </ul> </li> <li>waf</li> </ul> </li> <li>controller</li> <li>core<ul> <li>addonmanager</li> <li>aggregation<ul> <li>aggregator</li> <li>blockchainReputation</li> <li>fedavg</li> <li>krum</li> <li>median</li> <li>trimmedmean</li> <li>updatehandlers<ul> <li>cflupdatehandler</li> <li>dflupdatehandler</li> <li>sdflupdatehandler</li> <li>updatehandler</li> </ul> </li> </ul> </li> <li>datasets<ul> <li>changeablesubset</li> <li>cifar10<ul> <li>cifar10</li> </ul> </li> <li>cifar100<ul> <li>cifar100</li> </ul> </li> <li>datamodule</li> <li>emnist<ul> <li>emnist</li> </ul> </li> <li>fashionmnist<ul> <li>fashionmnist</li> </ul> </li> <li>mnist<ul> <li>mnist</li> </ul> </li> <li>nebuladataset</li> </ul> </li> <li>engine</li> <li>eventmanager</li> <li>models<ul> <li>cifar10<ul> <li>cnn</li> <li>cnnV2</li> <li>cnnV3</li> <li>fastermobilenet</li> <li>resnet</li> <li>simplemobilenet</li> </ul> </li> <li>cifar100<ul> <li>cnn</li> </ul> </li> <li>emnist<ul> <li>cnn</li> <li>mlp</li> </ul> </li> <li>fashionmnist<ul> <li>cnn</li> <li>mlp</li> </ul> </li> <li>mnist<ul> <li>cnn</li> <li>mlp</li> </ul> </li> <li>nebulamodel</li> <li>sentiment140<ul> <li>cnn</li> <li>rnn</li> </ul> </li> </ul> </li> <li>nebulaevents</li> <li>network<ul> <li>actions</li> <li>communications</li> <li>connection</li> <li>discoverer</li> <li>forwarder</li> <li>health</li> <li>messages</li> <li>propagator</li> </ul> </li> <li>pb<ul> <li>nebula_pb2</li> </ul> </li> <li>role</li> <li>training<ul> <li>lightning</li> <li>scikit</li> <li>siamese</li> </ul> </li> </ul> </li> <li>frontend<ul> <li>app</li> <li>database</li> </ul> </li> <li>node</li> <li>scenarios</li> <li>utils</li> </ul> </li> </ul>"},{"location":"api/controller/","title":"Documentation for Controller Module","text":""},{"location":"api/controller/#nebula.controller.NebulaEventHandler","title":"<code>NebulaEventHandler</code>","text":"<p>               Bases: <code>PatternMatchingEventHandler</code></p> <p>NebulaEventHandler handles file system events for .sh scripts.</p> <p>This class monitors the creation, modification, and deletion of .sh scripts in a specified directory.</p> Source code in <code>nebula/controller.py</code> <pre><code>class NebulaEventHandler(PatternMatchingEventHandler):\n    \"\"\"\n    NebulaEventHandler handles file system events for .sh scripts.\n\n    This class monitors the creation, modification, and deletion of .sh scripts\n    in a specified directory.\n    \"\"\"\n\n    patterns = [\"*.sh\", \"*.ps1\"]\n\n    def __init__(self):\n        super(NebulaEventHandler, self).__init__()\n        self.last_processed = {}\n        self.timeout_ns = 5 * 1e9\n        self.processing_files = set()\n        self.lock = threading.Lock()\n\n    def _should_process_event(self, src_path: str) -&gt; bool:\n        current_time_ns = time.time_ns()\n        logging.info(f\"Current time (ns): {current_time_ns}\")\n        with self.lock:\n            if src_path in self.last_processed:\n                logging.info(f\"Last processed time for {src_path}: {self.last_processed[src_path]}\")\n                last_time = self.last_processed[src_path]\n                if current_time_ns - last_time &lt; self.timeout_ns:\n                    return False\n            self.last_processed[src_path] = current_time_ns\n        return True\n\n    def _is_being_processed(self, src_path: str) -&gt; bool:\n        with self.lock:\n            if src_path in self.processing_files:\n                logging.info(f\"Skipping {src_path} as it is already being processed.\")\n                return True\n            self.processing_files.add(src_path)\n        return False\n\n    def _processing_done(self, src_path: str):\n        with self.lock:\n            if src_path in self.processing_files:\n                self.processing_files.remove(src_path)\n\n    def verify_nodes_ports(self, src_path):\n        parent_dir = os.path.dirname(src_path)\n        base_dir = os.path.basename(parent_dir)\n        scenario_path = os.path.join(os.path.dirname(parent_dir), base_dir)\n\n        try:\n            port_mapping = {}\n            new_port_start = 50000\n\n            participant_files = sorted(\n                f for f in os.listdir(scenario_path) if f.endswith(\".json\") and f.startswith(\"participant\")\n            )\n\n            for filename in participant_files:\n                file_path = os.path.join(scenario_path, filename)\n                with open(file_path) as json_file:\n                    node = json.load(json_file)\n                current_port = node[\"network_args\"][\"port\"]\n                port_mapping[current_port] = SocketUtils.find_free_port(start_port=new_port_start)\n                logging.info(\n                    f\"Participant file: {filename} | Current port: {current_port} | New port: {port_mapping[current_port]}\"\n                )\n                new_port_start = port_mapping[current_port] + 1\n\n            for filename in participant_files:\n                file_path = os.path.join(scenario_path, filename)\n                with open(file_path) as json_file:\n                    node = json.load(json_file)\n                current_port = node[\"network_args\"][\"port\"]\n                node[\"network_args\"][\"port\"] = port_mapping[current_port]\n                neighbors = node[\"network_args\"][\"neighbors\"]\n\n                for old_port, new_port in port_mapping.items():\n                    neighbors = neighbors.replace(f\":{old_port}\", f\":{new_port}\")\n\n                node[\"network_args\"][\"neighbors\"] = neighbors\n\n                with open(file_path, \"w\") as f:\n                    json.dump(node, f, indent=4)\n\n        except Exception as e:\n            print(f\"Error processing JSON files: {e}\")\n\n    def on_created(self, event):\n        \"\"\"\n        Handles the event when a file is created.\n        \"\"\"\n        if event.is_directory:\n            return\n        src_path = event.src_path\n        if not self._should_process_event(src_path):\n            return\n        if self._is_being_processed(src_path):\n            return\n        logging.info(\"File created: %s\" % src_path)\n        try:\n            self.verify_nodes_ports(src_path)\n            self.run_script(src_path)\n        finally:\n            self._processing_done(src_path)\n\n    def on_deleted(self, event):\n        \"\"\"\n        Handles the event when a file is deleted.\n        \"\"\"\n        if event.is_directory:\n            return\n        src_path = event.src_path\n        if not self._should_process_event(src_path):\n            return\n        if self._is_being_processed(src_path):\n            return\n        logging.info(\"File deleted: %s\" % src_path)\n        directory_script = os.path.dirname(src_path)\n        pids_file = os.path.join(directory_script, \"current_scenario_pids.txt\")\n        logging.info(f\"Killing processes from {pids_file}\")\n        try:\n            self.kill_script_processes(pids_file)\n            os.remove(pids_file)\n        except FileNotFoundError:\n            logging.warning(f\"{pids_file} not found.\")\n        except Exception as e:\n            logging.exception(f\"Error while killing processes: {e}\")\n        finally:\n            self._processing_done(src_path)\n\n    def run_script(self, script):\n        try:\n            logging.info(f\"Running script: {script}\")\n            if script.endswith(\".sh\"):\n                result = subprocess.run([\"bash\", script], capture_output=True, text=True)\n                logging.info(f\"Script output:\\n{result.stdout}\")\n                if result.stderr:\n                    logging.error(f\"Script error:\\n{result.stderr}\")\n            elif script.endswith(\".ps1\"):\n                subprocess.Popen(\n                    [\"powershell\", \"-ExecutionPolicy\", \"Bypass\", \"-File\", script],\n                    stdout=subprocess.PIPE,\n                    stderr=subprocess.PIPE,\n                    text=False,\n                )\n            else:\n                logging.error(\"Unsupported script format.\")\n                return\n        except Exception as e:\n            logging.exception(f\"Error while running script: {e}\")\n\n    def kill_script_processes(self, pids_file):\n        try:\n            with open(pids_file) as f:\n                pids = f.readlines()\n                for pid in pids:\n                    try:\n                        pid = int(pid.strip())\n                        if psutil.pid_exists(pid):\n                            process = psutil.Process(pid)\n                            children = process.children(recursive=True)\n                            logging.info(f\"Forcibly killing process {pid} and {len(children)} child processes...\")\n                            for child in children:\n                                try:\n                                    logging.info(f\"Forcibly killing child process {child.pid}\")\n                                    child.kill()\n                                except psutil.NoSuchProcess:\n                                    logging.warning(f\"Child process {child.pid} already terminated.\")\n                                except Exception as e:\n                                    logging.exception(f\"Error while forcibly killing child process {child.pid}: {e}\")\n                            try:\n                                logging.info(f\"Forcibly killing main process {pid}\")\n                                process.kill()\n                            except psutil.NoSuchProcess:\n                                logging.warning(f\"Process {pid} already terminated.\")\n                            except Exception as e:\n                                logging.exception(f\"Error while forcibly killing main process {pid}: {e}\")\n                        else:\n                            logging.warning(f\"PID {pid} does not exist.\")\n                    except ValueError:\n                        logging.exception(f\"Invalid PID value in file: {pid}\")\n                    except Exception as e:\n                        logging.exception(f\"Error while forcibly killing process {pid}: {e}\")\n        except FileNotFoundError:\n            logging.exception(f\"PID file not found: {pids_file}\")\n        except Exception as e:\n            logging.exception(f\"Error while reading PIDs from file: {e}\")\n</code></pre>"},{"location":"api/controller/#nebula.controller.NebulaEventHandler.on_created","title":"<code>on_created(event)</code>","text":"<p>Handles the event when a file is created.</p> Source code in <code>nebula/controller.py</code> <pre><code>def on_created(self, event):\n    \"\"\"\n    Handles the event when a file is created.\n    \"\"\"\n    if event.is_directory:\n        return\n    src_path = event.src_path\n    if not self._should_process_event(src_path):\n        return\n    if self._is_being_processed(src_path):\n        return\n    logging.info(\"File created: %s\" % src_path)\n    try:\n        self.verify_nodes_ports(src_path)\n        self.run_script(src_path)\n    finally:\n        self._processing_done(src_path)\n</code></pre>"},{"location":"api/controller/#nebula.controller.NebulaEventHandler.on_deleted","title":"<code>on_deleted(event)</code>","text":"<p>Handles the event when a file is deleted.</p> Source code in <code>nebula/controller.py</code> <pre><code>def on_deleted(self, event):\n    \"\"\"\n    Handles the event when a file is deleted.\n    \"\"\"\n    if event.is_directory:\n        return\n    src_path = event.src_path\n    if not self._should_process_event(src_path):\n        return\n    if self._is_being_processed(src_path):\n        return\n    logging.info(\"File deleted: %s\" % src_path)\n    directory_script = os.path.dirname(src_path)\n    pids_file = os.path.join(directory_script, \"current_scenario_pids.txt\")\n    logging.info(f\"Killing processes from {pids_file}\")\n    try:\n        self.kill_script_processes(pids_file)\n        os.remove(pids_file)\n    except FileNotFoundError:\n        logging.warning(f\"{pids_file} not found.\")\n    except Exception as e:\n        logging.exception(f\"Error while killing processes: {e}\")\n    finally:\n        self._processing_done(src_path)\n</code></pre>"},{"location":"api/node/","title":"Documentation for Node Module","text":""},{"location":"api/node/#nebula.node.main","title":"<code>main(config)</code>  <code>async</code>","text":"<p>Main function to start the NEBULA node.</p> <p>This function initiates the NEBULA core component deployed on each federation participant. It configures the node using the provided configuration object, setting up dataset partitions, selecting and initializing the appropriate model and data handler, and establishing training mechanisms. Additionally, it adjusts specific node parameters (such as indices and timing intervals) based on the participant's configuration, and deploys the node's network communications for federated learning.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>Configuration object containing settings for: - scenario (including federation and deployment parameters), - model selection and its corresponding hyperparameters, - dataset and data partitioning, - training strategy and related arguments, - device roles and security flags.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported model, dataset, or device role is specified.</p> <code>NotImplementedError</code> <p>If an unsupported training strategy (e.g., \"scikit\") is requested.</p> <p>Returns:</p> Type Description <p>Coroutine that initializes and starts the NEBULA node.</p> Source code in <code>nebula/node.py</code> <pre><code>async def main(config):\n    \"\"\"\n    Main function to start the NEBULA node.\n\n    This function initiates the NEBULA core component deployed on each federation participant.\n    It configures the node using the provided configuration object, setting up dataset partitions,\n    selecting and initializing the appropriate model and data handler, and establishing training\n    mechanisms. Additionally, it adjusts specific node parameters (such as indices and timing intervals)\n    based on the participant's configuration, and deploys the node's network communications for\n    federated learning.\n\n    Parameters:\n        config (Config): Configuration object containing settings for:\n            - scenario (including federation and deployment parameters),\n            - model selection and its corresponding hyperparameters,\n            - dataset and data partitioning,\n            - training strategy and related arguments,\n            - device roles and security flags.\n\n    Raises:\n        ValueError: If an unsupported model, dataset, or device role is specified.\n        NotImplementedError: If an unsupported training strategy (e.g., \"scikit\") is requested.\n\n    Returns:\n        Coroutine that initializes and starts the NEBULA node.\n    \"\"\"\n    n_nodes = config.participant[\"scenario_args\"][\"n_nodes\"]\n    model_name = config.participant[\"model_args\"][\"model\"]\n    idx = config.participant[\"device_args\"][\"idx\"]\n\n    additional_node_status = config.participant[\"mobility_args\"][\"additional_node\"][\"status\"]\n    additional_node_round = config.participant[\"mobility_args\"][\"additional_node\"][\"round_start\"]\n\n    # Adjust the total number of nodes and the index of the current node for CFL, as it doesn't require a specific partition for the server (not used for training)\n    if config.participant[\"scenario_args\"][\"federation\"] == \"CFL\":\n        n_nodes -= 1\n        if idx &gt; 0:\n            idx -= 1\n\n    dataset = None\n    dataset_name = config.participant[\"data_args\"][\"dataset\"]\n    handler = None\n    batch_size = None\n    num_workers = config.participant[\"data_args\"][\"num_workers\"]\n    model = None\n\n    if dataset_name == \"MNIST\":\n        batch_size = 32\n        handler = MNISTPartitionHandler\n        if model_name == \"MLP\":\n            model = MNISTModelMLP()\n        elif model_name == \"CNN\":\n            model = MNISTModelCNN()\n        else:\n            raise ValueError(f\"Model {model} not supported for dataset {dataset_name}\")\n    elif dataset_name == \"FashionMNIST\":\n        batch_size = 32\n        handler = FashionMNISTPartitionHandler\n        if model_name == \"MLP\":\n            model = FashionMNISTModelMLP()\n        elif model_name == \"CNN\":\n            model = FashionMNISTModelCNN()\n        else:\n            raise ValueError(f\"Model {model} not supported for dataset {dataset_name}\")\n    elif dataset_name == \"EMNIST\":\n        batch_size = 32\n        handler = EMNISTPartitionHandler\n        if model_name == \"MLP\":\n            model = EMNISTModelMLP()\n        elif model_name == \"CNN\":\n            model = EMNISTModelCNN()\n        else:\n            raise ValueError(f\"Model {model} not supported for dataset {dataset_name}\")\n    elif dataset_name == \"CIFAR10\":\n        batch_size = 32\n        handler = CIFAR10PartitionHandler\n        if model_name == \"ResNet9\":\n            model = CIFAR10ModelResNet(classifier=\"resnet9\")\n        elif model_name == \"fastermobilenet\":\n            model = FasterMobileNet()\n        elif model_name == \"simplemobilenet\":\n            model = SimpleMobileNetV1()\n        elif model_name == \"CNN\":\n            model = CIFAR10ModelCNN()\n        elif model_name == \"CNNv2\":\n            model = CIFAR10ModelCNN_V2()\n        elif model_name == \"CNNv3\":\n            model = CIFAR10ModelCNN_V3()\n        else:\n            raise ValueError(f\"Model {model} not supported for dataset {dataset_name}\")\n    elif dataset_name == \"CIFAR100\":\n        batch_size = 128\n        handler = CIFAR100PartitionHandler\n        if model_name == \"CNN\":\n            model = CIFAR100ModelCNN()\n        else:\n            raise ValueError(f\"Model {model} not supported for dataset {dataset_name}\")\n    else:\n        raise ValueError(f\"Dataset {dataset_name} not supported\")\n\n    dataset = NebulaPartition(handler=handler, config=config)\n    dataset.load_partition()\n    dataset.log_partition()\n\n    datamodule = DataModule(\n        train_set=dataset.train_set,\n        train_set_indices=dataset.train_indices,\n        test_set=dataset.test_set,\n        test_set_indices=dataset.test_indices,\n        local_test_set=dataset.local_test_set,\n        local_test_set_indices=dataset.local_test_indices,\n        num_workers=num_workers,\n        batch_size=batch_size,\n    )\n\n    trainer = None\n    trainer_str = config.participant[\"training_args\"][\"trainer\"]\n    if trainer_str == \"lightning\":\n        trainer = Lightning\n    elif trainer_str == \"scikit\":\n        raise NotImplementedError\n    elif trainer_str == \"siamese\":\n        trainer = Siamese\n    else:\n        raise ValueError(f\"Trainer {trainer_str} not supported\")\n\n    if config.participant[\"device_args\"][\"malicious\"]:\n        node_cls = MaliciousNode\n    else:\n        if config.participant[\"device_args\"][\"role\"] == Role.AGGREGATOR:\n            node_cls = AggregatorNode\n        elif config.participant[\"device_args\"][\"role\"] == Role.TRAINER:\n            node_cls = TrainerNode\n        elif config.participant[\"device_args\"][\"role\"] == Role.SERVER:\n            node_cls = ServerNode\n        elif config.participant[\"device_args\"][\"role\"] == Role.IDLE:\n            node_cls = IdleNode\n        else:\n            raise ValueError(f\"Role {config.participant['device_args']['role']} not supported\")\n\n    VARIABILITY = 0.5\n\n    def randomize_value(value, variability):\n        min_value = max(0, value - variability)\n        max_value = value + variability\n        return random.uniform(min_value, max_value)\n\n    config_keys = [\n        [\"reporter_args\", \"report_frequency\"],\n        [\"discoverer_args\", \"discovery_frequency\"],\n        [\"health_args\", \"health_interval\"],\n        [\"health_args\", \"grace_time_health\"],\n        [\"health_args\", \"check_alive_interval\"],\n        [\"health_args\", \"send_alive_interval\"],\n        [\"forwarder_args\", \"forwarder_interval\"],\n        [\"forwarder_args\", \"forward_messages_interval\"],\n    ]\n\n    for keys in config_keys:\n        value = config.participant\n        for key in keys[:-1]:\n            value = value[key]\n        value[keys[-1]] = randomize_value(value[keys[-1]], VARIABILITY)\n\n    logging.info(f\"Starting node {idx} with model {model_name}, trainer {trainer.__name__}, and as {node_cls.__name__}\")\n\n    node = node_cls(\n        model=model,\n        datamodule=datamodule,\n        config=config,\n        trainer=trainer,\n        security=False,\n    )\n    await node.start_communications()\n    await node.deploy_federation()\n\n    # If it is an additional node, it should wait until additional_node_round to connect to the network\n    # In order to do that, it should request the current round to the controller\n    if additional_node_status:\n        logging.info(f\"Waiting for round {additional_node_round} to start\")\n        logging.info(\"Waiting time to start finding federation\")\n\n        await asyncio.sleep(6000)  # DEBUG purposes\n\n    if node.cm is not None:\n        await node.cm.network_wait()\n</code></pre>"},{"location":"api/scenarios/","title":"Documentation for Scenarios Module","text":""},{"location":"api/scenarios/#nebula.scenarios.Scenario","title":"<code>Scenario</code>","text":"<p>Class to define a scenario for the NEBULA platform. It contains all the parameters needed to create a scenario and run it on the platform.</p> Source code in <code>nebula/scenarios.py</code> <pre><code>class Scenario:\n    \"\"\"\n    Class to define a scenario for the NEBULA platform.\n    It contains all the parameters needed to create a scenario and run it on the platform.\n    \"\"\"\n\n    def __init__(\n        self,\n        scenario_title,\n        scenario_description,\n        deployment,\n        federation,\n        topology,\n        nodes,\n        nodes_graph,\n        n_nodes,\n        matrix,\n        dataset,\n        iid,\n        partition_selection,\n        partition_parameter,\n        model,\n        agg_algorithm,\n        rounds,\n        logginglevel,\n        report_status_data_queue,\n        accelerator,\n        gpu_id,\n        network_subnet,\n        network_gateway,\n        epochs,\n        attacks,\n        poisoned_node_percent,\n        poisoned_sample_percent,\n        poisoned_noise_percent,\n        attack_params,\n        with_reputation,\n        reputation_metrics,\n        initial_reputation,\n        weighting_factor,\n        weight_model_arrival_latency,\n        weight_model_similarity,\n        weight_num_messages,\n        weight_fraction_params_changed,\n        # is_dynamic_topology,\n        # is_dynamic_aggregation,\n        # target_aggregation,\n        random_geo,\n        latitude,\n        longitude,\n        mobility,\n        mobility_type,\n        radius_federation,\n        scheme_mobility,\n        round_frequency,\n        mobile_participants_percent,\n        additional_participants,\n        schema_additional_participants,\n        random_topology_probability,\n    ):\n        \"\"\"\n        Initialize the scenario.\n\n        Args:\n            scenario_title (str): Title of the scenario.\n            scenario_description (str): Description of the scenario.\n            deployment (str): Type of deployment (e.g., 'docker', 'process').\n            federation (str): Type of federation.\n            topology (str): Network topology.\n            nodes (dict): Dictionary of nodes.\n            nodes_graph (dict): Graph of nodes.\n            n_nodes (int): Number of nodes.\n            matrix (list): Matrix of connections.\n            dataset (str): Dataset used.\n            iid (bool): Indicator if data is independent and identically distributed.\n            partition_selection (str): Method of partition selection.\n            partition_parameter (float): Parameter for partition selection.\n            model (str): Model used.\n            agg_algorithm (str): Aggregation algorithm.\n            rounds (int): Number of rounds.\n            logginglevel (str): Logging level.\n            report_status_data_queue (bool): Indicator to report information about the nodes of the scenario\n            accelerator (str): Accelerator used.\n            gpu_id (list) : Id list of the used gpu\n            network_subnet (str): Network subnet.\n            network_gateway (str): Network gateway.\n            epochs (int): Number of epochs.\n            attacks (list): List of attacks.\n            poisoned_node_percent (float): Percentage of poisoned nodes.\n            poisoned_sample_percent (float): Percentage of poisoned samples.\n            noise_type (str): The type of noise applied by the attack.\n            targeted (bool): Indicator if the attack is targeted.\n            target_label (int): The label to change when `targeted` is True.\n            target_changed_label (int): The label to which `target_label` will be changed .\n            attack_params (dict) : Attack parameters.\n            with_reputation (bool): Indicator if reputation is used.\n            reputation_metrics (list): List of reputation metrics.\n            initial_reputation (float): Initial reputation.\n            weighting_factor (str): dymanic or static weighting factor.\n            weight_model_arrival_latency (float): Weight of model arrival latency.\n            weight_model_similarity (float): Weight of model similarity.\n            weight_num_messages (float): Weight of number of messages.\n            weight_fraction_params_changed (float): Weight of fraction of parameters changed.\n            # is_dynamic_topology (bool): Indicator if topology is dynamic.\n            # is_dynamic_aggregation (bool): Indicator if aggregation is dynamic.\n            # target_aggregation (str): Target aggregation method.\n            random_geo (bool): Indicator if random geo is used.\n            latitude (float): Latitude for mobility.\n            longitude (float): Longitude for mobility.\n            mobility (bool): Indicator if mobility is used.\n            mobility_type (str): Type of mobility.\n            radius_federation (float): Radius of federation.\n            scheme_mobility (str): Scheme of mobility.\n            round_frequency (int): Frequency of rounds.\n            mobile_participants_percent (float): Percentage of mobile participants.\n            additional_participants (list): List of additional participants.\n            schema_additional_participants (str): Schema for additional participants.\n            random_topology_probability (float): Probability for random topology.\n        \"\"\"\n        self.scenario_title = scenario_title\n        self.scenario_description = scenario_description\n        self.deployment = deployment\n        self.federation = federation\n        self.topology = topology\n        self.nodes = nodes\n        self.nodes_graph = nodes_graph\n        self.n_nodes = n_nodes\n        self.matrix = matrix\n        self.dataset = dataset\n        self.iid = iid\n        self.partition_selection = partition_selection\n        self.partition_parameter = partition_parameter\n        self.model = model\n        self.agg_algorithm = agg_algorithm\n        self.rounds = rounds\n        self.logginglevel = logginglevel\n        self.report_status_data_queue = report_status_data_queue\n        self.accelerator = accelerator\n        self.gpu_id = gpu_id\n        self.network_subnet = network_subnet\n        self.network_gateway = network_gateway\n        self.epochs = epochs\n        self.attacks = attacks\n        self.poisoned_node_percent = poisoned_node_percent\n        self.poisoned_sample_percent = poisoned_sample_percent\n        self.poisoned_noise_percent = poisoned_noise_percent\n        self.attack_params = attack_params\n        self.with_reputation = with_reputation\n        self.reputation_metrics = reputation_metrics\n        self.initial_reputation = initial_reputation\n        self.weighting_factor = weighting_factor\n        self.weight_model_arrival_latency = weight_model_arrival_latency\n        self.weight_model_similarity = weight_model_similarity\n        self.weight_num_messages = weight_num_messages\n        self.weight_fraction_params_changed = weight_fraction_params_changed\n        # self.is_dynamic_topology = is_dynamic_topology\n        # self.is_dynamic_aggregation = is_dynamic_aggregation\n        # self.target_aggregation = target_aggregation\n        self.random_geo = random_geo\n        self.latitude = latitude\n        self.longitude = longitude\n        self.mobility = mobility\n        self.mobility_type = mobility_type\n        self.radius_federation = radius_federation\n        self.scheme_mobility = scheme_mobility\n        self.round_frequency = round_frequency\n        self.mobile_participants_percent = mobile_participants_percent\n        self.additional_participants = additional_participants\n        self.schema_additional_participants = schema_additional_participants\n        self.random_topology_probability = random_topology_probability\n\n    def attack_node_assign(\n        self,\n        nodes,\n        federation,\n        attack,\n        poisoned_node_percent,\n        poisoned_sample_percent,\n        poisoned_noise_percent,\n        attack_params,\n    ):\n        \"\"\"Identify which nodes will be attacked\"\"\"\n        import math\n        import random\n\n        nodes_index = []\n        # Get the nodes index\n        if federation == \"DFL\":\n            nodes_index = list(nodes.keys())\n        else:\n            for node in nodes:\n                if nodes[node][\"role\"] != \"server\":\n                    nodes_index.append(node)\n\n        mal_nodes_defined = any(nodes[node][\"malicious\"] for node in nodes)\n\n        attacked_nodes = []\n\n        if not mal_nodes_defined:\n            n_nodes = len(nodes_index)\n            # Number of attacked nodes, round up\n            num_attacked = int(math.ceil(poisoned_node_percent / 100 * n_nodes))\n            if num_attacked &gt; n_nodes:\n                num_attacked = n_nodes\n\n            # Get the index of attacked nodes\n            attacked_nodes = random.sample(nodes_index, num_attacked)\n\n        # Assign the role of each node\n        for node in nodes:\n            node_att = \"No Attack\"\n            malicious = False\n            with_reputation = self.with_reputation\n            attack_sample_percent = 0\n            poisoned_ratio = 0\n            if (str(nodes[node][\"id\"]) in attacked_nodes) or (nodes[node][\"malicious\"]):\n                malicious = True\n                with_reputation = False\n                node_att = attack\n                attack_sample_percent = poisoned_sample_percent / 100\n                poisoned_ratio = poisoned_noise_percent / 100\n                attack_params[\"poisoned_percent\"] = attack_sample_percent\n                attack_params[\"poisoned_ratio\"] = poisoned_ratio\n            nodes[node][\"malicious\"] = malicious\n            nodes[node][\"with_reputation\"] = with_reputation\n            nodes[node][\"attacks\"] = node_att\n            nodes[node][\"attack_params\"] = attack_params\n        return nodes\n\n    def mobility_assign(self, nodes, mobile_participants_percent):\n        \"\"\"Assign mobility to nodes\"\"\"\n        import random\n\n        # Number of mobile nodes, round down\n        num_mobile = math.floor(mobile_participants_percent / 100 * len(nodes))\n        if num_mobile &gt; len(nodes):\n            num_mobile = len(nodes)\n\n        # Get the index of mobile nodes\n        mobile_nodes = random.sample(list(nodes.keys()), num_mobile)\n\n        # Assign the role of each node\n        for node in nodes:\n            node_mob = False\n            if node in mobile_nodes:\n                node_mob = True\n            nodes[node][\"mobility\"] = node_mob\n        return nodes\n\n    @classmethod\n    def from_dict(cls, data):\n        return cls(**data)\n</code></pre>"},{"location":"api/scenarios/#nebula.scenarios.Scenario.__init__","title":"<code>__init__(scenario_title, scenario_description, deployment, federation, topology, nodes, nodes_graph, n_nodes, matrix, dataset, iid, partition_selection, partition_parameter, model, agg_algorithm, rounds, logginglevel, report_status_data_queue, accelerator, gpu_id, network_subnet, network_gateway, epochs, attacks, poisoned_node_percent, poisoned_sample_percent, poisoned_noise_percent, attack_params, with_reputation, reputation_metrics, initial_reputation, weighting_factor, weight_model_arrival_latency, weight_model_similarity, weight_num_messages, weight_fraction_params_changed, random_geo, latitude, longitude, mobility, mobility_type, radius_federation, scheme_mobility, round_frequency, mobile_participants_percent, additional_participants, schema_additional_participants, random_topology_probability)</code>","text":"<p>Initialize the scenario.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_title</code> <code>str</code> <p>Title of the scenario.</p> required <code>scenario_description</code> <code>str</code> <p>Description of the scenario.</p> required <code>deployment</code> <code>str</code> <p>Type of deployment (e.g., 'docker', 'process').</p> required <code>federation</code> <code>str</code> <p>Type of federation.</p> required <code>topology</code> <code>str</code> <p>Network topology.</p> required <code>nodes</code> <code>dict</code> <p>Dictionary of nodes.</p> required <code>nodes_graph</code> <code>dict</code> <p>Graph of nodes.</p> required <code>n_nodes</code> <code>int</code> <p>Number of nodes.</p> required <code>matrix</code> <code>list</code> <p>Matrix of connections.</p> required <code>dataset</code> <code>str</code> <p>Dataset used.</p> required <code>iid</code> <code>bool</code> <p>Indicator if data is independent and identically distributed.</p> required <code>partition_selection</code> <code>str</code> <p>Method of partition selection.</p> required <code>partition_parameter</code> <code>float</code> <p>Parameter for partition selection.</p> required <code>model</code> <code>str</code> <p>Model used.</p> required <code>agg_algorithm</code> <code>str</code> <p>Aggregation algorithm.</p> required <code>rounds</code> <code>int</code> <p>Number of rounds.</p> required <code>logginglevel</code> <code>str</code> <p>Logging level.</p> required <code>report_status_data_queue</code> <code>bool</code> <p>Indicator to report information about the nodes of the scenario</p> required <code>accelerator</code> <code>str</code> <p>Accelerator used.</p> required <code>gpu_id</code> <code>list) </code> <p>Id list of the used gpu</p> required <code>network_subnet</code> <code>str</code> <p>Network subnet.</p> required <code>network_gateway</code> <code>str</code> <p>Network gateway.</p> required <code>epochs</code> <code>int</code> <p>Number of epochs.</p> required <code>attacks</code> <code>list</code> <p>List of attacks.</p> required <code>poisoned_node_percent</code> <code>float</code> <p>Percentage of poisoned nodes.</p> required <code>poisoned_sample_percent</code> <code>float</code> <p>Percentage of poisoned samples.</p> required <code>noise_type</code> <code>str</code> <p>The type of noise applied by the attack.</p> required <code>targeted</code> <code>bool</code> <p>Indicator if the attack is targeted.</p> required <code>target_label</code> <code>int</code> <p>The label to change when <code>targeted</code> is True.</p> required <code>target_changed_label</code> <code>int</code> <p>The label to which <code>target_label</code> will be changed .</p> required <code>attack_params</code> <code>dict) </code> <p>Attack parameters.</p> required <code>with_reputation</code> <code>bool</code> <p>Indicator if reputation is used.</p> required <code>reputation_metrics</code> <code>list</code> <p>List of reputation metrics.</p> required <code>initial_reputation</code> <code>float</code> <p>Initial reputation.</p> required <code>weighting_factor</code> <code>str</code> <p>dymanic or static weighting factor.</p> required <code>weight_model_arrival_latency</code> <code>float</code> <p>Weight of model arrival latency.</p> required <code>weight_model_similarity</code> <code>float</code> <p>Weight of model similarity.</p> required <code>weight_num_messages</code> <code>float</code> <p>Weight of number of messages.</p> required <code>weight_fraction_params_changed</code> <code>float</code> <p>Weight of fraction of parameters changed.</p> required <code>#</code> <code>is_dynamic_topology (bool</code> <p>Indicator if topology is dynamic.</p> required <code>#</code> <code>is_dynamic_aggregation (bool</code> <p>Indicator if aggregation is dynamic.</p> required <code>#</code> <code>target_aggregation (str</code> <p>Target aggregation method.</p> required <code>random_geo</code> <code>bool</code> <p>Indicator if random geo is used.</p> required <code>latitude</code> <code>float</code> <p>Latitude for mobility.</p> required <code>longitude</code> <code>float</code> <p>Longitude for mobility.</p> required <code>mobility</code> <code>bool</code> <p>Indicator if mobility is used.</p> required <code>mobility_type</code> <code>str</code> <p>Type of mobility.</p> required <code>radius_federation</code> <code>float</code> <p>Radius of federation.</p> required <code>scheme_mobility</code> <code>str</code> <p>Scheme of mobility.</p> required <code>round_frequency</code> <code>int</code> <p>Frequency of rounds.</p> required <code>mobile_participants_percent</code> <code>float</code> <p>Percentage of mobile participants.</p> required <code>additional_participants</code> <code>list</code> <p>List of additional participants.</p> required <code>schema_additional_participants</code> <code>str</code> <p>Schema for additional participants.</p> required <code>random_topology_probability</code> <code>float</code> <p>Probability for random topology.</p> required Source code in <code>nebula/scenarios.py</code> <pre><code>def __init__(\n    self,\n    scenario_title,\n    scenario_description,\n    deployment,\n    federation,\n    topology,\n    nodes,\n    nodes_graph,\n    n_nodes,\n    matrix,\n    dataset,\n    iid,\n    partition_selection,\n    partition_parameter,\n    model,\n    agg_algorithm,\n    rounds,\n    logginglevel,\n    report_status_data_queue,\n    accelerator,\n    gpu_id,\n    network_subnet,\n    network_gateway,\n    epochs,\n    attacks,\n    poisoned_node_percent,\n    poisoned_sample_percent,\n    poisoned_noise_percent,\n    attack_params,\n    with_reputation,\n    reputation_metrics,\n    initial_reputation,\n    weighting_factor,\n    weight_model_arrival_latency,\n    weight_model_similarity,\n    weight_num_messages,\n    weight_fraction_params_changed,\n    # is_dynamic_topology,\n    # is_dynamic_aggregation,\n    # target_aggregation,\n    random_geo,\n    latitude,\n    longitude,\n    mobility,\n    mobility_type,\n    radius_federation,\n    scheme_mobility,\n    round_frequency,\n    mobile_participants_percent,\n    additional_participants,\n    schema_additional_participants,\n    random_topology_probability,\n):\n    \"\"\"\n    Initialize the scenario.\n\n    Args:\n        scenario_title (str): Title of the scenario.\n        scenario_description (str): Description of the scenario.\n        deployment (str): Type of deployment (e.g., 'docker', 'process').\n        federation (str): Type of federation.\n        topology (str): Network topology.\n        nodes (dict): Dictionary of nodes.\n        nodes_graph (dict): Graph of nodes.\n        n_nodes (int): Number of nodes.\n        matrix (list): Matrix of connections.\n        dataset (str): Dataset used.\n        iid (bool): Indicator if data is independent and identically distributed.\n        partition_selection (str): Method of partition selection.\n        partition_parameter (float): Parameter for partition selection.\n        model (str): Model used.\n        agg_algorithm (str): Aggregation algorithm.\n        rounds (int): Number of rounds.\n        logginglevel (str): Logging level.\n        report_status_data_queue (bool): Indicator to report information about the nodes of the scenario\n        accelerator (str): Accelerator used.\n        gpu_id (list) : Id list of the used gpu\n        network_subnet (str): Network subnet.\n        network_gateway (str): Network gateway.\n        epochs (int): Number of epochs.\n        attacks (list): List of attacks.\n        poisoned_node_percent (float): Percentage of poisoned nodes.\n        poisoned_sample_percent (float): Percentage of poisoned samples.\n        noise_type (str): The type of noise applied by the attack.\n        targeted (bool): Indicator if the attack is targeted.\n        target_label (int): The label to change when `targeted` is True.\n        target_changed_label (int): The label to which `target_label` will be changed .\n        attack_params (dict) : Attack parameters.\n        with_reputation (bool): Indicator if reputation is used.\n        reputation_metrics (list): List of reputation metrics.\n        initial_reputation (float): Initial reputation.\n        weighting_factor (str): dymanic or static weighting factor.\n        weight_model_arrival_latency (float): Weight of model arrival latency.\n        weight_model_similarity (float): Weight of model similarity.\n        weight_num_messages (float): Weight of number of messages.\n        weight_fraction_params_changed (float): Weight of fraction of parameters changed.\n        # is_dynamic_topology (bool): Indicator if topology is dynamic.\n        # is_dynamic_aggregation (bool): Indicator if aggregation is dynamic.\n        # target_aggregation (str): Target aggregation method.\n        random_geo (bool): Indicator if random geo is used.\n        latitude (float): Latitude for mobility.\n        longitude (float): Longitude for mobility.\n        mobility (bool): Indicator if mobility is used.\n        mobility_type (str): Type of mobility.\n        radius_federation (float): Radius of federation.\n        scheme_mobility (str): Scheme of mobility.\n        round_frequency (int): Frequency of rounds.\n        mobile_participants_percent (float): Percentage of mobile participants.\n        additional_participants (list): List of additional participants.\n        schema_additional_participants (str): Schema for additional participants.\n        random_topology_probability (float): Probability for random topology.\n    \"\"\"\n    self.scenario_title = scenario_title\n    self.scenario_description = scenario_description\n    self.deployment = deployment\n    self.federation = federation\n    self.topology = topology\n    self.nodes = nodes\n    self.nodes_graph = nodes_graph\n    self.n_nodes = n_nodes\n    self.matrix = matrix\n    self.dataset = dataset\n    self.iid = iid\n    self.partition_selection = partition_selection\n    self.partition_parameter = partition_parameter\n    self.model = model\n    self.agg_algorithm = agg_algorithm\n    self.rounds = rounds\n    self.logginglevel = logginglevel\n    self.report_status_data_queue = report_status_data_queue\n    self.accelerator = accelerator\n    self.gpu_id = gpu_id\n    self.network_subnet = network_subnet\n    self.network_gateway = network_gateway\n    self.epochs = epochs\n    self.attacks = attacks\n    self.poisoned_node_percent = poisoned_node_percent\n    self.poisoned_sample_percent = poisoned_sample_percent\n    self.poisoned_noise_percent = poisoned_noise_percent\n    self.attack_params = attack_params\n    self.with_reputation = with_reputation\n    self.reputation_metrics = reputation_metrics\n    self.initial_reputation = initial_reputation\n    self.weighting_factor = weighting_factor\n    self.weight_model_arrival_latency = weight_model_arrival_latency\n    self.weight_model_similarity = weight_model_similarity\n    self.weight_num_messages = weight_num_messages\n    self.weight_fraction_params_changed = weight_fraction_params_changed\n    # self.is_dynamic_topology = is_dynamic_topology\n    # self.is_dynamic_aggregation = is_dynamic_aggregation\n    # self.target_aggregation = target_aggregation\n    self.random_geo = random_geo\n    self.latitude = latitude\n    self.longitude = longitude\n    self.mobility = mobility\n    self.mobility_type = mobility_type\n    self.radius_federation = radius_federation\n    self.scheme_mobility = scheme_mobility\n    self.round_frequency = round_frequency\n    self.mobile_participants_percent = mobile_participants_percent\n    self.additional_participants = additional_participants\n    self.schema_additional_participants = schema_additional_participants\n    self.random_topology_probability = random_topology_probability\n</code></pre>"},{"location":"api/scenarios/#nebula.scenarios.Scenario.attack_node_assign","title":"<code>attack_node_assign(nodes, federation, attack, poisoned_node_percent, poisoned_sample_percent, poisoned_noise_percent, attack_params)</code>","text":"<p>Identify which nodes will be attacked</p> Source code in <code>nebula/scenarios.py</code> <pre><code>def attack_node_assign(\n    self,\n    nodes,\n    federation,\n    attack,\n    poisoned_node_percent,\n    poisoned_sample_percent,\n    poisoned_noise_percent,\n    attack_params,\n):\n    \"\"\"Identify which nodes will be attacked\"\"\"\n    import math\n    import random\n\n    nodes_index = []\n    # Get the nodes index\n    if federation == \"DFL\":\n        nodes_index = list(nodes.keys())\n    else:\n        for node in nodes:\n            if nodes[node][\"role\"] != \"server\":\n                nodes_index.append(node)\n\n    mal_nodes_defined = any(nodes[node][\"malicious\"] for node in nodes)\n\n    attacked_nodes = []\n\n    if not mal_nodes_defined:\n        n_nodes = len(nodes_index)\n        # Number of attacked nodes, round up\n        num_attacked = int(math.ceil(poisoned_node_percent / 100 * n_nodes))\n        if num_attacked &gt; n_nodes:\n            num_attacked = n_nodes\n\n        # Get the index of attacked nodes\n        attacked_nodes = random.sample(nodes_index, num_attacked)\n\n    # Assign the role of each node\n    for node in nodes:\n        node_att = \"No Attack\"\n        malicious = False\n        with_reputation = self.with_reputation\n        attack_sample_percent = 0\n        poisoned_ratio = 0\n        if (str(nodes[node][\"id\"]) in attacked_nodes) or (nodes[node][\"malicious\"]):\n            malicious = True\n            with_reputation = False\n            node_att = attack\n            attack_sample_percent = poisoned_sample_percent / 100\n            poisoned_ratio = poisoned_noise_percent / 100\n            attack_params[\"poisoned_percent\"] = attack_sample_percent\n            attack_params[\"poisoned_ratio\"] = poisoned_ratio\n        nodes[node][\"malicious\"] = malicious\n        nodes[node][\"with_reputation\"] = with_reputation\n        nodes[node][\"attacks\"] = node_att\n        nodes[node][\"attack_params\"] = attack_params\n    return nodes\n</code></pre>"},{"location":"api/scenarios/#nebula.scenarios.Scenario.mobility_assign","title":"<code>mobility_assign(nodes, mobile_participants_percent)</code>","text":"<p>Assign mobility to nodes</p> Source code in <code>nebula/scenarios.py</code> <pre><code>def mobility_assign(self, nodes, mobile_participants_percent):\n    \"\"\"Assign mobility to nodes\"\"\"\n    import random\n\n    # Number of mobile nodes, round down\n    num_mobile = math.floor(mobile_participants_percent / 100 * len(nodes))\n    if num_mobile &gt; len(nodes):\n        num_mobile = len(nodes)\n\n    # Get the index of mobile nodes\n    mobile_nodes = random.sample(list(nodes.keys()), num_mobile)\n\n    # Assign the role of each node\n    for node in nodes:\n        node_mob = False\n        if node in mobile_nodes:\n            node_mob = True\n        nodes[node][\"mobility\"] = node_mob\n    return nodes\n</code></pre>"},{"location":"api/utils/","title":"Documentation for Utils Module","text":""},{"location":"api/utils/#nebula.utils.DockerUtils","title":"<code>DockerUtils</code>","text":"<p>Utility class for Docker operations.</p> Source code in <code>nebula/utils.py</code> <pre><code>class DockerUtils:\n    \"\"\"\n    Utility class for Docker operations.\n    \"\"\"\n    @classmethod\n    def create_docker_network(cls, network_name, subnet=None, prefix=24):\n        try:\n            # Connect to Docker\n            client = docker.from_env()\n            base_subnet = \"192.168\"\n\n            # Obtain existing docker subnets\n            existing_subnets = []\n            networks = client.networks.list()\n\n            existing_network = next((n for n in networks if n.name == network_name), None)\n\n            if existing_network:\n                ipam_config = existing_network.attrs.get(\"IPAM\", {}).get(\"Config\", [])\n                if ipam_config:\n                    # Assume there's only one subnet per network for simplicity\n                    existing_subnet = ipam_config[0].get(\"Subnet\", \"\")\n                    potential_base = \".\".join(existing_subnet.split(\".\")[:3])  # Extract base from subnet\n                    logging.info(f\"Network '{network_name}' already exists with base {potential_base}\")\n                    return potential_base\n\n            for network in networks:\n                ipam_config = network.attrs.get(\"IPAM\", {}).get(\"Config\", [])\n                if ipam_config:\n                    for config in ipam_config:\n                        if \"Subnet\" in config:\n                            existing_subnets.append(config[\"Subnet\"])\n\n            # If no subnet is provided or it exists, find the next available one\n            if not subnet or subnet in existing_subnets:\n                for i in range(50, 255):  # Iterate over 192.168.50.0 to 192.168.254.0\n                    subnet = f\"{base_subnet}.{i}.0/{prefix}\"\n                    potential_base = f\"{base_subnet}.{i}\"\n                    if subnet not in existing_subnets:\n                        break\n                else:\n                    raise ValueError(\"No available subnets found.\")\n\n            # Create the Docker network\n            gateway = f\"{subnet.split('/')[0].rsplit('.', 1)[0]}.1\"\n            ipam_pool = docker.types.IPAMPool(subnet=subnet, gateway=gateway)\n            ipam_config = docker.types.IPAMConfig(pool_configs=[ipam_pool])\n            network = client.networks.create(name=network_name, driver=\"bridge\", ipam=ipam_config)\n\n            logging.info(f\"Network created: {network.name} with subnet {subnet}\")\n            return potential_base\n\n        except docker.errors.APIError:\n            logging.exception(\"Error interacting with Docker\")\n            return None\n        except Exception:\n            logging.exception(\"Unexpected error\")\n            return None\n        finally:\n            client.close()  # Ensure the Docker client is closed\n\n    @classmethod\n    def remove_docker_network(cls, network_name):\n        try:\n            # Connect to Docker\n            client = docker.from_env()\n\n            # Get the network by name\n            network = client.networks.get(network_name)\n\n            # Remove the network\n            network.remove()\n\n            logging.info(f\"Network {network_name} removed successfully.\")\n        except docker.errors.NotFound:\n            logging.exception(f\"Network {network_name} not found.\")\n        except docker.errors.APIError:\n            logging.exception(\"Error interacting with Docker\")\n        except Exception:\n            logging.exception(\"Unexpected error\")\n\n    @classmethod\n    def remove_docker_networks_by_prefix(cls, prefix):\n        try:\n            # Connect to Docker\n            client = docker.from_env()\n\n            # List all networks\n            networks = client.networks.list()\n\n            # Filter and remove networks with names starting with the prefix\n            for network in networks:\n                if network.name.startswith(prefix):\n                    network.remove()\n                    logging.info(f\"Network {network.name} removed successfully.\")\n\n        except docker.errors.NotFound:\n            logging.info(f\"One or more networks with prefix {prefix} not found.\")\n        except docker.errors.APIError:\n            logging.info(\"Error interacting with Docker\")\n        except Exception:\n            logging.info(\"Unexpected error\")\n\n    @classmethod\n    def remove_containers_by_prefix(cls, prefix):\n        try:\n            # Connect to Docker client\n            client = docker.from_env()\n\n            containers = client.containers.list(all=True)  # `all=True` to include stopped containers\n\n            # Iterate through containers and remove those with the matching prefix\n            for container in containers:\n                if container.name.startswith(prefix):\n                    logging.info(f\"Removing container: {container.name}\")\n                    container.remove(force=True)  # force=True to stop and remove if running\n                    logging.info(f\"Container {container.name} removed successfully.\")\n\n        except docker.errors.APIError:\n            logging.exception(\"Error interacting with Docker\")\n        except Exception:\n            logging.exception(\"Unexpected error\")\n</code></pre>"},{"location":"api/utils/#nebula.utils.FileUtils","title":"<code>FileUtils</code>","text":"<p>Utility class for file operations.</p> Source code in <code>nebula/utils.py</code> <pre><code>class FileUtils:\n    \"\"\"\n    Utility class for file operations.\n    \"\"\"\n    @classmethod\n    def check_path(cls, base_path, relative_path):\n        full_path = os.path.normpath(os.path.join(base_path, relative_path))\n        base_path = os.path.normpath(base_path)\n\n        if not full_path.startswith(base_path):\n            raise Exception(\"Not allowed\")\n        return full_path\n</code></pre>"},{"location":"api/utils/#nebula.utils.SocketUtils","title":"<code>SocketUtils</code>","text":"<p>Utility class for socket operations.</p> Source code in <code>nebula/utils.py</code> <pre><code>class SocketUtils:\n    \"\"\"\n    Utility class for socket operations.\n    \"\"\"\n    @classmethod\n    def is_port_open(cls, port):\n        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        try:\n            s.bind((\"127.0.0.1\", port))\n            s.close()\n            return True\n        except OSError:\n            return False\n\n    @classmethod\n    def find_free_port(cls, start_port=49152, end_port=65535):\n        for port in range(start_port, end_port + 1):\n            if cls.is_port_open(port):\n                return port\n        return None\n</code></pre>"},{"location":"api/addons/","title":"Documentation for Addons Module","text":"<p>This package consists of several modules that handle different aspects of the network simulation:</p> <ol> <li><code>env.py</code>:</li> <li>Manages the environment configuration and settings.</li> <li> <p>It initializes the system environment, loads configuration parameters, and ensures correct operation of other components based on the simulation's settings.</p> </li> <li> <p><code>functions.py</code>:</p> </li> <li>Contains utility functions that are used across different parts of the simulation.</li> <li> <p>It provides helper methods for common operations like data processing, mathematical calculations, and other reusable functionalities.</p> </li> <li> <p><code>mobility.py</code>:</p> </li> <li>Models and simulates the mobility of nodes within the network.</li> <li> <p>It handles dynamic aspects of the simulation, such as node movement and position updates, based on mobility models and the simulation's configuration.</p> </li> <li> <p><code>reporter.py</code>:</p> </li> <li>Responsible for collecting and reporting data during the simulation.</li> <li> <p>It tracks various system metrics, including node status and network performance, and periodically sends updates to a controller or dashboard for analysis and monitoring.</p> </li> <li> <p><code>topologymanager.py</code>:</p> </li> <li>Manages the topology of the network.</li> <li>It handles the creation and maintenance of the network's structure (e.g., nodes and their connections), including generating different types of topologies like ring, random, or fully connected based on simulation parameters.</li> </ol> <p>Each of these modules plays a critical role in simulating a network environment, enabling real-time tracking, topology management, mobility simulation, and efficient reporting of results.</p>"},{"location":"api/addons/env/","title":"Documentation for Env Module","text":""},{"location":"api/addons/env/#nebula.addons.env.check_environment","title":"<code>check_environment()</code>","text":"<p>Logs the current environment configuration for the NEBULA platform.</p> <p>This function gathers and logs information about the operating system, hardware, Python version, PyTorch version (if installed), CPU configuration, and GPU configuration (if applicable). It provides insights into the system's capabilities and current usage statistics.</p> <p>Returns:</p> Type Description <p>None</p> Notes <ul> <li>The function logs the NEBULA platform version using the <code>__version__</code> variable.</li> <li>It checks the system's CPU load, available memory, and detailed GPU statistics using the <code>pynvml</code>   library if running on Windows or Linux.</li> <li>If any of the libraries required for gathering information (like <code>torch</code>, <code>psutil</code>, or <code>pynvml</code>)   are not installed, appropriate log messages will be generated indicating the absence of that information.</li> <li>If any unexpected error occurs during execution, it will be logged as an exception.</li> </ul> Source code in <code>nebula/addons/env.py</code> <pre><code>def check_environment():\n    \"\"\"\n    Logs the current environment configuration for the NEBULA platform.\n\n    This function gathers and logs information about the operating system, hardware, Python version,\n    PyTorch version (if installed), CPU configuration, and GPU configuration (if applicable). It provides\n    insights into the system's capabilities and current usage statistics.\n\n    Returns:\n        None\n\n    Notes:\n        - The function logs the NEBULA platform version using the `__version__` variable.\n        - It checks the system's CPU load, available memory, and detailed GPU statistics using the `pynvml`\n          library if running on Windows or Linux.\n        - If any of the libraries required for gathering information (like `torch`, `psutil`, or `pynvml`)\n          are not installed, appropriate log messages will be generated indicating the absence of that information.\n        - If any unexpected error occurs during execution, it will be logged as an exception.\n    \"\"\"\n    logging.info(f\"NEBULA Platform version: {__version__}\")\n    # check_version()\n\n    logging.info(\"======== Running Environment ========\")\n    logging.info(\"OS: \" + platform.platform())\n    logging.info(\"Hardware: \" + platform.machine())\n    logging.info(\"Python version: \" + sys.version)\n\n    try:\n        import torch\n\n        logging.info(\"PyTorch version: \" + torch.__version__)\n    except ImportError:\n        logging.info(\"PyTorch is not installed properly\")\n    except Exception:  # noqa: S110\n        pass\n\n    logging.info(\"======== CPU Configuration ========\")\n    try:\n        import psutil\n\n        load1, load5, load15 = psutil.getloadavg()\n        cpu_usage = (load15 / os.cpu_count()) * 100\n\n        logging.info(f\"The CPU usage is : {cpu_usage:.0f}%\")\n        logging.info(\n            f\"Available CPU Memory: {psutil.virtual_memory().available / 1024 / 1024 / 1024:.1f} G / {psutil.virtual_memory().total / 1024 / 1024 / 1024}G\"\n        )\n    except ImportError:\n        logging.info(\"No CPU information available\")\n    except Exception:  # noqa: S110\n        pass\n\n    if sys.platform == \"win32\" or sys.platform == \"linux\":\n        logging.info(\"======== GPU Configuration ========\")\n        try:\n            import pynvml\n\n            pynvml.nvmlInit()\n            devices = pynvml.nvmlDeviceGetCount()\n            for i in range(devices):\n                handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n                gpu_percent = pynvml.nvmlDeviceGetUtilizationRates(handle).gpu\n                gpu_temp = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)\n                gpu_mem = pynvml.nvmlDeviceGetMemoryInfo(handle)\n                gpu_mem_percent = gpu_mem.used / gpu_mem.total * 100\n                gpu_power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0\n                gpu_clocks = pynvml.nvmlDeviceGetClockInfo(handle, pynvml.NVML_CLOCK_SM)\n                gpu_memory_clocks = pynvml.nvmlDeviceGetClockInfo(handle, pynvml.NVML_CLOCK_MEM)\n                gpu_utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)\n                gpu_fan_speed = pynvml.nvmlDeviceGetFanSpeed(handle)\n                logging.info(f\"GPU{i} percent: {gpu_percent}\")\n                logging.info(f\"GPU{i} temp: {gpu_temp}\")\n                logging.info(f\"GPU{i} mem percent: {gpu_mem_percent}\")\n                logging.info(f\"GPU{i} power: {gpu_power}\")\n                logging.info(f\"GPU{i} clocks: {gpu_clocks}\")\n                logging.info(f\"GPU{i} memory clocks: {gpu_memory_clocks}\")\n                logging.info(f\"GPU{i} utilization: {gpu_utilization.gpu}\")\n                logging.info(f\"GPU{i} fan speed: {gpu_fan_speed}\")\n        except ImportError:\n            logging.info(\"pynvml module not found, GPU information unavailable\")\n        except Exception:  # noqa: S110\n            pass\n    else:\n        logging.info(\"GPU information unavailable\")\n</code></pre>"},{"location":"api/addons/env/#nebula.addons.env.check_version","title":"<code>check_version()</code>","text":"<p>Checks the current version of NEBULA and compares it with the latest version available in the repository.</p> <p>This function retrieves the latest NEBULA version from the specified GitHub repository and compares it with the version defined in the local NEBULA package. If the versions do not match, it logs a message prompting the user to update to the latest version.</p> <p>Returns:</p> Type Description <p>None</p> <p>Raises:</p> Type Description <code>SystemExit</code> <p>If the version check fails or an exception occurs during the request.</p> Notes <ul> <li>The version information is expected to be defined in the <code>__init__.py</code> file of the NEBULA package   using the <code>__version__</code> variable.</li> <li>If the latest version is not the same as the local version, the program will exit after logging   the necessary information.</li> <li>An exception during the request will be logged, and the program will also exit.</li> </ul> Source code in <code>nebula/addons/env.py</code> <pre><code>def check_version():\n    \"\"\"\n    Checks the current version of NEBULA and compares it with the latest version available in the repository.\n\n    This function retrieves the latest NEBULA version from the specified GitHub repository and compares\n    it with the version defined in the local NEBULA package. If the versions do not match, it logs a message\n    prompting the user to update to the latest version.\n\n    Returns:\n        None\n\n    Raises:\n        SystemExit: If the version check fails or an exception occurs during the request.\n\n    Notes:\n        - The version information is expected to be defined in the `__init__.py` file of the NEBULA package\n          using the `__version__` variable.\n        - If the latest version is not the same as the local version, the program will exit after logging\n          the necessary information.\n        - An exception during the request will be logged, and the program will also exit.\n    \"\"\"\n    logging.info(\"Checking NEBULA version...\")\n    try:\n        r = requests.get(\"https://raw.githubusercontent.com/CyberDataLab/nebula/main/nebula/__init__.py\", timeout=5)\n        if r.status_code == 200:\n            version = re.search(r'^__version__\\s*=\\s*[\\'\"]([^\\'\"]*)[\\'\"]', r.text, re.MULTILINE).group(1)\n            if version != __version__:\n                logging.info(\n                    f\"Your NEBULA version is {__version__} and the latest version is {version}. Please update your NEBULA version.\"\n                )\n                logging.info(\n                    \"You can update your NEBULA version downloading the latest version from https://github.com/CyberDataLab/nebula\"\n                )\n                sys.exit(0)\n            else:\n                logging.info(f\"Your NEBULA version is {__version__} and it is the latest version.\")\n    except Exception:\n        logging.exception(\"Error while checking NEBULA version\")\n        sys.exit(0)\n</code></pre>"},{"location":"api/addons/functions/","title":"Documentation for Functions Module","text":""},{"location":"api/addons/functions/#nebula.addons.functions.print_msg_box","title":"<code>print_msg_box(msg, indent=1, width=None, title=None, logger_name=None)</code>","text":"<p>Prints a formatted message box to the logger with an optional title.</p> <p>This function creates a visually appealing message box format for logging messages. It allows for indentation, custom width, and inclusion of a title. If the message is multiline, each line will be included in the box.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>The message to be displayed inside the box. Must be a string.</p> required <code>indent</code> <code>int</code> <p>The number of spaces to indent the message box. Default is 1.</p> <code>1</code> <code>width</code> <code>int</code> <p>The width of the message box. If not provided, it will be calculated                    based on the longest line of the message and the title (if provided).</p> <code>None</code> <code>title</code> <code>str</code> <p>An optional title for the message box. Must be a string if provided.</p> <code>None</code> <code>logger_name</code> <code>str</code> <p>The name of the logger to use. If not provided, the root logger                           will be used.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>msg</code> or <code>title</code> is not a string.</p> <p>Returns:</p> Type Description <p>None</p> Notes <ul> <li>The message box is bordered with decorative characters to enhance visibility in the logs.</li> <li>If the <code>width</code> is not provided, it will automatically adjust to fit the content.</li> </ul> Source code in <code>nebula/addons/functions.py</code> <pre><code>def print_msg_box(msg, indent=1, width=None, title=None, logger_name=None):\n    \"\"\"\n    Prints a formatted message box to the logger with an optional title.\n\n    This function creates a visually appealing message box format for logging messages.\n    It allows for indentation, custom width, and inclusion of a title. If the message is\n    multiline, each line will be included in the box.\n\n    Args:\n        msg (str): The message to be displayed inside the box. Must be a string.\n        indent (int, optional): The number of spaces to indent the message box. Default is 1.\n        width (int, optional): The width of the message box. If not provided, it will be calculated\n                               based on the longest line of the message and the title (if provided).\n        title (str, optional): An optional title for the message box. Must be a string if provided.\n        logger_name (str, optional): The name of the logger to use. If not provided, the root logger\n                                      will be used.\n\n    Raises:\n        TypeError: If `msg` or `title` is not a string.\n\n    Returns:\n        None\n\n    Notes:\n        - The message box is bordered with decorative characters to enhance visibility in the logs.\n        - If the `width` is not provided, it will automatically adjust to fit the content.\n    \"\"\"\n    logger = logging.getLogger(logger_name) if logger_name else logging.getLogger()\n\n    if not isinstance(msg, str):\n        raise TypeError(\"msg parameter must be a string\")  # noqa: TRY003\n\n    lines = msg.split(\"\\n\")\n    space = \" \" * indent\n    if not width:\n        width = max(map(len, lines))\n        if title:\n            width = max(width, len(title))\n    box = f\"\\n\u2554{'\u2550' * (width + indent * 2)}\u2557\\n\"  # upper_border\n    if title:\n        if not isinstance(title, str):\n            raise TypeError(\"title parameter must be a string\")  # noqa: TRY003\n        box += f\"\u2551{space}{title:&lt;{width}}{space}\u2551\\n\"  # title\n        box += f\"\u2551{space}{'-' * len(title):&lt;{width}}{space}\u2551\\n\"  # underscore\n    box += \"\".join([f\"\u2551{space}{line:&lt;{width}}{space}\u2551\\n\" for line in lines])\n    box += f\"\u255a{'\u2550' * (width + indent * 2)}\u255d\"  # lower_border\n    logger.info(box)\n</code></pre>"},{"location":"api/addons/mobility/","title":"Documentation for Mobility Module","text":""},{"location":"api/addons/mobility/#nebula.addons.mobility.Mobility","title":"<code>Mobility</code>","text":"Source code in <code>nebula/addons/mobility.py</code> <pre><code>class Mobility:\n    def __init__(self, config, cm: \"CommunicationsManager\", verbose=False):\n        \"\"\"\n        Initializes the mobility module with specified configuration and communication manager.\n\n        This method sets up the mobility parameters required for the module, including grace time,\n        geographical change interval, mobility type, and other network conditions based on distance.\n        It also logs the initialized settings for the mobility system.\n\n        Args:\n            config (Config): Configuration object containing mobility parameters and settings.\n            cm (CommunicationsManager): An instance of the CommunicationsManager class used for handling\n                                         communication-related tasks within the mobility module.\n\n        Attributes:\n            grace_time (float): Time allocated for mobility processes to stabilize.\n            period (float): Interval at which geographic changes are made.\n            mobility (bool): Flag indicating whether mobility is enabled.\n            mobility_type (str): Type of mobility strategy to be used (e.g., random, nearest).\n            radius_federation (float): Radius for federation in meters.\n            scheme_mobility (str): Scheme to be used for managing mobility.\n            round_frequency (int): Number of rounds after which mobility changes are applied.\n            max_distance_with_direct_connections (float): Maximum distance for direct connections in meters.\n            max_movement_random_strategy (float): Maximum movement distance for the random strategy in meters.\n            max_movement_nearest_strategy (float): Maximum movement distance for the nearest strategy in meters.\n            max_initiate_approximation (float): Maximum distance for initiating approximation calculations.\n            network_conditions (dict): A dictionary containing network conditions (bandwidth and delay)\n                                       based on distance.\n            current_network_conditions (dict): A dictionary mapping addresses to their current network conditions.\n\n        Logs:\n            Mobility information upon initialization to provide insights into the current setup.\n\n        Raises:\n            KeyError: If the expected mobility configuration keys are not found in the provided config.\n        \"\"\"\n        logging.info(\"Starting mobility module...\")\n        self.config = config\n        self.cm = cm\n        self.grace_time = self.config.participant[\"mobility_args\"][\"grace_time_mobility\"]\n        self.period = self.config.participant[\"mobility_args\"][\"change_geo_interval\"]\n        self.mobility = self.config.participant[\"mobility_args\"][\"mobility\"]\n        self.mobility_type = self.config.participant[\"mobility_args\"][\"mobility_type\"]\n        self.radius_federation = float(self.config.participant[\"mobility_args\"][\"radius_federation\"])\n        self.scheme_mobility = self.config.participant[\"mobility_args\"][\"scheme_mobility\"]\n        self.round_frequency = int(self.config.participant[\"mobility_args\"][\"round_frequency\"])\n        # Protocol to change connections based on distance\n        self.max_distance_with_direct_connections = 300  # meters\n        self.max_movement_random_strategy = 100  # meters\n        self.max_movement_nearest_strategy = 100  # meters\n        self.max_initiate_approximation = self.max_distance_with_direct_connections * 1.2\n        # Logging box with mobility information\n        mobility_msg = f\"Mobility: {self.mobility}\\nMobility type: {self.mobility_type}\\nRadius federation: {self.radius_federation}\\nScheme mobility: {self.scheme_mobility}\\nEach {self.round_frequency} rounds\"\n        print_msg_box(msg=mobility_msg, indent=2, title=\"Mobility information\")\n        self._nodes_distances = {}\n        self._nodes_distances_lock = Locker(\"nodes_distances_lock\", async_lock=True)\n        self._verbose = verbose\n\n    @property\n    def round(self):\n        \"\"\"\n        Gets the current round number from the Communications Manager.\n\n        This property retrieves the current round number that is being managed by the\n        CommunicationsManager instance associated with this module. It provides an\n        interface to access the ongoing round of the communication process without\n        directly exposing the underlying method in the CommunicationsManager.\n\n        Returns:\n            int: The current round number managed by the CommunicationsManager.\n        \"\"\"\n        return self.cm.get_round()\n\n    async def start(self):\n        \"\"\"\n        Initiates the mobility process by starting the associated task.\n\n        This method creates and schedules an asynchronous task to run the\n        `run_mobility` coroutine, which handles the mobility operations\n        for the module. It allows the mobility operations to run concurrently\n        without blocking the execution of other tasks.\n\n        Returns:\n            asyncio.Task: An asyncio Task object representing the scheduled\n                           `run_mobility` operation.\n        \"\"\"\n        await EventManager.get_instance().subscribe_addonevent(GPSEvent, self.update_nodes_distances)\n        task = asyncio.create_task(self.run_mobility())\n        return task\n\n    async def update_nodes_distances(self, gpsevent: GPSEvent):\n        distances = await gpsevent.get_event_data()\n        async with self._nodes_distances_lock:\n            self._nodes_distances = dict(distances)\n\n    async def run_mobility(self):\n        \"\"\"\n        Executes the mobility operations in a continuous loop.\n\n        This coroutine manages the mobility behavior of the module. It first\n        checks whether mobility is enabled. If mobility is not enabled, the\n        function returns immediately.\n\n        If mobility is enabled, the function will wait for the specified\n        grace time before entering an infinite loop where it performs the\n        following operations:\n\n        1. Changes the geographical location by calling the `change_geo_location` method.\n        2. Adjusts connections based on the current distance by calling\n           the `change_connections_based_on_distance` method.\n        3. Sleeps for a specified period (`self.period`) before repeating the operations.\n\n        This allows for periodic updates to the module's geographical location\n        and network connections as per the defined mobility strategy.\n\n        Raises:\n            Exception: May raise exceptions if `change_geo_location` or\n                        `change_connections_based_on_distance` encounters errors.\n        \"\"\"\n        if not self.mobility:\n            return\n        # await asyncio.sleep(self.grace_time)\n        while True:\n            await self.change_geo_location()\n            await asyncio.sleep(self.period)\n\n    async def change_geo_location_random_strategy(self, latitude, longitude):\n        \"\"\"\n        Changes the geographical location of the entity using a random strategy.\n\n        This coroutine modifies the current geographical location by randomly\n        selecting a new position within a specified radius around the given\n        latitude and longitude. The new location is determined using polar\n        coordinates, where a random distance (radius) and angle are calculated.\n\n        Args:\n            latitude (float): The current latitude of the entity.\n            longitude (float): The current longitude of the entity.\n\n        Raises:\n            Exception: May raise exceptions if the `set_geo_location` method encounters errors.\n\n        Notes:\n            - The maximum movement distance is determined by `self.max_movement_random_strategy`.\n            - The calculated radius is converted from meters to degrees based on an approximate\n              conversion factor (1 degree is approximately 111 kilometers).\n        \"\"\"\n        if self._verbose:\n            logging.info(\"\ud83d\udccd  Changing geo location randomly\")\n        # radius_in_degrees = self.radius_federation / 111000\n        max_radius_in_degrees = self.max_movement_random_strategy / 111000\n        radius = random.uniform(0, max_radius_in_degrees)  # noqa: S311\n        angle = random.uniform(0, 2 * math.pi)  # noqa: S311\n        latitude += radius * math.cos(angle)\n        longitude += radius * math.sin(angle)\n        await self.set_geo_location(latitude, longitude)\n\n    async def change_geo_location_nearest_neighbor_strategy(\n        self, distance, latitude, longitude, neighbor_latitude, neighbor_longitude\n    ):\n        \"\"\"\n        Changes the geographical location of the entity towards the nearest neighbor.\n\n        This coroutine updates the current geographical location by calculating the direction\n        and distance to the nearest neighbor's coordinates. The movement towards the neighbor\n        is scaled based on the distance and the maximum movement allowed.\n\n        Args:\n            distance (float): The distance to the nearest neighbor.\n            latitude (float): The current latitude of the entity.\n            longitude (float): The current longitude of the entity.\n            neighbor_latitude (float): The latitude of the nearest neighbor.\n            neighbor_longitude (float): The longitude of the nearest neighbor.\n\n        Raises:\n            Exception: May raise exceptions if the `set_geo_location` method encounters errors.\n\n        Notes:\n            - The movement is scaled based on the maximum allowed distance defined by\n              `self.max_movement_nearest_strategy`.\n            - The angle to the neighbor is calculated using the arctangent of the difference in\n              coordinates to determine the direction of movement.\n            - The conversion from meters to degrees is based on approximate geographical conversion factors.\n        \"\"\"\n        logging.info(\"\ud83d\udccd  Changing geo location towards the nearest neighbor\")\n        scale_factor = min(1, self.max_movement_nearest_strategy / distance)\n        # Calcular el \u00e1ngulo hacia el vecino\n        angle = math.atan2(neighbor_longitude - longitude, neighbor_latitude - latitude)\n        # Conversi\u00f3n de movimiento m\u00e1ximo a grados\n        max_lat_change = self.max_movement_nearest_strategy / 111000  # Cambio en grados para latitud\n        max_lon_change = self.max_movement_nearest_strategy / (\n            111000 * math.cos(math.radians(latitude))\n        )  # Cambio en grados para longitud\n        # Aplicar escala y direcci\u00f3n\n        delta_lat = max_lat_change * math.cos(angle) * scale_factor\n        delta_lon = max_lon_change * math.sin(angle) * scale_factor\n        # Actualizar latitud y longitud\n        new_latitude = latitude + delta_lat\n        new_longitude = longitude + delta_lon\n        await self.set_geo_location(new_latitude, new_longitude)\n\n    async def set_geo_location(self, latitude, longitude):\n        \"\"\"\n        Sets the geographical location of the entity to the specified latitude and longitude.\n\n        This coroutine updates the latitude and longitude values in the configuration. If the\n        provided coordinates are out of bounds (latitude must be between -90 and 90, and\n        longitude must be between -180 and 180), the previous location is retained.\n\n        Args:\n            latitude (float): The new latitude to set.\n            longitude (float): The new longitude to set.\n\n        Raises:\n            None: This function does not raise any exceptions but retains the previous coordinates\n                  if the new ones are invalid.\n\n        Notes:\n            - The new location is logged for tracking purposes.\n            - The coordinates are expected to be in decimal degrees format.\n        \"\"\"\n\n        if latitude &lt; -90 or latitude &gt; 90 or longitude &lt; -180 or longitude &gt; 180:\n            # If the new location is out of bounds, we keep the old location\n            latitude = self.config.participant[\"mobility_args\"][\"latitude\"]\n            longitude = self.config.participant[\"mobility_args\"][\"longitude\"]\n\n        self.config.participant[\"mobility_args\"][\"latitude\"] = latitude\n        self.config.participant[\"mobility_args\"][\"longitude\"] = longitude\n        if self._verbose:\n            logging.info(f\"\ud83d\udccd  New geo location: {latitude}, {longitude}\")\n\n    async def change_geo_location(self):\n        \"\"\"\n        Changes the geographical location of the entity based on the current mobility strategy.\n\n        This coroutine checks the mobility type and decides whether to move towards the nearest neighbor\n        or change the geo location randomly. It uses the communications manager to obtain the current\n        connections and their distances.\n\n        If the number of undirected connections is greater than directed connections, the method will\n        attempt to find the nearest neighbor and move towards it if the distance exceeds a certain threshold.\n        Otherwise, it will randomly change the geo location.\n\n        Args:\n            None: This function does not take any arguments.\n\n        Raises:\n            Exception: If the neighbor's location or distance cannot be found.\n\n        Notes:\n            - The method expects the mobility type to be either \"topology\" or \"both\".\n            - It logs actions taken during the execution for tracking and debugging purposes.\n        \"\"\"\n        if self.mobility and (self.mobility_type == \"topology\" or self.mobility_type == \"both\"):\n            random.seed(time.time() + self.config.participant[\"device_args\"][\"idx\"])\n            latitude = float(self.config.participant[\"mobility_args\"][\"latitude\"])\n            longitude = float(self.config.participant[\"mobility_args\"][\"longitude\"])\n            if True:\n                # Get neighbor closer to me\n                async with self._nodes_distances_lock:\n                    sorted_list = sorted(self._nodes_distances.items(), key=lambda item: item[1][0])\n                    # Transformamos la lista para obtener solo direcci\u00f3n y coordenadas\n                    result = [(addr, dist, coords) for addr, (dist, coords) in sorted_list]\n\n                selected_neighbor = result[0] if result else None\n                if selected_neighbor:\n                    # logging.info(f\"\ud83d\udccd  Selected neighbor: {selected_neighbor}\")\n                    addr, dist, (lat, long) = selected_neighbor\n                    if dist &gt; self.max_initiate_approximation:\n                        # If the distance is too big, we move towards the neighbor\n                        logging.info(f\"Moving towards nearest neighbor: {addr}\")\n                        await self.change_geo_location_nearest_neighbor_strategy(\n                            dist,\n                            latitude,\n                            longitude,\n                            lat,\n                            long,\n                        )\n                    else:\n                        await self.change_geo_location_random_strategy(latitude, longitude)\n                else:\n                    await self.change_geo_location_random_strategy(latitude, longitude)\n            else:\n                await self.change_geo_location_random_strategy(latitude, longitude)\n        else:\n            logging.error(f\"\ud83d\udccd  Mobility type {self.mobility_type} not implemented\")\n            return\n</code></pre>"},{"location":"api/addons/mobility/#nebula.addons.mobility.Mobility.round","title":"<code>round</code>  <code>property</code>","text":"<p>Gets the current round number from the Communications Manager.</p> <p>This property retrieves the current round number that is being managed by the CommunicationsManager instance associated with this module. It provides an interface to access the ongoing round of the communication process without directly exposing the underlying method in the CommunicationsManager.</p> <p>Returns:</p> Name Type Description <code>int</code> <p>The current round number managed by the CommunicationsManager.</p>"},{"location":"api/addons/mobility/#nebula.addons.mobility.Mobility.__init__","title":"<code>__init__(config, cm, verbose=False)</code>","text":"<p>Initializes the mobility module with specified configuration and communication manager.</p> <p>This method sets up the mobility parameters required for the module, including grace time, geographical change interval, mobility type, and other network conditions based on distance. It also logs the initialized settings for the mobility system.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>Configuration object containing mobility parameters and settings.</p> required <code>cm</code> <code>CommunicationsManager</code> <p>An instance of the CommunicationsManager class used for handling                          communication-related tasks within the mobility module.</p> required <p>Attributes:</p> Name Type Description <code>grace_time</code> <code>float</code> <p>Time allocated for mobility processes to stabilize.</p> <code>period</code> <code>float</code> <p>Interval at which geographic changes are made.</p> <code>mobility</code> <code>bool</code> <p>Flag indicating whether mobility is enabled.</p> <code>mobility_type</code> <code>str</code> <p>Type of mobility strategy to be used (e.g., random, nearest).</p> <code>radius_federation</code> <code>float</code> <p>Radius for federation in meters.</p> <code>scheme_mobility</code> <code>str</code> <p>Scheme to be used for managing mobility.</p> <code>round_frequency</code> <code>int</code> <p>Number of rounds after which mobility changes are applied.</p> <code>max_distance_with_direct_connections</code> <code>float</code> <p>Maximum distance for direct connections in meters.</p> <code>max_movement_random_strategy</code> <code>float</code> <p>Maximum movement distance for the random strategy in meters.</p> <code>max_movement_nearest_strategy</code> <code>float</code> <p>Maximum movement distance for the nearest strategy in meters.</p> <code>max_initiate_approximation</code> <code>float</code> <p>Maximum distance for initiating approximation calculations.</p> <code>network_conditions</code> <code>dict</code> <p>A dictionary containing network conditions (bandwidth and delay)                        based on distance.</p> <code>current_network_conditions</code> <code>dict</code> <p>A dictionary mapping addresses to their current network conditions.</p> Logs <p>Mobility information upon initialization to provide insights into the current setup.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the expected mobility configuration keys are not found in the provided config.</p> Source code in <code>nebula/addons/mobility.py</code> <pre><code>def __init__(self, config, cm: \"CommunicationsManager\", verbose=False):\n    \"\"\"\n    Initializes the mobility module with specified configuration and communication manager.\n\n    This method sets up the mobility parameters required for the module, including grace time,\n    geographical change interval, mobility type, and other network conditions based on distance.\n    It also logs the initialized settings for the mobility system.\n\n    Args:\n        config (Config): Configuration object containing mobility parameters and settings.\n        cm (CommunicationsManager): An instance of the CommunicationsManager class used for handling\n                                     communication-related tasks within the mobility module.\n\n    Attributes:\n        grace_time (float): Time allocated for mobility processes to stabilize.\n        period (float): Interval at which geographic changes are made.\n        mobility (bool): Flag indicating whether mobility is enabled.\n        mobility_type (str): Type of mobility strategy to be used (e.g., random, nearest).\n        radius_federation (float): Radius for federation in meters.\n        scheme_mobility (str): Scheme to be used for managing mobility.\n        round_frequency (int): Number of rounds after which mobility changes are applied.\n        max_distance_with_direct_connections (float): Maximum distance for direct connections in meters.\n        max_movement_random_strategy (float): Maximum movement distance for the random strategy in meters.\n        max_movement_nearest_strategy (float): Maximum movement distance for the nearest strategy in meters.\n        max_initiate_approximation (float): Maximum distance for initiating approximation calculations.\n        network_conditions (dict): A dictionary containing network conditions (bandwidth and delay)\n                                   based on distance.\n        current_network_conditions (dict): A dictionary mapping addresses to their current network conditions.\n\n    Logs:\n        Mobility information upon initialization to provide insights into the current setup.\n\n    Raises:\n        KeyError: If the expected mobility configuration keys are not found in the provided config.\n    \"\"\"\n    logging.info(\"Starting mobility module...\")\n    self.config = config\n    self.cm = cm\n    self.grace_time = self.config.participant[\"mobility_args\"][\"grace_time_mobility\"]\n    self.period = self.config.participant[\"mobility_args\"][\"change_geo_interval\"]\n    self.mobility = self.config.participant[\"mobility_args\"][\"mobility\"]\n    self.mobility_type = self.config.participant[\"mobility_args\"][\"mobility_type\"]\n    self.radius_federation = float(self.config.participant[\"mobility_args\"][\"radius_federation\"])\n    self.scheme_mobility = self.config.participant[\"mobility_args\"][\"scheme_mobility\"]\n    self.round_frequency = int(self.config.participant[\"mobility_args\"][\"round_frequency\"])\n    # Protocol to change connections based on distance\n    self.max_distance_with_direct_connections = 300  # meters\n    self.max_movement_random_strategy = 100  # meters\n    self.max_movement_nearest_strategy = 100  # meters\n    self.max_initiate_approximation = self.max_distance_with_direct_connections * 1.2\n    # Logging box with mobility information\n    mobility_msg = f\"Mobility: {self.mobility}\\nMobility type: {self.mobility_type}\\nRadius federation: {self.radius_federation}\\nScheme mobility: {self.scheme_mobility}\\nEach {self.round_frequency} rounds\"\n    print_msg_box(msg=mobility_msg, indent=2, title=\"Mobility information\")\n    self._nodes_distances = {}\n    self._nodes_distances_lock = Locker(\"nodes_distances_lock\", async_lock=True)\n    self._verbose = verbose\n</code></pre>"},{"location":"api/addons/mobility/#nebula.addons.mobility.Mobility.change_geo_location","title":"<code>change_geo_location()</code>  <code>async</code>","text":"<p>Changes the geographical location of the entity based on the current mobility strategy.</p> <p>This coroutine checks the mobility type and decides whether to move towards the nearest neighbor or change the geo location randomly. It uses the communications manager to obtain the current connections and their distances.</p> <p>If the number of undirected connections is greater than directed connections, the method will attempt to find the nearest neighbor and move towards it if the distance exceeds a certain threshold. Otherwise, it will randomly change the geo location.</p> <p>Parameters:</p> Name Type Description Default <code>None</code> <p>This function does not take any arguments.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If the neighbor's location or distance cannot be found.</p> Notes <ul> <li>The method expects the mobility type to be either \"topology\" or \"both\".</li> <li>It logs actions taken during the execution for tracking and debugging purposes.</li> </ul> Source code in <code>nebula/addons/mobility.py</code> <pre><code>async def change_geo_location(self):\n    \"\"\"\n    Changes the geographical location of the entity based on the current mobility strategy.\n\n    This coroutine checks the mobility type and decides whether to move towards the nearest neighbor\n    or change the geo location randomly. It uses the communications manager to obtain the current\n    connections and their distances.\n\n    If the number of undirected connections is greater than directed connections, the method will\n    attempt to find the nearest neighbor and move towards it if the distance exceeds a certain threshold.\n    Otherwise, it will randomly change the geo location.\n\n    Args:\n        None: This function does not take any arguments.\n\n    Raises:\n        Exception: If the neighbor's location or distance cannot be found.\n\n    Notes:\n        - The method expects the mobility type to be either \"topology\" or \"both\".\n        - It logs actions taken during the execution for tracking and debugging purposes.\n    \"\"\"\n    if self.mobility and (self.mobility_type == \"topology\" or self.mobility_type == \"both\"):\n        random.seed(time.time() + self.config.participant[\"device_args\"][\"idx\"])\n        latitude = float(self.config.participant[\"mobility_args\"][\"latitude\"])\n        longitude = float(self.config.participant[\"mobility_args\"][\"longitude\"])\n        if True:\n            # Get neighbor closer to me\n            async with self._nodes_distances_lock:\n                sorted_list = sorted(self._nodes_distances.items(), key=lambda item: item[1][0])\n                # Transformamos la lista para obtener solo direcci\u00f3n y coordenadas\n                result = [(addr, dist, coords) for addr, (dist, coords) in sorted_list]\n\n            selected_neighbor = result[0] if result else None\n            if selected_neighbor:\n                # logging.info(f\"\ud83d\udccd  Selected neighbor: {selected_neighbor}\")\n                addr, dist, (lat, long) = selected_neighbor\n                if dist &gt; self.max_initiate_approximation:\n                    # If the distance is too big, we move towards the neighbor\n                    logging.info(f\"Moving towards nearest neighbor: {addr}\")\n                    await self.change_geo_location_nearest_neighbor_strategy(\n                        dist,\n                        latitude,\n                        longitude,\n                        lat,\n                        long,\n                    )\n                else:\n                    await self.change_geo_location_random_strategy(latitude, longitude)\n            else:\n                await self.change_geo_location_random_strategy(latitude, longitude)\n        else:\n            await self.change_geo_location_random_strategy(latitude, longitude)\n    else:\n        logging.error(f\"\ud83d\udccd  Mobility type {self.mobility_type} not implemented\")\n        return\n</code></pre>"},{"location":"api/addons/mobility/#nebula.addons.mobility.Mobility.change_geo_location_nearest_neighbor_strategy","title":"<code>change_geo_location_nearest_neighbor_strategy(distance, latitude, longitude, neighbor_latitude, neighbor_longitude)</code>  <code>async</code>","text":"<p>Changes the geographical location of the entity towards the nearest neighbor.</p> <p>This coroutine updates the current geographical location by calculating the direction and distance to the nearest neighbor's coordinates. The movement towards the neighbor is scaled based on the distance and the maximum movement allowed.</p> <p>Parameters:</p> Name Type Description Default <code>distance</code> <code>float</code> <p>The distance to the nearest neighbor.</p> required <code>latitude</code> <code>float</code> <p>The current latitude of the entity.</p> required <code>longitude</code> <code>float</code> <p>The current longitude of the entity.</p> required <code>neighbor_latitude</code> <code>float</code> <p>The latitude of the nearest neighbor.</p> required <code>neighbor_longitude</code> <code>float</code> <p>The longitude of the nearest neighbor.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>May raise exceptions if the <code>set_geo_location</code> method encounters errors.</p> Notes <ul> <li>The movement is scaled based on the maximum allowed distance defined by   <code>self.max_movement_nearest_strategy</code>.</li> <li>The angle to the neighbor is calculated using the arctangent of the difference in   coordinates to determine the direction of movement.</li> <li>The conversion from meters to degrees is based on approximate geographical conversion factors.</li> </ul> Source code in <code>nebula/addons/mobility.py</code> <pre><code>async def change_geo_location_nearest_neighbor_strategy(\n    self, distance, latitude, longitude, neighbor_latitude, neighbor_longitude\n):\n    \"\"\"\n    Changes the geographical location of the entity towards the nearest neighbor.\n\n    This coroutine updates the current geographical location by calculating the direction\n    and distance to the nearest neighbor's coordinates. The movement towards the neighbor\n    is scaled based on the distance and the maximum movement allowed.\n\n    Args:\n        distance (float): The distance to the nearest neighbor.\n        latitude (float): The current latitude of the entity.\n        longitude (float): The current longitude of the entity.\n        neighbor_latitude (float): The latitude of the nearest neighbor.\n        neighbor_longitude (float): The longitude of the nearest neighbor.\n\n    Raises:\n        Exception: May raise exceptions if the `set_geo_location` method encounters errors.\n\n    Notes:\n        - The movement is scaled based on the maximum allowed distance defined by\n          `self.max_movement_nearest_strategy`.\n        - The angle to the neighbor is calculated using the arctangent of the difference in\n          coordinates to determine the direction of movement.\n        - The conversion from meters to degrees is based on approximate geographical conversion factors.\n    \"\"\"\n    logging.info(\"\ud83d\udccd  Changing geo location towards the nearest neighbor\")\n    scale_factor = min(1, self.max_movement_nearest_strategy / distance)\n    # Calcular el \u00e1ngulo hacia el vecino\n    angle = math.atan2(neighbor_longitude - longitude, neighbor_latitude - latitude)\n    # Conversi\u00f3n de movimiento m\u00e1ximo a grados\n    max_lat_change = self.max_movement_nearest_strategy / 111000  # Cambio en grados para latitud\n    max_lon_change = self.max_movement_nearest_strategy / (\n        111000 * math.cos(math.radians(latitude))\n    )  # Cambio en grados para longitud\n    # Aplicar escala y direcci\u00f3n\n    delta_lat = max_lat_change * math.cos(angle) * scale_factor\n    delta_lon = max_lon_change * math.sin(angle) * scale_factor\n    # Actualizar latitud y longitud\n    new_latitude = latitude + delta_lat\n    new_longitude = longitude + delta_lon\n    await self.set_geo_location(new_latitude, new_longitude)\n</code></pre>"},{"location":"api/addons/mobility/#nebula.addons.mobility.Mobility.change_geo_location_random_strategy","title":"<code>change_geo_location_random_strategy(latitude, longitude)</code>  <code>async</code>","text":"<p>Changes the geographical location of the entity using a random strategy.</p> <p>This coroutine modifies the current geographical location by randomly selecting a new position within a specified radius around the given latitude and longitude. The new location is determined using polar coordinates, where a random distance (radius) and angle are calculated.</p> <p>Parameters:</p> Name Type Description Default <code>latitude</code> <code>float</code> <p>The current latitude of the entity.</p> required <code>longitude</code> <code>float</code> <p>The current longitude of the entity.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>May raise exceptions if the <code>set_geo_location</code> method encounters errors.</p> Notes <ul> <li>The maximum movement distance is determined by <code>self.max_movement_random_strategy</code>.</li> <li>The calculated radius is converted from meters to degrees based on an approximate   conversion factor (1 degree is approximately 111 kilometers).</li> </ul> Source code in <code>nebula/addons/mobility.py</code> <pre><code>async def change_geo_location_random_strategy(self, latitude, longitude):\n    \"\"\"\n    Changes the geographical location of the entity using a random strategy.\n\n    This coroutine modifies the current geographical location by randomly\n    selecting a new position within a specified radius around the given\n    latitude and longitude. The new location is determined using polar\n    coordinates, where a random distance (radius) and angle are calculated.\n\n    Args:\n        latitude (float): The current latitude of the entity.\n        longitude (float): The current longitude of the entity.\n\n    Raises:\n        Exception: May raise exceptions if the `set_geo_location` method encounters errors.\n\n    Notes:\n        - The maximum movement distance is determined by `self.max_movement_random_strategy`.\n        - The calculated radius is converted from meters to degrees based on an approximate\n          conversion factor (1 degree is approximately 111 kilometers).\n    \"\"\"\n    if self._verbose:\n        logging.info(\"\ud83d\udccd  Changing geo location randomly\")\n    # radius_in_degrees = self.radius_federation / 111000\n    max_radius_in_degrees = self.max_movement_random_strategy / 111000\n    radius = random.uniform(0, max_radius_in_degrees)  # noqa: S311\n    angle = random.uniform(0, 2 * math.pi)  # noqa: S311\n    latitude += radius * math.cos(angle)\n    longitude += radius * math.sin(angle)\n    await self.set_geo_location(latitude, longitude)\n</code></pre>"},{"location":"api/addons/mobility/#nebula.addons.mobility.Mobility.run_mobility","title":"<code>run_mobility()</code>  <code>async</code>","text":"<p>Executes the mobility operations in a continuous loop.</p> <p>This coroutine manages the mobility behavior of the module. It first checks whether mobility is enabled. If mobility is not enabled, the function returns immediately.</p> <p>If mobility is enabled, the function will wait for the specified grace time before entering an infinite loop where it performs the following operations:</p> <ol> <li>Changes the geographical location by calling the <code>change_geo_location</code> method.</li> <li>Adjusts connections based on the current distance by calling    the <code>change_connections_based_on_distance</code> method.</li> <li>Sleeps for a specified period (<code>self.period</code>) before repeating the operations.</li> </ol> <p>This allows for periodic updates to the module's geographical location and network connections as per the defined mobility strategy.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>May raise exceptions if <code>change_geo_location</code> or         <code>change_connections_based_on_distance</code> encounters errors.</p> Source code in <code>nebula/addons/mobility.py</code> <pre><code>async def run_mobility(self):\n    \"\"\"\n    Executes the mobility operations in a continuous loop.\n\n    This coroutine manages the mobility behavior of the module. It first\n    checks whether mobility is enabled. If mobility is not enabled, the\n    function returns immediately.\n\n    If mobility is enabled, the function will wait for the specified\n    grace time before entering an infinite loop where it performs the\n    following operations:\n\n    1. Changes the geographical location by calling the `change_geo_location` method.\n    2. Adjusts connections based on the current distance by calling\n       the `change_connections_based_on_distance` method.\n    3. Sleeps for a specified period (`self.period`) before repeating the operations.\n\n    This allows for periodic updates to the module's geographical location\n    and network connections as per the defined mobility strategy.\n\n    Raises:\n        Exception: May raise exceptions if `change_geo_location` or\n                    `change_connections_based_on_distance` encounters errors.\n    \"\"\"\n    if not self.mobility:\n        return\n    # await asyncio.sleep(self.grace_time)\n    while True:\n        await self.change_geo_location()\n        await asyncio.sleep(self.period)\n</code></pre>"},{"location":"api/addons/mobility/#nebula.addons.mobility.Mobility.set_geo_location","title":"<code>set_geo_location(latitude, longitude)</code>  <code>async</code>","text":"<p>Sets the geographical location of the entity to the specified latitude and longitude.</p> <p>This coroutine updates the latitude and longitude values in the configuration. If the provided coordinates are out of bounds (latitude must be between -90 and 90, and longitude must be between -180 and 180), the previous location is retained.</p> <p>Parameters:</p> Name Type Description Default <code>latitude</code> <code>float</code> <p>The new latitude to set.</p> required <code>longitude</code> <code>float</code> <p>The new longitude to set.</p> required <p>Raises:</p> Type Description <code>None</code> <p>This function does not raise any exceptions but retains the previous coordinates   if the new ones are invalid.</p> Notes <ul> <li>The new location is logged for tracking purposes.</li> <li>The coordinates are expected to be in decimal degrees format.</li> </ul> Source code in <code>nebula/addons/mobility.py</code> <pre><code>async def set_geo_location(self, latitude, longitude):\n    \"\"\"\n    Sets the geographical location of the entity to the specified latitude and longitude.\n\n    This coroutine updates the latitude and longitude values in the configuration. If the\n    provided coordinates are out of bounds (latitude must be between -90 and 90, and\n    longitude must be between -180 and 180), the previous location is retained.\n\n    Args:\n        latitude (float): The new latitude to set.\n        longitude (float): The new longitude to set.\n\n    Raises:\n        None: This function does not raise any exceptions but retains the previous coordinates\n              if the new ones are invalid.\n\n    Notes:\n        - The new location is logged for tracking purposes.\n        - The coordinates are expected to be in decimal degrees format.\n    \"\"\"\n\n    if latitude &lt; -90 or latitude &gt; 90 or longitude &lt; -180 or longitude &gt; 180:\n        # If the new location is out of bounds, we keep the old location\n        latitude = self.config.participant[\"mobility_args\"][\"latitude\"]\n        longitude = self.config.participant[\"mobility_args\"][\"longitude\"]\n\n    self.config.participant[\"mobility_args\"][\"latitude\"] = latitude\n    self.config.participant[\"mobility_args\"][\"longitude\"] = longitude\n    if self._verbose:\n        logging.info(f\"\ud83d\udccd  New geo location: {latitude}, {longitude}\")\n</code></pre>"},{"location":"api/addons/mobility/#nebula.addons.mobility.Mobility.start","title":"<code>start()</code>  <code>async</code>","text":"<p>Initiates the mobility process by starting the associated task.</p> <p>This method creates and schedules an asynchronous task to run the <code>run_mobility</code> coroutine, which handles the mobility operations for the module. It allows the mobility operations to run concurrently without blocking the execution of other tasks.</p> <p>Returns:</p> Type Description <p>asyncio.Task: An asyncio Task object representing the scheduled            <code>run_mobility</code> operation.</p> Source code in <code>nebula/addons/mobility.py</code> <pre><code>async def start(self):\n    \"\"\"\n    Initiates the mobility process by starting the associated task.\n\n    This method creates and schedules an asynchronous task to run the\n    `run_mobility` coroutine, which handles the mobility operations\n    for the module. It allows the mobility operations to run concurrently\n    without blocking the execution of other tasks.\n\n    Returns:\n        asyncio.Task: An asyncio Task object representing the scheduled\n                       `run_mobility` operation.\n    \"\"\"\n    await EventManager.get_instance().subscribe_addonevent(GPSEvent, self.update_nodes_distances)\n    task = asyncio.create_task(self.run_mobility())\n    return task\n</code></pre>"},{"location":"api/addons/reporter/","title":"Documentation for Reporter Module","text":""},{"location":"api/addons/reporter/#nebula.addons.reporter.Reporter","title":"<code>Reporter</code>","text":"Source code in <code>nebula/addons/reporter.py</code> <pre><code>class Reporter:\n    def __init__(self, config, trainer, cm: \"CommunicationsManager\"):\n        \"\"\"\n        Initializes the reporter module for sending periodic updates to a dashboard controller.\n\n        This initializer sets up the configuration parameters required to report metrics and statistics\n        about the network, participant, and trainer. It connects to a specified URL endpoint where\n        these metrics will be logged, and it initializes values used for tracking network traffic.\n\n        Args:\n            config (dict): The configuration dictionary containing all setup parameters.\n            trainer (Trainer): The trainer object responsible for managing training sessions.\n            cm (CommunicationsManager): The communications manager handling network connections\n                                        and interactions.\n\n        Attributes:\n            frequency (int): The frequency at which the reporter sends updates.\n            grace_time (int): Grace period before starting the reporting.\n            data_queue (Queue): An asyncio queue for managing data to be reported.\n            url (str): The endpoint URL for reporting updates.\n            counter (int): Counter for tracking the number of reports sent.\n            first_net_metrics (bool): Flag indicating if this is the first collection of network metrics.\n            prev_bytes_sent (int), prev_bytes_recv (int), prev_packets_sent (int), prev_packets_recv (int):\n                Metrics for tracking network data sent and received.\n            acc_bytes_sent (int), acc_bytes_recv (int), acc_packets_sent (int), acc_packets_recv (int):\n                Accumulators for network traffic.\n\n        Raises:\n            None\n\n        Notes:\n            - Logs the start of the reporter module.\n            - Initializes both current and accumulated metrics for traffic monitoring.\n        \"\"\"\n        logging.info(\"Starting reporter module\")\n        self.config = config\n        self.trainer = trainer\n        self.cm = cm\n        self.frequency = self.config.participant[\"reporter_args\"][\"report_frequency\"]\n        self.grace_time = self.config.participant[\"reporter_args\"][\"grace_time_reporter\"]\n        self.data_queue = asyncio.Queue()\n        self.url = f\"http://{self.config.participant['scenario_args']['controller']}/platform/dashboard/{self.config.participant['scenario_args']['name']}/node/update\"\n        self.counter = 0\n\n        self.first_net_metrics = True\n        self.prev_bytes_sent = 0\n        self.prev_bytes_recv = 0\n        self.prev_packets_sent = 0\n        self.prev_packets_recv = 0\n\n        self.acc_bytes_sent = 0\n        self.acc_bytes_recv = 0\n        self.acc_packets_sent = 0\n        self.acc_packets_recv = 0\n\n    async def enqueue_data(self, name, value):\n        \"\"\"\n        Asynchronously enqueues data for reporting.\n\n        This function adds a named data value pair to the data queue, which will later be processed\n        and sent to the designated reporting endpoint. The queue enables handling of reporting tasks\n        independently of other processes.\n\n        Args:\n            name (str): The name or identifier for the data item.\n            value (Any): The value of the data item to be reported.\n\n        Returns:\n            None\n\n        Notes:\n            - This function is asynchronous to allow non-blocking data enqueueing.\n            - Uses asyncio's queue to manage data, ensuring concurrency.\n        \"\"\"\n        await self.data_queue.put((name, value))\n\n    async def start(self):\n        \"\"\"\n        Starts the reporter module after a grace period.\n\n        This asynchronous function initiates the reporting process following a designated grace period.\n        It creates a background task to run the reporting loop, allowing data to be reported at defined intervals.\n\n        Returns:\n            asyncio.Task: The task for the reporter loop, which handles the data reporting asynchronously.\n\n        Notes:\n            - The grace period allows for a delay before the first reporting cycle.\n            - The reporter loop runs in the background, ensuring continuous data updates.\n        \"\"\"\n        await asyncio.sleep(self.grace_time)\n        task = asyncio.create_task(self.run_reporter())\n        return task\n\n    async def run_reporter(self):\n        \"\"\"\n        Runs the continuous reporting loop.\n\n        This asynchronous function performs periodic reporting tasks such as reporting resource usage,\n        data queue contents, and, optionally, status updates to the controller. The loop runs indefinitely,\n        updating the counter with each cycle to track the frequency of specific tasks.\n\n        Key Actions:\n            - Regularly reports the resource status.\n            - Reloads the configuration file every 50 cycles to reflect any updates.\n\n        Notes:\n            - The reporting frequency is determined by the 'report_frequency' setting in the config file.\n        \"\"\"\n        while True:\n            if self.config.participant[\"reporter_args\"][\"report_status_data_queue\"]:\n                if self.config.participant[\"scenario_args\"][\"controller\"] != \"nebula-test\":\n                    await self.__report_status_to_controller()\n                await self.__report_data_queue()\n            await self.__report_resources()\n            self.counter += 1\n            if self.counter % 50 == 0:\n                logging.info(\"Reloading config file...\")\n                self.cm.engine.config.reload_config_file()\n            await asyncio.sleep(self.frequency)\n\n    async def report_scenario_finished(self):\n        \"\"\"\n        Reports the scenario completion status to the controller.\n\n        This asynchronous function notifies the scenario controller that the participant has finished\n        its tasks. It sends a POST request to the designated controller URL, including the participant's\n        ID in the JSON payload.\n\n        URL Construction:\n            - The URL is dynamically built using the controller address and scenario name\n              from the configuration settings.\n\n        Parameters:\n            - idx (int): The unique identifier for this participant, sent in the request data.\n\n        Returns:\n            - bool: True if the report was successful (status 200), False otherwise.\n\n        Error Handling:\n            - Logs an error if the response status is not 200, indicating that the controller\n              might be temporarily overloaded.\n            - Logs exceptions if the connection attempt to the controller fails.\n        \"\"\"\n        url = f\"http://{self.config.participant['scenario_args']['controller']}/platform/dashboard/{self.config.participant['scenario_args']['name']}/node/done\"\n        data = json.dumps({\"idx\": self.config.participant[\"device_args\"][\"idx\"]})\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"User-Agent\": f\"NEBULA Participant {self.config.participant['device_args']['idx']}\",\n        }\n        try:\n            async with aiohttp.ClientSession() as session, session.post(url, data=data, headers=headers) as response:\n                if response.status != 200:\n                    logging.error(\n                        f\"Error received from controller: {response.status} (probably there is overhead in the controller, trying again in the next round)\"\n                    )\n                    text = await response.text()\n                    logging.debug(text)\n                else:\n                    logging.info(\n                        f\"Participant {self.config.participant['device_args']['idx']} reported scenario finished\"\n                    )\n                    return True\n        except aiohttp.ClientError:\n            logging.exception(f\"Error connecting to the controller at {url}\")\n        return False\n\n    async def __report_data_queue(self):\n        \"\"\"\n        Processes and reports queued data entries.\n\n        This asynchronous function iterates over the data queue, retrieving each name-value pair\n        and sending it to the trainer's logging mechanism. Once logged, each item is marked as done.\n\n        Functionality:\n            - Retrieves and logs all entries in the data queue until it is empty.\n            - Assumes that `log_data` can handle asynchronous execution for optimal performance.\n\n        Parameters:\n            - name (str): The identifier for the data entry (e.g., metric name).\n            - value (Any): The value of the data entry to be logged.\n\n        Returns:\n            - None\n\n        Notes:\n            - Each processed item is marked as done in the queue.\n        \"\"\"\n\n        while not self.data_queue.empty():\n            name, value = await self.data_queue.get()\n            await self.trainer.logger.log_data({name: value})  # Assuming log_data can be made async\n            self.data_queue.task_done()\n\n    async def __report_status_to_controller(self):\n        \"\"\"\n        Sends the participant's status to the controller.\n\n        This asynchronous function transmits the current participant configuration to the controller's\n        URL endpoint. It handles both client and general exceptions to ensure robust communication\n        with the controller, retrying in case of errors.\n\n        Functionality:\n            - Initiates a session to post participant data to the controller.\n            - Logs the response status, indicating issues when status is non-200.\n            - Retries after a short delay in case of connection errors or unhandled exceptions.\n\n        Parameters:\n            - None (uses internal `self.config.participant` data to build the payload).\n\n        Returns:\n            - None\n\n        Notes:\n            - Uses the participant index to specify the User-Agent in headers.\n            - Delays for 5 seconds upon general exceptions to avoid rapid retry loops.\n        \"\"\"\n        try:\n            async with (\n                aiohttp.ClientSession() as session,\n                session.post(\n                    self.url,\n                    data=json.dumps(self.config.participant),\n                    headers={\n                        \"Content-Type\": \"application/json\",\n                        \"User-Agent\": f\"NEBULA Participant {self.config.participant['device_args']['idx']}\",\n                    },\n                ) as response,\n            ):\n                if response.status != 200:\n                    logging.error(\n                        f\"Error received from controller: {response.status} (probably there is overhead in the controller, trying again in the next round)\"\n                    )\n                    text = await response.text()\n                    logging.debug(text)\n        except aiohttp.ClientError:\n            logging.exception(f\"Error connecting to the controller at {self.url}\")\n        except Exception:\n            logging.exception(\"Error sending status to controller, will try again in a few seconds\")\n            await asyncio.sleep(5)\n\n    async def __report_resources(self):\n        \"\"\"\n        Reports system resource usage metrics.\n\n        This asynchronous function gathers and logs CPU usage data for the participant's device,\n        and attempts to retrieve the CPU temperature (Linux systems only). Additionally, it measures\n        CPU usage specifically for the current process.\n\n        Functionality:\n            - Gathers total CPU usage (percentage) and attempts to retrieve CPU temperature.\n            - Uses `psutil` for non-blocking access to system data on Linux.\n            - Records CPU usage of the current process for finer monitoring.\n\n        Parameters:\n            - None\n\n        Notes:\n            - On non-Linux platforms, CPU temperature will default to 0.\n            - Uses `asyncio.to_thread` to call CPU and sensor readings without blocking the event loop.\n        \"\"\"\n        cpu_percent = psutil.cpu_percent()\n        cpu_temp = 0\n        try:\n            if sys.platform == \"linux\":\n                sensors = await asyncio.to_thread(psutil.sensors_temperatures)\n                cpu_temp = sensors.get(\"coretemp\")[0].current if sensors.get(\"coretemp\") else 0\n        except Exception:  # noqa: S110\n            pass\n\n        pid = os.getpid()\n        cpu_percent_process = await asyncio.to_thread(psutil.Process(pid).cpu_percent, interval=1)\n\n        process = psutil.Process(pid)\n        memory_process = await asyncio.to_thread(lambda: process.memory_info().rss / (1024**2))\n        memory_percent_process = process.memory_percent()\n        memory_info = await asyncio.to_thread(psutil.virtual_memory)\n        memory_percent = memory_info.percent\n        memory_used = memory_info.used / (1024**2)\n\n        disk_percent = psutil.disk_usage(\"/\").percent\n\n        net_io_counters = await asyncio.to_thread(psutil.net_io_counters)\n        bytes_sent = net_io_counters.bytes_sent\n        bytes_recv = net_io_counters.bytes_recv\n        packets_sent = net_io_counters.packets_sent\n        packets_recv = net_io_counters.packets_recv\n\n        if self.first_net_metrics:\n            bytes_sent_diff = 0\n            bytes_recv_diff = 0\n            packets_sent_diff = 0\n            packets_recv_diff = 0\n            self.first_net_metrics = False\n        else:\n            bytes_sent_diff = bytes_sent - self.prev_bytes_sent\n            bytes_recv_diff = bytes_recv - self.prev_bytes_recv\n            packets_sent_diff = packets_sent - self.prev_packets_sent\n            packets_recv_diff = packets_recv - self.prev_packets_recv\n\n        self.prev_bytes_sent = bytes_sent\n        self.prev_bytes_recv = bytes_recv\n        self.prev_packets_sent = packets_sent\n        self.prev_packets_recv = packets_recv\n\n        self.acc_bytes_sent += bytes_sent_diff\n        self.acc_bytes_recv += bytes_recv_diff\n        self.acc_packets_sent += packets_sent_diff\n        self.acc_packets_recv += packets_recv_diff\n\n        current_connections = await self.cm.get_addrs_current_connections(only_direct=True)\n\n        resources = {\n            \"W-CPU/CPU global (%)\": cpu_percent,\n            \"W-CPU/CPU process (%)\": cpu_percent_process,\n            \"W-CPU/CPU temperature (\u00b0)\": cpu_temp,\n            \"Z-RAM/RAM global (%)\": memory_percent,\n            \"Z-RAM/RAM global (MB)\": memory_used,\n            \"Z-RAM/RAM process (%)\": memory_percent_process,\n            \"Z-RAM/RAM process (MB)\": memory_process,\n            \"Y-Disk/Disk (%)\": disk_percent,\n            \"X-Network/Network (bytes sent)\": round(self.acc_bytes_sent / (1024**2), 3),\n            \"X-Network/Network (bytes received)\": round(self.acc_bytes_recv / (1024**2), 3),\n            \"X-Network/Network (packets sent)\": self.acc_packets_sent,\n            \"X-Network/Network (packets received)\": self.acc_packets_recv,\n            \"X-Network/Connections\": len(current_connections),\n        }\n        self.trainer.logger.log_data(resources)\n\n        if importlib.util.find_spec(\"pynvml\") is not None:\n            try:\n                import pynvml\n\n                await asyncio.to_thread(pynvml.nvmlInit)\n                devices = await asyncio.to_thread(pynvml.nvmlDeviceGetCount)\n                for i in range(devices):\n                    handle = await asyncio.to_thread(pynvml.nvmlDeviceGetHandleByIndex, i)\n                    gpu_percent = (await asyncio.to_thread(pynvml.nvmlDeviceGetUtilizationRates, handle)).gpu\n                    gpu_temp = await asyncio.to_thread(\n                        pynvml.nvmlDeviceGetTemperature,\n                        handle,\n                        pynvml.NVML_TEMPERATURE_GPU,\n                    )\n                    gpu_mem = await asyncio.to_thread(pynvml.nvmlDeviceGetMemoryInfo, handle)\n                    gpu_mem_percent = round(gpu_mem.used / gpu_mem.total * 100, 3)\n                    gpu_power = await asyncio.to_thread(pynvml.nvmlDeviceGetPowerUsage, handle) / 1000.0\n                    gpu_clocks = await asyncio.to_thread(pynvml.nvmlDeviceGetClockInfo, handle, pynvml.NVML_CLOCK_SM)\n                    gpu_memory_clocks = await asyncio.to_thread(\n                        pynvml.nvmlDeviceGetClockInfo, handle, pynvml.NVML_CLOCK_MEM\n                    )\n                    gpu_fan_speed = await asyncio.to_thread(pynvml.nvmlDeviceGetFanSpeed, handle)\n                    gpu_info = {\n                        f\"W-GPU/GPU{i} (%)\": gpu_percent,\n                        f\"W-GPU/GPU{i} temperature (\u00b0)\": gpu_temp,\n                        f\"W-GPU/GPU{i} memory (%)\": gpu_mem_percent,\n                        f\"W-GPU/GPU{i} power\": gpu_power,\n                        f\"W-GPU/GPU{i} clocks\": gpu_clocks,\n                        f\"W-GPU/GPU{i} memory clocks\": gpu_memory_clocks,\n                        f\"W-GPU/GPU{i} fan speed\": gpu_fan_speed,\n                    }\n                    self.trainer.logger.log_data(gpu_info)\n            except Exception:  # noqa: S110\n                pass\n</code></pre>"},{"location":"api/addons/reporter/#nebula.addons.reporter.Reporter.__init__","title":"<code>__init__(config, trainer, cm)</code>","text":"<p>Initializes the reporter module for sending periodic updates to a dashboard controller.</p> <p>This initializer sets up the configuration parameters required to report metrics and statistics about the network, participant, and trainer. It connects to a specified URL endpoint where these metrics will be logged, and it initializes values used for tracking network traffic.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>The configuration dictionary containing all setup parameters.</p> required <code>trainer</code> <code>Trainer</code> <p>The trainer object responsible for managing training sessions.</p> required <code>cm</code> <code>CommunicationsManager</code> <p>The communications manager handling network connections                         and interactions.</p> required <p>Attributes:</p> Name Type Description <code>frequency</code> <code>int</code> <p>The frequency at which the reporter sends updates.</p> <code>grace_time</code> <code>int</code> <p>Grace period before starting the reporting.</p> <code>data_queue</code> <code>Queue</code> <p>An asyncio queue for managing data to be reported.</p> <code>url</code> <code>str</code> <p>The endpoint URL for reporting updates.</p> <code>counter</code> <code>int</code> <p>Counter for tracking the number of reports sent.</p> <code>first_net_metrics</code> <code>bool</code> <p>Flag indicating if this is the first collection of network metrics.</p> <code>prev_bytes_sent</code> <code>int), prev_bytes_recv (int), prev_packets_sent (int), prev_packets_recv (int</code> <p>Metrics for tracking network data sent and received.</p> <code>acc_bytes_sent</code> <code>int), acc_bytes_recv (int), acc_packets_sent (int), acc_packets_recv (int</code> <p>Accumulators for network traffic.</p> Notes <ul> <li>Logs the start of the reporter module.</li> <li>Initializes both current and accumulated metrics for traffic monitoring.</li> </ul> Source code in <code>nebula/addons/reporter.py</code> <pre><code>def __init__(self, config, trainer, cm: \"CommunicationsManager\"):\n    \"\"\"\n    Initializes the reporter module for sending periodic updates to a dashboard controller.\n\n    This initializer sets up the configuration parameters required to report metrics and statistics\n    about the network, participant, and trainer. It connects to a specified URL endpoint where\n    these metrics will be logged, and it initializes values used for tracking network traffic.\n\n    Args:\n        config (dict): The configuration dictionary containing all setup parameters.\n        trainer (Trainer): The trainer object responsible for managing training sessions.\n        cm (CommunicationsManager): The communications manager handling network connections\n                                    and interactions.\n\n    Attributes:\n        frequency (int): The frequency at which the reporter sends updates.\n        grace_time (int): Grace period before starting the reporting.\n        data_queue (Queue): An asyncio queue for managing data to be reported.\n        url (str): The endpoint URL for reporting updates.\n        counter (int): Counter for tracking the number of reports sent.\n        first_net_metrics (bool): Flag indicating if this is the first collection of network metrics.\n        prev_bytes_sent (int), prev_bytes_recv (int), prev_packets_sent (int), prev_packets_recv (int):\n            Metrics for tracking network data sent and received.\n        acc_bytes_sent (int), acc_bytes_recv (int), acc_packets_sent (int), acc_packets_recv (int):\n            Accumulators for network traffic.\n\n    Raises:\n        None\n\n    Notes:\n        - Logs the start of the reporter module.\n        - Initializes both current and accumulated metrics for traffic monitoring.\n    \"\"\"\n    logging.info(\"Starting reporter module\")\n    self.config = config\n    self.trainer = trainer\n    self.cm = cm\n    self.frequency = self.config.participant[\"reporter_args\"][\"report_frequency\"]\n    self.grace_time = self.config.participant[\"reporter_args\"][\"grace_time_reporter\"]\n    self.data_queue = asyncio.Queue()\n    self.url = f\"http://{self.config.participant['scenario_args']['controller']}/platform/dashboard/{self.config.participant['scenario_args']['name']}/node/update\"\n    self.counter = 0\n\n    self.first_net_metrics = True\n    self.prev_bytes_sent = 0\n    self.prev_bytes_recv = 0\n    self.prev_packets_sent = 0\n    self.prev_packets_recv = 0\n\n    self.acc_bytes_sent = 0\n    self.acc_bytes_recv = 0\n    self.acc_packets_sent = 0\n    self.acc_packets_recv = 0\n</code></pre>"},{"location":"api/addons/reporter/#nebula.addons.reporter.Reporter.__report_data_queue","title":"<code>__report_data_queue()</code>  <code>async</code>","text":"<p>Processes and reports queued data entries.</p> <p>This asynchronous function iterates over the data queue, retrieving each name-value pair and sending it to the trainer's logging mechanism. Once logged, each item is marked as done.</p> Functionality <ul> <li>Retrieves and logs all entries in the data queue until it is empty.</li> <li>Assumes that <code>log_data</code> can handle asynchronous execution for optimal performance.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>-</code> <code>name (str</code> <p>The identifier for the data entry (e.g., metric name).</p> required <code>-</code> <code>value (Any</code> <p>The value of the data entry to be logged.</p> required <p>Returns:</p> Type Description <ul> <li>None</li> </ul> Notes <ul> <li>Each processed item is marked as done in the queue.</li> </ul> Source code in <code>nebula/addons/reporter.py</code> <pre><code>async def __report_data_queue(self):\n    \"\"\"\n    Processes and reports queued data entries.\n\n    This asynchronous function iterates over the data queue, retrieving each name-value pair\n    and sending it to the trainer's logging mechanism. Once logged, each item is marked as done.\n\n    Functionality:\n        - Retrieves and logs all entries in the data queue until it is empty.\n        - Assumes that `log_data` can handle asynchronous execution for optimal performance.\n\n    Parameters:\n        - name (str): The identifier for the data entry (e.g., metric name).\n        - value (Any): The value of the data entry to be logged.\n\n    Returns:\n        - None\n\n    Notes:\n        - Each processed item is marked as done in the queue.\n    \"\"\"\n\n    while not self.data_queue.empty():\n        name, value = await self.data_queue.get()\n        await self.trainer.logger.log_data({name: value})  # Assuming log_data can be made async\n        self.data_queue.task_done()\n</code></pre>"},{"location":"api/addons/reporter/#nebula.addons.reporter.Reporter.__report_resources","title":"<code>__report_resources()</code>  <code>async</code>","text":"<p>Reports system resource usage metrics.</p> <p>This asynchronous function gathers and logs CPU usage data for the participant's device, and attempts to retrieve the CPU temperature (Linux systems only). Additionally, it measures CPU usage specifically for the current process.</p> Functionality <ul> <li>Gathers total CPU usage (percentage) and attempts to retrieve CPU temperature.</li> <li>Uses <code>psutil</code> for non-blocking access to system data on Linux.</li> <li>Records CPU usage of the current process for finer monitoring.</li> </ul> Notes <ul> <li>On non-Linux platforms, CPU temperature will default to 0.</li> <li>Uses <code>asyncio.to_thread</code> to call CPU and sensor readings without blocking the event loop.</li> </ul> Source code in <code>nebula/addons/reporter.py</code> <pre><code>async def __report_resources(self):\n    \"\"\"\n    Reports system resource usage metrics.\n\n    This asynchronous function gathers and logs CPU usage data for the participant's device,\n    and attempts to retrieve the CPU temperature (Linux systems only). Additionally, it measures\n    CPU usage specifically for the current process.\n\n    Functionality:\n        - Gathers total CPU usage (percentage) and attempts to retrieve CPU temperature.\n        - Uses `psutil` for non-blocking access to system data on Linux.\n        - Records CPU usage of the current process for finer monitoring.\n\n    Parameters:\n        - None\n\n    Notes:\n        - On non-Linux platforms, CPU temperature will default to 0.\n        - Uses `asyncio.to_thread` to call CPU and sensor readings without blocking the event loop.\n    \"\"\"\n    cpu_percent = psutil.cpu_percent()\n    cpu_temp = 0\n    try:\n        if sys.platform == \"linux\":\n            sensors = await asyncio.to_thread(psutil.sensors_temperatures)\n            cpu_temp = sensors.get(\"coretemp\")[0].current if sensors.get(\"coretemp\") else 0\n    except Exception:  # noqa: S110\n        pass\n\n    pid = os.getpid()\n    cpu_percent_process = await asyncio.to_thread(psutil.Process(pid).cpu_percent, interval=1)\n\n    process = psutil.Process(pid)\n    memory_process = await asyncio.to_thread(lambda: process.memory_info().rss / (1024**2))\n    memory_percent_process = process.memory_percent()\n    memory_info = await asyncio.to_thread(psutil.virtual_memory)\n    memory_percent = memory_info.percent\n    memory_used = memory_info.used / (1024**2)\n\n    disk_percent = psutil.disk_usage(\"/\").percent\n\n    net_io_counters = await asyncio.to_thread(psutil.net_io_counters)\n    bytes_sent = net_io_counters.bytes_sent\n    bytes_recv = net_io_counters.bytes_recv\n    packets_sent = net_io_counters.packets_sent\n    packets_recv = net_io_counters.packets_recv\n\n    if self.first_net_metrics:\n        bytes_sent_diff = 0\n        bytes_recv_diff = 0\n        packets_sent_diff = 0\n        packets_recv_diff = 0\n        self.first_net_metrics = False\n    else:\n        bytes_sent_diff = bytes_sent - self.prev_bytes_sent\n        bytes_recv_diff = bytes_recv - self.prev_bytes_recv\n        packets_sent_diff = packets_sent - self.prev_packets_sent\n        packets_recv_diff = packets_recv - self.prev_packets_recv\n\n    self.prev_bytes_sent = bytes_sent\n    self.prev_bytes_recv = bytes_recv\n    self.prev_packets_sent = packets_sent\n    self.prev_packets_recv = packets_recv\n\n    self.acc_bytes_sent += bytes_sent_diff\n    self.acc_bytes_recv += bytes_recv_diff\n    self.acc_packets_sent += packets_sent_diff\n    self.acc_packets_recv += packets_recv_diff\n\n    current_connections = await self.cm.get_addrs_current_connections(only_direct=True)\n\n    resources = {\n        \"W-CPU/CPU global (%)\": cpu_percent,\n        \"W-CPU/CPU process (%)\": cpu_percent_process,\n        \"W-CPU/CPU temperature (\u00b0)\": cpu_temp,\n        \"Z-RAM/RAM global (%)\": memory_percent,\n        \"Z-RAM/RAM global (MB)\": memory_used,\n        \"Z-RAM/RAM process (%)\": memory_percent_process,\n        \"Z-RAM/RAM process (MB)\": memory_process,\n        \"Y-Disk/Disk (%)\": disk_percent,\n        \"X-Network/Network (bytes sent)\": round(self.acc_bytes_sent / (1024**2), 3),\n        \"X-Network/Network (bytes received)\": round(self.acc_bytes_recv / (1024**2), 3),\n        \"X-Network/Network (packets sent)\": self.acc_packets_sent,\n        \"X-Network/Network (packets received)\": self.acc_packets_recv,\n        \"X-Network/Connections\": len(current_connections),\n    }\n    self.trainer.logger.log_data(resources)\n\n    if importlib.util.find_spec(\"pynvml\") is not None:\n        try:\n            import pynvml\n\n            await asyncio.to_thread(pynvml.nvmlInit)\n            devices = await asyncio.to_thread(pynvml.nvmlDeviceGetCount)\n            for i in range(devices):\n                handle = await asyncio.to_thread(pynvml.nvmlDeviceGetHandleByIndex, i)\n                gpu_percent = (await asyncio.to_thread(pynvml.nvmlDeviceGetUtilizationRates, handle)).gpu\n                gpu_temp = await asyncio.to_thread(\n                    pynvml.nvmlDeviceGetTemperature,\n                    handle,\n                    pynvml.NVML_TEMPERATURE_GPU,\n                )\n                gpu_mem = await asyncio.to_thread(pynvml.nvmlDeviceGetMemoryInfo, handle)\n                gpu_mem_percent = round(gpu_mem.used / gpu_mem.total * 100, 3)\n                gpu_power = await asyncio.to_thread(pynvml.nvmlDeviceGetPowerUsage, handle) / 1000.0\n                gpu_clocks = await asyncio.to_thread(pynvml.nvmlDeviceGetClockInfo, handle, pynvml.NVML_CLOCK_SM)\n                gpu_memory_clocks = await asyncio.to_thread(\n                    pynvml.nvmlDeviceGetClockInfo, handle, pynvml.NVML_CLOCK_MEM\n                )\n                gpu_fan_speed = await asyncio.to_thread(pynvml.nvmlDeviceGetFanSpeed, handle)\n                gpu_info = {\n                    f\"W-GPU/GPU{i} (%)\": gpu_percent,\n                    f\"W-GPU/GPU{i} temperature (\u00b0)\": gpu_temp,\n                    f\"W-GPU/GPU{i} memory (%)\": gpu_mem_percent,\n                    f\"W-GPU/GPU{i} power\": gpu_power,\n                    f\"W-GPU/GPU{i} clocks\": gpu_clocks,\n                    f\"W-GPU/GPU{i} memory clocks\": gpu_memory_clocks,\n                    f\"W-GPU/GPU{i} fan speed\": gpu_fan_speed,\n                }\n                self.trainer.logger.log_data(gpu_info)\n        except Exception:  # noqa: S110\n            pass\n</code></pre>"},{"location":"api/addons/reporter/#nebula.addons.reporter.Reporter.__report_status_to_controller","title":"<code>__report_status_to_controller()</code>  <code>async</code>","text":"<p>Sends the participant's status to the controller.</p> <p>This asynchronous function transmits the current participant configuration to the controller's URL endpoint. It handles both client and general exceptions to ensure robust communication with the controller, retrying in case of errors.</p> Functionality <ul> <li>Initiates a session to post participant data to the controller.</li> <li>Logs the response status, indicating issues when status is non-200.</li> <li>Retries after a short delay in case of connection errors or unhandled exceptions.</li> </ul> <p>Returns:</p> Type Description <ul> <li>None</li> </ul> Notes <ul> <li>Uses the participant index to specify the User-Agent in headers.</li> <li>Delays for 5 seconds upon general exceptions to avoid rapid retry loops.</li> </ul> Source code in <code>nebula/addons/reporter.py</code> <pre><code>async def __report_status_to_controller(self):\n    \"\"\"\n    Sends the participant's status to the controller.\n\n    This asynchronous function transmits the current participant configuration to the controller's\n    URL endpoint. It handles both client and general exceptions to ensure robust communication\n    with the controller, retrying in case of errors.\n\n    Functionality:\n        - Initiates a session to post participant data to the controller.\n        - Logs the response status, indicating issues when status is non-200.\n        - Retries after a short delay in case of connection errors or unhandled exceptions.\n\n    Parameters:\n        - None (uses internal `self.config.participant` data to build the payload).\n\n    Returns:\n        - None\n\n    Notes:\n        - Uses the participant index to specify the User-Agent in headers.\n        - Delays for 5 seconds upon general exceptions to avoid rapid retry loops.\n    \"\"\"\n    try:\n        async with (\n            aiohttp.ClientSession() as session,\n            session.post(\n                self.url,\n                data=json.dumps(self.config.participant),\n                headers={\n                    \"Content-Type\": \"application/json\",\n                    \"User-Agent\": f\"NEBULA Participant {self.config.participant['device_args']['idx']}\",\n                },\n            ) as response,\n        ):\n            if response.status != 200:\n                logging.error(\n                    f\"Error received from controller: {response.status} (probably there is overhead in the controller, trying again in the next round)\"\n                )\n                text = await response.text()\n                logging.debug(text)\n    except aiohttp.ClientError:\n        logging.exception(f\"Error connecting to the controller at {self.url}\")\n    except Exception:\n        logging.exception(\"Error sending status to controller, will try again in a few seconds\")\n        await asyncio.sleep(5)\n</code></pre>"},{"location":"api/addons/reporter/#nebula.addons.reporter.Reporter.enqueue_data","title":"<code>enqueue_data(name, value)</code>  <code>async</code>","text":"<p>Asynchronously enqueues data for reporting.</p> <p>This function adds a named data value pair to the data queue, which will later be processed and sent to the designated reporting endpoint. The queue enables handling of reporting tasks independently of other processes.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name or identifier for the data item.</p> required <code>value</code> <code>Any</code> <p>The value of the data item to be reported.</p> required <p>Returns:</p> Type Description <p>None</p> Notes <ul> <li>This function is asynchronous to allow non-blocking data enqueueing.</li> <li>Uses asyncio's queue to manage data, ensuring concurrency.</li> </ul> Source code in <code>nebula/addons/reporter.py</code> <pre><code>async def enqueue_data(self, name, value):\n    \"\"\"\n    Asynchronously enqueues data for reporting.\n\n    This function adds a named data value pair to the data queue, which will later be processed\n    and sent to the designated reporting endpoint. The queue enables handling of reporting tasks\n    independently of other processes.\n\n    Args:\n        name (str): The name or identifier for the data item.\n        value (Any): The value of the data item to be reported.\n\n    Returns:\n        None\n\n    Notes:\n        - This function is asynchronous to allow non-blocking data enqueueing.\n        - Uses asyncio's queue to manage data, ensuring concurrency.\n    \"\"\"\n    await self.data_queue.put((name, value))\n</code></pre>"},{"location":"api/addons/reporter/#nebula.addons.reporter.Reporter.report_scenario_finished","title":"<code>report_scenario_finished()</code>  <code>async</code>","text":"<p>Reports the scenario completion status to the controller.</p> <p>This asynchronous function notifies the scenario controller that the participant has finished its tasks. It sends a POST request to the designated controller URL, including the participant's ID in the JSON payload.</p> URL Construction <ul> <li>The URL is dynamically built using the controller address and scenario name   from the configuration settings.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>-</code> <code>idx (int</code> <p>The unique identifier for this participant, sent in the request data.</p> required <p>Returns:</p> Type Description <ul> <li>bool: True if the report was successful (status 200), False otherwise.</li> </ul> Error Handling <ul> <li>Logs an error if the response status is not 200, indicating that the controller   might be temporarily overloaded.</li> <li>Logs exceptions if the connection attempt to the controller fails.</li> </ul> Source code in <code>nebula/addons/reporter.py</code> <pre><code>async def report_scenario_finished(self):\n    \"\"\"\n    Reports the scenario completion status to the controller.\n\n    This asynchronous function notifies the scenario controller that the participant has finished\n    its tasks. It sends a POST request to the designated controller URL, including the participant's\n    ID in the JSON payload.\n\n    URL Construction:\n        - The URL is dynamically built using the controller address and scenario name\n          from the configuration settings.\n\n    Parameters:\n        - idx (int): The unique identifier for this participant, sent in the request data.\n\n    Returns:\n        - bool: True if the report was successful (status 200), False otherwise.\n\n    Error Handling:\n        - Logs an error if the response status is not 200, indicating that the controller\n          might be temporarily overloaded.\n        - Logs exceptions if the connection attempt to the controller fails.\n    \"\"\"\n    url = f\"http://{self.config.participant['scenario_args']['controller']}/platform/dashboard/{self.config.participant['scenario_args']['name']}/node/done\"\n    data = json.dumps({\"idx\": self.config.participant[\"device_args\"][\"idx\"]})\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"User-Agent\": f\"NEBULA Participant {self.config.participant['device_args']['idx']}\",\n    }\n    try:\n        async with aiohttp.ClientSession() as session, session.post(url, data=data, headers=headers) as response:\n            if response.status != 200:\n                logging.error(\n                    f\"Error received from controller: {response.status} (probably there is overhead in the controller, trying again in the next round)\"\n                )\n                text = await response.text()\n                logging.debug(text)\n            else:\n                logging.info(\n                    f\"Participant {self.config.participant['device_args']['idx']} reported scenario finished\"\n                )\n                return True\n    except aiohttp.ClientError:\n        logging.exception(f\"Error connecting to the controller at {url}\")\n    return False\n</code></pre>"},{"location":"api/addons/reporter/#nebula.addons.reporter.Reporter.run_reporter","title":"<code>run_reporter()</code>  <code>async</code>","text":"<p>Runs the continuous reporting loop.</p> <p>This asynchronous function performs periodic reporting tasks such as reporting resource usage, data queue contents, and, optionally, status updates to the controller. The loop runs indefinitely, updating the counter with each cycle to track the frequency of specific tasks.</p> Key Actions <ul> <li>Regularly reports the resource status.</li> <li>Reloads the configuration file every 50 cycles to reflect any updates.</li> </ul> Notes <ul> <li>The reporting frequency is determined by the 'report_frequency' setting in the config file.</li> </ul> Source code in <code>nebula/addons/reporter.py</code> <pre><code>async def run_reporter(self):\n    \"\"\"\n    Runs the continuous reporting loop.\n\n    This asynchronous function performs periodic reporting tasks such as reporting resource usage,\n    data queue contents, and, optionally, status updates to the controller. The loop runs indefinitely,\n    updating the counter with each cycle to track the frequency of specific tasks.\n\n    Key Actions:\n        - Regularly reports the resource status.\n        - Reloads the configuration file every 50 cycles to reflect any updates.\n\n    Notes:\n        - The reporting frequency is determined by the 'report_frequency' setting in the config file.\n    \"\"\"\n    while True:\n        if self.config.participant[\"reporter_args\"][\"report_status_data_queue\"]:\n            if self.config.participant[\"scenario_args\"][\"controller\"] != \"nebula-test\":\n                await self.__report_status_to_controller()\n            await self.__report_data_queue()\n        await self.__report_resources()\n        self.counter += 1\n        if self.counter % 50 == 0:\n            logging.info(\"Reloading config file...\")\n            self.cm.engine.config.reload_config_file()\n        await asyncio.sleep(self.frequency)\n</code></pre>"},{"location":"api/addons/reporter/#nebula.addons.reporter.Reporter.start","title":"<code>start()</code>  <code>async</code>","text":"<p>Starts the reporter module after a grace period.</p> <p>This asynchronous function initiates the reporting process following a designated grace period. It creates a background task to run the reporting loop, allowing data to be reported at defined intervals.</p> <p>Returns:</p> Type Description <p>asyncio.Task: The task for the reporter loop, which handles the data reporting asynchronously.</p> Notes <ul> <li>The grace period allows for a delay before the first reporting cycle.</li> <li>The reporter loop runs in the background, ensuring continuous data updates.</li> </ul> Source code in <code>nebula/addons/reporter.py</code> <pre><code>async def start(self):\n    \"\"\"\n    Starts the reporter module after a grace period.\n\n    This asynchronous function initiates the reporting process following a designated grace period.\n    It creates a background task to run the reporting loop, allowing data to be reported at defined intervals.\n\n    Returns:\n        asyncio.Task: The task for the reporter loop, which handles the data reporting asynchronously.\n\n    Notes:\n        - The grace period allows for a delay before the first reporting cycle.\n        - The reporter loop runs in the background, ensuring continuous data updates.\n    \"\"\"\n    await asyncio.sleep(self.grace_time)\n    task = asyncio.create_task(self.run_reporter())\n    return task\n</code></pre>"},{"location":"api/addons/topologymanager/","title":"Documentation for Topologymanager Module","text":""},{"location":"api/addons/topologymanager/#nebula.addons.topologymanager.TopologyManager","title":"<code>TopologyManager</code>","text":"Source code in <code>nebula/addons/topologymanager.py</code> <pre><code>class TopologyManager:\n    def __init__(\n        self,\n        scenario_name=None,\n        n_nodes=5,\n        b_symmetric=True,\n        undirected_neighbor_num=5,\n        topology=None,\n    ):\n        \"\"\"\n        Initializes a network topology for the scenario.\n\n        This constructor sets up a network topology with a given number of nodes, neighbors, and other parameters.\n        It includes options to specify whether the topology should be symmetric and the number of undirected neighbors for each node.\n        It also checks for constraints on the number of neighbors and the structure of the network.\n\n        Parameters:\n            - scenario_name (str, optional): Name of the scenario.\n            - n_nodes (int): Number of nodes in the network (default 5).\n            - b_symmetric (bool): Whether the topology is symmetric (default True).\n            - undirected_neighbor_num (int): Number of undirected neighbors for each node (default 5).\n            - topology (list, optional): Predefined topology, a list of nodes and connections (default None).\n\n        Raises:\n            - ValueError: If `undirected_neighbor_num` is less than 2.\n\n        Attributes:\n            - scenario_name (str): Name of the scenario.\n            - n_nodes (int): Number of nodes in the network.\n            - b_symmetric (bool): Whether the topology is symmetric.\n            - undirected_neighbor_num (int): Number of undirected neighbors.\n            - topology (list): Topology of the network.\n            - nodes (np.ndarray): Array of nodes initialized with zeroes.\n            - b_fully_connected (bool): Flag indicating if the topology is fully connected.\n        \"\"\"\n        self.scenario_name = scenario_name\n        if topology is None:\n            topology = []\n        self.n_nodes = n_nodes\n        self.b_symmetric = b_symmetric\n        self.undirected_neighbor_num = undirected_neighbor_num\n        self.topology = topology\n        # Initialize nodes with array of tuples (0,0,0) with size n_nodes\n        self.nodes = np.zeros((n_nodes, 3), dtype=np.int32)\n\n        self.b_fully_connected = False\n        if self.undirected_neighbor_num &lt; 2:\n            raise ValueError(\"undirected_neighbor_num must be greater than 2\")  # noqa: TRY003\n        # If the number of neighbors is larger than the number of nodes, then the topology is fully connected\n        if self.undirected_neighbor_num &gt;= self.n_nodes - 1 and self.b_symmetric:\n            self.b_fully_connected = True\n\n    def __getstate__(self):\n        \"\"\"\n        Serializes the object state for saving.\n\n        This method defines which attributes of the class should be serialized when the object is pickled (saved to a file).\n        It returns a dictionary containing the attributes that need to be preserved.\n\n        Returns:\n            dict: A dictionary containing the relevant attributes of the object for serialization.\n                - scenario_name (str): Name of the scenario.\n                - n_nodes (int): Number of nodes in the network.\n                - topology (list): Topology of the network.\n                - nodes (np.ndarray): Array of nodes in the network.\n        \"\"\"\n        # Return the attributes of the class that should be serialized\n        return {\n            \"scenario_name\": self.scenario_name,\n            \"n_nodes\": self.n_nodes,\n            \"topology\": self.topology,\n            \"nodes\": self.nodes,\n        }\n\n    def __setstate__(self, state):\n        \"\"\"\n        Restores the object state from the serialized data.\n\n        This method is called during deserialization (unpickling) to restore the object's state\n        by setting the attributes using the provided state dictionary.\n\n        Args:\n            state (dict): A dictionary containing the serialized data, including:\n                - scenario_name (str): Name of the scenario.\n                - n_nodes (int): Number of nodes in the network.\n                - topology (list): Topology of the network.\n                - nodes (np.ndarray): Array of nodes in the network.\n        \"\"\"\n        # Set the attributes of the class from the serialized state\n        self.scenario_name = state[\"scenario_name\"]\n        self.n_nodes = state[\"n_nodes\"]\n        self.topology = state[\"topology\"]\n        self.nodes = state[\"nodes\"]\n\n    def get_node_color(self, role):\n        \"\"\"\n        Returns the color associated with a given role.\n\n        The method maps roles to specific colors for visualization or representation purposes.\n\n        Args:\n            role (Role): The role for which the color is to be determined.\n\n        Returns:\n            str: The color associated with the given role. Defaults to \"red\" if the role is not recognized.\n        \"\"\"\n        role_colors = {\n            Role.AGGREGATOR: \"orange\",\n            Role.SERVER: \"green\",\n            Role.TRAINER: \"#6182bd\",\n            Role.PROXY: \"purple\",\n        }\n        return role_colors.get(role, \"red\")\n\n    def add_legend(self, roles):\n        \"\"\"\n        Adds a legend to the plot for different roles, associating each role with a color.\n\n        The method iterates through the provided roles and assigns the corresponding color to each one.\n        The colors are predefined in the legend_map, which associates each role with a specific color.\n\n        Args:\n            roles (iterable): A collection of roles for which the legend should be displayed.\n\n        Returns:\n            None: The function modifies the plot directly by adding the legend.\n        \"\"\"\n        legend_map = {\n            Role.AGGREGATOR: \"orange\",\n            Role.SERVER: \"green\",\n            Role.TRAINER: \"#6182bd\",\n            Role.PROXY: \"purple\",\n            Role.IDLE: \"red\",\n        }\n        for role, color in legend_map.items():\n            if role in roles:\n                plt.scatter([], [], c=color, label=role)\n        plt.legend()\n\n    def draw_graph(self, plot=False, path=None):\n        \"\"\"\n        Draws the network graph based on the topology and saves it as an image.\n\n        This method generates a visualization of the network's topology using NetworkX and Matplotlib.\n        It assigns colors to the nodes based on their role, draws the network's nodes and edges,\n        adds labels to the nodes, and includes a legend for clarity.\n        The resulting plot is saved as an image file.\n\n        Args:\n            plot (bool, optional): Whether to display the plot. Default is False.\n            path (str, optional): The file path where the image will be saved. If None, the image is saved\n                                  to a default location based on the scenario name.\n\n        Returns:\n            None: The method saves the plot as an image at the specified path.\n        \"\"\"\n        g = nx.from_numpy_array(self.topology)\n        pos = nx.spring_layout(g, k=0.15, iterations=20, seed=42)\n\n        fig = plt.figure(num=\"Network topology\", dpi=100, figsize=(6, 6), frameon=False)\n        ax = fig.add_axes([0, 0, 1, 1])\n        ax.set_xlim([-1.3, 1.3])\n        ax.set_ylim([-1.3, 1.3])\n        labels = {}\n        color_map = []\n        for k in range(self.n_nodes):\n            role = str(self.nodes[k][2])\n            color_map.append(self.get_node_color(role))\n            labels[k] = f\"P{k}\\n\" + str(self.nodes[k][0]) + \":\" + str(self.nodes[k][1])\n\n        nx.draw_networkx_nodes(g, pos, node_color=color_map, linewidths=2)\n        nx.draw_networkx_labels(g, pos, labels, font_size=10, font_weight=\"bold\")\n        nx.draw_networkx_edges(g, pos, width=2)\n\n        self.add_legend([str(node[2]) for node in self.nodes])\n        plt.savefig(f\"{path}\", dpi=100, bbox_inches=\"tight\", pad_inches=0)\n        plt.close()\n\n    def generate_topology(self):\n        \"\"\"\n        Generates the network topology based on the configured settings.\n\n        This method generates the network topology for the given scenario. It checks whether the topology\n        should be fully connected, symmetric, or asymmetric and then generates the network accordingly.\n\n        - If the topology is fully connected, all nodes will be directly connected to each other.\n        - If the topology is symmetric, neighbors will be chosen symmetrically between nodes.\n        - If the topology is asymmetric, neighbors will be picked randomly without symmetry.\n\n        Returns:\n            None: The method modifies the internal topology of the network.\n        \"\"\"\n        if self.b_fully_connected:\n            self.__fully_connected()\n            return\n\n        if self.topology is not None and len(self.topology) &gt; 0:\n            # Topology was already provided\n            return\n\n        if self.b_symmetric:\n            self.__randomly_pick_neighbors_symmetric()\n        else:\n            self.__randomly_pick_neighbors_asymmetric()\n\n    def generate_server_topology(self):\n        \"\"\"\n        Generates a server topology where the first node (usually the server) is connected to all other nodes.\n\n        This method initializes a topology matrix where the first node (typically the server) is connected to\n        every other node in the network. The first row and the first column of the matrix are set to 1, representing\n        connections to and from the server. The diagonal is set to 0 to indicate that no node is connected to itself.\n\n        Returns:\n            None: The method modifies the internal `self.topology` matrix.\n        \"\"\"\n        self.topology = np.zeros((self.n_nodes, self.n_nodes), dtype=np.float32)\n        self.topology[0, :] = 1\n        self.topology[:, 0] = 1\n        np.fill_diagonal(self.topology, 0)\n\n    def generate_ring_topology(self, increase_convergence=False):\n        \"\"\"\n        Generates a ring topology for the network.\n\n        In a ring topology, each node is connected to two other nodes in a circular fashion, forming a closed loop.\n        This method uses a private method to generate the topology, with an optional parameter to control whether\n        the convergence speed of the network should be increased.\n\n        Args:\n            increase_convergence (bool): Optional flag to increase the convergence speed in the topology.\n                                          Defaults to False.\n\n        Returns:\n            None: The method modifies the internal `self.topology` matrix to reflect the generated ring topology.\n        \"\"\"\n        self.__ring_topology(increase_convergence=increase_convergence)\n\n    def generate_random_topology(self, probability):\n        \"\"\"\n        Generates a random topology using Erdos-Renyi model with given probability.\n\n        Args:\n            probability (float): Probability of edge creation between any two nodes (0-1)\n\n        Returns:\n            None: Updates self.topology with the generated random topology\n        \"\"\"\n        random_graph = nx.erdos_renyi_graph(self.n_nodes, probability)\n        self.topology = nx.to_numpy_array(random_graph, dtype=np.float32)\n        np.fill_diagonal(self.topology, 0)  # No self-loops\n\n    @staticmethod\n    def get_coordinates(random_geo=True):\n        \"\"\"\n        Generates random geographical coordinates within predefined bounds for either Spain or Switzerland.\n\n        The method returns a random geographical coordinate (latitude, longitude). The bounds for random coordinates are\n        defined for two regions: Spain and Switzerland. The region is chosen randomly, and then the latitude and longitude\n        are selected within the corresponding bounds.\n\n        Parameters:\n            random_geo (bool): If set to True, the method generates random coordinates within the predefined bounds\n                                for Spain or Switzerland. If set to False, this method could be modified to return fixed\n                                coordinates.\n\n        Returns:\n            tuple: A tuple containing the latitude and longitude of the generated point.\n        \"\"\"\n        if random_geo:\n            #  Espa\u00f1a min_lat, max_lat, min_lon, max_lon                  Suiza min_lat, max_lat, min_lon, max_lon\n            bounds = (36.0, 43.0, -9.0, 3.3) if random.randint(0, 1) == 0 else (45.8, 47.8, 5.9, 10.5)  # noqa: S311\n\n            min_latitude, max_latitude, min_longitude, max_longitude = bounds\n            latitude = random.uniform(min_latitude, max_latitude)  # noqa: S311\n            longitude = random.uniform(min_longitude, max_longitude)  # noqa: S311\n\n            return latitude, longitude\n\n    def add_nodes(self, nodes):\n        \"\"\"\n        Sets the nodes of the topology.\n\n        This method updates the `nodes` attribute with the given list or array of nodes.\n\n        Parameters:\n            nodes (array-like): The new set of nodes to be assigned to the topology. It should be in a format compatible\n                                 with the existing `nodes` structure, typically an array or list.\n\n        Returns:\n            None\n        \"\"\"\n        self.nodes = nodes\n\n    def update_nodes(self, config_participants):\n        \"\"\"\n        Updates the nodes of the topology based on the provided configuration.\n\n        This method assigns a new set of nodes to the `nodes` attribute, typically based on the configuration of the participants.\n\n        Parameters:\n            config_participants (array-like): A new set of nodes, usually derived from the participants' configuration, to be assigned to the topology.\n\n        Returns:\n            None\n        \"\"\"\n        self.nodes = config_participants\n\n    def get_neighbors_string(self, node_idx):\n        \"\"\"\n        Retrieves the neighbors of a given node as a string representation.\n\n        This method checks the `topology` attribute to find the neighbors of the node at the specified index (`node_idx`). It then returns a string that lists the coordinates of each neighbor.\n\n        Parameters:\n            node_idx (int): The index of the node for which neighbors are to be retrieved.\n\n        Returns:\n            str: A space-separated string of neighbors' coordinates in the format \"latitude:longitude\".\n        \"\"\"\n        # logging.info(f\"Topology: {self.topology}\")\n        # logging.info(f\"Nodes: {self.nodes}\")\n        neighbors_data = []\n        for i, node in enumerate(self.topology[node_idx]):\n            if node == 1:\n                neighbors_data.append(self.nodes[i])\n\n        neighbors_data_strings = [f\"{i[0]}:{i[1]}\" for i in neighbors_data]\n        neighbors_data_string = \" \".join(neighbors_data_strings)\n        logging.info(f\"Neighbors of node participant_{node_idx}: {neighbors_data_string}\")\n        return neighbors_data_string\n\n    def __ring_topology(self, increase_convergence=False):\n        \"\"\"\n        Generates a ring topology for the nodes.\n\n        This method creates a ring topology for the network using the Watts-Strogatz model. Each node is connected to two neighbors, forming a ring. Optionally, additional random connections are added to increase convergence, making the network more connected.\n\n        Parameters:\n            increase_convergence (bool): If set to True, random connections will be added between nodes to increase the network's connectivity.\n\n        Returns:\n            None: The `topology` attribute of the class is updated with the generated ring topology.\n        \"\"\"\n        topology_ring = np.array(\n            nx.to_numpy_matrix(nx.watts_strogatz_graph(self.n_nodes, 2, 0)),\n            dtype=np.float32,\n        )\n\n        if increase_convergence:\n            # Create random links between nodes in topology_ring\n            for i in range(self.n_nodes):\n                for j in range(self.n_nodes):\n                    if topology_ring[i][j] == 0 and random.random() &lt; 0.1:  # noqa: S311\n                        topology_ring[i][j] = 1\n                        topology_ring[j][i] = 1\n\n        np.fill_diagonal(topology_ring, 0)\n        self.topology = topology_ring\n\n    def __randomly_pick_neighbors_symmetric(self):\n        \"\"\"\n        Generates a symmetric random topology by combining a ring topology with additional random links.\n\n        This method first creates a ring topology using the Watts-Strogatz model, where each node is connected to two neighbors. Then, it randomly adds links to each node (up to the specified number of neighbors) to form a symmetric topology. The result is a topology where each node has a fixed number of undirected neighbors, and the connections are symmetric between nodes.\n\n        Parameters:\n            None\n\n        Returns:\n            None: The `topology` attribute of the class is updated with the generated symmetric topology.\n        \"\"\"\n        # First generate a ring topology\n        topology_ring = np.array(\n            nx.to_numpy_matrix(nx.watts_strogatz_graph(self.n_nodes, 2, 0)),\n            dtype=np.float32,\n        )\n\n        np.fill_diagonal(topology_ring, 0)\n\n        # After, randomly add some links for each node (symmetric)\n        # If undirected_neighbor_num is X, then each node has X links to other nodes\n        k = int(self.undirected_neighbor_num)\n        topology_random_link = np.array(\n            nx.to_numpy_matrix(nx.watts_strogatz_graph(self.n_nodes, k, 0)),\n            dtype=np.float32,\n        )\n\n        # generate symmetric topology\n        topology_symmetric = topology_ring.copy()\n        for i in range(self.n_nodes):\n            for j in range(self.n_nodes):\n                if topology_symmetric[i][j] == 0 and topology_random_link[i][j] == 1:\n                    topology_symmetric[i][j] = topology_random_link[i][j]\n\n        np.fill_diagonal(topology_symmetric, 0)\n\n        self.topology = topology_symmetric\n\n    def __randomly_pick_neighbors_asymmetric(self):\n        \"\"\"\n        Generates an asymmetric random topology by combining a ring topology with additional random links and random deletions.\n\n        This method first creates a ring topology using the Watts-Strogatz model, where each node is connected to two neighbors. Then, it randomly adds links to each node to create a topology with a specified number of undirected neighbors. After that, it randomly deletes some of the links to introduce asymmetry. The result is a topology where nodes have a varying number of directed and undirected links, and the structure is asymmetric.\n\n        Parameters:\n            None\n\n        Returns:\n            None: The `topology` attribute of the class is updated with the generated asymmetric topology.\n        \"\"\"\n        # randomly add some links for each node (symmetric)\n        k = self.undirected_neighbor_num\n        topology_random_link = np.array(\n            nx.to_numpy_matrix(nx.watts_strogatz_graph(self.n_nodes, k, 0)),\n            dtype=np.float32,\n        )\n\n        np.fill_diagonal(topology_random_link, 0)\n\n        # first generate a ring topology\n        topology_ring = np.array(\n            nx.to_numpy_matrix(nx.watts_strogatz_graph(self.n_nodes, 2, 0)),\n            dtype=np.float32,\n        )\n\n        np.fill_diagonal(topology_ring, 0)\n\n        for i in range(self.n_nodes):\n            for j in range(self.n_nodes):\n                if topology_ring[i][j] == 0 and topology_random_link[i][j] == 1:\n                    topology_ring[i][j] = topology_random_link[i][j]\n\n        np.fill_diagonal(topology_ring, 0)\n\n        # randomly delete some links\n        out_link_set = set()\n        for i in range(self.n_nodes):\n            len_row_zero = 0\n            for j in range(self.n_nodes):\n                if topology_ring[i][j] == 0:\n                    len_row_zero += 1\n            random_selection = np.random.randint(2, size=len_row_zero)\n            index_of_zero = 0\n            for j in range(self.n_nodes):\n                out_link = j * self.n_nodes + i\n                if topology_ring[i][j] == 0:\n                    if random_selection[index_of_zero] == 1 and out_link not in out_link_set:\n                        topology_ring[i][j] = 1\n                        out_link_set.add(i * self.n_nodes + j)\n                    index_of_zero += 1\n\n        np.fill_diagonal(topology_ring, 0)\n\n        self.topology = topology_ring\n\n    def __fully_connected(self):\n        \"\"\"\n        Generates a fully connected topology where each node is connected to every other node.\n\n        This method creates a fully connected network by generating a Watts-Strogatz graph with the number of nodes set to `n_nodes` and the number of neighbors set to `n_nodes - 1`. The resulting graph is then converted into a numpy matrix and all missing links (i.e., non-ones in the adjacency matrix) are set to 1 to ensure complete connectivity. The diagonal elements are filled with zeros to avoid self-loops.\n\n        Parameters:\n            None\n\n        Returns:\n            None: The `topology` attribute of the class is updated with the generated fully connected topology.\n        \"\"\"\n        topology_fully_connected = np.array(\n            nx.to_numpy_matrix(nx.watts_strogatz_graph(self.n_nodes, self.n_nodes - 1, 0)),\n            dtype=np.float32,\n        )\n\n        np.fill_diagonal(topology_fully_connected, 0)\n\n        for i in range(self.n_nodes):\n            for j in range(self.n_nodes):\n                if topology_fully_connected[i][j] != 1:\n                    topology_fully_connected[i][j] = 1\n\n        np.fill_diagonal(topology_fully_connected, 0)\n\n        self.topology = topology_fully_connected\n</code></pre>"},{"location":"api/addons/topologymanager/#nebula.addons.topologymanager.TopologyManager.__fully_connected","title":"<code>__fully_connected()</code>","text":"<p>Generates a fully connected topology where each node is connected to every other node.</p> <p>This method creates a fully connected network by generating a Watts-Strogatz graph with the number of nodes set to <code>n_nodes</code> and the number of neighbors set to <code>n_nodes - 1</code>. The resulting graph is then converted into a numpy matrix and all missing links (i.e., non-ones in the adjacency matrix) are set to 1 to ensure complete connectivity. The diagonal elements are filled with zeros to avoid self-loops.</p> <p>Returns:</p> Name Type Description <code>None</code> <p>The <code>topology</code> attribute of the class is updated with the generated fully connected topology.</p> Source code in <code>nebula/addons/topologymanager.py</code> <pre><code>def __fully_connected(self):\n    \"\"\"\n    Generates a fully connected topology where each node is connected to every other node.\n\n    This method creates a fully connected network by generating a Watts-Strogatz graph with the number of nodes set to `n_nodes` and the number of neighbors set to `n_nodes - 1`. The resulting graph is then converted into a numpy matrix and all missing links (i.e., non-ones in the adjacency matrix) are set to 1 to ensure complete connectivity. The diagonal elements are filled with zeros to avoid self-loops.\n\n    Parameters:\n        None\n\n    Returns:\n        None: The `topology` attribute of the class is updated with the generated fully connected topology.\n    \"\"\"\n    topology_fully_connected = np.array(\n        nx.to_numpy_matrix(nx.watts_strogatz_graph(self.n_nodes, self.n_nodes - 1, 0)),\n        dtype=np.float32,\n    )\n\n    np.fill_diagonal(topology_fully_connected, 0)\n\n    for i in range(self.n_nodes):\n        for j in range(self.n_nodes):\n            if topology_fully_connected[i][j] != 1:\n                topology_fully_connected[i][j] = 1\n\n    np.fill_diagonal(topology_fully_connected, 0)\n\n    self.topology = topology_fully_connected\n</code></pre>"},{"location":"api/addons/topologymanager/#nebula.addons.topologymanager.TopologyManager.__getstate__","title":"<code>__getstate__()</code>","text":"<p>Serializes the object state for saving.</p> <p>This method defines which attributes of the class should be serialized when the object is pickled (saved to a file). It returns a dictionary containing the attributes that need to be preserved.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the relevant attributes of the object for serialization. - scenario_name (str): Name of the scenario. - n_nodes (int): Number of nodes in the network. - topology (list): Topology of the network. - nodes (np.ndarray): Array of nodes in the network.</p> Source code in <code>nebula/addons/topologymanager.py</code> <pre><code>def __getstate__(self):\n    \"\"\"\n    Serializes the object state for saving.\n\n    This method defines which attributes of the class should be serialized when the object is pickled (saved to a file).\n    It returns a dictionary containing the attributes that need to be preserved.\n\n    Returns:\n        dict: A dictionary containing the relevant attributes of the object for serialization.\n            - scenario_name (str): Name of the scenario.\n            - n_nodes (int): Number of nodes in the network.\n            - topology (list): Topology of the network.\n            - nodes (np.ndarray): Array of nodes in the network.\n    \"\"\"\n    # Return the attributes of the class that should be serialized\n    return {\n        \"scenario_name\": self.scenario_name,\n        \"n_nodes\": self.n_nodes,\n        \"topology\": self.topology,\n        \"nodes\": self.nodes,\n    }\n</code></pre>"},{"location":"api/addons/topologymanager/#nebula.addons.topologymanager.TopologyManager.__init__","title":"<code>__init__(scenario_name=None, n_nodes=5, b_symmetric=True, undirected_neighbor_num=5, topology=None)</code>","text":"<p>Initializes a network topology for the scenario.</p> <p>This constructor sets up a network topology with a given number of nodes, neighbors, and other parameters. It includes options to specify whether the topology should be symmetric and the number of undirected neighbors for each node. It also checks for constraints on the number of neighbors and the structure of the network.</p> <p>Parameters:</p> Name Type Description Default <code>-</code> <code>scenario_name (str</code> <p>Name of the scenario.</p> required <code>-</code> <code>n_nodes (int</code> <p>Number of nodes in the network (default 5).</p> required <code>-</code> <code>b_symmetric (bool</code> <p>Whether the topology is symmetric (default True).</p> required <code>-</code> <code>undirected_neighbor_num (int</code> <p>Number of undirected neighbors for each node (default 5).</p> required <code>-</code> <code>topology (list</code> <p>Predefined topology, a list of nodes and connections (default None).</p> required <p>Raises:</p> Type Description <code>-ValueError</code> <p>If <code>undirected_neighbor_num</code> is less than 2.</p> <p>Attributes:</p> Name Type Description <code>-</code> <code>scenario_name (str</code> <p>Name of the scenario.</p> <code>-</code> <code>n_nodes (int</code> <p>Number of nodes in the network.</p> <code>-</code> <code>b_symmetric (bool</code> <p>Whether the topology is symmetric.</p> <code>-</code> <code>undirected_neighbor_num (int</code> <p>Number of undirected neighbors.</p> <code>-</code> <code>topology (list</code> <p>Topology of the network.</p> <code>-</code> <code>nodes (np.ndarray</code> <p>Array of nodes initialized with zeroes.</p> <code>-</code> <code>b_fully_connected (bool</code> <p>Flag indicating if the topology is fully connected.</p> Source code in <code>nebula/addons/topologymanager.py</code> <pre><code>def __init__(\n    self,\n    scenario_name=None,\n    n_nodes=5,\n    b_symmetric=True,\n    undirected_neighbor_num=5,\n    topology=None,\n):\n    \"\"\"\n    Initializes a network topology for the scenario.\n\n    This constructor sets up a network topology with a given number of nodes, neighbors, and other parameters.\n    It includes options to specify whether the topology should be symmetric and the number of undirected neighbors for each node.\n    It also checks for constraints on the number of neighbors and the structure of the network.\n\n    Parameters:\n        - scenario_name (str, optional): Name of the scenario.\n        - n_nodes (int): Number of nodes in the network (default 5).\n        - b_symmetric (bool): Whether the topology is symmetric (default True).\n        - undirected_neighbor_num (int): Number of undirected neighbors for each node (default 5).\n        - topology (list, optional): Predefined topology, a list of nodes and connections (default None).\n\n    Raises:\n        - ValueError: If `undirected_neighbor_num` is less than 2.\n\n    Attributes:\n        - scenario_name (str): Name of the scenario.\n        - n_nodes (int): Number of nodes in the network.\n        - b_symmetric (bool): Whether the topology is symmetric.\n        - undirected_neighbor_num (int): Number of undirected neighbors.\n        - topology (list): Topology of the network.\n        - nodes (np.ndarray): Array of nodes initialized with zeroes.\n        - b_fully_connected (bool): Flag indicating if the topology is fully connected.\n    \"\"\"\n    self.scenario_name = scenario_name\n    if topology is None:\n        topology = []\n    self.n_nodes = n_nodes\n    self.b_symmetric = b_symmetric\n    self.undirected_neighbor_num = undirected_neighbor_num\n    self.topology = topology\n    # Initialize nodes with array of tuples (0,0,0) with size n_nodes\n    self.nodes = np.zeros((n_nodes, 3), dtype=np.int32)\n\n    self.b_fully_connected = False\n    if self.undirected_neighbor_num &lt; 2:\n        raise ValueError(\"undirected_neighbor_num must be greater than 2\")  # noqa: TRY003\n    # If the number of neighbors is larger than the number of nodes, then the topology is fully connected\n    if self.undirected_neighbor_num &gt;= self.n_nodes - 1 and self.b_symmetric:\n        self.b_fully_connected = True\n</code></pre>"},{"location":"api/addons/topologymanager/#nebula.addons.topologymanager.TopologyManager.__randomly_pick_neighbors_asymmetric","title":"<code>__randomly_pick_neighbors_asymmetric()</code>","text":"<p>Generates an asymmetric random topology by combining a ring topology with additional random links and random deletions.</p> <p>This method first creates a ring topology using the Watts-Strogatz model, where each node is connected to two neighbors. Then, it randomly adds links to each node to create a topology with a specified number of undirected neighbors. After that, it randomly deletes some of the links to introduce asymmetry. The result is a topology where nodes have a varying number of directed and undirected links, and the structure is asymmetric.</p> <p>Returns:</p> Name Type Description <code>None</code> <p>The <code>topology</code> attribute of the class is updated with the generated asymmetric topology.</p> Source code in <code>nebula/addons/topologymanager.py</code> <pre><code>def __randomly_pick_neighbors_asymmetric(self):\n    \"\"\"\n    Generates an asymmetric random topology by combining a ring topology with additional random links and random deletions.\n\n    This method first creates a ring topology using the Watts-Strogatz model, where each node is connected to two neighbors. Then, it randomly adds links to each node to create a topology with a specified number of undirected neighbors. After that, it randomly deletes some of the links to introduce asymmetry. The result is a topology where nodes have a varying number of directed and undirected links, and the structure is asymmetric.\n\n    Parameters:\n        None\n\n    Returns:\n        None: The `topology` attribute of the class is updated with the generated asymmetric topology.\n    \"\"\"\n    # randomly add some links for each node (symmetric)\n    k = self.undirected_neighbor_num\n    topology_random_link = np.array(\n        nx.to_numpy_matrix(nx.watts_strogatz_graph(self.n_nodes, k, 0)),\n        dtype=np.float32,\n    )\n\n    np.fill_diagonal(topology_random_link, 0)\n\n    # first generate a ring topology\n    topology_ring = np.array(\n        nx.to_numpy_matrix(nx.watts_strogatz_graph(self.n_nodes, 2, 0)),\n        dtype=np.float32,\n    )\n\n    np.fill_diagonal(topology_ring, 0)\n\n    for i in range(self.n_nodes):\n        for j in range(self.n_nodes):\n            if topology_ring[i][j] == 0 and topology_random_link[i][j] == 1:\n                topology_ring[i][j] = topology_random_link[i][j]\n\n    np.fill_diagonal(topology_ring, 0)\n\n    # randomly delete some links\n    out_link_set = set()\n    for i in range(self.n_nodes):\n        len_row_zero = 0\n        for j in range(self.n_nodes):\n            if topology_ring[i][j] == 0:\n                len_row_zero += 1\n        random_selection = np.random.randint(2, size=len_row_zero)\n        index_of_zero = 0\n        for j in range(self.n_nodes):\n            out_link = j * self.n_nodes + i\n            if topology_ring[i][j] == 0:\n                if random_selection[index_of_zero] == 1 and out_link not in out_link_set:\n                    topology_ring[i][j] = 1\n                    out_link_set.add(i * self.n_nodes + j)\n                index_of_zero += 1\n\n    np.fill_diagonal(topology_ring, 0)\n\n    self.topology = topology_ring\n</code></pre>"},{"location":"api/addons/topologymanager/#nebula.addons.topologymanager.TopologyManager.__randomly_pick_neighbors_symmetric","title":"<code>__randomly_pick_neighbors_symmetric()</code>","text":"<p>Generates a symmetric random topology by combining a ring topology with additional random links.</p> <p>This method first creates a ring topology using the Watts-Strogatz model, where each node is connected to two neighbors. Then, it randomly adds links to each node (up to the specified number of neighbors) to form a symmetric topology. The result is a topology where each node has a fixed number of undirected neighbors, and the connections are symmetric between nodes.</p> <p>Returns:</p> Name Type Description <code>None</code> <p>The <code>topology</code> attribute of the class is updated with the generated symmetric topology.</p> Source code in <code>nebula/addons/topologymanager.py</code> <pre><code>def __randomly_pick_neighbors_symmetric(self):\n    \"\"\"\n    Generates a symmetric random topology by combining a ring topology with additional random links.\n\n    This method first creates a ring topology using the Watts-Strogatz model, where each node is connected to two neighbors. Then, it randomly adds links to each node (up to the specified number of neighbors) to form a symmetric topology. The result is a topology where each node has a fixed number of undirected neighbors, and the connections are symmetric between nodes.\n\n    Parameters:\n        None\n\n    Returns:\n        None: The `topology` attribute of the class is updated with the generated symmetric topology.\n    \"\"\"\n    # First generate a ring topology\n    topology_ring = np.array(\n        nx.to_numpy_matrix(nx.watts_strogatz_graph(self.n_nodes, 2, 0)),\n        dtype=np.float32,\n    )\n\n    np.fill_diagonal(topology_ring, 0)\n\n    # After, randomly add some links for each node (symmetric)\n    # If undirected_neighbor_num is X, then each node has X links to other nodes\n    k = int(self.undirected_neighbor_num)\n    topology_random_link = np.array(\n        nx.to_numpy_matrix(nx.watts_strogatz_graph(self.n_nodes, k, 0)),\n        dtype=np.float32,\n    )\n\n    # generate symmetric topology\n    topology_symmetric = topology_ring.copy()\n    for i in range(self.n_nodes):\n        for j in range(self.n_nodes):\n            if topology_symmetric[i][j] == 0 and topology_random_link[i][j] == 1:\n                topology_symmetric[i][j] = topology_random_link[i][j]\n\n    np.fill_diagonal(topology_symmetric, 0)\n\n    self.topology = topology_symmetric\n</code></pre>"},{"location":"api/addons/topologymanager/#nebula.addons.topologymanager.TopologyManager.__ring_topology","title":"<code>__ring_topology(increase_convergence=False)</code>","text":"<p>Generates a ring topology for the nodes.</p> <p>This method creates a ring topology for the network using the Watts-Strogatz model. Each node is connected to two neighbors, forming a ring. Optionally, additional random connections are added to increase convergence, making the network more connected.</p> <p>Parameters:</p> Name Type Description Default <code>increase_convergence</code> <code>bool</code> <p>If set to True, random connections will be added between nodes to increase the network's connectivity.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>None</code> <p>The <code>topology</code> attribute of the class is updated with the generated ring topology.</p> Source code in <code>nebula/addons/topologymanager.py</code> <pre><code>def __ring_topology(self, increase_convergence=False):\n    \"\"\"\n    Generates a ring topology for the nodes.\n\n    This method creates a ring topology for the network using the Watts-Strogatz model. Each node is connected to two neighbors, forming a ring. Optionally, additional random connections are added to increase convergence, making the network more connected.\n\n    Parameters:\n        increase_convergence (bool): If set to True, random connections will be added between nodes to increase the network's connectivity.\n\n    Returns:\n        None: The `topology` attribute of the class is updated with the generated ring topology.\n    \"\"\"\n    topology_ring = np.array(\n        nx.to_numpy_matrix(nx.watts_strogatz_graph(self.n_nodes, 2, 0)),\n        dtype=np.float32,\n    )\n\n    if increase_convergence:\n        # Create random links between nodes in topology_ring\n        for i in range(self.n_nodes):\n            for j in range(self.n_nodes):\n                if topology_ring[i][j] == 0 and random.random() &lt; 0.1:  # noqa: S311\n                    topology_ring[i][j] = 1\n                    topology_ring[j][i] = 1\n\n    np.fill_diagonal(topology_ring, 0)\n    self.topology = topology_ring\n</code></pre>"},{"location":"api/addons/topologymanager/#nebula.addons.topologymanager.TopologyManager.__setstate__","title":"<code>__setstate__(state)</code>","text":"<p>Restores the object state from the serialized data.</p> <p>This method is called during deserialization (unpickling) to restore the object's state by setting the attributes using the provided state dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>A dictionary containing the serialized data, including: - scenario_name (str): Name of the scenario. - n_nodes (int): Number of nodes in the network. - topology (list): Topology of the network. - nodes (np.ndarray): Array of nodes in the network.</p> required Source code in <code>nebula/addons/topologymanager.py</code> <pre><code>def __setstate__(self, state):\n    \"\"\"\n    Restores the object state from the serialized data.\n\n    This method is called during deserialization (unpickling) to restore the object's state\n    by setting the attributes using the provided state dictionary.\n\n    Args:\n        state (dict): A dictionary containing the serialized data, including:\n            - scenario_name (str): Name of the scenario.\n            - n_nodes (int): Number of nodes in the network.\n            - topology (list): Topology of the network.\n            - nodes (np.ndarray): Array of nodes in the network.\n    \"\"\"\n    # Set the attributes of the class from the serialized state\n    self.scenario_name = state[\"scenario_name\"]\n    self.n_nodes = state[\"n_nodes\"]\n    self.topology = state[\"topology\"]\n    self.nodes = state[\"nodes\"]\n</code></pre>"},{"location":"api/addons/topologymanager/#nebula.addons.topologymanager.TopologyManager.add_legend","title":"<code>add_legend(roles)</code>","text":"<p>Adds a legend to the plot for different roles, associating each role with a color.</p> <p>The method iterates through the provided roles and assigns the corresponding color to each one. The colors are predefined in the legend_map, which associates each role with a specific color.</p> <p>Parameters:</p> Name Type Description Default <code>roles</code> <code>iterable</code> <p>A collection of roles for which the legend should be displayed.</p> required <p>Returns:</p> Name Type Description <code>None</code> <p>The function modifies the plot directly by adding the legend.</p> Source code in <code>nebula/addons/topologymanager.py</code> <pre><code>def add_legend(self, roles):\n    \"\"\"\n    Adds a legend to the plot for different roles, associating each role with a color.\n\n    The method iterates through the provided roles and assigns the corresponding color to each one.\n    The colors are predefined in the legend_map, which associates each role with a specific color.\n\n    Args:\n        roles (iterable): A collection of roles for which the legend should be displayed.\n\n    Returns:\n        None: The function modifies the plot directly by adding the legend.\n    \"\"\"\n    legend_map = {\n        Role.AGGREGATOR: \"orange\",\n        Role.SERVER: \"green\",\n        Role.TRAINER: \"#6182bd\",\n        Role.PROXY: \"purple\",\n        Role.IDLE: \"red\",\n    }\n    for role, color in legend_map.items():\n        if role in roles:\n            plt.scatter([], [], c=color, label=role)\n    plt.legend()\n</code></pre>"},{"location":"api/addons/topologymanager/#nebula.addons.topologymanager.TopologyManager.add_nodes","title":"<code>add_nodes(nodes)</code>","text":"<p>Sets the nodes of the topology.</p> <p>This method updates the <code>nodes</code> attribute with the given list or array of nodes.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>array - like</code> <p>The new set of nodes to be assigned to the topology. It should be in a format compatible                  with the existing <code>nodes</code> structure, typically an array or list.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>nebula/addons/topologymanager.py</code> <pre><code>def add_nodes(self, nodes):\n    \"\"\"\n    Sets the nodes of the topology.\n\n    This method updates the `nodes` attribute with the given list or array of nodes.\n\n    Parameters:\n        nodes (array-like): The new set of nodes to be assigned to the topology. It should be in a format compatible\n                             with the existing `nodes` structure, typically an array or list.\n\n    Returns:\n        None\n    \"\"\"\n    self.nodes = nodes\n</code></pre>"},{"location":"api/addons/topologymanager/#nebula.addons.topologymanager.TopologyManager.draw_graph","title":"<code>draw_graph(plot=False, path=None)</code>","text":"<p>Draws the network graph based on the topology and saves it as an image.</p> <p>This method generates a visualization of the network's topology using NetworkX and Matplotlib. It assigns colors to the nodes based on their role, draws the network's nodes and edges, adds labels to the nodes, and includes a legend for clarity. The resulting plot is saved as an image file.</p> <p>Parameters:</p> Name Type Description Default <code>plot</code> <code>bool</code> <p>Whether to display the plot. Default is False.</p> <code>False</code> <code>path</code> <code>str</code> <p>The file path where the image will be saved. If None, the image is saved                   to a default location based on the scenario name.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>None</code> <p>The method saves the plot as an image at the specified path.</p> Source code in <code>nebula/addons/topologymanager.py</code> <pre><code>def draw_graph(self, plot=False, path=None):\n    \"\"\"\n    Draws the network graph based on the topology and saves it as an image.\n\n    This method generates a visualization of the network's topology using NetworkX and Matplotlib.\n    It assigns colors to the nodes based on their role, draws the network's nodes and edges,\n    adds labels to the nodes, and includes a legend for clarity.\n    The resulting plot is saved as an image file.\n\n    Args:\n        plot (bool, optional): Whether to display the plot. Default is False.\n        path (str, optional): The file path where the image will be saved. If None, the image is saved\n                              to a default location based on the scenario name.\n\n    Returns:\n        None: The method saves the plot as an image at the specified path.\n    \"\"\"\n    g = nx.from_numpy_array(self.topology)\n    pos = nx.spring_layout(g, k=0.15, iterations=20, seed=42)\n\n    fig = plt.figure(num=\"Network topology\", dpi=100, figsize=(6, 6), frameon=False)\n    ax = fig.add_axes([0, 0, 1, 1])\n    ax.set_xlim([-1.3, 1.3])\n    ax.set_ylim([-1.3, 1.3])\n    labels = {}\n    color_map = []\n    for k in range(self.n_nodes):\n        role = str(self.nodes[k][2])\n        color_map.append(self.get_node_color(role))\n        labels[k] = f\"P{k}\\n\" + str(self.nodes[k][0]) + \":\" + str(self.nodes[k][1])\n\n    nx.draw_networkx_nodes(g, pos, node_color=color_map, linewidths=2)\n    nx.draw_networkx_labels(g, pos, labels, font_size=10, font_weight=\"bold\")\n    nx.draw_networkx_edges(g, pos, width=2)\n\n    self.add_legend([str(node[2]) for node in self.nodes])\n    plt.savefig(f\"{path}\", dpi=100, bbox_inches=\"tight\", pad_inches=0)\n    plt.close()\n</code></pre>"},{"location":"api/addons/topologymanager/#nebula.addons.topologymanager.TopologyManager.generate_random_topology","title":"<code>generate_random_topology(probability)</code>","text":"<p>Generates a random topology using Erdos-Renyi model with given probability.</p> <p>Parameters:</p> Name Type Description Default <code>probability</code> <code>float</code> <p>Probability of edge creation between any two nodes (0-1)</p> required <p>Returns:</p> Name Type Description <code>None</code> <p>Updates self.topology with the generated random topology</p> Source code in <code>nebula/addons/topologymanager.py</code> <pre><code>def generate_random_topology(self, probability):\n    \"\"\"\n    Generates a random topology using Erdos-Renyi model with given probability.\n\n    Args:\n        probability (float): Probability of edge creation between any two nodes (0-1)\n\n    Returns:\n        None: Updates self.topology with the generated random topology\n    \"\"\"\n    random_graph = nx.erdos_renyi_graph(self.n_nodes, probability)\n    self.topology = nx.to_numpy_array(random_graph, dtype=np.float32)\n    np.fill_diagonal(self.topology, 0)  # No self-loops\n</code></pre>"},{"location":"api/addons/topologymanager/#nebula.addons.topologymanager.TopologyManager.generate_ring_topology","title":"<code>generate_ring_topology(increase_convergence=False)</code>","text":"<p>Generates a ring topology for the network.</p> <p>In a ring topology, each node is connected to two other nodes in a circular fashion, forming a closed loop. This method uses a private method to generate the topology, with an optional parameter to control whether the convergence speed of the network should be increased.</p> <p>Parameters:</p> Name Type Description Default <code>increase_convergence</code> <code>bool</code> <p>Optional flag to increase the convergence speed in the topology.                           Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>None</code> <p>The method modifies the internal <code>self.topology</code> matrix to reflect the generated ring topology.</p> Source code in <code>nebula/addons/topologymanager.py</code> <pre><code>def generate_ring_topology(self, increase_convergence=False):\n    \"\"\"\n    Generates a ring topology for the network.\n\n    In a ring topology, each node is connected to two other nodes in a circular fashion, forming a closed loop.\n    This method uses a private method to generate the topology, with an optional parameter to control whether\n    the convergence speed of the network should be increased.\n\n    Args:\n        increase_convergence (bool): Optional flag to increase the convergence speed in the topology.\n                                      Defaults to False.\n\n    Returns:\n        None: The method modifies the internal `self.topology` matrix to reflect the generated ring topology.\n    \"\"\"\n    self.__ring_topology(increase_convergence=increase_convergence)\n</code></pre>"},{"location":"api/addons/topologymanager/#nebula.addons.topologymanager.TopologyManager.generate_server_topology","title":"<code>generate_server_topology()</code>","text":"<p>Generates a server topology where the first node (usually the server) is connected to all other nodes.</p> <p>This method initializes a topology matrix where the first node (typically the server) is connected to every other node in the network. The first row and the first column of the matrix are set to 1, representing connections to and from the server. The diagonal is set to 0 to indicate that no node is connected to itself.</p> <p>Returns:</p> Name Type Description <code>None</code> <p>The method modifies the internal <code>self.topology</code> matrix.</p> Source code in <code>nebula/addons/topologymanager.py</code> <pre><code>def generate_server_topology(self):\n    \"\"\"\n    Generates a server topology where the first node (usually the server) is connected to all other nodes.\n\n    This method initializes a topology matrix where the first node (typically the server) is connected to\n    every other node in the network. The first row and the first column of the matrix are set to 1, representing\n    connections to and from the server. The diagonal is set to 0 to indicate that no node is connected to itself.\n\n    Returns:\n        None: The method modifies the internal `self.topology` matrix.\n    \"\"\"\n    self.topology = np.zeros((self.n_nodes, self.n_nodes), dtype=np.float32)\n    self.topology[0, :] = 1\n    self.topology[:, 0] = 1\n    np.fill_diagonal(self.topology, 0)\n</code></pre>"},{"location":"api/addons/topologymanager/#nebula.addons.topologymanager.TopologyManager.generate_topology","title":"<code>generate_topology()</code>","text":"<p>Generates the network topology based on the configured settings.</p> <p>This method generates the network topology for the given scenario. It checks whether the topology should be fully connected, symmetric, or asymmetric and then generates the network accordingly.</p> <ul> <li>If the topology is fully connected, all nodes will be directly connected to each other.</li> <li>If the topology is symmetric, neighbors will be chosen symmetrically between nodes.</li> <li>If the topology is asymmetric, neighbors will be picked randomly without symmetry.</li> </ul> <p>Returns:</p> Name Type Description <code>None</code> <p>The method modifies the internal topology of the network.</p> Source code in <code>nebula/addons/topologymanager.py</code> <pre><code>def generate_topology(self):\n    \"\"\"\n    Generates the network topology based on the configured settings.\n\n    This method generates the network topology for the given scenario. It checks whether the topology\n    should be fully connected, symmetric, or asymmetric and then generates the network accordingly.\n\n    - If the topology is fully connected, all nodes will be directly connected to each other.\n    - If the topology is symmetric, neighbors will be chosen symmetrically between nodes.\n    - If the topology is asymmetric, neighbors will be picked randomly without symmetry.\n\n    Returns:\n        None: The method modifies the internal topology of the network.\n    \"\"\"\n    if self.b_fully_connected:\n        self.__fully_connected()\n        return\n\n    if self.topology is not None and len(self.topology) &gt; 0:\n        # Topology was already provided\n        return\n\n    if self.b_symmetric:\n        self.__randomly_pick_neighbors_symmetric()\n    else:\n        self.__randomly_pick_neighbors_asymmetric()\n</code></pre>"},{"location":"api/addons/topologymanager/#nebula.addons.topologymanager.TopologyManager.get_coordinates","title":"<code>get_coordinates(random_geo=True)</code>  <code>staticmethod</code>","text":"<p>Generates random geographical coordinates within predefined bounds for either Spain or Switzerland.</p> <p>The method returns a random geographical coordinate (latitude, longitude). The bounds for random coordinates are defined for two regions: Spain and Switzerland. The region is chosen randomly, and then the latitude and longitude are selected within the corresponding bounds.</p> <p>Parameters:</p> Name Type Description Default <code>random_geo</code> <code>bool</code> <p>If set to True, the method generates random coordinates within the predefined bounds                 for Spain or Switzerland. If set to False, this method could be modified to return fixed                 coordinates.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing the latitude and longitude of the generated point.</p> Source code in <code>nebula/addons/topologymanager.py</code> <pre><code>@staticmethod\ndef get_coordinates(random_geo=True):\n    \"\"\"\n    Generates random geographical coordinates within predefined bounds for either Spain or Switzerland.\n\n    The method returns a random geographical coordinate (latitude, longitude). The bounds for random coordinates are\n    defined for two regions: Spain and Switzerland. The region is chosen randomly, and then the latitude and longitude\n    are selected within the corresponding bounds.\n\n    Parameters:\n        random_geo (bool): If set to True, the method generates random coordinates within the predefined bounds\n                            for Spain or Switzerland. If set to False, this method could be modified to return fixed\n                            coordinates.\n\n    Returns:\n        tuple: A tuple containing the latitude and longitude of the generated point.\n    \"\"\"\n    if random_geo:\n        #  Espa\u00f1a min_lat, max_lat, min_lon, max_lon                  Suiza min_lat, max_lat, min_lon, max_lon\n        bounds = (36.0, 43.0, -9.0, 3.3) if random.randint(0, 1) == 0 else (45.8, 47.8, 5.9, 10.5)  # noqa: S311\n\n        min_latitude, max_latitude, min_longitude, max_longitude = bounds\n        latitude = random.uniform(min_latitude, max_latitude)  # noqa: S311\n        longitude = random.uniform(min_longitude, max_longitude)  # noqa: S311\n\n        return latitude, longitude\n</code></pre>"},{"location":"api/addons/topologymanager/#nebula.addons.topologymanager.TopologyManager.get_neighbors_string","title":"<code>get_neighbors_string(node_idx)</code>","text":"<p>Retrieves the neighbors of a given node as a string representation.</p> <p>This method checks the <code>topology</code> attribute to find the neighbors of the node at the specified index (<code>node_idx</code>). It then returns a string that lists the coordinates of each neighbor.</p> <p>Parameters:</p> Name Type Description Default <code>node_idx</code> <code>int</code> <p>The index of the node for which neighbors are to be retrieved.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>A space-separated string of neighbors' coordinates in the format \"latitude:longitude\".</p> Source code in <code>nebula/addons/topologymanager.py</code> <pre><code>def get_neighbors_string(self, node_idx):\n    \"\"\"\n    Retrieves the neighbors of a given node as a string representation.\n\n    This method checks the `topology` attribute to find the neighbors of the node at the specified index (`node_idx`). It then returns a string that lists the coordinates of each neighbor.\n\n    Parameters:\n        node_idx (int): The index of the node for which neighbors are to be retrieved.\n\n    Returns:\n        str: A space-separated string of neighbors' coordinates in the format \"latitude:longitude\".\n    \"\"\"\n    # logging.info(f\"Topology: {self.topology}\")\n    # logging.info(f\"Nodes: {self.nodes}\")\n    neighbors_data = []\n    for i, node in enumerate(self.topology[node_idx]):\n        if node == 1:\n            neighbors_data.append(self.nodes[i])\n\n    neighbors_data_strings = [f\"{i[0]}:{i[1]}\" for i in neighbors_data]\n    neighbors_data_string = \" \".join(neighbors_data_strings)\n    logging.info(f\"Neighbors of node participant_{node_idx}: {neighbors_data_string}\")\n    return neighbors_data_string\n</code></pre>"},{"location":"api/addons/topologymanager/#nebula.addons.topologymanager.TopologyManager.get_node_color","title":"<code>get_node_color(role)</code>","text":"<p>Returns the color associated with a given role.</p> <p>The method maps roles to specific colors for visualization or representation purposes.</p> <p>Parameters:</p> Name Type Description Default <code>role</code> <code>Role</code> <p>The role for which the color is to be determined.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The color associated with the given role. Defaults to \"red\" if the role is not recognized.</p> Source code in <code>nebula/addons/topologymanager.py</code> <pre><code>def get_node_color(self, role):\n    \"\"\"\n    Returns the color associated with a given role.\n\n    The method maps roles to specific colors for visualization or representation purposes.\n\n    Args:\n        role (Role): The role for which the color is to be determined.\n\n    Returns:\n        str: The color associated with the given role. Defaults to \"red\" if the role is not recognized.\n    \"\"\"\n    role_colors = {\n        Role.AGGREGATOR: \"orange\",\n        Role.SERVER: \"green\",\n        Role.TRAINER: \"#6182bd\",\n        Role.PROXY: \"purple\",\n    }\n    return role_colors.get(role, \"red\")\n</code></pre>"},{"location":"api/addons/topologymanager/#nebula.addons.topologymanager.TopologyManager.update_nodes","title":"<code>update_nodes(config_participants)</code>","text":"<p>Updates the nodes of the topology based on the provided configuration.</p> <p>This method assigns a new set of nodes to the <code>nodes</code> attribute, typically based on the configuration of the participants.</p> <p>Parameters:</p> Name Type Description Default <code>config_participants</code> <code>array - like</code> <p>A new set of nodes, usually derived from the participants' configuration, to be assigned to the topology.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>nebula/addons/topologymanager.py</code> <pre><code>def update_nodes(self, config_participants):\n    \"\"\"\n    Updates the nodes of the topology based on the provided configuration.\n\n    This method assigns a new set of nodes to the `nodes` attribute, typically based on the configuration of the participants.\n\n    Parameters:\n        config_participants (array-like): A new set of nodes, usually derived from the participants' configuration, to be assigned to the topology.\n\n    Returns:\n        None\n    \"\"\"\n    self.nodes = config_participants\n</code></pre>"},{"location":"api/addons/attacks/","title":"Documentation for Attacks Module","text":""},{"location":"api/addons/attacks/attacks/","title":"Documentation for Attacks Module","text":""},{"location":"api/addons/attacks/attacks/#nebula.addons.attacks.attacks.Attack","title":"<code>Attack</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for implementing various attack behaviors by dynamically injecting malicious behavior into existing functions or methods.</p> <p>This class provides an interface for replacing benign functions with malicious behaviors and for defining specific attack implementations. Subclasses must implement the <code>attack</code> and <code>_inject_malicious_behaviour</code> methods.</p> Source code in <code>nebula/addons/attacks/attacks.py</code> <pre><code>class Attack(ABC):\n    \"\"\"\n    Base class for implementing various attack behaviors by dynamically injecting\n    malicious behavior into existing functions or methods.\n\n    This class provides an interface for replacing benign functions with malicious\n    behaviors and for defining specific attack implementations. Subclasses must\n    implement the `attack` and `_inject_malicious_behaviour` methods.\n    \"\"\"\n\n    async def _replace_benign_function(function_route: str, malicious_behaviour):\n        \"\"\"\n        Dynamically replace a method in a class with a malicious behavior.\n\n        Args:\n            function_route (str): The route to the class and method to be replaced, in the format 'module.class.method'.\n            malicious_behaviour (callable): The malicious function that will replace the target method.\n\n        Raises:\n            AttributeError: If the specified class does not have the target method.\n            ImportError: If the module specified in `function_route` cannot be imported.\n            Exception: If any other error occurs during the process.\n\n        Returns:\n            None\n        \"\"\"\n        try:\n            *module_route, class_and_func = function_route.rsplit(\".\", maxsplit=1)\n            module = \".\".join(module_route)\n            class_name, function_name = class_and_func.split(\".\")\n\n            # Import the module\n            module_obj = importlib.import_module(module)\n\n            # Retrieve the class\n            changing_class = getattr(module_obj, class_name)\n\n            # Verify the class has the target method\n            if not hasattr(changing_class, function_name):\n                raise AttributeError(f\"Class '{class_name}' has no method named: '{function_name}'.\")\n\n            # Replace the original method with the malicious behavior\n            setattr(changing_class, function_name, malicious_behaviour)\n            print(f\"Function '{function_name}' has been replaced with '{malicious_behaviour.__name__}'.\")\n        except Exception as e:\n            logging.exception(f\"Error replacing function: {e}\")\n\n    @abstractmethod\n    async def attack(self):\n        \"\"\"\n        Abstract method to define the attack logic.\n\n        Subclasses must implement this method to specify the actions to perform\n        during an attack.\n\n        Raises:\n            NotImplementedError: If the method is not implemented in a subclass.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    async def _inject_malicious_behaviour(self, target_function: callable, *args, **kwargs) -&gt; None:\n        \"\"\"\n        Abstract method to inject a malicious behavior into an existing function.\n\n        This method must be implemented in subclasses to define how the malicious\n        behavior should interact with the target function.\n\n        Args:\n            target_function (callable): The function to inject the malicious behavior into.\n            *args: Positional arguments for the malicious behavior.\n            **kwargs: Keyword arguments for the malicious behavior.\n\n        Raises:\n            NotImplementedError: If the method is not implemented in a subclass.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api/addons/attacks/attacks/#nebula.addons.attacks.attacks.Attack.attack","title":"<code>attack()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Abstract method to define the attack logic.</p> <p>Subclasses must implement this method to specify the actions to perform during an attack.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the method is not implemented in a subclass.</p> Source code in <code>nebula/addons/attacks/attacks.py</code> <pre><code>@abstractmethod\nasync def attack(self):\n    \"\"\"\n    Abstract method to define the attack logic.\n\n    Subclasses must implement this method to specify the actions to perform\n    during an attack.\n\n    Raises:\n        NotImplementedError: If the method is not implemented in a subclass.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/addons/attacks/attacks/#nebula.addons.attacks.attacks.create_attack","title":"<code>create_attack(engine)</code>","text":"<p>Creates an attack object based on the attack name specified in the engine configuration.</p> <p>This function uses a predefined map of available attacks (<code>ATTACK_MAP</code>) to instantiate the corresponding attack class based on the attack name in the configuration. The attack parameters are also extracted from the configuration and passed when creating the attack object.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>object</code> <p>The training engine object containing the configuration for the attack.</p> required <p>Returns:</p> Name Type Description <code>Attack</code> <code>Attack</code> <p>An instance of the specified attack class.</p> <p>Raises:</p> Type Description <code>AttackException</code> <p>If the specified attack name is not found in the <code>ATTACK_MAP</code>.</p> Source code in <code>nebula/addons/attacks/attacks.py</code> <pre><code>def create_attack(engine) -&gt; Attack:\n    \"\"\"\n    Creates an attack object based on the attack name specified in the engine configuration.\n\n    This function uses a predefined map of available attacks (`ATTACK_MAP`) to instantiate\n    the corresponding attack class based on the attack name in the configuration. The attack\n    parameters are also extracted from the configuration and passed when creating the attack object.\n\n    Args:\n        engine (object): The training engine object containing the configuration for the attack.\n\n    Returns:\n        Attack: An instance of the specified attack class.\n\n    Raises:\n        AttackException: If the specified attack name is not found in the `ATTACK_MAP`.\n    \"\"\"\n    from nebula.addons.attacks.communications.delayerattack import DelayerAttack\n    from nebula.addons.attacks.communications.floodingattack import FloodingAttack\n    from nebula.addons.attacks.dataset.datapoison import SamplePoisoningAttack\n    from nebula.addons.attacks.dataset.labelflipping import LabelFlippingAttack\n    from nebula.addons.attacks.model.gllneuroninversion import GLLNeuronInversionAttack\n    from nebula.addons.attacks.model.modelpoison import ModelPoisonAttack\n    from nebula.addons.attacks.model.swappingweights import SwappingWeightsAttack\n\n    ATTACK_MAP = {\n        \"GLL Neuron Inversion\": GLLNeuronInversionAttack,\n        \"Swapping Weights\": SwappingWeightsAttack,\n        \"Delayer\": DelayerAttack,\n        \"Flooding\": FloodingAttack,\n        \"Label Flipping\": LabelFlippingAttack,\n        \"Sample Poisoning\": SamplePoisoningAttack,\n        \"Model Poisoning\": ModelPoisonAttack,\n    }\n\n    # Get attack name and parameters from the engine configuration\n    attack_name = engine.config.participant[\"adversarial_args\"][\"attacks\"]\n    attack_params = engine.config.participant[\"adversarial_args\"].get(\"attack_params\", {}).items()\n\n    # Look up the attack class based on the attack name\n    attack = ATTACK_MAP.get(attack_name)\n\n    # If the attack is found, return an instance of the attack class\n    if attack:\n        return attack(engine, dict(attack_params))\n    else:\n        # If the attack name is not found, raise an exception\n        raise AttackException(f\"Attack {attack_name} not found\")\n</code></pre>"},{"location":"api/addons/attacks/communications/","title":"Documentation for Communications Module","text":""},{"location":"api/addons/attacks/communications/communicationattack/","title":"Documentation for Communicationattack Module","text":""},{"location":"api/addons/attacks/communications/communicationattack/#nebula.addons.attacks.communications.communicationattack.CommunicationAttack","title":"<code>CommunicationAttack</code>","text":"<p>               Bases: <code>Attack</code></p> Source code in <code>nebula/addons/attacks/communications/communicationattack.py</code> <pre><code>class CommunicationAttack(Attack):\n    def __init__(\n        self,\n        engine,\n        target_class,\n        target_method,\n        round_start_attack,\n        round_stop_attack,\n        attack_interval,\n        decorator_args=None,\n        selectivity_percentage: int = 100,\n        selection_interval: int = None,\n    ):\n        super().__init__()\n        self.engine = engine\n        self.target_class = target_class\n        self.target_method = target_method\n        self.decorator_args = decorator_args\n        self.round_start_attack = round_start_attack\n        self.round_stop_attack = round_stop_attack\n        self.attack_interval = attack_interval\n        self.original_method = getattr(target_class, target_method, None)\n        self.selectivity_percentage = selectivity_percentage\n        self.selection_interval = selection_interval\n        self.last_selection_round = 0\n        self.targets = set()\n\n        if not self.original_method:\n            raise AttributeError(f\"Method {target_method} not found in class {target_class}\")\n\n    @abstractmethod\n    def decorator(self, *args):\n        \"\"\"Decorator that adds malicious behavior to the execution of the original method.\"\"\"\n        pass\n\n    async def select_targets(self):\n        if self.selectivity_percentage != 100:\n            if self.selection_interval:\n                if self.last_selection_round % self.selection_interval == 0:\n                    logging.info(\"Recalculating targets...\")\n                    all_nodes = await self.engine.cm.get_addrs_current_connections(only_direct=True)\n                    num_targets = max(1, int(len(all_nodes) * (self.selectivity_percentage / 100)))\n                    self.targets = set(random.sample(list(all_nodes), num_targets))\n            elif not self.targets:\n                logging.info(\"Calculating targets...\")\n                all_nodes = await self.engine.cm.get_addrs_current_connections(only_direct=True)\n                num_targets = max(1, int(len(all_nodes) * (self.selectivity_percentage / 100)))\n                self.targets = set(random.sample(list(all_nodes), num_targets))\n        else:\n            logging.info(\"All neighbors selected as targets\")\n            self.targets = await self.engine.cm.get_addrs_current_connections(only_direct=True)\n\n        logging.info(f\"Selected {self.selectivity_percentage}% targets from neighbors: {self.targets}\")\n        self.last_selection_round += 1\n\n    async def _inject_malicious_behaviour(self):\n        \"\"\"Inject malicious behavior into the target method.\"\"\"\n        decorated_method = self.decorator(self.decorator_args)(self.original_method)\n\n        setattr(\n            self.target_class,\n            self.target_method,\n            types.MethodType(decorated_method, self.target_class),\n        )\n\n    async def _restore_original_behaviour(self):\n        \"\"\"Restore the original behavior of the target method.\"\"\"\n        setattr(self.target_class, self.target_method, self.original_method)\n\n    async def attack(self):\n        \"\"\"Perform the attack logic based on the current round.\"\"\"\n        if self.engine.round not in range(self.round_start_attack, self.round_stop_attack + 1):\n            pass\n        elif self.engine.round == self.round_stop_attack:\n            logging.info(f\"[{self.__class__.__name__}] Stoping attack\")\n            await self._restore_original_behaviour()\n        elif (self.engine.round == self.round_start_attack) or (\n            (self.engine.round - self.round_start_attack) % self.attack_interval == 0\n        ):\n            await self.select_targets()\n            logging.info(f\"[{self.__class__.__name__}] Performing attack\")\n            await self._inject_malicious_behaviour()\n        else:\n            await self._restore_original_behaviour()\n</code></pre>"},{"location":"api/addons/attacks/communications/communicationattack/#nebula.addons.attacks.communications.communicationattack.CommunicationAttack.attack","title":"<code>attack()</code>  <code>async</code>","text":"<p>Perform the attack logic based on the current round.</p> Source code in <code>nebula/addons/attacks/communications/communicationattack.py</code> <pre><code>async def attack(self):\n    \"\"\"Perform the attack logic based on the current round.\"\"\"\n    if self.engine.round not in range(self.round_start_attack, self.round_stop_attack + 1):\n        pass\n    elif self.engine.round == self.round_stop_attack:\n        logging.info(f\"[{self.__class__.__name__}] Stoping attack\")\n        await self._restore_original_behaviour()\n    elif (self.engine.round == self.round_start_attack) or (\n        (self.engine.round - self.round_start_attack) % self.attack_interval == 0\n    ):\n        await self.select_targets()\n        logging.info(f\"[{self.__class__.__name__}] Performing attack\")\n        await self._inject_malicious_behaviour()\n    else:\n        await self._restore_original_behaviour()\n</code></pre>"},{"location":"api/addons/attacks/communications/communicationattack/#nebula.addons.attacks.communications.communicationattack.CommunicationAttack.decorator","title":"<code>decorator(*args)</code>  <code>abstractmethod</code>","text":"<p>Decorator that adds malicious behavior to the execution of the original method.</p> Source code in <code>nebula/addons/attacks/communications/communicationattack.py</code> <pre><code>@abstractmethod\ndef decorator(self, *args):\n    \"\"\"Decorator that adds malicious behavior to the execution of the original method.\"\"\"\n    pass\n</code></pre>"},{"location":"api/addons/attacks/communications/delayerattack/","title":"Documentation for Delayerattack Module","text":""},{"location":"api/addons/attacks/communications/delayerattack/#nebula.addons.attacks.communications.delayerattack.DelayerAttack","title":"<code>DelayerAttack</code>","text":"<p>               Bases: <code>CommunicationAttack</code></p> <p>Implements an attack that delays the execution of a target method by a specified amount of time.</p> Source code in <code>nebula/addons/attacks/communications/delayerattack.py</code> <pre><code>class DelayerAttack(CommunicationAttack):\n    \"\"\"\n    Implements an attack that delays the execution of a target method by a specified amount of time.\n    \"\"\"\n\n    def __init__(self, engine, attack_params: dict):\n        \"\"\"\n        Initializes the DelayerAttack with the engine and attack parameters.\n\n        Args:\n            engine: The engine managing the attack context.\n            attack_params (dict): Parameters for the attack, including the delay duration.\n        \"\"\"\n        try:\n            self.delay = int(attack_params[\"delay\"])\n            round_start = int(attack_params[\"round_start_attack\"])\n            round_stop = int(attack_params[\"round_stop_attack\"])\n            attack_interval = int(attack_params[\"attack_interval\"])\n            self.target_percentage = int(attack_params[\"target_percentage\"])\n            self.selection_interval = int(attack_params[\"selection_interval\"])\n        except KeyError as e:\n            raise ValueError(f\"Missing required attack parameter: {e}\")\n        except ValueError:\n            raise ValueError(\"Invalid value in attack_params. Ensure all values are integers.\")\n\n        super().__init__(\n            engine,\n            engine._cm,\n            \"send_model\",\n            round_start,\n            round_stop,\n            attack_interval,\n            self.delay,\n            self.target_percentage,\n            self.selection_interval,\n        )\n\n    def decorator(self, delay: int):\n        \"\"\"\n        Decorator that adds a delay to the execution of the original method.\n\n        Args:\n            delay (int): The time in seconds to delay the method execution.\n\n        Returns:\n            function: A decorator function that wraps the target method with the delay logic.\n        \"\"\"\n\n        def decorator(func):\n            @wraps(func)\n            async def wrapper(*args, **kwargs):\n                if len(args) &gt; 1:\n                    dest_addr = args[1]\n                    if dest_addr in self.targets:\n                        logging.info(f\"[DelayerAttack] Delaying model propagation to {dest_addr} by {delay} seconds\")\n                        await asyncio.sleep(delay)\n                _, *new_args = args  # Exclude self argument\n                return await func(*new_args)\n\n            return wrapper\n\n        return decorator\n</code></pre>"},{"location":"api/addons/attacks/communications/delayerattack/#nebula.addons.attacks.communications.delayerattack.DelayerAttack.__init__","title":"<code>__init__(engine, attack_params)</code>","text":"<p>Initializes the DelayerAttack with the engine and attack parameters.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <p>The engine managing the attack context.</p> required <code>attack_params</code> <code>dict</code> <p>Parameters for the attack, including the delay duration.</p> required Source code in <code>nebula/addons/attacks/communications/delayerattack.py</code> <pre><code>def __init__(self, engine, attack_params: dict):\n    \"\"\"\n    Initializes the DelayerAttack with the engine and attack parameters.\n\n    Args:\n        engine: The engine managing the attack context.\n        attack_params (dict): Parameters for the attack, including the delay duration.\n    \"\"\"\n    try:\n        self.delay = int(attack_params[\"delay\"])\n        round_start = int(attack_params[\"round_start_attack\"])\n        round_stop = int(attack_params[\"round_stop_attack\"])\n        attack_interval = int(attack_params[\"attack_interval\"])\n        self.target_percentage = int(attack_params[\"target_percentage\"])\n        self.selection_interval = int(attack_params[\"selection_interval\"])\n    except KeyError as e:\n        raise ValueError(f\"Missing required attack parameter: {e}\")\n    except ValueError:\n        raise ValueError(\"Invalid value in attack_params. Ensure all values are integers.\")\n\n    super().__init__(\n        engine,\n        engine._cm,\n        \"send_model\",\n        round_start,\n        round_stop,\n        attack_interval,\n        self.delay,\n        self.target_percentage,\n        self.selection_interval,\n    )\n</code></pre>"},{"location":"api/addons/attacks/communications/delayerattack/#nebula.addons.attacks.communications.delayerattack.DelayerAttack.decorator","title":"<code>decorator(delay)</code>","text":"<p>Decorator that adds a delay to the execution of the original method.</p> <p>Parameters:</p> Name Type Description Default <code>delay</code> <code>int</code> <p>The time in seconds to delay the method execution.</p> required <p>Returns:</p> Name Type Description <code>function</code> <p>A decorator function that wraps the target method with the delay logic.</p> Source code in <code>nebula/addons/attacks/communications/delayerattack.py</code> <pre><code>def decorator(self, delay: int):\n    \"\"\"\n    Decorator that adds a delay to the execution of the original method.\n\n    Args:\n        delay (int): The time in seconds to delay the method execution.\n\n    Returns:\n        function: A decorator function that wraps the target method with the delay logic.\n    \"\"\"\n\n    def decorator(func):\n        @wraps(func)\n        async def wrapper(*args, **kwargs):\n            if len(args) &gt; 1:\n                dest_addr = args[1]\n                if dest_addr in self.targets:\n                    logging.info(f\"[DelayerAttack] Delaying model propagation to {dest_addr} by {delay} seconds\")\n                    await asyncio.sleep(delay)\n            _, *new_args = args  # Exclude self argument\n            return await func(*new_args)\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"api/addons/attacks/communications/floodingattack/","title":"Documentation for Floodingattack Module","text":""},{"location":"api/addons/attacks/communications/floodingattack/#nebula.addons.attacks.communications.floodingattack.FloodingAttack","title":"<code>FloodingAttack</code>","text":"<p>               Bases: <code>CommunicationAttack</code></p> <p>Implements an attack that delays the execution of a target method by a specified amount of time.</p> Source code in <code>nebula/addons/attacks/communications/floodingattack.py</code> <pre><code>class FloodingAttack(CommunicationAttack):\n    \"\"\"\n    Implements an attack that delays the execution of a target method by a specified amount of time.\n    \"\"\"\n\n    def __init__(self, engine, attack_params: dict):\n        \"\"\"\n        Initializes the DelayerAttack with the engine and attack parameters.\n\n        Args:\n            engine: The engine managing the attack context.\n            attack_params (dict): Parameters for the attack, including the delay duration.\n        \"\"\"\n        try:\n            round_start = int(attack_params[\"round_start_attack\"])\n            round_stop = int(attack_params[\"round_stop_attack\"])\n            attack_interval = int(attack_params[\"attack_interval\"])\n            self.flooding_factor = int(attack_params[\"flooding_factor\"])\n            self.target_percentage = int(attack_params[\"target_percentage\"])\n            self.selection_interval = int(attack_params[\"selection_interval\"])\n        except KeyError as e:\n            raise ValueError(f\"Missing required attack parameter: {e}\")\n        except ValueError:\n            raise ValueError(\"Invalid value in attack_params. Ensure all values are integers.\")\n\n        self.verbose = False\n\n        super().__init__(\n            engine,\n            engine._cm,\n            \"send_model\",\n            round_start,\n            round_stop,\n            attack_interval,\n            self.flooding_factor,\n            self.target_percentage,\n            self.selection_interval,\n        )\n\n    def decorator(self, flooding_factor: int):\n        \"\"\"\n        Decorator that adds a delay to the execution of the original method.\n\n        Args:\n            flooding_factor (int): The number of times to repeat the function execution.\n\n        Returns:\n            function: A decorator function that wraps the target method with the delay logic.\n        \"\"\"\n\n        def decorator(func):\n            @wraps(func)\n            async def wrapper(*args, **kwargs):\n                if len(args) &gt; 1:\n                    dest_addr = args[1]\n                    if dest_addr in self.targets:\n                        logging.info(f\"[FloodingAttack] Flooding message to {dest_addr} by {flooding_factor} times\")\n                        for i in range(flooding_factor):\n                            if self.verbose:\n                                logging.info(\n                                    f\"[FloodingAttack] Sending duplicate {i + 1}/{flooding_factor} to {dest_addr}\"\n                                )\n                            _, dest_addr, _, serialized_model, weight = args  # Exclude self argument\n                            new_args = [dest_addr, i, serialized_model, weight]\n                            await func(*new_args, **kwargs)\n                _, dest_addr, _, serialized_model, weight = args  # Exclude self argument\n                new_args = [dest_addr, i, serialized_model, weight]\n                return await func(*new_args)\n\n            return wrapper\n\n        return decorator\n</code></pre>"},{"location":"api/addons/attacks/communications/floodingattack/#nebula.addons.attacks.communications.floodingattack.FloodingAttack.__init__","title":"<code>__init__(engine, attack_params)</code>","text":"<p>Initializes the DelayerAttack with the engine and attack parameters.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <p>The engine managing the attack context.</p> required <code>attack_params</code> <code>dict</code> <p>Parameters for the attack, including the delay duration.</p> required Source code in <code>nebula/addons/attacks/communications/floodingattack.py</code> <pre><code>def __init__(self, engine, attack_params: dict):\n    \"\"\"\n    Initializes the DelayerAttack with the engine and attack parameters.\n\n    Args:\n        engine: The engine managing the attack context.\n        attack_params (dict): Parameters for the attack, including the delay duration.\n    \"\"\"\n    try:\n        round_start = int(attack_params[\"round_start_attack\"])\n        round_stop = int(attack_params[\"round_stop_attack\"])\n        attack_interval = int(attack_params[\"attack_interval\"])\n        self.flooding_factor = int(attack_params[\"flooding_factor\"])\n        self.target_percentage = int(attack_params[\"target_percentage\"])\n        self.selection_interval = int(attack_params[\"selection_interval\"])\n    except KeyError as e:\n        raise ValueError(f\"Missing required attack parameter: {e}\")\n    except ValueError:\n        raise ValueError(\"Invalid value in attack_params. Ensure all values are integers.\")\n\n    self.verbose = False\n\n    super().__init__(\n        engine,\n        engine._cm,\n        \"send_model\",\n        round_start,\n        round_stop,\n        attack_interval,\n        self.flooding_factor,\n        self.target_percentage,\n        self.selection_interval,\n    )\n</code></pre>"},{"location":"api/addons/attacks/communications/floodingattack/#nebula.addons.attacks.communications.floodingattack.FloodingAttack.decorator","title":"<code>decorator(flooding_factor)</code>","text":"<p>Decorator that adds a delay to the execution of the original method.</p> <p>Parameters:</p> Name Type Description Default <code>flooding_factor</code> <code>int</code> <p>The number of times to repeat the function execution.</p> required <p>Returns:</p> Name Type Description <code>function</code> <p>A decorator function that wraps the target method with the delay logic.</p> Source code in <code>nebula/addons/attacks/communications/floodingattack.py</code> <pre><code>def decorator(self, flooding_factor: int):\n    \"\"\"\n    Decorator that adds a delay to the execution of the original method.\n\n    Args:\n        flooding_factor (int): The number of times to repeat the function execution.\n\n    Returns:\n        function: A decorator function that wraps the target method with the delay logic.\n    \"\"\"\n\n    def decorator(func):\n        @wraps(func)\n        async def wrapper(*args, **kwargs):\n            if len(args) &gt; 1:\n                dest_addr = args[1]\n                if dest_addr in self.targets:\n                    logging.info(f\"[FloodingAttack] Flooding message to {dest_addr} by {flooding_factor} times\")\n                    for i in range(flooding_factor):\n                        if self.verbose:\n                            logging.info(\n                                f\"[FloodingAttack] Sending duplicate {i + 1}/{flooding_factor} to {dest_addr}\"\n                            )\n                        _, dest_addr, _, serialized_model, weight = args  # Exclude self argument\n                        new_args = [dest_addr, i, serialized_model, weight]\n                        await func(*new_args, **kwargs)\n            _, dest_addr, _, serialized_model, weight = args  # Exclude self argument\n            new_args = [dest_addr, i, serialized_model, weight]\n            return await func(*new_args)\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"api/addons/attacks/dataset/","title":"Documentation for Dataset Module","text":""},{"location":"api/addons/attacks/dataset/datapoison/","title":"Documentation for Datapoison Module","text":"<p>This module contains functions for applying data poisoning techniques, including the application of noise to tensors and modification of datasets to simulate poisoning attacks.</p> <p>Functions: - apply_noise: Applies noise to a tensor based on the specified noise type and poisoning ratio. - datapoison: Adds noise to a specified portion of a dataset for data poisoning purposes. - add_x_to_image: Adds an 'X' mark to the top-left corner of an image. - poison_to_nlp_rawdata: Poisons NLP data by setting word vectors to zero with a given probability.</p>"},{"location":"api/addons/attacks/dataset/datapoison/#nebula.addons.attacks.dataset.datapoison.SamplePoisoningAttack","title":"<code>SamplePoisoningAttack</code>","text":"<p>               Bases: <code>DatasetAttack</code></p> <p>Implements a data poisoning attack on a training dataset.</p> <p>This attack introduces noise or modifies specific data points to influence the behavior of a machine learning model.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>object</code> <p>The training engine object, including the associated              datamodule.</p> required <code>attack_params</code> <code>dict</code> <p>Attack parameters including: - poisoned_percent (float): The percentage of data points to be poisoned. - poisoned_ratio (float): The ratio of poisoned data relative to the total dataset. - targeted (bool): Whether the attack is targeted at a specific label. - target_label (int): The target label for the attack (used if targeted is True). - noise_type (str): The type of noise to introduce during the attack.</p> required Source code in <code>nebula/addons/attacks/dataset/datapoison.py</code> <pre><code>class SamplePoisoningAttack(DatasetAttack):\n    \"\"\"\n    Implements a data poisoning attack on a training dataset.\n\n    This attack introduces noise or modifies specific data points to influence\n    the behavior of a machine learning model.\n\n    Args:\n        engine (object): The training engine object, including the associated\n                         datamodule.\n        attack_params (dict): Attack parameters including:\n            - poisoned_percent (float): The percentage of data points to be poisoned.\n            - poisoned_ratio (float): The ratio of poisoned data relative to the total dataset.\n            - targeted (bool): Whether the attack is targeted at a specific label.\n            - target_label (int): The target label for the attack (used if targeted is True).\n            - noise_type (str): The type of noise to introduce during the attack.\n    \"\"\"\n\n    def __init__(self, engine, attack_params):\n        \"\"\"\n        Initializes the SamplePoisoningAttack with the specified engine and parameters.\n\n        Args:\n            engine (object): The training engine object.\n            attack_params (dict): Dictionary of attack parameters.\n        \"\"\"\n        try:\n            round_start = int(attack_params[\"round_start_attack\"])\n            round_stop = int(attack_params[\"round_stop_attack\"])\n            attack_interval = int(attack_params[\"attack_interval\"])\n        except KeyError as e:\n            raise ValueError(f\"Missing required attack parameter: {e}\")\n        except ValueError:\n            raise ValueError(\"Invalid value in attack_params. Ensure all values are integers.\")\n\n        super().__init__(engine, round_start, round_stop, attack_interval)\n        self.datamodule = engine._trainer.datamodule\n        self.poisoned_percent = float(attack_params[\"poisoned_percent\"])\n        self.poisoned_ratio = float(attack_params[\"poisoned_ratio\"])\n        self.targeted = attack_params[\"targeted\"]\n        self.target_label = int(attack_params[\"target_label\"])\n        self.noise_type = attack_params[\"noise_type\"]\n\n    def apply_noise(self, t, noise_type, poisoned_ratio):\n        \"\"\"\n        Applies noise to a tensor based on the specified noise type and poisoning ratio.\n\n        Args:\n            t (torch.Tensor): The input tensor to which noise will be applied.\n            noise_type (str): The type of noise to apply. Supported types are:\n                - \"salt\": Salt noise (binary salt-and-pepper noise with only 'salt').\n                - \"gaussian\": Gaussian noise with mean 0 and specified variance.\n                - \"s&amp;p\": Salt-and-pepper noise.\n                - \"nlp_rawdata\": Applies a custom NLP raw data poisoning function.\n            poisoned_ratio (float): The ratio or variance of noise to be applied, depending on the noise type.\n\n        Returns:\n            torch.Tensor: The tensor with noise applied. If the noise type is not supported,\n                          returns the original tensor with an error message printed.\n\n        Raises:\n            ValueError: If the specified noise_type is not supported.\n\n        Notes:\n           - The \"nlp_rawdata\" noise type requires the custom `poison_to_nlp_rawdata` function.\n           - Noise for types \"salt\", \"gaussian\", and \"s&amp;p\" is generated using `random_noise` from\n             the `skimage.util` package, and returned as a `torch.Tensor`.\n        \"\"\"\n        if noise_type == \"salt\":\n            return torch.tensor(random_noise(t, mode=noise_type, amount=poisoned_ratio))\n        elif noise_type == \"gaussian\":\n            return torch.tensor(random_noise(t, mode=noise_type, mean=0, var=poisoned_ratio, clip=True))\n        elif noise_type == \"s&amp;p\":\n            return torch.tensor(random_noise(t, mode=noise_type, amount=poisoned_ratio))\n        elif noise_type == \"nlp_rawdata\":\n            return self.poison_to_nlp_rawdata(t, poisoned_ratio)\n        else:\n            print(\"ERROR: poison attack type not supported.\")\n            return t\n\n    def datapoison(\n        self,\n        dataset,\n        indices,\n        poisoned_percent,\n        poisoned_ratio,\n        targeted=False,\n        target_label=3,\n        noise_type=\"salt\",\n    ):\n        \"\"\"\n        Adds noise to a specified portion of a dataset for data poisoning purposes.\n\n        This function applies noise to randomly selected samples within a dataset.\n        Noise can be targeted or non-targeted. In non-targeted poisoning, random samples\n        are chosen and altered using the specified noise type and ratio. In targeted poisoning,\n        only samples with a specified label are altered by adding an 'X' pattern.\n\n        Args:\n            dataset (Dataset): The dataset to poison, expected to have `.data` and `.targets` attributes.\n            indices (list of int): The list of indices in the dataset to consider for poisoning.\n            poisoned_percent (float): The percentage of `indices` to poison, as a fraction (0 &lt;= poisoned_percent &lt;= 1).\n            poisoned_ratio (float): The intensity or probability parameter for the noise, depending on the noise type.\n            targeted (bool, optional): If True, applies targeted poisoning by adding an 'X' only to samples with `target_label`.\n                                       Default is False.\n            target_label (int, optional): The label to target when `targeted` is True. Default is 3.\n            noise_type (str, optional): The type of noise to apply in non-targeted poisoning. Supported types are:\n                                        - \"salt\": Applies salt noise.\n                                        - \"gaussian\": Applies Gaussian noise.\n                                        - \"s&amp;p\": Applies salt-and-pepper noise.\n                                        Default is \"salt\".\n\n        Returns:\n            Dataset: A deep copy of the original dataset with poisoned data in `.data`.\n\n        Raises:\n            ValueError: If `poisoned_percent` is not between 0 and 1, or if `noise_type` is unsupported.\n\n        Notes:\n            - Non-targeted poisoning randomly selects samples from `indices` based on `poisoned_percent`.\n            - Targeted poisoning modifies only samples with `target_label` by adding an 'X' pattern, regardless of `poisoned_ratio`.\n        \"\"\"\n        new_dataset = copy.deepcopy(dataset)\n        train_data = new_dataset.data\n        targets = new_dataset.targets\n        num_indices = len(indices)\n        if not isinstance(noise_type, str):\n            noise_type = noise_type[0]\n\n        if not targeted:\n            num_poisoned = int(poisoned_percent * num_indices)\n            if num_indices == 0:\n                return new_dataset\n            if num_poisoned &gt; num_indices:\n                return new_dataset\n            poisoned_indice = random.sample(indices, num_poisoned)\n\n            for i in poisoned_indice:\n                t = train_data[i]\n                poisoned = self.apply_noise(t, noise_type, poisoned_ratio)\n                train_data[i] = poisoned\n        else:\n            for i in indices:\n                if int(targets[i]) == int(target_label):\n                    t = train_data[i]\n                    poisoned = self.add_x_to_image(t)\n                    train_data[i] = poisoned\n        new_dataset.data = train_data\n        return new_dataset\n\n    def add_x_to_image(self, img):\n        \"\"\"\n        Adds a 10x10 pixel 'X' mark to the top-left corner of an image.\n\n        This function modifies the input image by setting specific pixels in the\n        top-left 10x10 region to a high intensity value, forming an 'X' shape.\n        Pixels on or below the main diagonal and above the secondary diagonal\n        are set to 255 (white).\n\n        Args:\n            img (array-like): A 2D array or image tensor representing pixel values.\n                              It is expected to be in grayscale, where each pixel\n                              has a single intensity value.\n\n        Returns:\n            torch.Tensor: A tensor representation of the modified image with the 'X' mark.\n        \"\"\"\n        for i in range(0, 10):\n            for j in range(0, 10):\n                if i + j &lt;= 9 or i == j:\n                    img[i][j] = 255\n        return torch.tensor(img)\n\n    def poison_to_nlp_rawdata(self, text_data, poisoned_ratio):\n        \"\"\"\n        Poisons NLP data by setting word vectors to zero with a given probability.\n\n        This function randomly selects a portion of non-zero word vectors in the\n        input text data and sets them to zero vectors based on the specified\n        poisoning ratio. This simulates a form of data corruption by partially\n        nullifying the information in the input data.\n\n        Args:\n            text_data (list of torch.Tensor): A list where each entry is a tensor\n                representing a word vector. Non-zero vectors are assumed to represent valid words.\n            poisoned_ratio (float): The fraction of non-zero word vectors to set to zero,\n                where 0 &lt;= poisoned_ratio &lt;= 1.\n\n        Returns:\n            list of torch.Tensor: The modified text data with some word vectors set to zero.\n\n        Raises:\n            ValueError: If `poisoned_ratio` is greater than 1 or less than 0.\n\n        Notes:\n            - `poisoned_ratio` controls the percentage of non-zero vectors to poison.\n            - If `num_poisoned_token` is zero or exceeds the number of non-zero vectors,\n              the function returns the original `text_data` without modification.\n        \"\"\"\n        non_zero_vector_indice = [i for i in range(0, len(text_data)) if text_data[i][0] != 0]\n        non_zero_vector_len = len(non_zero_vector_indice)\n\n        num_poisoned_token = int(poisoned_ratio * non_zero_vector_len)\n        if num_poisoned_token == 0:\n            return text_data\n        if num_poisoned_token &gt; non_zero_vector_len:\n            return text_data\n\n        poisoned_token_indice = random.sample(non_zero_vector_indice, num_poisoned_token)\n        zero_vector = torch.Tensor(np.zeros(len(text_data[0][0])))\n        for i in poisoned_token_indice:\n            text_data[i] = zero_vector\n        return text_data\n\n    def get_malicious_dataset(self):\n        \"\"\"\n        Generates a poisoned dataset based on the specified parameters.\n\n        Returns:\n            Dataset: A modified version of the training dataset with poisoned data.\n        \"\"\"\n        return self.datapoison(\n            self.datamodule.train_set,\n            self.datamodule.train_set_indices,\n            self.poisoned_percent,\n            self.poisoned_ratio,\n            self.targeted,\n            self.target_label,\n            self.noise_type,\n        )\n</code></pre>"},{"location":"api/addons/attacks/dataset/datapoison/#nebula.addons.attacks.dataset.datapoison.SamplePoisoningAttack.__init__","title":"<code>__init__(engine, attack_params)</code>","text":"<p>Initializes the SamplePoisoningAttack with the specified engine and parameters.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>object</code> <p>The training engine object.</p> required <code>attack_params</code> <code>dict</code> <p>Dictionary of attack parameters.</p> required Source code in <code>nebula/addons/attacks/dataset/datapoison.py</code> <pre><code>def __init__(self, engine, attack_params):\n    \"\"\"\n    Initializes the SamplePoisoningAttack with the specified engine and parameters.\n\n    Args:\n        engine (object): The training engine object.\n        attack_params (dict): Dictionary of attack parameters.\n    \"\"\"\n    try:\n        round_start = int(attack_params[\"round_start_attack\"])\n        round_stop = int(attack_params[\"round_stop_attack\"])\n        attack_interval = int(attack_params[\"attack_interval\"])\n    except KeyError as e:\n        raise ValueError(f\"Missing required attack parameter: {e}\")\n    except ValueError:\n        raise ValueError(\"Invalid value in attack_params. Ensure all values are integers.\")\n\n    super().__init__(engine, round_start, round_stop, attack_interval)\n    self.datamodule = engine._trainer.datamodule\n    self.poisoned_percent = float(attack_params[\"poisoned_percent\"])\n    self.poisoned_ratio = float(attack_params[\"poisoned_ratio\"])\n    self.targeted = attack_params[\"targeted\"]\n    self.target_label = int(attack_params[\"target_label\"])\n    self.noise_type = attack_params[\"noise_type\"]\n</code></pre>"},{"location":"api/addons/attacks/dataset/datapoison/#nebula.addons.attacks.dataset.datapoison.SamplePoisoningAttack.add_x_to_image","title":"<code>add_x_to_image(img)</code>","text":"<p>Adds a 10x10 pixel 'X' mark to the top-left corner of an image.</p> <p>This function modifies the input image by setting specific pixels in the top-left 10x10 region to a high intensity value, forming an 'X' shape. Pixels on or below the main diagonal and above the secondary diagonal are set to 255 (white).</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>array - like</code> <p>A 2D array or image tensor representing pixel values.               It is expected to be in grayscale, where each pixel               has a single intensity value.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: A tensor representation of the modified image with the 'X' mark.</p> Source code in <code>nebula/addons/attacks/dataset/datapoison.py</code> <pre><code>def add_x_to_image(self, img):\n    \"\"\"\n    Adds a 10x10 pixel 'X' mark to the top-left corner of an image.\n\n    This function modifies the input image by setting specific pixels in the\n    top-left 10x10 region to a high intensity value, forming an 'X' shape.\n    Pixels on or below the main diagonal and above the secondary diagonal\n    are set to 255 (white).\n\n    Args:\n        img (array-like): A 2D array or image tensor representing pixel values.\n                          It is expected to be in grayscale, where each pixel\n                          has a single intensity value.\n\n    Returns:\n        torch.Tensor: A tensor representation of the modified image with the 'X' mark.\n    \"\"\"\n    for i in range(0, 10):\n        for j in range(0, 10):\n            if i + j &lt;= 9 or i == j:\n                img[i][j] = 255\n    return torch.tensor(img)\n</code></pre>"},{"location":"api/addons/attacks/dataset/datapoison/#nebula.addons.attacks.dataset.datapoison.SamplePoisoningAttack.apply_noise","title":"<code>apply_noise(t, noise_type, poisoned_ratio)</code>","text":"<p>Applies noise to a tensor based on the specified noise type and poisoning ratio.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>The input tensor to which noise will be applied.</p> required <code>noise_type</code> <code>str</code> <p>The type of noise to apply. Supported types are: - \"salt\": Salt noise (binary salt-and-pepper noise with only 'salt'). - \"gaussian\": Gaussian noise with mean 0 and specified variance. - \"s&amp;p\": Salt-and-pepper noise. - \"nlp_rawdata\": Applies a custom NLP raw data poisoning function.</p> required <code>poisoned_ratio</code> <code>float</code> <p>The ratio or variance of noise to be applied, depending on the noise type.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: The tensor with noise applied. If the noise type is not supported,           returns the original tensor with an error message printed.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified noise_type is not supported.</p> Notes <ul> <li>The \"nlp_rawdata\" noise type requires the custom <code>poison_to_nlp_rawdata</code> function.</li> <li>Noise for types \"salt\", \"gaussian\", and \"s&amp;p\" is generated using <code>random_noise</code> from   the <code>skimage.util</code> package, and returned as a <code>torch.Tensor</code>.</li> </ul> Source code in <code>nebula/addons/attacks/dataset/datapoison.py</code> <pre><code>def apply_noise(self, t, noise_type, poisoned_ratio):\n    \"\"\"\n    Applies noise to a tensor based on the specified noise type and poisoning ratio.\n\n    Args:\n        t (torch.Tensor): The input tensor to which noise will be applied.\n        noise_type (str): The type of noise to apply. Supported types are:\n            - \"salt\": Salt noise (binary salt-and-pepper noise with only 'salt').\n            - \"gaussian\": Gaussian noise with mean 0 and specified variance.\n            - \"s&amp;p\": Salt-and-pepper noise.\n            - \"nlp_rawdata\": Applies a custom NLP raw data poisoning function.\n        poisoned_ratio (float): The ratio or variance of noise to be applied, depending on the noise type.\n\n    Returns:\n        torch.Tensor: The tensor with noise applied. If the noise type is not supported,\n                      returns the original tensor with an error message printed.\n\n    Raises:\n        ValueError: If the specified noise_type is not supported.\n\n    Notes:\n       - The \"nlp_rawdata\" noise type requires the custom `poison_to_nlp_rawdata` function.\n       - Noise for types \"salt\", \"gaussian\", and \"s&amp;p\" is generated using `random_noise` from\n         the `skimage.util` package, and returned as a `torch.Tensor`.\n    \"\"\"\n    if noise_type == \"salt\":\n        return torch.tensor(random_noise(t, mode=noise_type, amount=poisoned_ratio))\n    elif noise_type == \"gaussian\":\n        return torch.tensor(random_noise(t, mode=noise_type, mean=0, var=poisoned_ratio, clip=True))\n    elif noise_type == \"s&amp;p\":\n        return torch.tensor(random_noise(t, mode=noise_type, amount=poisoned_ratio))\n    elif noise_type == \"nlp_rawdata\":\n        return self.poison_to_nlp_rawdata(t, poisoned_ratio)\n    else:\n        print(\"ERROR: poison attack type not supported.\")\n        return t\n</code></pre>"},{"location":"api/addons/attacks/dataset/datapoison/#nebula.addons.attacks.dataset.datapoison.SamplePoisoningAttack.datapoison","title":"<code>datapoison(dataset, indices, poisoned_percent, poisoned_ratio, targeted=False, target_label=3, noise_type='salt')</code>","text":"<p>Adds noise to a specified portion of a dataset for data poisoning purposes.</p> <p>This function applies noise to randomly selected samples within a dataset. Noise can be targeted or non-targeted. In non-targeted poisoning, random samples are chosen and altered using the specified noise type and ratio. In targeted poisoning, only samples with a specified label are altered by adding an 'X' pattern.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>The dataset to poison, expected to have <code>.data</code> and <code>.targets</code> attributes.</p> required <code>indices</code> <code>list of int</code> <p>The list of indices in the dataset to consider for poisoning.</p> required <code>poisoned_percent</code> <code>float</code> <p>The percentage of <code>indices</code> to poison, as a fraction (0 &lt;= poisoned_percent &lt;= 1).</p> required <code>poisoned_ratio</code> <code>float</code> <p>The intensity or probability parameter for the noise, depending on the noise type.</p> required <code>targeted</code> <code>bool</code> <p>If True, applies targeted poisoning by adding an 'X' only to samples with <code>target_label</code>.                        Default is False.</p> <code>False</code> <code>target_label</code> <code>int</code> <p>The label to target when <code>targeted</code> is True. Default is 3.</p> <code>3</code> <code>noise_type</code> <code>str</code> <p>The type of noise to apply in non-targeted poisoning. Supported types are:                         - \"salt\": Applies salt noise.                         - \"gaussian\": Applies Gaussian noise.                         - \"s&amp;p\": Applies salt-and-pepper noise.                         Default is \"salt\".</p> <code>'salt'</code> <p>Returns:</p> Name Type Description <code>Dataset</code> <p>A deep copy of the original dataset with poisoned data in <code>.data</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>poisoned_percent</code> is not between 0 and 1, or if <code>noise_type</code> is unsupported.</p> Notes <ul> <li>Non-targeted poisoning randomly selects samples from <code>indices</code> based on <code>poisoned_percent</code>.</li> <li>Targeted poisoning modifies only samples with <code>target_label</code> by adding an 'X' pattern, regardless of <code>poisoned_ratio</code>.</li> </ul> Source code in <code>nebula/addons/attacks/dataset/datapoison.py</code> <pre><code>def datapoison(\n    self,\n    dataset,\n    indices,\n    poisoned_percent,\n    poisoned_ratio,\n    targeted=False,\n    target_label=3,\n    noise_type=\"salt\",\n):\n    \"\"\"\n    Adds noise to a specified portion of a dataset for data poisoning purposes.\n\n    This function applies noise to randomly selected samples within a dataset.\n    Noise can be targeted or non-targeted. In non-targeted poisoning, random samples\n    are chosen and altered using the specified noise type and ratio. In targeted poisoning,\n    only samples with a specified label are altered by adding an 'X' pattern.\n\n    Args:\n        dataset (Dataset): The dataset to poison, expected to have `.data` and `.targets` attributes.\n        indices (list of int): The list of indices in the dataset to consider for poisoning.\n        poisoned_percent (float): The percentage of `indices` to poison, as a fraction (0 &lt;= poisoned_percent &lt;= 1).\n        poisoned_ratio (float): The intensity or probability parameter for the noise, depending on the noise type.\n        targeted (bool, optional): If True, applies targeted poisoning by adding an 'X' only to samples with `target_label`.\n                                   Default is False.\n        target_label (int, optional): The label to target when `targeted` is True. Default is 3.\n        noise_type (str, optional): The type of noise to apply in non-targeted poisoning. Supported types are:\n                                    - \"salt\": Applies salt noise.\n                                    - \"gaussian\": Applies Gaussian noise.\n                                    - \"s&amp;p\": Applies salt-and-pepper noise.\n                                    Default is \"salt\".\n\n    Returns:\n        Dataset: A deep copy of the original dataset with poisoned data in `.data`.\n\n    Raises:\n        ValueError: If `poisoned_percent` is not between 0 and 1, or if `noise_type` is unsupported.\n\n    Notes:\n        - Non-targeted poisoning randomly selects samples from `indices` based on `poisoned_percent`.\n        - Targeted poisoning modifies only samples with `target_label` by adding an 'X' pattern, regardless of `poisoned_ratio`.\n    \"\"\"\n    new_dataset = copy.deepcopy(dataset)\n    train_data = new_dataset.data\n    targets = new_dataset.targets\n    num_indices = len(indices)\n    if not isinstance(noise_type, str):\n        noise_type = noise_type[0]\n\n    if not targeted:\n        num_poisoned = int(poisoned_percent * num_indices)\n        if num_indices == 0:\n            return new_dataset\n        if num_poisoned &gt; num_indices:\n            return new_dataset\n        poisoned_indice = random.sample(indices, num_poisoned)\n\n        for i in poisoned_indice:\n            t = train_data[i]\n            poisoned = self.apply_noise(t, noise_type, poisoned_ratio)\n            train_data[i] = poisoned\n    else:\n        for i in indices:\n            if int(targets[i]) == int(target_label):\n                t = train_data[i]\n                poisoned = self.add_x_to_image(t)\n                train_data[i] = poisoned\n    new_dataset.data = train_data\n    return new_dataset\n</code></pre>"},{"location":"api/addons/attacks/dataset/datapoison/#nebula.addons.attacks.dataset.datapoison.SamplePoisoningAttack.get_malicious_dataset","title":"<code>get_malicious_dataset()</code>","text":"<p>Generates a poisoned dataset based on the specified parameters.</p> <p>Returns:</p> Name Type Description <code>Dataset</code> <p>A modified version of the training dataset with poisoned data.</p> Source code in <code>nebula/addons/attacks/dataset/datapoison.py</code> <pre><code>def get_malicious_dataset(self):\n    \"\"\"\n    Generates a poisoned dataset based on the specified parameters.\n\n    Returns:\n        Dataset: A modified version of the training dataset with poisoned data.\n    \"\"\"\n    return self.datapoison(\n        self.datamodule.train_set,\n        self.datamodule.train_set_indices,\n        self.poisoned_percent,\n        self.poisoned_ratio,\n        self.targeted,\n        self.target_label,\n        self.noise_type,\n    )\n</code></pre>"},{"location":"api/addons/attacks/dataset/datapoison/#nebula.addons.attacks.dataset.datapoison.SamplePoisoningAttack.poison_to_nlp_rawdata","title":"<code>poison_to_nlp_rawdata(text_data, poisoned_ratio)</code>","text":"<p>Poisons NLP data by setting word vectors to zero with a given probability.</p> <p>This function randomly selects a portion of non-zero word vectors in the input text data and sets them to zero vectors based on the specified poisoning ratio. This simulates a form of data corruption by partially nullifying the information in the input data.</p> <p>Parameters:</p> Name Type Description Default <code>text_data</code> <code>list of torch.Tensor</code> <p>A list where each entry is a tensor representing a word vector. Non-zero vectors are assumed to represent valid words.</p> required <code>poisoned_ratio</code> <code>float</code> <p>The fraction of non-zero word vectors to set to zero, where 0 &lt;= poisoned_ratio &lt;= 1.</p> required <p>Returns:</p> Type Description <p>list of torch.Tensor: The modified text data with some word vectors set to zero.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>poisoned_ratio</code> is greater than 1 or less than 0.</p> Notes <ul> <li><code>poisoned_ratio</code> controls the percentage of non-zero vectors to poison.</li> <li>If <code>num_poisoned_token</code> is zero or exceeds the number of non-zero vectors,   the function returns the original <code>text_data</code> without modification.</li> </ul> Source code in <code>nebula/addons/attacks/dataset/datapoison.py</code> <pre><code>def poison_to_nlp_rawdata(self, text_data, poisoned_ratio):\n    \"\"\"\n    Poisons NLP data by setting word vectors to zero with a given probability.\n\n    This function randomly selects a portion of non-zero word vectors in the\n    input text data and sets them to zero vectors based on the specified\n    poisoning ratio. This simulates a form of data corruption by partially\n    nullifying the information in the input data.\n\n    Args:\n        text_data (list of torch.Tensor): A list where each entry is a tensor\n            representing a word vector. Non-zero vectors are assumed to represent valid words.\n        poisoned_ratio (float): The fraction of non-zero word vectors to set to zero,\n            where 0 &lt;= poisoned_ratio &lt;= 1.\n\n    Returns:\n        list of torch.Tensor: The modified text data with some word vectors set to zero.\n\n    Raises:\n        ValueError: If `poisoned_ratio` is greater than 1 or less than 0.\n\n    Notes:\n        - `poisoned_ratio` controls the percentage of non-zero vectors to poison.\n        - If `num_poisoned_token` is zero or exceeds the number of non-zero vectors,\n          the function returns the original `text_data` without modification.\n    \"\"\"\n    non_zero_vector_indice = [i for i in range(0, len(text_data)) if text_data[i][0] != 0]\n    non_zero_vector_len = len(non_zero_vector_indice)\n\n    num_poisoned_token = int(poisoned_ratio * non_zero_vector_len)\n    if num_poisoned_token == 0:\n        return text_data\n    if num_poisoned_token &gt; non_zero_vector_len:\n        return text_data\n\n    poisoned_token_indice = random.sample(non_zero_vector_indice, num_poisoned_token)\n    zero_vector = torch.Tensor(np.zeros(len(text_data[0][0])))\n    for i in poisoned_token_indice:\n        text_data[i] = zero_vector\n    return text_data\n</code></pre>"},{"location":"api/addons/attacks/dataset/datasetattack/","title":"Documentation for Datasetattack Module","text":""},{"location":"api/addons/attacks/dataset/datasetattack/#nebula.addons.attacks.dataset.datasetattack.DatasetAttack","title":"<code>DatasetAttack</code>","text":"<p>               Bases: <code>Attack</code></p> <p>Implements an attack that replaces the training dataset with a malicious version during specific rounds of the engine's execution.</p> <p>This attack modifies the dataset used by the engine's trainer to introduce malicious data, potentially impacting the model's training process.</p> Source code in <code>nebula/addons/attacks/dataset/datasetattack.py</code> <pre><code>class DatasetAttack(Attack):\n    \"\"\"\n    Implements an attack that replaces the training dataset with a malicious version\n    during specific rounds of the engine's execution.\n\n    This attack modifies the dataset used by the engine's trainer to introduce malicious\n    data, potentially impacting the model's training process.\n    \"\"\"\n\n    def __init__(self, engine, round_start_attack, round_stop_attack, attack_interval):\n        \"\"\"\n        Initializes the DatasetAttack with the given engine.\n\n        Args:\n            engine: The engine managing the attack context.\n        \"\"\"\n        self.engine = engine\n        self.round_start_attack = round_start_attack\n        self.round_stop_attack = round_stop_attack\n        self.attack_interval = attack_interval\n\n    async def attack(self):\n        \"\"\"\n        Performs the attack by replacing the training dataset with a malicious version.\n\n        During the specified rounds of the attack, the engine's trainer is provided\n        with a malicious dataset. The attack is stopped when the engine reaches the\n        designated stop round.\n        \"\"\"\n        if self.engine.round not in range(self.round_start_attack, self.round_stop_attack + 1):\n            pass\n        elif  self.engine.round == self.round_stop_attack:\n            logging.info(f\"[{self.__class__.__name__}] Stopping attack\")\n        elif self.engine.round &gt;= self.round_start_attack and ((self.engine.round - self.round_start_attack) % self.attack_interval == 0):\n            logging.info(f\"[{self.__class__.__name__}] Performing attack\")\n            self.engine.trainer.datamodule.train_set = self.get_malicious_dataset()\n\n    async def _inject_malicious_behaviour(self, target_function, *args, **kwargs):\n        \"\"\"\n        Abstract method for injecting malicious behavior into a target function.\n\n        This method is not implemented in this class and must be overridden by subclasses\n        if additional malicious behavior is required.\n\n        Args:\n            target_function (callable): The function to inject the malicious behavior into.\n            *args: Positional arguments for the malicious behavior.\n            **kwargs: Keyword arguments for the malicious behavior.\n\n        Raises:\n            NotImplementedError: This method is not implemented in this class.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_malicious_dataset(self):\n        \"\"\"\n        Abstract method to retrieve the malicious dataset.\n\n        Subclasses must implement this method to define how the malicious dataset\n        is created or retrieved.\n\n        Raises:\n            NotImplementedError: If the method is not implemented in a subclass.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api/addons/attacks/dataset/datasetattack/#nebula.addons.attacks.dataset.datasetattack.DatasetAttack.__init__","title":"<code>__init__(engine, round_start_attack, round_stop_attack, attack_interval)</code>","text":"<p>Initializes the DatasetAttack with the given engine.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <p>The engine managing the attack context.</p> required Source code in <code>nebula/addons/attacks/dataset/datasetattack.py</code> <pre><code>def __init__(self, engine, round_start_attack, round_stop_attack, attack_interval):\n    \"\"\"\n    Initializes the DatasetAttack with the given engine.\n\n    Args:\n        engine: The engine managing the attack context.\n    \"\"\"\n    self.engine = engine\n    self.round_start_attack = round_start_attack\n    self.round_stop_attack = round_stop_attack\n    self.attack_interval = attack_interval\n</code></pre>"},{"location":"api/addons/attacks/dataset/datasetattack/#nebula.addons.attacks.dataset.datasetattack.DatasetAttack.attack","title":"<code>attack()</code>  <code>async</code>","text":"<p>Performs the attack by replacing the training dataset with a malicious version.</p> <p>During the specified rounds of the attack, the engine's trainer is provided with a malicious dataset. The attack is stopped when the engine reaches the designated stop round.</p> Source code in <code>nebula/addons/attacks/dataset/datasetattack.py</code> <pre><code>async def attack(self):\n    \"\"\"\n    Performs the attack by replacing the training dataset with a malicious version.\n\n    During the specified rounds of the attack, the engine's trainer is provided\n    with a malicious dataset. The attack is stopped when the engine reaches the\n    designated stop round.\n    \"\"\"\n    if self.engine.round not in range(self.round_start_attack, self.round_stop_attack + 1):\n        pass\n    elif  self.engine.round == self.round_stop_attack:\n        logging.info(f\"[{self.__class__.__name__}] Stopping attack\")\n    elif self.engine.round &gt;= self.round_start_attack and ((self.engine.round - self.round_start_attack) % self.attack_interval == 0):\n        logging.info(f\"[{self.__class__.__name__}] Performing attack\")\n        self.engine.trainer.datamodule.train_set = self.get_malicious_dataset()\n</code></pre>"},{"location":"api/addons/attacks/dataset/datasetattack/#nebula.addons.attacks.dataset.datasetattack.DatasetAttack.get_malicious_dataset","title":"<code>get_malicious_dataset()</code>  <code>abstractmethod</code>","text":"<p>Abstract method to retrieve the malicious dataset.</p> <p>Subclasses must implement this method to define how the malicious dataset is created or retrieved.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the method is not implemented in a subclass.</p> Source code in <code>nebula/addons/attacks/dataset/datasetattack.py</code> <pre><code>@abstractmethod\ndef get_malicious_dataset(self):\n    \"\"\"\n    Abstract method to retrieve the malicious dataset.\n\n    Subclasses must implement this method to define how the malicious dataset\n    is created or retrieved.\n\n    Raises:\n        NotImplementedError: If the method is not implemented in a subclass.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/addons/attacks/dataset/labelflipping/","title":"Documentation for Labelflipping Module","text":"<p>This module provides a function for label flipping in datasets, allowing for the simulation of label noise as a form of data poisoning. The main function modifies the labels of specific samples in a dataset based on a specified percentage and target conditions.</p> <p>Function: - labelFlipping: Flips the labels of a specified portion of a dataset to random values or to a specific target label.</p>"},{"location":"api/addons/attacks/dataset/labelflipping/#nebula.addons.attacks.dataset.labelflipping.LabelFlippingAttack","title":"<code>LabelFlippingAttack</code>","text":"<p>               Bases: <code>DatasetAttack</code></p> <p>Implements an attack that flips the labels of a portion of the training dataset.</p> <p>This attack alters the labels of certain data points in the training set to mislead the training process.</p> Source code in <code>nebula/addons/attacks/dataset/labelflipping.py</code> <pre><code>class LabelFlippingAttack(DatasetAttack):\n    \"\"\"\n    Implements an attack that flips the labels of a portion of the training dataset.\n\n    This attack alters the labels of certain data points in the training set to\n    mislead the training process.\n    \"\"\"\n\n    def __init__(self, engine, attack_params):\n        \"\"\"\n        Initializes the LabelFlippingAttack with the engine and attack parameters.\n\n        Args:\n            engine: The engine managing the attack context.\n            attack_params (dict): Parameters for the attack, including the percentage of\n                                  poisoned data, targeting options, and label specifications.\n        \"\"\"\n        try:\n            round_start = int(attack_params[\"round_start_attack\"])\n            round_stop = int(attack_params[\"round_stop_attack\"])\n            attack_interval = int(attack_params[\"attack_interval\"])\n        except KeyError as e:\n            raise ValueError(f\"Missing required attack parameter: {e}\")\n        except ValueError:\n            raise ValueError(\"Invalid value in attack_params. Ensure all values are integers.\")\n\n        super().__init__(engine, round_start, round_stop, attack_interval)\n        self.datamodule = engine._trainer.datamodule\n        self.poisoned_percent = float(attack_params[\"poisoned_percent\"])\n        self.targeted = attack_params[\"targeted\"]\n        self.target_label = int(attack_params[\"target_label\"])\n        self.target_changed_label = int(attack_params[\"target_changed_label\"])\n\n    def labelFlipping(\n        self,\n        dataset,\n        indices,\n        poisoned_percent=0,\n        targeted=False,\n        target_label=4,\n        target_changed_label=7,\n    ):\n        \"\"\"\n        Flips the labels of a specified portion of a dataset to random values or to a specific target label.\n\n        This function modifies the labels of selected samples in the dataset based on the specified\n        poisoning percentage. Labels can be flipped either randomly or targeted to change from a specific\n        label to another specified label.\n\n        Args:\n            dataset (Dataset): The dataset containing training data, expected to be a PyTorch dataset\n                               with a `.targets` attribute.\n            indices (list of int): The list of indices in the dataset to consider for label flipping.\n            poisoned_percent (float, optional): The ratio of labels to change, expressed as a fraction\n                                                (0 &lt;= poisoned_percent &lt;= 1). Default is 0.\n            targeted (bool, optional): If True, flips only labels matching `target_label` to `target_changed_label`.\n                                       Default is False.\n            target_label (int, optional): The label to change when `targeted` is True. Default is 4.\n            target_changed_label (int, optional): The label to which `target_label` will be changed. Default is 7.\n\n        Returns:\n            Dataset: A deep copy of the original dataset with modified labels in `.targets`.\n\n        Raises:\n            ValueError: If `poisoned_percent` is not between 0 and 1, or if `flipping_percent` is invalid.\n\n        Notes:\n            - When not in targeted mode, labels are flipped for a random selection of indices based on the specified\n              `poisoned_percent`. The new label is chosen randomly from the existing classes.\n            - In targeted mode, labels that match `target_label` are directly changed to `target_changed_label`.\n        \"\"\"\n        new_dataset = copy.deepcopy(dataset)\n\n        targets = torch.tensor(new_dataset.targets) if isinstance(new_dataset.targets, list) else new_dataset.targets\n\n        num_indices = len(indices)\n        class_list = list(set(targets.tolist()))\n        if not targeted:\n            num_flipped = int(poisoned_percent * num_indices)\n            if num_indices == 0:\n                return new_dataset\n            if num_flipped &gt; num_indices:\n                return new_dataset\n            flipped_indice = random.sample(indices, num_flipped)\n\n            for i in flipped_indice:\n                t = targets[i]\n                flipped = torch.tensor(random.sample(class_list, 1)[0])\n                while t == flipped:\n                    flipped = torch.tensor(random.sample(class_list, 1)[0])\n                targets[i] = flipped\n        else:\n            for i in indices:\n                if int(targets[i]) == int(target_label):\n                    targets[i] = torch.tensor(target_changed_label)\n        new_dataset.targets = targets\n        return new_dataset\n\n    def get_malicious_dataset(self):\n        \"\"\"\n        Creates a malicious dataset by flipping the labels of selected data points.\n\n        Returns:\n            Dataset: The modified dataset with flipped labels.\n        \"\"\"\n        return self.labelFlipping(\n            self.datamodule.train_set,\n            self.datamodule.train_set_indices,\n            self.poisoned_percent,\n            self.targeted,\n            self.target_label,\n            self.target_changed_label,\n        )\n</code></pre>"},{"location":"api/addons/attacks/dataset/labelflipping/#nebula.addons.attacks.dataset.labelflipping.LabelFlippingAttack.__init__","title":"<code>__init__(engine, attack_params)</code>","text":"<p>Initializes the LabelFlippingAttack with the engine and attack parameters.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <p>The engine managing the attack context.</p> required <code>attack_params</code> <code>dict</code> <p>Parameters for the attack, including the percentage of                   poisoned data, targeting options, and label specifications.</p> required Source code in <code>nebula/addons/attacks/dataset/labelflipping.py</code> <pre><code>def __init__(self, engine, attack_params):\n    \"\"\"\n    Initializes the LabelFlippingAttack with the engine and attack parameters.\n\n    Args:\n        engine: The engine managing the attack context.\n        attack_params (dict): Parameters for the attack, including the percentage of\n                              poisoned data, targeting options, and label specifications.\n    \"\"\"\n    try:\n        round_start = int(attack_params[\"round_start_attack\"])\n        round_stop = int(attack_params[\"round_stop_attack\"])\n        attack_interval = int(attack_params[\"attack_interval\"])\n    except KeyError as e:\n        raise ValueError(f\"Missing required attack parameter: {e}\")\n    except ValueError:\n        raise ValueError(\"Invalid value in attack_params. Ensure all values are integers.\")\n\n    super().__init__(engine, round_start, round_stop, attack_interval)\n    self.datamodule = engine._trainer.datamodule\n    self.poisoned_percent = float(attack_params[\"poisoned_percent\"])\n    self.targeted = attack_params[\"targeted\"]\n    self.target_label = int(attack_params[\"target_label\"])\n    self.target_changed_label = int(attack_params[\"target_changed_label\"])\n</code></pre>"},{"location":"api/addons/attacks/dataset/labelflipping/#nebula.addons.attacks.dataset.labelflipping.LabelFlippingAttack.get_malicious_dataset","title":"<code>get_malicious_dataset()</code>","text":"<p>Creates a malicious dataset by flipping the labels of selected data points.</p> <p>Returns:</p> Name Type Description <code>Dataset</code> <p>The modified dataset with flipped labels.</p> Source code in <code>nebula/addons/attacks/dataset/labelflipping.py</code> <pre><code>def get_malicious_dataset(self):\n    \"\"\"\n    Creates a malicious dataset by flipping the labels of selected data points.\n\n    Returns:\n        Dataset: The modified dataset with flipped labels.\n    \"\"\"\n    return self.labelFlipping(\n        self.datamodule.train_set,\n        self.datamodule.train_set_indices,\n        self.poisoned_percent,\n        self.targeted,\n        self.target_label,\n        self.target_changed_label,\n    )\n</code></pre>"},{"location":"api/addons/attacks/dataset/labelflipping/#nebula.addons.attacks.dataset.labelflipping.LabelFlippingAttack.labelFlipping","title":"<code>labelFlipping(dataset, indices, poisoned_percent=0, targeted=False, target_label=4, target_changed_label=7)</code>","text":"<p>Flips the labels of a specified portion of a dataset to random values or to a specific target label.</p> <p>This function modifies the labels of selected samples in the dataset based on the specified poisoning percentage. Labels can be flipped either randomly or targeted to change from a specific label to another specified label.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>The dataset containing training data, expected to be a PyTorch dataset                with a <code>.targets</code> attribute.</p> required <code>indices</code> <code>list of int</code> <p>The list of indices in the dataset to consider for label flipping.</p> required <code>poisoned_percent</code> <code>float</code> <p>The ratio of labels to change, expressed as a fraction                                 (0 &lt;= poisoned_percent &lt;= 1). Default is 0.</p> <code>0</code> <code>targeted</code> <code>bool</code> <p>If True, flips only labels matching <code>target_label</code> to <code>target_changed_label</code>.                        Default is False.</p> <code>False</code> <code>target_label</code> <code>int</code> <p>The label to change when <code>targeted</code> is True. Default is 4.</p> <code>4</code> <code>target_changed_label</code> <code>int</code> <p>The label to which <code>target_label</code> will be changed. Default is 7.</p> <code>7</code> <p>Returns:</p> Name Type Description <code>Dataset</code> <p>A deep copy of the original dataset with modified labels in <code>.targets</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>poisoned_percent</code> is not between 0 and 1, or if <code>flipping_percent</code> is invalid.</p> Notes <ul> <li>When not in targeted mode, labels are flipped for a random selection of indices based on the specified   <code>poisoned_percent</code>. The new label is chosen randomly from the existing classes.</li> <li>In targeted mode, labels that match <code>target_label</code> are directly changed to <code>target_changed_label</code>.</li> </ul> Source code in <code>nebula/addons/attacks/dataset/labelflipping.py</code> <pre><code>def labelFlipping(\n    self,\n    dataset,\n    indices,\n    poisoned_percent=0,\n    targeted=False,\n    target_label=4,\n    target_changed_label=7,\n):\n    \"\"\"\n    Flips the labels of a specified portion of a dataset to random values or to a specific target label.\n\n    This function modifies the labels of selected samples in the dataset based on the specified\n    poisoning percentage. Labels can be flipped either randomly or targeted to change from a specific\n    label to another specified label.\n\n    Args:\n        dataset (Dataset): The dataset containing training data, expected to be a PyTorch dataset\n                           with a `.targets` attribute.\n        indices (list of int): The list of indices in the dataset to consider for label flipping.\n        poisoned_percent (float, optional): The ratio of labels to change, expressed as a fraction\n                                            (0 &lt;= poisoned_percent &lt;= 1). Default is 0.\n        targeted (bool, optional): If True, flips only labels matching `target_label` to `target_changed_label`.\n                                   Default is False.\n        target_label (int, optional): The label to change when `targeted` is True. Default is 4.\n        target_changed_label (int, optional): The label to which `target_label` will be changed. Default is 7.\n\n    Returns:\n        Dataset: A deep copy of the original dataset with modified labels in `.targets`.\n\n    Raises:\n        ValueError: If `poisoned_percent` is not between 0 and 1, or if `flipping_percent` is invalid.\n\n    Notes:\n        - When not in targeted mode, labels are flipped for a random selection of indices based on the specified\n          `poisoned_percent`. The new label is chosen randomly from the existing classes.\n        - In targeted mode, labels that match `target_label` are directly changed to `target_changed_label`.\n    \"\"\"\n    new_dataset = copy.deepcopy(dataset)\n\n    targets = torch.tensor(new_dataset.targets) if isinstance(new_dataset.targets, list) else new_dataset.targets\n\n    num_indices = len(indices)\n    class_list = list(set(targets.tolist()))\n    if not targeted:\n        num_flipped = int(poisoned_percent * num_indices)\n        if num_indices == 0:\n            return new_dataset\n        if num_flipped &gt; num_indices:\n            return new_dataset\n        flipped_indice = random.sample(indices, num_flipped)\n\n        for i in flipped_indice:\n            t = targets[i]\n            flipped = torch.tensor(random.sample(class_list, 1)[0])\n            while t == flipped:\n                flipped = torch.tensor(random.sample(class_list, 1)[0])\n            targets[i] = flipped\n    else:\n        for i in indices:\n            if int(targets[i]) == int(target_label):\n                targets[i] = torch.tensor(target_changed_label)\n    new_dataset.targets = targets\n    return new_dataset\n</code></pre>"},{"location":"api/addons/attacks/model/","title":"Documentation for Model Module","text":""},{"location":"api/addons/attacks/model/gllneuroninversion/","title":"Documentation for Gllneuroninversion Module","text":""},{"location":"api/addons/attacks/model/gllneuroninversion/#nebula.addons.attacks.model.gllneuroninversion.GLLNeuronInversionAttack","title":"<code>GLLNeuronInversionAttack</code>","text":"<p>               Bases: <code>ModelAttack</code></p> <p>Implements a neuron inversion attack on the received model weights.</p> <p>This attack aims to invert the values of neurons in specific layers by replacing their values with random noise, potentially disrupting the model's functionality during aggregation.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>object</code> <p>The training engine object that manages the aggregator.</p> required <code>_</code> <code>any</code> <p>A placeholder argument (not used in this class).</p> required Source code in <code>nebula/addons/attacks/model/gllneuroninversion.py</code> <pre><code>class GLLNeuronInversionAttack(ModelAttack):\n    \"\"\"\n    Implements a neuron inversion attack on the received model weights.\n\n    This attack aims to invert the values of neurons in specific layers\n    by replacing their values with random noise, potentially disrupting the model's\n    functionality during aggregation.\n\n    Args:\n        engine (object): The training engine object that manages the aggregator.\n        _ (any): A placeholder argument (not used in this class).\n    \"\"\"\n\n    def __init__(self, engine, attack_params):\n        \"\"\"\n        Initializes the GLLNeuronInversionAttack with the specified engine.\n\n        Args:\n            engine (object): The training engine object.\n            _ (any): A placeholder argument (not used in this class).\n        \"\"\"\n        try:\n            round_start = int(attack_params[\"round_start_attack\"])\n            round_stop = int(attack_params[\"round_stop_attack\"])\n            attack_interval = int(attack_params[\"attack_interval\"])\n        except KeyError as e:\n            raise ValueError(f\"Missing required attack parameter: {e}\")\n        except ValueError:\n            raise ValueError(\"Invalid value in attack_params. Ensure all values are integers.\")\n\n        super().__init__(engine, round_start, round_stop, attack_interval)\n\n    def model_attack(self, received_weights):\n        \"\"\"\n        Performs the neuron inversion attack by modifying the weights of a specific\n        layer with random noise.\n\n        This attack replaces the weights of a chosen layer with random values,\n        which may disrupt the functionality of the model.\n\n        Args:\n            received_weights (dict): The aggregated model weights to be modified.\n\n        Returns:\n            dict: The modified model weights after applying the neuron inversion attack.\n        \"\"\"\n        logging.info(\"[GLLNeuronInversionAttack] Performing neuron inversion attack\")\n        lkeys = list(received_weights.keys())\n        logging.info(f\"Layer inverted: {lkeys[-2]}\")\n        received_weights[lkeys[-2]].data = torch.rand(received_weights[lkeys[-2]].shape) * 10000\n        return received_weights\n</code></pre>"},{"location":"api/addons/attacks/model/gllneuroninversion/#nebula.addons.attacks.model.gllneuroninversion.GLLNeuronInversionAttack.__init__","title":"<code>__init__(engine, attack_params)</code>","text":"<p>Initializes the GLLNeuronInversionAttack with the specified engine.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>object</code> <p>The training engine object.</p> required <code>_</code> <code>any</code> <p>A placeholder argument (not used in this class).</p> required Source code in <code>nebula/addons/attacks/model/gllneuroninversion.py</code> <pre><code>def __init__(self, engine, attack_params):\n    \"\"\"\n    Initializes the GLLNeuronInversionAttack with the specified engine.\n\n    Args:\n        engine (object): The training engine object.\n        _ (any): A placeholder argument (not used in this class).\n    \"\"\"\n    try:\n        round_start = int(attack_params[\"round_start_attack\"])\n        round_stop = int(attack_params[\"round_stop_attack\"])\n        attack_interval = int(attack_params[\"attack_interval\"])\n    except KeyError as e:\n        raise ValueError(f\"Missing required attack parameter: {e}\")\n    except ValueError:\n        raise ValueError(\"Invalid value in attack_params. Ensure all values are integers.\")\n\n    super().__init__(engine, round_start, round_stop, attack_interval)\n</code></pre>"},{"location":"api/addons/attacks/model/gllneuroninversion/#nebula.addons.attacks.model.gllneuroninversion.GLLNeuronInversionAttack.model_attack","title":"<code>model_attack(received_weights)</code>","text":"<p>Performs the neuron inversion attack by modifying the weights of a specific layer with random noise.</p> <p>This attack replaces the weights of a chosen layer with random values, which may disrupt the functionality of the model.</p> <p>Parameters:</p> Name Type Description Default <code>received_weights</code> <code>dict</code> <p>The aggregated model weights to be modified.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>The modified model weights after applying the neuron inversion attack.</p> Source code in <code>nebula/addons/attacks/model/gllneuroninversion.py</code> <pre><code>def model_attack(self, received_weights):\n    \"\"\"\n    Performs the neuron inversion attack by modifying the weights of a specific\n    layer with random noise.\n\n    This attack replaces the weights of a chosen layer with random values,\n    which may disrupt the functionality of the model.\n\n    Args:\n        received_weights (dict): The aggregated model weights to be modified.\n\n    Returns:\n        dict: The modified model weights after applying the neuron inversion attack.\n    \"\"\"\n    logging.info(\"[GLLNeuronInversionAttack] Performing neuron inversion attack\")\n    lkeys = list(received_weights.keys())\n    logging.info(f\"Layer inverted: {lkeys[-2]}\")\n    received_weights[lkeys[-2]].data = torch.rand(received_weights[lkeys[-2]].shape) * 10000\n    return received_weights\n</code></pre>"},{"location":"api/addons/attacks/model/modelattack/","title":"Documentation for Modelattack Module","text":""},{"location":"api/addons/attacks/model/modelattack/#nebula.addons.attacks.model.modelattack.ModelAttack","title":"<code>ModelAttack</code>","text":"<p>               Bases: <code>Attack</code></p> <p>Base class for implementing model attacks, which modify the behavior of model aggregation methods.</p> <p>This class defines a decorator for introducing malicious behavior into the aggregation process and requires subclasses to implement the model-specific attack logic.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>object</code> <p>The engine object that manages the aggregator for              model aggregation.</p> required Source code in <code>nebula/addons/attacks/model/modelattack.py</code> <pre><code>class ModelAttack(Attack):\n    \"\"\"\n    Base class for implementing model attacks, which modify the behavior of\n    model aggregation methods.\n\n    This class defines a decorator for introducing malicious behavior into the\n    aggregation process and requires subclasses to implement the model-specific\n    attack logic.\n\n    Args:\n        engine (object): The engine object that manages the aggregator for\n                         model aggregation.\n    \"\"\"\n\n    def __init__(self, engine, round_start_attack, round_stop_attack, attack_interval):\n        \"\"\"\n        Initializes the ModelAttack with the specified engine.\n\n        Args:\n            engine (object): The engine object that includes the aggregator.\n        \"\"\"\n        super().__init__()\n        self.engine = engine\n        self.aggregator = engine._aggregator\n        self.original_aggregation = engine.aggregator.run_aggregation\n        self.round_start_attack = round_start_attack\n        self.round_stop_attack = round_stop_attack\n        self.attack_interval = attack_interval\n\n    def aggregator_decorator(self):\n        \"\"\"\n        Decorator that adds a delay to the execution of the original method.\n\n        Args:\n            delay (int or float): The time in seconds to delay the method execution.\n\n        Returns:\n            function: A decorator function that wraps the target method with\n                      the delay logic and potentially modifies the aggregation\n                      behavior to inject malicious changes.\n        \"\"\"\n\n        # The actual decorator function that will be applied to the target method\n        def decorator(func):\n            @wraps(func)  # Preserves the metadata of the original function\n            def wrapper(*args):\n                _, *new_args = args  # Exclude self argument\n                accum = func(*new_args)\n                logging.info(f\"malicious_aggregate | original aggregation result={accum}\")\n\n                if new_args is not None:\n                    accum = self.model_attack(accum)\n                    logging.info(f\"malicious_aggregate | attack aggregation result={accum}\")\n                return accum\n\n            return wrapper\n\n        return decorator\n\n    @abstractmethod\n    def model_attack(self, received_weights):\n        \"\"\"\n        Abstract method that applies the specific model attack logic.\n\n        This method should be implemented in subclasses to define the attack\n        logic on the received model weights.\n\n        Args:\n            received_weights (any): The aggregated model weights to be modified.\n\n        Returns:\n            any: The modified model weights after applying the attack.\n        \"\"\"\n        raise NotImplementedError\n\n    async def _inject_malicious_behaviour(self):\n        \"\"\"\n        Modifies the `propagate` method of the aggregator to include the delay\n        introduced by the decorator.\n\n        This method wraps the original aggregation method with the malicious\n        decorator to inject the attack behavior into the aggregation process.\n        \"\"\"\n        decorated_aggregation = self.aggregator_decorator()(self.aggregator.run_aggregation)\n        self.aggregator.run_aggregation = types.MethodType(decorated_aggregation, self.aggregator)\n\n    async def _restore_original_behaviour(self):\n        \"\"\"\n        Restores the original behaviour of the `run_aggregation` method.\n        \"\"\"\n        self.aggregator.run_aggregation = self.original_aggregation\n\n    async def attack(self):\n        \"\"\"\n        Initiates the malicious attack by injecting the malicious behavior\n        into the aggregation process.\n\n        This method logs the attack and calls the method to modify the aggregator.\n        \"\"\"\n        if self.engine.round not in range(self.round_start_attack, self.round_stop_attack + 1):\n            pass\n        elif self.engine.round == self.round_stop_attack:\n            logging.info(f\"[{self.__class__.__name__}] Stopping attack\")\n            await self._restore_original_behaviour()\n        elif (self.engine.round == self.round_start_attack) or ((self.engine.round - self.round_start_attack) % self.attack_interval == 0):\n            logging.info(f\"[{self.__class__.__name__}] Performing attack\")\n            await self._inject_malicious_behaviour()\n        else:\n            await self._restore_original_behaviour()\n</code></pre>"},{"location":"api/addons/attacks/model/modelattack/#nebula.addons.attacks.model.modelattack.ModelAttack.__init__","title":"<code>__init__(engine, round_start_attack, round_stop_attack, attack_interval)</code>","text":"<p>Initializes the ModelAttack with the specified engine.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>object</code> <p>The engine object that includes the aggregator.</p> required Source code in <code>nebula/addons/attacks/model/modelattack.py</code> <pre><code>def __init__(self, engine, round_start_attack, round_stop_attack, attack_interval):\n    \"\"\"\n    Initializes the ModelAttack with the specified engine.\n\n    Args:\n        engine (object): The engine object that includes the aggregator.\n    \"\"\"\n    super().__init__()\n    self.engine = engine\n    self.aggregator = engine._aggregator\n    self.original_aggregation = engine.aggregator.run_aggregation\n    self.round_start_attack = round_start_attack\n    self.round_stop_attack = round_stop_attack\n    self.attack_interval = attack_interval\n</code></pre>"},{"location":"api/addons/attacks/model/modelattack/#nebula.addons.attacks.model.modelattack.ModelAttack.aggregator_decorator","title":"<code>aggregator_decorator()</code>","text":"<p>Decorator that adds a delay to the execution of the original method.</p> <p>Parameters:</p> Name Type Description Default <code>delay</code> <code>int or float</code> <p>The time in seconds to delay the method execution.</p> required <p>Returns:</p> Name Type Description <code>function</code> <p>A decorator function that wraps the target method with       the delay logic and potentially modifies the aggregation       behavior to inject malicious changes.</p> Source code in <code>nebula/addons/attacks/model/modelattack.py</code> <pre><code>def aggregator_decorator(self):\n    \"\"\"\n    Decorator that adds a delay to the execution of the original method.\n\n    Args:\n        delay (int or float): The time in seconds to delay the method execution.\n\n    Returns:\n        function: A decorator function that wraps the target method with\n                  the delay logic and potentially modifies the aggregation\n                  behavior to inject malicious changes.\n    \"\"\"\n\n    # The actual decorator function that will be applied to the target method\n    def decorator(func):\n        @wraps(func)  # Preserves the metadata of the original function\n        def wrapper(*args):\n            _, *new_args = args  # Exclude self argument\n            accum = func(*new_args)\n            logging.info(f\"malicious_aggregate | original aggregation result={accum}\")\n\n            if new_args is not None:\n                accum = self.model_attack(accum)\n                logging.info(f\"malicious_aggregate | attack aggregation result={accum}\")\n            return accum\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"api/addons/attacks/model/modelattack/#nebula.addons.attacks.model.modelattack.ModelAttack.attack","title":"<code>attack()</code>  <code>async</code>","text":"<p>Initiates the malicious attack by injecting the malicious behavior into the aggregation process.</p> <p>This method logs the attack and calls the method to modify the aggregator.</p> Source code in <code>nebula/addons/attacks/model/modelattack.py</code> <pre><code>async def attack(self):\n    \"\"\"\n    Initiates the malicious attack by injecting the malicious behavior\n    into the aggregation process.\n\n    This method logs the attack and calls the method to modify the aggregator.\n    \"\"\"\n    if self.engine.round not in range(self.round_start_attack, self.round_stop_attack + 1):\n        pass\n    elif self.engine.round == self.round_stop_attack:\n        logging.info(f\"[{self.__class__.__name__}] Stopping attack\")\n        await self._restore_original_behaviour()\n    elif (self.engine.round == self.round_start_attack) or ((self.engine.round - self.round_start_attack) % self.attack_interval == 0):\n        logging.info(f\"[{self.__class__.__name__}] Performing attack\")\n        await self._inject_malicious_behaviour()\n    else:\n        await self._restore_original_behaviour()\n</code></pre>"},{"location":"api/addons/attacks/model/modelattack/#nebula.addons.attacks.model.modelattack.ModelAttack.model_attack","title":"<code>model_attack(received_weights)</code>  <code>abstractmethod</code>","text":"<p>Abstract method that applies the specific model attack logic.</p> <p>This method should be implemented in subclasses to define the attack logic on the received model weights.</p> <p>Parameters:</p> Name Type Description Default <code>received_weights</code> <code>any</code> <p>The aggregated model weights to be modified.</p> required <p>Returns:</p> Name Type Description <code>any</code> <p>The modified model weights after applying the attack.</p> Source code in <code>nebula/addons/attacks/model/modelattack.py</code> <pre><code>@abstractmethod\ndef model_attack(self, received_weights):\n    \"\"\"\n    Abstract method that applies the specific model attack logic.\n\n    This method should be implemented in subclasses to define the attack\n    logic on the received model weights.\n\n    Args:\n        received_weights (any): The aggregated model weights to be modified.\n\n    Returns:\n        any: The modified model weights after applying the attack.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/addons/attacks/model/modelpoison/","title":"Documentation for Modelpoison Module","text":"<p>This module provides a function for adding noise to a machine learning model's parameters, simulating data poisoning attacks. The main function allows for the injection of various types of noise into the model parameters, effectively altering them to test the model's robustness against malicious manipulations.</p> <p>Function: - modelpoison: Modifies the parameters of a model by injecting noise according to a specified ratio   and type of noise (e.g., Gaussian, salt, salt-and-pepper).</p>"},{"location":"api/addons/attacks/model/modelpoison/#nebula.addons.attacks.model.modelpoison.ModelPoisonAttack","title":"<code>ModelPoisonAttack</code>","text":"<p>               Bases: <code>ModelAttack</code></p> <p>Implements a model poisoning attack by modifying the received model weights during the aggregation process.</p> <p>This attack introduces specific modifications to the model weights to influence the global model's behavior.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>object</code> <p>The training engine object that manages the aggregator.</p> required <code>attack_params</code> <code>dict</code> <p>Parameters for the attack, including: - poisoned_ratio (float): The ratio of model weights to be poisoned. - noise_type (str): The type of noise to introduce during the attack.</p> required Source code in <code>nebula/addons/attacks/model/modelpoison.py</code> <pre><code>class ModelPoisonAttack(ModelAttack):\n    \"\"\"\n    Implements a model poisoning attack by modifying the received model weights\n    during the aggregation process.\n\n    This attack introduces specific modifications to the model weights to\n    influence the global model's behavior.\n\n    Args:\n        engine (object): The training engine object that manages the aggregator.\n        attack_params (dict): Parameters for the attack, including:\n            - poisoned_ratio (float): The ratio of model weights to be poisoned.\n            - noise_type (str): The type of noise to introduce during the attack.\n    \"\"\"\n\n    def __init__(self, engine, attack_params):\n        \"\"\"\n        Initializes the ModelPoisonAttack with the specified engine and parameters.\n\n        Args:\n            engine (object): The training engine object.\n            attack_params (dict): Dictionary of attack parameters.\n        \"\"\"\n        try:\n            round_start = int(attack_params[\"round_start_attack\"])\n            round_stop = int(attack_params[\"round_stop_attack\"])\n            attack_interval = int(attack_params[\"attack_interval\"])\n        except KeyError as e:\n            raise ValueError(f\"Missing required attack parameter: {e}\")\n        except ValueError:\n            raise ValueError(\"Invalid value in attack_params. Ensure all values are integers.\")\n\n        super().__init__(engine, round_start, round_stop, attack_interval)\n\n        self.poisoned_ratio = float(attack_params[\"poisoned_ratio\"])\n        self.noise_type = attack_params[\"noise_type\"].lower()\n\n    def modelPoison(self, model: OrderedDict, poisoned_ratio, noise_type=\"gaussian\"):\n        \"\"\"\n        Adds random noise to the parameters of a model for the purpose of data poisoning.\n\n        This function modifies the model's parameters by injecting noise according to the specified\n        noise type and ratio. Various types of noise can be applied, including salt noise, Gaussian\n        noise, and salt-and-pepper noise.\n\n        Args:\n            model (OrderedDict): The model's parameters organized as an `OrderedDict`. Each key corresponds\n                                 to a layer, and each value is a tensor representing the parameters of that layer.\n            poisoned_ratio (float): The proportion of noise to apply, expressed as a fraction (0 &lt;= poisoned_ratio &lt;= 1).\n            noise_type (str, optional): The type of noise to apply to the model parameters. Supported types are:\n                                        - \"salt\": Applies salt noise, replacing random elements with 1.\n                                        - \"gaussian\": Applies Gaussian-distributed additive noise.\n                                        - \"s&amp;p\": Applies salt-and-pepper noise, replacing random elements with either 1 or low_val.\n                                        Default is \"gaussian\".\n\n        Returns:\n            OrderedDict: A new `OrderedDict` containing the model parameters with noise added.\n\n        Raises:\n            ValueError: If `poisoned_ratio` is not between 0 and 1, or if `noise_type` is unsupported.\n\n        Notes:\n            - If a layer's tensor is a single point (0-dimensional), it will be reshaped for processing.\n            - Unsupported noise types will result in an error message, and the original tensor will be retained.\n        \"\"\"\n        poisoned_model = OrderedDict()\n        if not isinstance(noise_type, str):\n            noise_type = noise_type[0]\n\n        for layer in model:\n            bt = model[layer]\n            t = bt.detach().clone()\n            single_point = False\n            if len(t.shape) == 0:\n                t = t.view(-1)\n                single_point = True\n            # print(t)\n            if noise_type == \"salt\":\n                # Replaces random pixels with 1.\n                poisoned = torch.tensor(random_noise(t, mode=noise_type, amount=poisoned_ratio))\n            elif noise_type == \"gaussian\":\n                # Gaussian-distributed additive noise.\n                poisoned = torch.tensor(random_noise(t, mode=noise_type, mean=0, var=poisoned_ratio, clip=True))\n            elif noise_type == \"s&amp;p\":\n                # Replaces random pixels with either 1 or low_val, where low_val is 0 for unsigned images or -1 for signed images.\n                poisoned = torch.tensor(random_noise(t, mode=noise_type, amount=poisoned_ratio))\n            else:\n                print(\"ERROR: poison attack type not supported.\")\n                poisoned = t\n            if single_point:\n                poisoned = poisoned[0]\n            poisoned_model[layer] = poisoned\n\n        return poisoned_model\n\n    def model_attack(self, received_weights):\n        \"\"\"\n        Applies the model poisoning attack by modifying the received model weights.\n\n        Args:\n            received_weights (any): The aggregated model weights to be poisoned.\n\n        Returns:\n            any: The modified model weights after applying the poisoning attack.\n        \"\"\"\n        logging.info(\"[ModelPoisonAttack] Performing model poison attack\")\n        received_weights = self.modelPoison(received_weights, self.poisoned_ratio, self.noise_type)\n        return received_weights\n</code></pre>"},{"location":"api/addons/attacks/model/modelpoison/#nebula.addons.attacks.model.modelpoison.ModelPoisonAttack.__init__","title":"<code>__init__(engine, attack_params)</code>","text":"<p>Initializes the ModelPoisonAttack with the specified engine and parameters.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>object</code> <p>The training engine object.</p> required <code>attack_params</code> <code>dict</code> <p>Dictionary of attack parameters.</p> required Source code in <code>nebula/addons/attacks/model/modelpoison.py</code> <pre><code>def __init__(self, engine, attack_params):\n    \"\"\"\n    Initializes the ModelPoisonAttack with the specified engine and parameters.\n\n    Args:\n        engine (object): The training engine object.\n        attack_params (dict): Dictionary of attack parameters.\n    \"\"\"\n    try:\n        round_start = int(attack_params[\"round_start_attack\"])\n        round_stop = int(attack_params[\"round_stop_attack\"])\n        attack_interval = int(attack_params[\"attack_interval\"])\n    except KeyError as e:\n        raise ValueError(f\"Missing required attack parameter: {e}\")\n    except ValueError:\n        raise ValueError(\"Invalid value in attack_params. Ensure all values are integers.\")\n\n    super().__init__(engine, round_start, round_stop, attack_interval)\n\n    self.poisoned_ratio = float(attack_params[\"poisoned_ratio\"])\n    self.noise_type = attack_params[\"noise_type\"].lower()\n</code></pre>"},{"location":"api/addons/attacks/model/modelpoison/#nebula.addons.attacks.model.modelpoison.ModelPoisonAttack.modelPoison","title":"<code>modelPoison(model, poisoned_ratio, noise_type='gaussian')</code>","text":"<p>Adds random noise to the parameters of a model for the purpose of data poisoning.</p> <p>This function modifies the model's parameters by injecting noise according to the specified noise type and ratio. Various types of noise can be applied, including salt noise, Gaussian noise, and salt-and-pepper noise.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>OrderedDict</code> <p>The model's parameters organized as an <code>OrderedDict</code>. Each key corresponds                  to a layer, and each value is a tensor representing the parameters of that layer.</p> required <code>poisoned_ratio</code> <code>float</code> <p>The proportion of noise to apply, expressed as a fraction (0 &lt;= poisoned_ratio &lt;= 1).</p> required <code>noise_type</code> <code>str</code> <p>The type of noise to apply to the model parameters. Supported types are:                         - \"salt\": Applies salt noise, replacing random elements with 1.                         - \"gaussian\": Applies Gaussian-distributed additive noise.                         - \"s&amp;p\": Applies salt-and-pepper noise, replacing random elements with either 1 or low_val.                         Default is \"gaussian\".</p> <code>'gaussian'</code> <p>Returns:</p> Name Type Description <code>OrderedDict</code> <p>A new <code>OrderedDict</code> containing the model parameters with noise added.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>poisoned_ratio</code> is not between 0 and 1, or if <code>noise_type</code> is unsupported.</p> Notes <ul> <li>If a layer's tensor is a single point (0-dimensional), it will be reshaped for processing.</li> <li>Unsupported noise types will result in an error message, and the original tensor will be retained.</li> </ul> Source code in <code>nebula/addons/attacks/model/modelpoison.py</code> <pre><code>def modelPoison(self, model: OrderedDict, poisoned_ratio, noise_type=\"gaussian\"):\n    \"\"\"\n    Adds random noise to the parameters of a model for the purpose of data poisoning.\n\n    This function modifies the model's parameters by injecting noise according to the specified\n    noise type and ratio. Various types of noise can be applied, including salt noise, Gaussian\n    noise, and salt-and-pepper noise.\n\n    Args:\n        model (OrderedDict): The model's parameters organized as an `OrderedDict`. Each key corresponds\n                             to a layer, and each value is a tensor representing the parameters of that layer.\n        poisoned_ratio (float): The proportion of noise to apply, expressed as a fraction (0 &lt;= poisoned_ratio &lt;= 1).\n        noise_type (str, optional): The type of noise to apply to the model parameters. Supported types are:\n                                    - \"salt\": Applies salt noise, replacing random elements with 1.\n                                    - \"gaussian\": Applies Gaussian-distributed additive noise.\n                                    - \"s&amp;p\": Applies salt-and-pepper noise, replacing random elements with either 1 or low_val.\n                                    Default is \"gaussian\".\n\n    Returns:\n        OrderedDict: A new `OrderedDict` containing the model parameters with noise added.\n\n    Raises:\n        ValueError: If `poisoned_ratio` is not between 0 and 1, or if `noise_type` is unsupported.\n\n    Notes:\n        - If a layer's tensor is a single point (0-dimensional), it will be reshaped for processing.\n        - Unsupported noise types will result in an error message, and the original tensor will be retained.\n    \"\"\"\n    poisoned_model = OrderedDict()\n    if not isinstance(noise_type, str):\n        noise_type = noise_type[0]\n\n    for layer in model:\n        bt = model[layer]\n        t = bt.detach().clone()\n        single_point = False\n        if len(t.shape) == 0:\n            t = t.view(-1)\n            single_point = True\n        # print(t)\n        if noise_type == \"salt\":\n            # Replaces random pixels with 1.\n            poisoned = torch.tensor(random_noise(t, mode=noise_type, amount=poisoned_ratio))\n        elif noise_type == \"gaussian\":\n            # Gaussian-distributed additive noise.\n            poisoned = torch.tensor(random_noise(t, mode=noise_type, mean=0, var=poisoned_ratio, clip=True))\n        elif noise_type == \"s&amp;p\":\n            # Replaces random pixels with either 1 or low_val, where low_val is 0 for unsigned images or -1 for signed images.\n            poisoned = torch.tensor(random_noise(t, mode=noise_type, amount=poisoned_ratio))\n        else:\n            print(\"ERROR: poison attack type not supported.\")\n            poisoned = t\n        if single_point:\n            poisoned = poisoned[0]\n        poisoned_model[layer] = poisoned\n\n    return poisoned_model\n</code></pre>"},{"location":"api/addons/attacks/model/modelpoison/#nebula.addons.attacks.model.modelpoison.ModelPoisonAttack.model_attack","title":"<code>model_attack(received_weights)</code>","text":"<p>Applies the model poisoning attack by modifying the received model weights.</p> <p>Parameters:</p> Name Type Description Default <code>received_weights</code> <code>any</code> <p>The aggregated model weights to be poisoned.</p> required <p>Returns:</p> Name Type Description <code>any</code> <p>The modified model weights after applying the poisoning attack.</p> Source code in <code>nebula/addons/attacks/model/modelpoison.py</code> <pre><code>def model_attack(self, received_weights):\n    \"\"\"\n    Applies the model poisoning attack by modifying the received model weights.\n\n    Args:\n        received_weights (any): The aggregated model weights to be poisoned.\n\n    Returns:\n        any: The modified model weights after applying the poisoning attack.\n    \"\"\"\n    logging.info(\"[ModelPoisonAttack] Performing model poison attack\")\n    received_weights = self.modelPoison(received_weights, self.poisoned_ratio, self.noise_type)\n    return received_weights\n</code></pre>"},{"location":"api/addons/attacks/model/swappingweights/","title":"Documentation for Swappingweights Module","text":""},{"location":"api/addons/attacks/model/swappingweights/#nebula.addons.attacks.model.swappingweights.SwappingWeightsAttack","title":"<code>SwappingWeightsAttack</code>","text":"<p>               Bases: <code>ModelAttack</code></p> <p>Implements a swapping weights attack on the received model weights.</p> <p>This attack performs stochastic swapping of weights in a specified layer of the model, potentially disrupting its performance. The attack is not deterministic, and its performance can vary. The code may not work as expected for some layers due to reshaping, and its computational cost scales quadratically with the layer size. It should not be applied to the last layer, as it would make the attack detectable due to high loss on the malicious node.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>object</code> <p>The training engine object that manages the aggregator.</p> required <code>attack_params</code> <code>dict</code> <p>Parameters for the attack, including: - layer_idx (int): The index of the layer where the weights will be swapped.</p> required Source code in <code>nebula/addons/attacks/model/swappingweights.py</code> <pre><code>class SwappingWeightsAttack(ModelAttack):\n    \"\"\"\n    Implements a swapping weights attack on the received model weights.\n\n    This attack performs stochastic swapping of weights in a specified layer of the model,\n    potentially disrupting its performance. The attack is not deterministic, and its performance\n    can vary. The code may not work as expected for some layers due to reshaping, and its\n    computational cost scales quadratically with the layer size. It should not be applied to\n    the last layer, as it would make the attack detectable due to high loss on the malicious node.\n\n    Args:\n        engine (object): The training engine object that manages the aggregator.\n        attack_params (dict): Parameters for the attack, including:\n            - layer_idx (int): The index of the layer where the weights will be swapped.\n    \"\"\"\n\n    def __init__(self, engine, attack_params):\n        \"\"\"\n        Initializes the SwappingWeightsAttack with the specified engine and parameters.\n\n        Args:\n            engine (object): The training engine object.\n            attack_params (dict): Dictionary of attack parameters, including the layer index.\n        \"\"\"\n        try:\n            round_start = int(attack_params[\"round_start_attack\"])\n            round_stop = int(attack_params[\"round_stop_attack\"])\n            attack_interval = int(attack_params[\"attack_interval\"])\n        except KeyError as e:\n            raise ValueError(f\"Missing required attack parameter: {e}\")\n        except ValueError:\n            raise ValueError(\"Invalid value in attack_params. Ensure all values are integers.\")\n\n        super().__init__(engine, round_start, round_stop, attack_interval)\n\n        self.layer_idx = int(attack_params[\"layer_idx\"])\n\n    def model_attack(self, received_weights):\n        \"\"\"\n        Performs the swapping weights attack by computing a similarity matrix and\n        swapping the weights of a specified layer based on their similarity.\n\n        This method applies a greedy algorithm to swap weights in the selected layer\n        in a way that could potentially disrupt the training process. The attack also\n        ensures that some rows are fully permuted, and others are swapped based on\n        similarity.\n\n        Args:\n            received_weights (dict): The aggregated model weights to be modified.\n\n        Returns:\n            dict: The modified model weights after performing the swapping attack.\n        \"\"\"\n        logging.info(\"[SwappingWeightsAttack] Performing swapping weights attack\")\n        lkeys = list(received_weights.keys())\n        wm = received_weights[lkeys[self.layer_idx]]\n\n        # Compute similarity matrix\n        sm = torch.zeros((wm.shape[0], wm.shape[0]))\n        for j in range(wm.shape[0]):\n            sm[j] = pairwise_cosine_similarity(wm[j].reshape(1, -1), wm.reshape(wm.shape[0], -1))\n\n        # Check rows/cols where greedy approach is optimal\n        nsort = np.full(sm.shape[0], -1)\n        rows = []\n        for j in range(sm.shape[0]):\n            k = torch.argmin(sm[j])\n            if torch.argmin(sm[:, k]) == j:\n                nsort[j] = k\n                rows.append(j)\n        not_rows = np.array([i for i in range(sm.shape[0]) if i not in rows])\n\n        # Ensure the rest of the rows are fully permuted (not optimal, but good enough)\n        nrs = copy.deepcopy(not_rows)\n        nrs = np.random.permutation(nrs)\n        while np.any(nrs == not_rows):\n            nrs = np.random.permutation(nrs)\n        nsort[not_rows] = nrs\n        nsort = torch.tensor(nsort)\n\n        # Apply permutation to weights\n        received_weights[lkeys[self.layer_idx]] = received_weights[lkeys[self.layer_idx]][nsort]\n        received_weights[lkeys[self.layer_idx + 1]] = received_weights[lkeys[self.layer_idx + 1]][nsort]\n        if self.layer_idx + 2 &lt; len(lkeys):\n            received_weights[lkeys[self.layer_idx + 2]] = received_weights[lkeys[self.layer_idx + 2]][:, nsort]\n\n        return received_weights\n</code></pre>"},{"location":"api/addons/attacks/model/swappingweights/#nebula.addons.attacks.model.swappingweights.SwappingWeightsAttack.__init__","title":"<code>__init__(engine, attack_params)</code>","text":"<p>Initializes the SwappingWeightsAttack with the specified engine and parameters.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>object</code> <p>The training engine object.</p> required <code>attack_params</code> <code>dict</code> <p>Dictionary of attack parameters, including the layer index.</p> required Source code in <code>nebula/addons/attacks/model/swappingweights.py</code> <pre><code>def __init__(self, engine, attack_params):\n    \"\"\"\n    Initializes the SwappingWeightsAttack with the specified engine and parameters.\n\n    Args:\n        engine (object): The training engine object.\n        attack_params (dict): Dictionary of attack parameters, including the layer index.\n    \"\"\"\n    try:\n        round_start = int(attack_params[\"round_start_attack\"])\n        round_stop = int(attack_params[\"round_stop_attack\"])\n        attack_interval = int(attack_params[\"attack_interval\"])\n    except KeyError as e:\n        raise ValueError(f\"Missing required attack parameter: {e}\")\n    except ValueError:\n        raise ValueError(\"Invalid value in attack_params. Ensure all values are integers.\")\n\n    super().__init__(engine, round_start, round_stop, attack_interval)\n\n    self.layer_idx = int(attack_params[\"layer_idx\"])\n</code></pre>"},{"location":"api/addons/attacks/model/swappingweights/#nebula.addons.attacks.model.swappingweights.SwappingWeightsAttack.model_attack","title":"<code>model_attack(received_weights)</code>","text":"<p>Performs the swapping weights attack by computing a similarity matrix and swapping the weights of a specified layer based on their similarity.</p> <p>This method applies a greedy algorithm to swap weights in the selected layer in a way that could potentially disrupt the training process. The attack also ensures that some rows are fully permuted, and others are swapped based on similarity.</p> <p>Parameters:</p> Name Type Description Default <code>received_weights</code> <code>dict</code> <p>The aggregated model weights to be modified.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>The modified model weights after performing the swapping attack.</p> Source code in <code>nebula/addons/attacks/model/swappingweights.py</code> <pre><code>def model_attack(self, received_weights):\n    \"\"\"\n    Performs the swapping weights attack by computing a similarity matrix and\n    swapping the weights of a specified layer based on their similarity.\n\n    This method applies a greedy algorithm to swap weights in the selected layer\n    in a way that could potentially disrupt the training process. The attack also\n    ensures that some rows are fully permuted, and others are swapped based on\n    similarity.\n\n    Args:\n        received_weights (dict): The aggregated model weights to be modified.\n\n    Returns:\n        dict: The modified model weights after performing the swapping attack.\n    \"\"\"\n    logging.info(\"[SwappingWeightsAttack] Performing swapping weights attack\")\n    lkeys = list(received_weights.keys())\n    wm = received_weights[lkeys[self.layer_idx]]\n\n    # Compute similarity matrix\n    sm = torch.zeros((wm.shape[0], wm.shape[0]))\n    for j in range(wm.shape[0]):\n        sm[j] = pairwise_cosine_similarity(wm[j].reshape(1, -1), wm.reshape(wm.shape[0], -1))\n\n    # Check rows/cols where greedy approach is optimal\n    nsort = np.full(sm.shape[0], -1)\n    rows = []\n    for j in range(sm.shape[0]):\n        k = torch.argmin(sm[j])\n        if torch.argmin(sm[:, k]) == j:\n            nsort[j] = k\n            rows.append(j)\n    not_rows = np.array([i for i in range(sm.shape[0]) if i not in rows])\n\n    # Ensure the rest of the rows are fully permuted (not optimal, but good enough)\n    nrs = copy.deepcopy(not_rows)\n    nrs = np.random.permutation(nrs)\n    while np.any(nrs == not_rows):\n        nrs = np.random.permutation(nrs)\n    nsort[not_rows] = nrs\n    nsort = torch.tensor(nsort)\n\n    # Apply permutation to weights\n    received_weights[lkeys[self.layer_idx]] = received_weights[lkeys[self.layer_idx]][nsort]\n    received_weights[lkeys[self.layer_idx + 1]] = received_weights[lkeys[self.layer_idx + 1]][nsort]\n    if self.layer_idx + 2 &lt; len(lkeys):\n        received_weights[lkeys[self.layer_idx + 2]] = received_weights[lkeys[self.layer_idx + 2]][:, nsort]\n\n    return received_weights\n</code></pre>"},{"location":"api/addons/blockchain/","title":"Documentation for Blockchain Module","text":""},{"location":"api/addons/blockchain/blockchain_deployer/","title":"Documentation for Blockchain_deployer Module","text":""},{"location":"api/addons/blockchain/blockchain_deployer/#nebula.addons.blockchain.blockchain_deployer.BlockchainDeployer","title":"<code>BlockchainDeployer</code>","text":"<p>Creates files (docker-compose.yaml and genesis.json) for deploying blockchain network</p> Source code in <code>nebula/addons/blockchain/blockchain_deployer.py</code> <pre><code>class BlockchainDeployer:\n    \"\"\"\n    Creates files (docker-compose.yaml and genesis.json) for deploying blockchain network\n    \"\"\"\n\n    def __init__(self, n_validator=3, config_dir=\".\", input_dir=\".\"):\n        # root dir of blockchain folder\n        self.__input_dir = input_dir\n\n        # config folder for storing generated files for deployment\n        self.__config_dir = config_dir\n\n        # random but static id of boot node to be assigned to all other nodes\n        self.__boot_id = None\n\n        # ip address of boot node (needs to be static)\n        self.__boot_ip = \"172.25.0.101\"\n\n        # ip address of non-validator node (needs to be static)\n        self.__rpc_ip = \"172.25.0.104\"\n\n        # ip address of oracle (needs to be static)\n        self.__oracle_ip = \"172.25.0.105\"\n\n        # temporary yaml parameter to store config before dump\n        self.__yaml = \"\"\n\n        # list of reserved addresses which need to be excluded in random address generation\n        self.__reserved_addresses = set()\n\n        # load original genesis dict\n        self.__genesis = self.__load_genesis()\n\n        # create blockchain directory in scenario's config directory\n        self.__setup_dir()\n\n        # add a boot node to the yaml file\n        self.__add_boot_node()\n\n        # add n validator nodes to the genesis.json and yaml file\n        self.__add_validator(n_validator)\n\n        # add non-validator node to the yaml file\n        self.__add_rpc()\n\n        # add oracle node to the genesis.json and yaml file\n        self.__add_oracle()\n\n        # dump config files into scenario's config directory\n        self.__export_config()\n\n    def __setup_dir(self) -&gt; None:\n        if not os.path.exists(self.__config_dir):\n            os.makedirs(self.__config_dir, exist_ok=True)\n\n    def __get_unreserved_address(self) -&gt; tuple[int, int]:\n        \"\"\"\n        Computes a randomized port and last 8 bits of an ip address, where both are not yet used\n        Returns: Randomized and unreserved lat 8 bit of ip and port\n\n        \"\"\"\n\n        # extract reserved ports and ip addresses\n        reserved_ips = [address[0] for address in self.__reserved_addresses]\n        reserved_ports = [address[1] for address in self.__reserved_addresses]\n\n        # get randomized ip and port in range still unreserved\n        ip = random.choice([number for number in range(10, 254) if number not in reserved_ips])\n        port = random.choice([number for number in range(30310, 30360) if number not in reserved_ports])\n\n        # add network address to list of reserved addresses\n        self.__reserved_addresses.add((ip, port))\n        return ip, port\n\n    def __copy_dir(self, source_path) -&gt; None:\n        \"\"\"\n        Copy blockchain folder with current files such as chaincode to config folder\n        Args:\n            source_path: Path of dir to copy\n\n        Returns: None\n\n        \"\"\"\n\n        curr_path = os.path.dirname(os.path.abspath(__file__))\n\n        if not os.path.exists(self.__config_dir):\n            os.makedirs(self.__config_dir, exist_ok=True)\n\n        target_dir = os.path.join(self.__config_dir, source_path)\n        source_dir = os.path.join(curr_path, source_path)\n        shutil.copytree(str(source_dir), target_dir, dirs_exist_ok=True)\n\n    @staticmethod\n    def __load_genesis() -&gt; dict[str, int | str | dict]:\n        \"\"\"\n        Load original genesis config\n        Returns: Genesis json dict\n\n        \"\"\"\n        return {\n            \"config\": {\n                \"chainId\": 19265019,  # unique id not used by any public Ethereum network\n                # block number at which the defined EIP hard fork policies are applied\n                \"homesteadBlock\": 0,\n                \"eip150Block\": 0,\n                \"eip155Block\": 0,\n                \"eip158Block\": 0,\n                \"byzantiumBlock\": 0,\n                \"constantinopleBlock\": 0,\n                \"petersburgBlock\": 0,\n                \"istanbulBlock\": 0,\n                \"muirGlacierBlock\": 0,\n                \"berlinBlock\": 0,\n                # Proof-of-Authority settings\n                \"clique\": {\n                    \"period\": 1,\n                    \"epoch\": 10000,\n                },  # block time (time in seconds between two blocks)  # number of blocks after reset the pending votes\n            },\n            # unique continuous id of transactions used by PoA\n            \"nonce\": \"0x0\",\n            # UNIX timestamp of block creation\n            \"timestamp\": \"0x5a8efd25\",\n            # strictly formated string containing all public wallet addresses of all validators (PoA)\n            # will be replaced by public addresses of randomly generated validator node\n            \"extraData\": \"0x0000000000000000000000000000000000000000000000000000000000000000187c1c14c75bA185A59c621Fbe5dda26D488852DF20C144e8aE3e1aCF7071C4883B759D1B428e7930000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\",\n            # maximum gas (computational cost) per transaction\n            \"gasLimit\": \"9000000000000\",  # \"8000000\" is default for Ethereum but too low for heavy load\n            # difficulty for PoW\n            \"difficulty\": \"0x1\",\n            # root hash of block\n            \"mixHash\": \"0x0000000000000000000000000000000000000000000000000000000000000000\",\n            # validator of genesis block\n            \"coinbase\": \"0x0000000000000000000000000000000000000000\",\n            # prefunded public wallet addresses (Oracle)\n            \"alloc\": {\n                # will be replaced by Oracle's randomized address\n                \"0x61DE01FcD560da4D6e05E58bCD34C8Dc92CE36D1\": {\n                    \"balance\": \"0x200000000000000000000000000000000000000000000000000000000000000\"\n                }\n            },\n            # block number of genesis block\n            \"number\": \"0x0\",\n            # gas used to validate genesis block\n            \"gasUsed\": \"0x0\",\n            # hash of parent block (0x0 since first block)\n            \"parentHash\": \"0x0000000000000000000000000000000000000000000000000000000000000000\",\n        }\n\n    def __add_boot_node(self) -&gt; None:\n        \"\"\"\n        Adds boot node to docker-compose.yaml\n        Returns: None\n\n        \"\"\"\n\n        # create random private key and create account from it\n        acc = w3.eth.account.create()\n\n        # store id of boot node to be inserted into all other nodes\n        self.__boot_id = str(keys.PrivateKey(acc.key).public_key)[2:]\n\n        # add service to yaml string\n        self.__yaml += textwrap.dedent(\n            f\"\"\"\n            geth-bootnode:\n                hostname: geth-bootnode\n                environment:\n                  - nodekeyhex={w3.to_hex(acc.key)[2:]}\n                build:\n                  dockerfile: {self.__input_dir}/geth/boot.dockerfile\n                container_name: boot\n                networks:\n                  chainnet:\n                    ipv4_address: {self.__boot_ip}\n            \"\"\"\n        )\n\n    def __add_validator(self, cnt) -&gt; None:\n        \"\"\"\n        Randomly generates and adds number(cnt) of validator nodes to yaml and genesis.json\n        Args:\n            cnt: number of validator nodes to cresate\n\n        Returns: None\n\n        \"\"\"\n        validator_addresses = list()\n\n        for id in range(cnt):\n            # create random private key and create account from it\n            acc = w3.eth.account.create()\n            validator_addresses.append(acc.address[2:])\n\n            # get random network address\n            ip, port = self.__get_unreserved_address()\n\n            self.__yaml += textwrap.dedent(\n                f\"\"\"\n                geth-validator-{id}:\n                    hostname: geth-validator-{id}\n                    depends_on:\n                      - geth-bootnode\n                    environment:\n                      - address={acc.address}\n                      - bootnodeId={self.__boot_id}\n                      - bootnodeIp={self.__boot_ip}\n                      - port={port}\n                    build:\n                      dockerfile: {self.__input_dir}/geth/validator.dockerfile\n                      args:\n                        privatekey: {w3.to_hex(acc.key)[2:]}\n                        password: {w3.to_hex(w3.eth.account.create().key)}\n                    container_name: validator_{id}\n                    networks:\n                      chainnet:\n                        ipv4_address: 172.25.0.{ip}\n                \"\"\"\n            )\n\n        # create specific Ethereum extra data string for PoA with all public addresses of validators\n        extra_data = \"0x\" + \"0\" * 64 + \"\".join([a for a in validator_addresses]) + 65 * \"0\" + 65 * \"0\"\n        self.__genesis[\"extraData\"] = extra_data\n\n    def __add_oracle(self) -&gt; None:\n        \"\"\"\n        Adds Oracle node to yaml and genesis.json\n        Returns: None\n\n        \"\"\"\n\n        # create random private key and create account from it\n        acc = w3.eth.account.create()\n\n        # prefund oracle by allocating all funds to its public wallet address\n        self.__genesis[\"alloc\"] = {\n            acc.address: {\"balance\": \"0x200000000000000000000000000000000000000000000000000000000000000\"}\n        }\n\n        self.__yaml += textwrap.dedent(\n            f\"\"\"\n            oracle:\n               hostname: oracle\n               depends_on:\n                 - geth-rpc\n                 - geth-bootnode\n               environment:\n                 - PRIVATE_KEY={w3.to_hex(acc.key)[2:]}\n                 - RPC_IP={self.__rpc_ip}\n               build:\n                 dockerfile: {self.__input_dir}/geth/oracle.dockerfile\n                 context: {self.__input_dir}\n               ports:\n                 - 8081:8081\n               container_name: oracle\n               networks:\n                 chainnet:\n                   ipv4_address: {self.__oracle_ip}\n            \"\"\"\n        )\n\n    def __add_rpc(self):\n        \"\"\"\n        Add non-validator node to yaml\n        Returns: None\n\n        \"\"\"\n        # create random private key and create account from it\n        acc = w3.eth.account.create()\n\n        self.__yaml += textwrap.dedent(\n            f\"\"\"\n            geth-rpc:\n                 hostname: geth-rpc\n                 depends_on:\n                   - geth-bootnode\n                 environment:\n                   - address={acc.address}\n                   - bootnodeId={self.__boot_id}\n                   - bootnodeIp={self.__boot_ip}\n                 build:\n                   dockerfile: {self.__input_dir}/geth/rpc.dockerfile\n                 ports:\n                   - 8545:8545\n                 container_name: rpc\n                 networks:\n                   chainnet:\n                     ipv4_address: {self.__rpc_ip}\n            \"\"\"\n        )\n\n    def __add_network(self) -&gt; None:\n        \"\"\"\n        Adds network config to docker-compose.yaml to create a private network for docker compose\n        Returns: None\n\n        \"\"\"\n        self.__yaml += textwrap.dedent(\n            \"\"\"\n            networks:\n              chainnet:\n                name: chainnet\n                driver: bridge\n                ipam:\n                  config:\n                  - subnet: 172.25.0.0/24\n            \"\"\"\n        )\n\n    def __export_config(self) -&gt; None:\n        \"\"\"\n        Writes configured yaml and genesis files to config folder for deplyoment\n        Returns: None\n\n        \"\"\"\n\n        # format yaml and add docker compose properties\n        final_str = textwrap.indent(f\"\"\"{self.__yaml}\"\"\", \"  \")\n\n        self.__yaml = textwrap.dedent(\n            \"\"\"\n                    version: \"3.8\"\n                    name: blockchain\n                    services:\n                    \"\"\"\n        )\n\n        self.__yaml += final_str\n\n        # add network config last\n        self.__add_network()\n\n        with open(f\"{self.__config_dir}/blockchain-docker-compose.yml\", \"w+\") as file:\n            file.write(self.__yaml)\n\n        with open(f\"{self.__input_dir}/geth/genesis.json\", \"w+\") as file:\n            json.dump(self.__genesis, file, indent=4)\n\n        source = os.path.join(self.__input_dir, \"geth\", \"genesis.json\")\n        shutil.copy(source, os.path.join(self.__config_dir, \"genesis.json\"))\n\n        source = os.path.join(self.__input_dir, \"chaincode\", \"reputation_system.sol\")\n        shutil.copy(source, os.path.join(self.__config_dir, \"reputation_system.sol\"))\n</code></pre>"},{"location":"api/addons/blockchain/blockchain_deployer/#nebula.addons.blockchain.blockchain_deployer.BlockchainDeployer.__add_boot_node","title":"<code>__add_boot_node()</code>","text":"<p>Adds boot node to docker-compose.yaml Returns: None</p> Source code in <code>nebula/addons/blockchain/blockchain_deployer.py</code> <pre><code>def __add_boot_node(self) -&gt; None:\n    \"\"\"\n    Adds boot node to docker-compose.yaml\n    Returns: None\n\n    \"\"\"\n\n    # create random private key and create account from it\n    acc = w3.eth.account.create()\n\n    # store id of boot node to be inserted into all other nodes\n    self.__boot_id = str(keys.PrivateKey(acc.key).public_key)[2:]\n\n    # add service to yaml string\n    self.__yaml += textwrap.dedent(\n        f\"\"\"\n        geth-bootnode:\n            hostname: geth-bootnode\n            environment:\n              - nodekeyhex={w3.to_hex(acc.key)[2:]}\n            build:\n              dockerfile: {self.__input_dir}/geth/boot.dockerfile\n            container_name: boot\n            networks:\n              chainnet:\n                ipv4_address: {self.__boot_ip}\n        \"\"\"\n    )\n</code></pre>"},{"location":"api/addons/blockchain/blockchain_deployer/#nebula.addons.blockchain.blockchain_deployer.BlockchainDeployer.__add_network","title":"<code>__add_network()</code>","text":"<p>Adds network config to docker-compose.yaml to create a private network for docker compose Returns: None</p> Source code in <code>nebula/addons/blockchain/blockchain_deployer.py</code> <pre><code>def __add_network(self) -&gt; None:\n    \"\"\"\n    Adds network config to docker-compose.yaml to create a private network for docker compose\n    Returns: None\n\n    \"\"\"\n    self.__yaml += textwrap.dedent(\n        \"\"\"\n        networks:\n          chainnet:\n            name: chainnet\n            driver: bridge\n            ipam:\n              config:\n              - subnet: 172.25.0.0/24\n        \"\"\"\n    )\n</code></pre>"},{"location":"api/addons/blockchain/blockchain_deployer/#nebula.addons.blockchain.blockchain_deployer.BlockchainDeployer.__add_oracle","title":"<code>__add_oracle()</code>","text":"<p>Adds Oracle node to yaml and genesis.json Returns: None</p> Source code in <code>nebula/addons/blockchain/blockchain_deployer.py</code> <pre><code>def __add_oracle(self) -&gt; None:\n    \"\"\"\n    Adds Oracle node to yaml and genesis.json\n    Returns: None\n\n    \"\"\"\n\n    # create random private key and create account from it\n    acc = w3.eth.account.create()\n\n    # prefund oracle by allocating all funds to its public wallet address\n    self.__genesis[\"alloc\"] = {\n        acc.address: {\"balance\": \"0x200000000000000000000000000000000000000000000000000000000000000\"}\n    }\n\n    self.__yaml += textwrap.dedent(\n        f\"\"\"\n        oracle:\n           hostname: oracle\n           depends_on:\n             - geth-rpc\n             - geth-bootnode\n           environment:\n             - PRIVATE_KEY={w3.to_hex(acc.key)[2:]}\n             - RPC_IP={self.__rpc_ip}\n           build:\n             dockerfile: {self.__input_dir}/geth/oracle.dockerfile\n             context: {self.__input_dir}\n           ports:\n             - 8081:8081\n           container_name: oracle\n           networks:\n             chainnet:\n               ipv4_address: {self.__oracle_ip}\n        \"\"\"\n    )\n</code></pre>"},{"location":"api/addons/blockchain/blockchain_deployer/#nebula.addons.blockchain.blockchain_deployer.BlockchainDeployer.__add_rpc","title":"<code>__add_rpc()</code>","text":"<p>Add non-validator node to yaml Returns: None</p> Source code in <code>nebula/addons/blockchain/blockchain_deployer.py</code> <pre><code>def __add_rpc(self):\n    \"\"\"\n    Add non-validator node to yaml\n    Returns: None\n\n    \"\"\"\n    # create random private key and create account from it\n    acc = w3.eth.account.create()\n\n    self.__yaml += textwrap.dedent(\n        f\"\"\"\n        geth-rpc:\n             hostname: geth-rpc\n             depends_on:\n               - geth-bootnode\n             environment:\n               - address={acc.address}\n               - bootnodeId={self.__boot_id}\n               - bootnodeIp={self.__boot_ip}\n             build:\n               dockerfile: {self.__input_dir}/geth/rpc.dockerfile\n             ports:\n               - 8545:8545\n             container_name: rpc\n             networks:\n               chainnet:\n                 ipv4_address: {self.__rpc_ip}\n        \"\"\"\n    )\n</code></pre>"},{"location":"api/addons/blockchain/blockchain_deployer/#nebula.addons.blockchain.blockchain_deployer.BlockchainDeployer.__add_validator","title":"<code>__add_validator(cnt)</code>","text":"<p>Randomly generates and adds number(cnt) of validator nodes to yaml and genesis.json Args:     cnt: number of validator nodes to cresate</p> <p>Returns: None</p> Source code in <code>nebula/addons/blockchain/blockchain_deployer.py</code> <pre><code>def __add_validator(self, cnt) -&gt; None:\n    \"\"\"\n    Randomly generates and adds number(cnt) of validator nodes to yaml and genesis.json\n    Args:\n        cnt: number of validator nodes to cresate\n\n    Returns: None\n\n    \"\"\"\n    validator_addresses = list()\n\n    for id in range(cnt):\n        # create random private key and create account from it\n        acc = w3.eth.account.create()\n        validator_addresses.append(acc.address[2:])\n\n        # get random network address\n        ip, port = self.__get_unreserved_address()\n\n        self.__yaml += textwrap.dedent(\n            f\"\"\"\n            geth-validator-{id}:\n                hostname: geth-validator-{id}\n                depends_on:\n                  - geth-bootnode\n                environment:\n                  - address={acc.address}\n                  - bootnodeId={self.__boot_id}\n                  - bootnodeIp={self.__boot_ip}\n                  - port={port}\n                build:\n                  dockerfile: {self.__input_dir}/geth/validator.dockerfile\n                  args:\n                    privatekey: {w3.to_hex(acc.key)[2:]}\n                    password: {w3.to_hex(w3.eth.account.create().key)}\n                container_name: validator_{id}\n                networks:\n                  chainnet:\n                    ipv4_address: 172.25.0.{ip}\n            \"\"\"\n        )\n\n    # create specific Ethereum extra data string for PoA with all public addresses of validators\n    extra_data = \"0x\" + \"0\" * 64 + \"\".join([a for a in validator_addresses]) + 65 * \"0\" + 65 * \"0\"\n    self.__genesis[\"extraData\"] = extra_data\n</code></pre>"},{"location":"api/addons/blockchain/blockchain_deployer/#nebula.addons.blockchain.blockchain_deployer.BlockchainDeployer.__copy_dir","title":"<code>__copy_dir(source_path)</code>","text":"<p>Copy blockchain folder with current files such as chaincode to config folder Args:     source_path: Path of dir to copy</p> <p>Returns: None</p> Source code in <code>nebula/addons/blockchain/blockchain_deployer.py</code> <pre><code>def __copy_dir(self, source_path) -&gt; None:\n    \"\"\"\n    Copy blockchain folder with current files such as chaincode to config folder\n    Args:\n        source_path: Path of dir to copy\n\n    Returns: None\n\n    \"\"\"\n\n    curr_path = os.path.dirname(os.path.abspath(__file__))\n\n    if not os.path.exists(self.__config_dir):\n        os.makedirs(self.__config_dir, exist_ok=True)\n\n    target_dir = os.path.join(self.__config_dir, source_path)\n    source_dir = os.path.join(curr_path, source_path)\n    shutil.copytree(str(source_dir), target_dir, dirs_exist_ok=True)\n</code></pre>"},{"location":"api/addons/blockchain/blockchain_deployer/#nebula.addons.blockchain.blockchain_deployer.BlockchainDeployer.__export_config","title":"<code>__export_config()</code>","text":"<p>Writes configured yaml and genesis files to config folder for deplyoment Returns: None</p> Source code in <code>nebula/addons/blockchain/blockchain_deployer.py</code> <pre><code>def __export_config(self) -&gt; None:\n    \"\"\"\n    Writes configured yaml and genesis files to config folder for deplyoment\n    Returns: None\n\n    \"\"\"\n\n    # format yaml and add docker compose properties\n    final_str = textwrap.indent(f\"\"\"{self.__yaml}\"\"\", \"  \")\n\n    self.__yaml = textwrap.dedent(\n        \"\"\"\n                version: \"3.8\"\n                name: blockchain\n                services:\n                \"\"\"\n    )\n\n    self.__yaml += final_str\n\n    # add network config last\n    self.__add_network()\n\n    with open(f\"{self.__config_dir}/blockchain-docker-compose.yml\", \"w+\") as file:\n        file.write(self.__yaml)\n\n    with open(f\"{self.__input_dir}/geth/genesis.json\", \"w+\") as file:\n        json.dump(self.__genesis, file, indent=4)\n\n    source = os.path.join(self.__input_dir, \"geth\", \"genesis.json\")\n    shutil.copy(source, os.path.join(self.__config_dir, \"genesis.json\"))\n\n    source = os.path.join(self.__input_dir, \"chaincode\", \"reputation_system.sol\")\n    shutil.copy(source, os.path.join(self.__config_dir, \"reputation_system.sol\"))\n</code></pre>"},{"location":"api/addons/blockchain/blockchain_deployer/#nebula.addons.blockchain.blockchain_deployer.BlockchainDeployer.__get_unreserved_address","title":"<code>__get_unreserved_address()</code>","text":"<p>Computes a randomized port and last 8 bits of an ip address, where both are not yet used Returns: Randomized and unreserved lat 8 bit of ip and port</p> Source code in <code>nebula/addons/blockchain/blockchain_deployer.py</code> <pre><code>def __get_unreserved_address(self) -&gt; tuple[int, int]:\n    \"\"\"\n    Computes a randomized port and last 8 bits of an ip address, where both are not yet used\n    Returns: Randomized and unreserved lat 8 bit of ip and port\n\n    \"\"\"\n\n    # extract reserved ports and ip addresses\n    reserved_ips = [address[0] for address in self.__reserved_addresses]\n    reserved_ports = [address[1] for address in self.__reserved_addresses]\n\n    # get randomized ip and port in range still unreserved\n    ip = random.choice([number for number in range(10, 254) if number not in reserved_ips])\n    port = random.choice([number for number in range(30310, 30360) if number not in reserved_ports])\n\n    # add network address to list of reserved addresses\n    self.__reserved_addresses.add((ip, port))\n    return ip, port\n</code></pre>"},{"location":"api/addons/blockchain/blockchain_deployer/#nebula.addons.blockchain.blockchain_deployer.BlockchainDeployer.__load_genesis","title":"<code>__load_genesis()</code>  <code>staticmethod</code>","text":"<p>Load original genesis config Returns: Genesis json dict</p> Source code in <code>nebula/addons/blockchain/blockchain_deployer.py</code> <pre><code>@staticmethod\ndef __load_genesis() -&gt; dict[str, int | str | dict]:\n    \"\"\"\n    Load original genesis config\n    Returns: Genesis json dict\n\n    \"\"\"\n    return {\n        \"config\": {\n            \"chainId\": 19265019,  # unique id not used by any public Ethereum network\n            # block number at which the defined EIP hard fork policies are applied\n            \"homesteadBlock\": 0,\n            \"eip150Block\": 0,\n            \"eip155Block\": 0,\n            \"eip158Block\": 0,\n            \"byzantiumBlock\": 0,\n            \"constantinopleBlock\": 0,\n            \"petersburgBlock\": 0,\n            \"istanbulBlock\": 0,\n            \"muirGlacierBlock\": 0,\n            \"berlinBlock\": 0,\n            # Proof-of-Authority settings\n            \"clique\": {\n                \"period\": 1,\n                \"epoch\": 10000,\n            },  # block time (time in seconds between two blocks)  # number of blocks after reset the pending votes\n        },\n        # unique continuous id of transactions used by PoA\n        \"nonce\": \"0x0\",\n        # UNIX timestamp of block creation\n        \"timestamp\": \"0x5a8efd25\",\n        # strictly formated string containing all public wallet addresses of all validators (PoA)\n        # will be replaced by public addresses of randomly generated validator node\n        \"extraData\": \"0x0000000000000000000000000000000000000000000000000000000000000000187c1c14c75bA185A59c621Fbe5dda26D488852DF20C144e8aE3e1aCF7071C4883B759D1B428e7930000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\",\n        # maximum gas (computational cost) per transaction\n        \"gasLimit\": \"9000000000000\",  # \"8000000\" is default for Ethereum but too low for heavy load\n        # difficulty for PoW\n        \"difficulty\": \"0x1\",\n        # root hash of block\n        \"mixHash\": \"0x0000000000000000000000000000000000000000000000000000000000000000\",\n        # validator of genesis block\n        \"coinbase\": \"0x0000000000000000000000000000000000000000\",\n        # prefunded public wallet addresses (Oracle)\n        \"alloc\": {\n            # will be replaced by Oracle's randomized address\n            \"0x61DE01FcD560da4D6e05E58bCD34C8Dc92CE36D1\": {\n                \"balance\": \"0x200000000000000000000000000000000000000000000000000000000000000\"\n            }\n        },\n        # block number of genesis block\n        \"number\": \"0x0\",\n        # gas used to validate genesis block\n        \"gasUsed\": \"0x0\",\n        # hash of parent block (0x0 since first block)\n        \"parentHash\": \"0x0000000000000000000000000000000000000000000000000000000000000000\",\n    }\n</code></pre>"},{"location":"api/addons/blockchain/chaincode/","title":"Documentation for Chaincode Module","text":""},{"location":"api/addons/blockchain/geth/","title":"Documentation for Geth Module","text":""},{"location":"api/addons/blockchain/oracle/","title":"Documentation for Oracle Module","text":""},{"location":"api/addons/blockchain/oracle/app/","title":"Documentation for App Module","text":""},{"location":"api/addons/blockchain/oracle/app/#nebula.addons.blockchain.oracle.app.Oracle","title":"<code>Oracle</code>","text":"Source code in <code>nebula/addons/blockchain/oracle/app.py</code> <pre><code>class Oracle:\n    def __init__(self):\n        # header file, required for interacting with chain code\n        self.__contract_abi = dict()\n\n        # stores gas expenses for experiments\n        self.__gas_store = list()\n\n        # stores timing records for experiments\n        self.__time_store = list()\n\n        # stores reputation records for experiments\n        self.__reputation_store = list()\n\n        # current (03.2024) average amount of WEI to pay for a unit of gas\n        self.__gas_price_per_unit = 27.3\n\n        # current (03.2024) average price in USD per WEI\n        self.__price_USD_per_WEI = 0.00001971\n\n        # static ip address of non-validator node (RPC)\n        self.__blockchain_address = \"http://172.25.0.104:8545\"\n\n        # executes RPC request to non-validator node until ready\n        self.__ready = self.wait_for_blockchain()\n\n        # creates an account from the primary key stored in the envs\n        self.acc = self.__create_account()\n\n        # create Web3 object for making transactions\n        self.__web3 = self.__initialize_web3()\n\n        # create a Web3 contract object from the compiled chaincode\n        self.contract_obj = self.__compile_chaincode()\n\n        # deploy the contract to the blockchain network\n        self.__contract_address = self.deploy_chaincode()\n\n        # update the contract object with the address\n        self.contract_obj = self.__web3.eth.contract(\n            abi=self.contract_obj.abi,\n            bytecode=self.contract_obj.bytecode,\n            address=self.contract_address,\n        )\n\n    @property\n    def contract_abi(self):\n        return self.__contract_abi\n\n    @property\n    def contract_address(self):\n        return self.__contract_address\n\n    @retry((Exception, requests.exceptions.HTTPError), tries=20, delay=10)\n    def wait_for_blockchain(self) -&gt; bool:\n        \"\"\"\n        Executes REST post request for a selected RPC method to check if blockchain\n        is up and running\n        Returns: None\n\n        \"\"\"\n        headers = {\"Content-type\": \"application/json\", \"Accept\": \"application/json\"}\n\n        data = {\"jsonrpc\": \"2.0\", \"method\": \"eth_accounts\", \"id\": 1, \"params\": []}\n\n        request = requests.post(url=self.__blockchain_address, json=data, headers=headers)\n\n        # raise Exception if status is an error one\n        request.raise_for_status()\n\n        print(\"ORACLE: RPC node up and running\", flush=True)\n\n        return True\n\n    def __initialize_web3(self):\n        \"\"\"\n        Initializes Web3 object and configures it for PoA protocol\n        Returns: Web3 object\n\n        \"\"\"\n\n        # initialize Web3 object with ip of non-validator node\n        web3 = Web3(Web3.HTTPProvider(self.__blockchain_address, request_kwargs={\"timeout\": 20}))  # 10\n\n        # inject Proof-of-Authority settings to object\n        web3.middleware_onion.inject(geth_poa_middleware, layer=0)\n\n        # automatically sign transactions if available for execution\n        web3.middleware_onion.add(construct_sign_and_send_raw_middleware(self.acc))\n\n        # inject local account as default\n        web3.eth.default_account = self.acc.address\n\n        # return initialized object for executing transaction\n        print(f\"SUCCESS: Account created at {self.acc.address}\")\n        return web3\n\n    def __compile_chaincode(self):\n        \"\"\"\n        Compile raw chaincode and create Web3 contract object with it\n        Returns: Web3 contract object\n\n        \"\"\"\n\n        # open raw solidity file\n        with open(\"reputation_system.sol\") as file:\n            simple_storage_file = file.read()\n\n        # set compiler version\n        install_solc(\"0.8.22\")\n\n        # compile solidity code\n        compiled_sol = compile_standard(\n            {\n                \"language\": \"Solidity\",\n                \"sources\": {\"reputation_system.sol\": {\"content\": simple_storage_file}},\n                \"settings\": {\n                    \"evmVersion\": \"paris\",\n                    \"outputSelection\": {\"*\": {\"*\": [\"abi\", \"metadata\", \"evm.bytecode\", \"evm.sourceMap\"]}},\n                    \"optimizer\": {\"enabled\": True, \"runs\": 1000},\n                },\n            },\n            solc_version=\"0.8.22\",\n        )\n\n        # store compiled code as json\n        with open(\"compiled_code.json\", \"w\") as file:\n            json.dump(compiled_sol, file)\n\n        # retrieve bytecode from the compiled contract\n        contract_bytecode = compiled_sol[\"contracts\"][\"reputation_system.sol\"][\"ReputationSystem\"][\"evm\"][\"bytecode\"][\n            \"object\"\n        ]\n\n        # retrieve ABI from compiled contract\n        self.__contract_abi = json.loads(\n            compiled_sol[\"contracts\"][\"reputation_system.sol\"][\"ReputationSystem\"][\"metadata\"]\n        )[\"output\"][\"abi\"]\n\n        print(\"Oracle: Solidity files compiled and bytecode ready\", flush=True)\n\n        # return draft Web3 contract object\n        return self.__web3.eth.contract(abi=self.__contract_abi, bytecode=contract_bytecode)\n\n    @staticmethod\n    def __create_account():\n        \"\"\"\n        Retrieves the private key from the envs, set during docker build\n        Returns: Web3 account object\n\n        \"\"\"\n\n        # retrieve private key, set during ducker build\n        private_key = os.environ.get(\"PRIVATE_KEY\")\n\n        # return Web3 account object\n        return Account.from_key(\"0x\" + private_key)\n\n    @retry((Exception, requests.exceptions.HTTPError), tries=3, delay=4)\n    def transfer_funds(self, address):\n        \"\"\"\n        Creates transaction to blockchain network for assigning funds to Cores\n        Args:\n            address: public wallet address of Core to assign funds to\n\n        Returns: Transaction receipt\n\n        \"\"\"\n\n        # create raw transaction with all required parameters to change state of ledger\n        raw_transaction = {\n            \"chainId\": self.__web3.eth.chain_id,\n            \"from\": self.acc.address,\n            \"value\": self.__web3.to_wei(\"500\", \"ether\"),\n            \"to\": self.__web3.to_checksum_address(address),\n            \"nonce\": self.__web3.eth.get_transaction_count(self.acc.address, \"pending\"),\n            \"gasPrice\": self.__web3.to_wei(self.__gas_price_per_unit, \"gwei\"),\n            \"gas\": self.__web3.to_wei(\"22000\", \"wei\"),\n        }\n\n        # sign transaction with private key and execute it\n        tx_receipt = self.__sign_and_deploy(raw_transaction)\n\n        # return transaction receipt\n        return f\"SUCESS: {tx_receipt}\"\n\n    def __sign_and_deploy(self, trx_hash):\n        \"\"\"\n        Signs a function call to the chain code with the primary key and awaits the receipt\n        Args:\n            trx_hash: Transformed dictionary of all properties relevant for call to chain code\n\n        Returns: transaction receipt confirming the successful write to the ledger\n\n        \"\"\"\n\n        # transaction is signed with private key\n        signed_transaction = self.__web3.eth.account.sign_transaction(trx_hash, private_key=self.acc.key)\n\n        # confirmation that transaction was passed from non-validator node to validator nodes\n        executed_transaction = self.__web3.eth.send_raw_transaction(signed_transaction.rawTransaction)\n\n        # non-validator node awaited the successful validation by validation nodes and returns receipt\n        transaction_receipt = self.__web3.eth.wait_for_transaction_receipt(executed_transaction, timeout=20)  # 5\n\n        # report used gas for experiment\n        self.report_gas(transaction_receipt.gasUsed, 0)\n\n        return transaction_receipt\n\n    @retry(Exception, tries=20, delay=5)\n    def deploy_chaincode(self):\n        \"\"\"\n        Creates transaction to deploy chain code on the blockchain network by\n        sending transaction to non-validator node\n        Returns: address of chain code on the network\n\n        \"\"\"\n\n        # create raw transaction with all properties to deploy contract\n        raw_transaction = self.contract_obj.constructor().build_transaction({\n            \"chainId\": self.__web3.eth.chain_id,\n            \"from\": self.acc.address,\n            \"value\": self.__web3.to_wei(\"3\", \"ether\"),\n            \"gasPrice\": self.__web3.to_wei(self.__gas_price_per_unit, \"gwei\"),\n            \"nonce\": self.__web3.eth.get_transaction_count(self.acc.address, \"pending\"),\n        })\n\n        # sign transaction with private key and executes it\n        tx_receipt = self.__sign_and_deploy(raw_transaction)\n\n        # store the address received from the non-validator node\n        contract_address = tx_receipt[\"contractAddress\"]\n\n        # returns contract address to provide to the cores later\n        return contract_address\n\n    def get_balance(self, addr):\n        \"\"\"\n        Creates transaction to blockchain network to request balance for parameter address\n        Args:\n            addr: public wallet address of account\n\n        Returns: current balance in ether (ETH)\n\n        \"\"\"\n\n        # converts address type required for making a transaction\n        cAddr = self.__web3.to_checksum_address(addr)\n\n        # executes the transaction directly, no signing required\n        balance = self.__web3.eth.get_balance(cAddr, \"pending\")\n\n        # returns JSON response with ether balance to requesting core\n        return {\"address\": cAddr, \"balance_eth\": self.__web3.from_wei(balance, \"ether\")}\n\n    def report_gas(self, amount: int, aggregation_round: int) -&gt; None:\n        \"\"\"\n        Experiment method for collecting and reporting gas usage statistics\n        Args:\n            aggregation_round: Aggregation round of sender\n            amount: Amount of gas spent in WEI\n\n        Returns: None\n\n        \"\"\"\n\n        # store the recorded gas for experiment\n        self.__gas_store.append((amount, aggregation_round))\n\n    def get_gas_report(self) -&gt; Mapping[str, str]:\n        \"\"\"\n        Experiment method for requesting the summed up records of reported gas usage\n        Returns: JSON with name:value (WEI/USD) for every reported node\n\n        \"\"\"\n        # sum up all reported costs\n        total_wei = sum(record[0] for record in self.__gas_store)\n\n        # convert sum in WEI to USD by computing with gas price USD per WEI\n        total_usd = round(total_wei * self.__price_USD_per_WEI)\n\n        return {\"Sum (WEI)\": total_wei, \"Sum (USD)\": f\"{total_usd:,}\"}\n\n    @property\n    def gas_store(self):\n        \"\"\"\n        Experiment method for requesting the detailed records of the gas reports\n        Returns: list of records of type: list[(node, timestamp, gas)]\n\n        \"\"\"\n        return self.__gas_store\n\n    def report_time(self, time_s: float, aggregation_round: int) -&gt; None:\n        \"\"\"\n        Experiment method for collecting and reporting time statistics\n        Args:\n            aggregation_round: Aggregation round of node\n            method: Name of node which reports time\n            time_s: Amount of time spend on method\n\n        Returns: None\n\n        \"\"\"\n\n        # store the recorded time for experiment\n        self.__time_store.append((time_s, aggregation_round))\n\n    def report_reputation(self, records: list, aggregation_round: int, sender: str) -&gt; None:\n        \"\"\"\n        Experiment method for collecting and reporting reputations statistics\n        Args:\n            aggregation_round: Current aggregation round of sender\n            records: list of (name:reputation) records\n            sender: node reporting its local view\n\n        Returns: None\n\n        \"\"\"\n\n        # store the recorded reputation for experiment\n        self.__reputation_store.extend([(record[0], record[1], aggregation_round, sender) for record in records])\n\n    @property\n    def time_store(self) -&gt; list:\n        \"\"\"\n        Experiment method for requesting all records of nodes which reported timings\n        Returns: JSON with method:(sum_time, n_calls) for every reported node\n\n        \"\"\"\n        return self.__time_store\n\n    @property\n    def reputation_store(self) -&gt; list:\n        \"\"\"\n        Experiment method for requesting all records of reputations\n        Returns: list with (name, reputation, timestamp)\n\n        \"\"\"\n        return self.__reputation_store\n\n    @property\n    def ready(self) -&gt; bool:\n        \"\"\"\n        Returns true if the Oracle is ready itself and the chain code was deployed successfully\n        Returns: True if ready False otherwise\n\n        \"\"\"\n        return self.__ready\n</code></pre>"},{"location":"api/addons/blockchain/oracle/app/#nebula.addons.blockchain.oracle.app.Oracle.gas_store","title":"<code>gas_store</code>  <code>property</code>","text":"<p>Experiment method for requesting the detailed records of the gas reports Returns: list of records of type: list[(node, timestamp, gas)]</p>"},{"location":"api/addons/blockchain/oracle/app/#nebula.addons.blockchain.oracle.app.Oracle.ready","title":"<code>ready</code>  <code>property</code>","text":"<p>Returns true if the Oracle is ready itself and the chain code was deployed successfully Returns: True if ready False otherwise</p>"},{"location":"api/addons/blockchain/oracle/app/#nebula.addons.blockchain.oracle.app.Oracle.reputation_store","title":"<code>reputation_store</code>  <code>property</code>","text":"<p>Experiment method for requesting all records of reputations Returns: list with (name, reputation, timestamp)</p>"},{"location":"api/addons/blockchain/oracle/app/#nebula.addons.blockchain.oracle.app.Oracle.time_store","title":"<code>time_store</code>  <code>property</code>","text":"<p>Experiment method for requesting all records of nodes which reported timings Returns: JSON with method:(sum_time, n_calls) for every reported node</p>"},{"location":"api/addons/blockchain/oracle/app/#nebula.addons.blockchain.oracle.app.Oracle.__compile_chaincode","title":"<code>__compile_chaincode()</code>","text":"<p>Compile raw chaincode and create Web3 contract object with it Returns: Web3 contract object</p> Source code in <code>nebula/addons/blockchain/oracle/app.py</code> <pre><code>def __compile_chaincode(self):\n    \"\"\"\n    Compile raw chaincode and create Web3 contract object with it\n    Returns: Web3 contract object\n\n    \"\"\"\n\n    # open raw solidity file\n    with open(\"reputation_system.sol\") as file:\n        simple_storage_file = file.read()\n\n    # set compiler version\n    install_solc(\"0.8.22\")\n\n    # compile solidity code\n    compiled_sol = compile_standard(\n        {\n            \"language\": \"Solidity\",\n            \"sources\": {\"reputation_system.sol\": {\"content\": simple_storage_file}},\n            \"settings\": {\n                \"evmVersion\": \"paris\",\n                \"outputSelection\": {\"*\": {\"*\": [\"abi\", \"metadata\", \"evm.bytecode\", \"evm.sourceMap\"]}},\n                \"optimizer\": {\"enabled\": True, \"runs\": 1000},\n            },\n        },\n        solc_version=\"0.8.22\",\n    )\n\n    # store compiled code as json\n    with open(\"compiled_code.json\", \"w\") as file:\n        json.dump(compiled_sol, file)\n\n    # retrieve bytecode from the compiled contract\n    contract_bytecode = compiled_sol[\"contracts\"][\"reputation_system.sol\"][\"ReputationSystem\"][\"evm\"][\"bytecode\"][\n        \"object\"\n    ]\n\n    # retrieve ABI from compiled contract\n    self.__contract_abi = json.loads(\n        compiled_sol[\"contracts\"][\"reputation_system.sol\"][\"ReputationSystem\"][\"metadata\"]\n    )[\"output\"][\"abi\"]\n\n    print(\"Oracle: Solidity files compiled and bytecode ready\", flush=True)\n\n    # return draft Web3 contract object\n    return self.__web3.eth.contract(abi=self.__contract_abi, bytecode=contract_bytecode)\n</code></pre>"},{"location":"api/addons/blockchain/oracle/app/#nebula.addons.blockchain.oracle.app.Oracle.__create_account","title":"<code>__create_account()</code>  <code>staticmethod</code>","text":"<p>Retrieves the private key from the envs, set during docker build Returns: Web3 account object</p> Source code in <code>nebula/addons/blockchain/oracle/app.py</code> <pre><code>@staticmethod\ndef __create_account():\n    \"\"\"\n    Retrieves the private key from the envs, set during docker build\n    Returns: Web3 account object\n\n    \"\"\"\n\n    # retrieve private key, set during ducker build\n    private_key = os.environ.get(\"PRIVATE_KEY\")\n\n    # return Web3 account object\n    return Account.from_key(\"0x\" + private_key)\n</code></pre>"},{"location":"api/addons/blockchain/oracle/app/#nebula.addons.blockchain.oracle.app.Oracle.__initialize_web3","title":"<code>__initialize_web3()</code>","text":"<p>Initializes Web3 object and configures it for PoA protocol Returns: Web3 object</p> Source code in <code>nebula/addons/blockchain/oracle/app.py</code> <pre><code>def __initialize_web3(self):\n    \"\"\"\n    Initializes Web3 object and configures it for PoA protocol\n    Returns: Web3 object\n\n    \"\"\"\n\n    # initialize Web3 object with ip of non-validator node\n    web3 = Web3(Web3.HTTPProvider(self.__blockchain_address, request_kwargs={\"timeout\": 20}))  # 10\n\n    # inject Proof-of-Authority settings to object\n    web3.middleware_onion.inject(geth_poa_middleware, layer=0)\n\n    # automatically sign transactions if available for execution\n    web3.middleware_onion.add(construct_sign_and_send_raw_middleware(self.acc))\n\n    # inject local account as default\n    web3.eth.default_account = self.acc.address\n\n    # return initialized object for executing transaction\n    print(f\"SUCCESS: Account created at {self.acc.address}\")\n    return web3\n</code></pre>"},{"location":"api/addons/blockchain/oracle/app/#nebula.addons.blockchain.oracle.app.Oracle.__sign_and_deploy","title":"<code>__sign_and_deploy(trx_hash)</code>","text":"<p>Signs a function call to the chain code with the primary key and awaits the receipt Args:     trx_hash: Transformed dictionary of all properties relevant for call to chain code</p> <p>Returns: transaction receipt confirming the successful write to the ledger</p> Source code in <code>nebula/addons/blockchain/oracle/app.py</code> <pre><code>def __sign_and_deploy(self, trx_hash):\n    \"\"\"\n    Signs a function call to the chain code with the primary key and awaits the receipt\n    Args:\n        trx_hash: Transformed dictionary of all properties relevant for call to chain code\n\n    Returns: transaction receipt confirming the successful write to the ledger\n\n    \"\"\"\n\n    # transaction is signed with private key\n    signed_transaction = self.__web3.eth.account.sign_transaction(trx_hash, private_key=self.acc.key)\n\n    # confirmation that transaction was passed from non-validator node to validator nodes\n    executed_transaction = self.__web3.eth.send_raw_transaction(signed_transaction.rawTransaction)\n\n    # non-validator node awaited the successful validation by validation nodes and returns receipt\n    transaction_receipt = self.__web3.eth.wait_for_transaction_receipt(executed_transaction, timeout=20)  # 5\n\n    # report used gas for experiment\n    self.report_gas(transaction_receipt.gasUsed, 0)\n\n    return transaction_receipt\n</code></pre>"},{"location":"api/addons/blockchain/oracle/app/#nebula.addons.blockchain.oracle.app.Oracle.deploy_chaincode","title":"<code>deploy_chaincode()</code>","text":"<p>Creates transaction to deploy chain code on the blockchain network by sending transaction to non-validator node Returns: address of chain code on the network</p> Source code in <code>nebula/addons/blockchain/oracle/app.py</code> <pre><code>@retry(Exception, tries=20, delay=5)\ndef deploy_chaincode(self):\n    \"\"\"\n    Creates transaction to deploy chain code on the blockchain network by\n    sending transaction to non-validator node\n    Returns: address of chain code on the network\n\n    \"\"\"\n\n    # create raw transaction with all properties to deploy contract\n    raw_transaction = self.contract_obj.constructor().build_transaction({\n        \"chainId\": self.__web3.eth.chain_id,\n        \"from\": self.acc.address,\n        \"value\": self.__web3.to_wei(\"3\", \"ether\"),\n        \"gasPrice\": self.__web3.to_wei(self.__gas_price_per_unit, \"gwei\"),\n        \"nonce\": self.__web3.eth.get_transaction_count(self.acc.address, \"pending\"),\n    })\n\n    # sign transaction with private key and executes it\n    tx_receipt = self.__sign_and_deploy(raw_transaction)\n\n    # store the address received from the non-validator node\n    contract_address = tx_receipt[\"contractAddress\"]\n\n    # returns contract address to provide to the cores later\n    return contract_address\n</code></pre>"},{"location":"api/addons/blockchain/oracle/app/#nebula.addons.blockchain.oracle.app.Oracle.get_balance","title":"<code>get_balance(addr)</code>","text":"<p>Creates transaction to blockchain network to request balance for parameter address Args:     addr: public wallet address of account</p> <p>Returns: current balance in ether (ETH)</p> Source code in <code>nebula/addons/blockchain/oracle/app.py</code> <pre><code>def get_balance(self, addr):\n    \"\"\"\n    Creates transaction to blockchain network to request balance for parameter address\n    Args:\n        addr: public wallet address of account\n\n    Returns: current balance in ether (ETH)\n\n    \"\"\"\n\n    # converts address type required for making a transaction\n    cAddr = self.__web3.to_checksum_address(addr)\n\n    # executes the transaction directly, no signing required\n    balance = self.__web3.eth.get_balance(cAddr, \"pending\")\n\n    # returns JSON response with ether balance to requesting core\n    return {\"address\": cAddr, \"balance_eth\": self.__web3.from_wei(balance, \"ether\")}\n</code></pre>"},{"location":"api/addons/blockchain/oracle/app/#nebula.addons.blockchain.oracle.app.Oracle.get_gas_report","title":"<code>get_gas_report()</code>","text":"<p>Experiment method for requesting the summed up records of reported gas usage Returns: JSON with name:value (WEI/USD) for every reported node</p> Source code in <code>nebula/addons/blockchain/oracle/app.py</code> <pre><code>def get_gas_report(self) -&gt; Mapping[str, str]:\n    \"\"\"\n    Experiment method for requesting the summed up records of reported gas usage\n    Returns: JSON with name:value (WEI/USD) for every reported node\n\n    \"\"\"\n    # sum up all reported costs\n    total_wei = sum(record[0] for record in self.__gas_store)\n\n    # convert sum in WEI to USD by computing with gas price USD per WEI\n    total_usd = round(total_wei * self.__price_USD_per_WEI)\n\n    return {\"Sum (WEI)\": total_wei, \"Sum (USD)\": f\"{total_usd:,}\"}\n</code></pre>"},{"location":"api/addons/blockchain/oracle/app/#nebula.addons.blockchain.oracle.app.Oracle.report_gas","title":"<code>report_gas(amount, aggregation_round)</code>","text":"<p>Experiment method for collecting and reporting gas usage statistics Args:     aggregation_round: Aggregation round of sender     amount: Amount of gas spent in WEI</p> <p>Returns: None</p> Source code in <code>nebula/addons/blockchain/oracle/app.py</code> <pre><code>def report_gas(self, amount: int, aggregation_round: int) -&gt; None:\n    \"\"\"\n    Experiment method for collecting and reporting gas usage statistics\n    Args:\n        aggregation_round: Aggregation round of sender\n        amount: Amount of gas spent in WEI\n\n    Returns: None\n\n    \"\"\"\n\n    # store the recorded gas for experiment\n    self.__gas_store.append((amount, aggregation_round))\n</code></pre>"},{"location":"api/addons/blockchain/oracle/app/#nebula.addons.blockchain.oracle.app.Oracle.report_reputation","title":"<code>report_reputation(records, aggregation_round, sender)</code>","text":"<p>Experiment method for collecting and reporting reputations statistics Args:     aggregation_round: Current aggregation round of sender     records: list of (name:reputation) records     sender: node reporting its local view</p> <p>Returns: None</p> Source code in <code>nebula/addons/blockchain/oracle/app.py</code> <pre><code>def report_reputation(self, records: list, aggregation_round: int, sender: str) -&gt; None:\n    \"\"\"\n    Experiment method for collecting and reporting reputations statistics\n    Args:\n        aggregation_round: Current aggregation round of sender\n        records: list of (name:reputation) records\n        sender: node reporting its local view\n\n    Returns: None\n\n    \"\"\"\n\n    # store the recorded reputation for experiment\n    self.__reputation_store.extend([(record[0], record[1], aggregation_round, sender) for record in records])\n</code></pre>"},{"location":"api/addons/blockchain/oracle/app/#nebula.addons.blockchain.oracle.app.Oracle.report_time","title":"<code>report_time(time_s, aggregation_round)</code>","text":"<p>Experiment method for collecting and reporting time statistics Args:     aggregation_round: Aggregation round of node     method: Name of node which reports time     time_s: Amount of time spend on method</p> <p>Returns: None</p> Source code in <code>nebula/addons/blockchain/oracle/app.py</code> <pre><code>def report_time(self, time_s: float, aggregation_round: int) -&gt; None:\n    \"\"\"\n    Experiment method for collecting and reporting time statistics\n    Args:\n        aggregation_round: Aggregation round of node\n        method: Name of node which reports time\n        time_s: Amount of time spend on method\n\n    Returns: None\n\n    \"\"\"\n\n    # store the recorded time for experiment\n    self.__time_store.append((time_s, aggregation_round))\n</code></pre>"},{"location":"api/addons/blockchain/oracle/app/#nebula.addons.blockchain.oracle.app.Oracle.transfer_funds","title":"<code>transfer_funds(address)</code>","text":"<p>Creates transaction to blockchain network for assigning funds to Cores Args:     address: public wallet address of Core to assign funds to</p> <p>Returns: Transaction receipt</p> Source code in <code>nebula/addons/blockchain/oracle/app.py</code> <pre><code>@retry((Exception, requests.exceptions.HTTPError), tries=3, delay=4)\ndef transfer_funds(self, address):\n    \"\"\"\n    Creates transaction to blockchain network for assigning funds to Cores\n    Args:\n        address: public wallet address of Core to assign funds to\n\n    Returns: Transaction receipt\n\n    \"\"\"\n\n    # create raw transaction with all required parameters to change state of ledger\n    raw_transaction = {\n        \"chainId\": self.__web3.eth.chain_id,\n        \"from\": self.acc.address,\n        \"value\": self.__web3.to_wei(\"500\", \"ether\"),\n        \"to\": self.__web3.to_checksum_address(address),\n        \"nonce\": self.__web3.eth.get_transaction_count(self.acc.address, \"pending\"),\n        \"gasPrice\": self.__web3.to_wei(self.__gas_price_per_unit, \"gwei\"),\n        \"gas\": self.__web3.to_wei(\"22000\", \"wei\"),\n    }\n\n    # sign transaction with private key and execute it\n    tx_receipt = self.__sign_and_deploy(raw_transaction)\n\n    # return transaction receipt\n    return f\"SUCESS: {tx_receipt}\"\n</code></pre>"},{"location":"api/addons/blockchain/oracle/app/#nebula.addons.blockchain.oracle.app.Oracle.wait_for_blockchain","title":"<code>wait_for_blockchain()</code>","text":"<p>Executes REST post request for a selected RPC method to check if blockchain is up and running Returns: None</p> Source code in <code>nebula/addons/blockchain/oracle/app.py</code> <pre><code>@retry((Exception, requests.exceptions.HTTPError), tries=20, delay=10)\ndef wait_for_blockchain(self) -&gt; bool:\n    \"\"\"\n    Executes REST post request for a selected RPC method to check if blockchain\n    is up and running\n    Returns: None\n\n    \"\"\"\n    headers = {\"Content-type\": \"application/json\", \"Accept\": \"application/json\"}\n\n    data = {\"jsonrpc\": \"2.0\", \"method\": \"eth_accounts\", \"id\": 1, \"params\": []}\n\n    request = requests.post(url=self.__blockchain_address, json=data, headers=headers)\n\n    # raise Exception if status is an error one\n    request.raise_for_status()\n\n    print(\"ORACLE: RPC node up and running\", flush=True)\n\n    return True\n</code></pre>"},{"location":"api/addons/blockchain/oracle/app/#nebula.addons.blockchain.oracle.app.error_handler","title":"<code>error_handler(func)</code>","text":"<p>Adds default status and header to all REST responses used for Oracle</p> Source code in <code>nebula/addons/blockchain/oracle/app.py</code> <pre><code>def error_handler(func):\n    \"\"\"Adds default status and header to all REST responses used for Oracle\"\"\"\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs), 200, {\"Content-Type\": \"application/json\"}\n        except Exception as e:\n            return jsonify({\"error\": str(e)}), 500, {\"Content-Type\": \"application/json\"}\n\n    return wrapper\n</code></pre>"},{"location":"api/addons/gps/","title":"Documentation for Gps Module","text":""},{"location":"api/addons/gps/gpsmodule/","title":"Documentation for Gpsmodule Module","text":""},{"location":"api/addons/gps/nebulagps/","title":"Documentation for Nebulagps Module","text":""},{"location":"api/addons/gps/nebulagps/#nebula.addons.gps.nebulagps.NebulaGPS","title":"<code>NebulaGPS</code>","text":"<p>               Bases: <code>GPSModule</code></p> Source code in <code>nebula/addons/gps/nebulagps.py</code> <pre><code>class NebulaGPS(GPSModule):\n    BROADCAST_IP = \"255.255.255.255\"  # Broadcast IP\n    BROADCAST_PORT = 50001  # Port used for GPS\n    INTERFACE = \"eth2\"  # Interface to avoid network conditions\n\n    def __init__(self, config, addr, update_interval: float = 5.0, verbose=False):\n        self._config = config\n        self._addr = addr\n        self.update_interval = update_interval  # Frequency\n        self.running = False\n        self._node_locations = {}  # Dictionary for storing node locations\n        self._broadcast_socket = None\n        self._nodes_location_lock = Locker(\"nodes_location_lock\", async_lock=True)\n        self._verbose = verbose\n\n    async def start(self):\n        \"\"\"Inicia el servicio de GPS, enviando y recibiendo ubicaciones.\"\"\"\n        logging.info(\"Starting NebulaGPS service...\")\n        self.running = True\n\n        # Create broadcast socket\n        self._broadcast_socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n        self._broadcast_socket.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, 1)\n\n        # Bind socket on eth2 to also receive data\n        self._broadcast_socket.bind((\"\", self.BROADCAST_PORT))\n\n        # Start sending and receiving tasks\n        asyncio.create_task(self._send_location_loop())\n        asyncio.create_task(self._receive_location_loop())\n        asyncio.create_task(self._notify_geolocs())\n\n    async def stop(self):\n        \"\"\"Stops the GPS service.\"\"\"\n        logging.info(\"Stopping NebulaGPS service...\")\n        self.running = False\n        if self._broadcast_socket:\n            self._broadcast_socket.close()\n            self._broadcast_socket = None\n\n    async def is_running(self):\n        return self.running\n\n    async def get_geoloc(self):\n        latitude = self._config.participant[\"mobility_args\"][\"latitude\"]\n        longitude = self._config.participant[\"mobility_args\"][\"longitude\"]\n        return (latitude, longitude)\n\n    async def calculate_distance(self, self_lat, self_long, other_lat, other_long):\n        distance_m = distance.distance((self_lat, self_long), (other_lat, other_long)).m\n        return distance_m\n\n    async def _send_location_loop(self):\n        \"\"\"Send the geolocation periodically by broadcast.\"\"\"\n        while self.running:\n            latitude, longitude = await self.get_geoloc()  # Obtener ubicaci\u00f3n actual\n            message = f\"GPS-UPDATE {self._addr} {latitude} {longitude}\"\n            self._broadcast_socket.sendto(message.encode(), (self.BROADCAST_IP, self.BROADCAST_PORT))\n            if self._verbose:\n                logging.info(f\"Sent GPS location: ({latitude}, {longitude})\")\n            await asyncio.sleep(self.update_interval)\n\n    async def _receive_location_loop(self):\n        \"\"\"Escucha y almacena geolocalizaciones de otros nodos.\"\"\"\n        while self.running:\n            try:\n                data, addr = await asyncio.get_running_loop().run_in_executor(\n                    None, self._broadcast_socket.recvfrom, 1024\n                )\n                message = data.decode().strip()\n                if message.startswith(\"GPS-UPDATE\"):\n                    _, sender_addr, lat, lon = message.split()\n                    if sender_addr != self._addr:\n                        async with self._nodes_location_lock:\n                            self._node_locations[sender_addr] = (float(lat), float(lon))\n                    if self._verbose:\n                        logging.info(f\"Received GPS from {addr[0]}: {lat}, {lon}\")\n            except Exception as e:\n                logging.error(f\"Error receiving GPS update: {e}\")\n\n    async def _notify_geolocs(self):\n        while True:\n            await asyncio.sleep(self.update_interval)\n            await self._nodes_location_lock.acquire_async()\n            geolocs: dict = self._node_locations.copy()\n            await self._nodes_location_lock.release_async()\n            if geolocs:\n                distances = {}\n                self_lat, self_long = await self.get_geoloc()\n                for addr, (lat, long) in geolocs.items():\n                    dist = await self.calculate_distance(self_lat, self_long, lat, long)\n                    distances[addr] = (dist, (lat, long))\n                gpsevent = GPSEvent(distances)\n                asyncio.create_task(EventManager.get_instance().publish_addonevent(gpsevent))\n</code></pre>"},{"location":"api/addons/gps/nebulagps/#nebula.addons.gps.nebulagps.NebulaGPS.start","title":"<code>start()</code>  <code>async</code>","text":"<p>Inicia el servicio de GPS, enviando y recibiendo ubicaciones.</p> Source code in <code>nebula/addons/gps/nebulagps.py</code> <pre><code>async def start(self):\n    \"\"\"Inicia el servicio de GPS, enviando y recibiendo ubicaciones.\"\"\"\n    logging.info(\"Starting NebulaGPS service...\")\n    self.running = True\n\n    # Create broadcast socket\n    self._broadcast_socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    self._broadcast_socket.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, 1)\n\n    # Bind socket on eth2 to also receive data\n    self._broadcast_socket.bind((\"\", self.BROADCAST_PORT))\n\n    # Start sending and receiving tasks\n    asyncio.create_task(self._send_location_loop())\n    asyncio.create_task(self._receive_location_loop())\n    asyncio.create_task(self._notify_geolocs())\n</code></pre>"},{"location":"api/addons/gps/nebulagps/#nebula.addons.gps.nebulagps.NebulaGPS.stop","title":"<code>stop()</code>  <code>async</code>","text":"<p>Stops the GPS service.</p> Source code in <code>nebula/addons/gps/nebulagps.py</code> <pre><code>async def stop(self):\n    \"\"\"Stops the GPS service.\"\"\"\n    logging.info(\"Stopping NebulaGPS service...\")\n    self.running = False\n    if self._broadcast_socket:\n        self._broadcast_socket.close()\n        self._broadcast_socket = None\n</code></pre>"},{"location":"api/addons/networksimulation/","title":"Documentation for Networksimulation Module","text":""},{"location":"api/addons/networksimulation/nebulanetworksimulator/","title":"Documentation for Nebulanetworksimulator Module","text":""},{"location":"api/addons/networksimulation/networksimulator/","title":"Documentation for Networksimulator Module","text":""},{"location":"api/addons/reputation/","title":"Documentation for Reputation Module","text":""},{"location":"api/addons/reputation/reputation/","title":"Documentation for Reputation Module","text":""},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Metrics","title":"<code>Metrics</code>","text":"Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>class Metrics:\n    def __init__(\n        self,\n        num_round=None,\n        current_round=None,\n        fraction_changed=None,\n        threshold=None,\n        latency=None,\n    ):\n        \"\"\"\n        Initialize a Metrics instance to store various evaluation metrics for a participant.\n\n        Args:\n            num_round (optional): The current round number.\n            current_round (optional): The round when the metric is measured.\n            fraction_changed (optional): Fraction of parameters changed.\n            threshold (optional): Threshold used for evaluating changes.\n            latency (optional): Latency value for model arrival.\n        \"\"\"\n        self.fraction_of_params_changed = {\n            \"fraction_changed\": fraction_changed,\n            \"threshold\": threshold,\n            \"round\": num_round\n        }\n\n        self.model_arrival_latency = {\n            \"latency\": latency,\n            \"round\": num_round,\n            \"round_received\": current_round\n        }\n\n        self.messages = []\n\n        self.similarity = []\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Metrics.__init__","title":"<code>__init__(num_round=None, current_round=None, fraction_changed=None, threshold=None, latency=None)</code>","text":"<p>Initialize a Metrics instance to store various evaluation metrics for a participant.</p> <p>Parameters:</p> Name Type Description Default <code>num_round</code> <code>optional</code> <p>The current round number.</p> <code>None</code> <code>current_round</code> <code>optional</code> <p>The round when the metric is measured.</p> <code>None</code> <code>fraction_changed</code> <code>optional</code> <p>Fraction of parameters changed.</p> <code>None</code> <code>threshold</code> <code>optional</code> <p>Threshold used for evaluating changes.</p> <code>None</code> <code>latency</code> <code>optional</code> <p>Latency value for model arrival.</p> <code>None</code> Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>def __init__(\n    self,\n    num_round=None,\n    current_round=None,\n    fraction_changed=None,\n    threshold=None,\n    latency=None,\n):\n    \"\"\"\n    Initialize a Metrics instance to store various evaluation metrics for a participant.\n\n    Args:\n        num_round (optional): The current round number.\n        current_round (optional): The round when the metric is measured.\n        fraction_changed (optional): Fraction of parameters changed.\n        threshold (optional): Threshold used for evaluating changes.\n        latency (optional): Latency value for model arrival.\n    \"\"\"\n    self.fraction_of_params_changed = {\n        \"fraction_changed\": fraction_changed,\n        \"threshold\": threshold,\n        \"round\": num_round\n    }\n\n    self.model_arrival_latency = {\n        \"latency\": latency,\n        \"round\": num_round,\n        \"round_received\": current_round\n    }\n\n    self.messages = []\n\n    self.similarity = []\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation","title":"<code>Reputation</code>","text":"<p>Class to define and manage the reputation of a participant in the network.</p> <p>The class handles collection of metrics, calculation of static and dynamic reputation, updating history, and communication of reputation scores to neighbors.</p> Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>class Reputation:\n    \"\"\"\n    Class to define and manage the reputation of a participant in the network.\n\n    The class handles collection of metrics, calculation of static and dynamic reputation,\n    updating history, and communication of reputation scores to neighbors.\n    \"\"\"\n    def __init__(self, engine: \"Engine\", config: \"Config\"):\n        \"\"\"\n        Initialize the Reputation system.\n\n        Args:\n            engine (Engine): The engine instance providing the runtime context.\n            config (Config): The configuration object with participant settings.\n        \"\"\"\n        self._engine = engine\n        self._config = config\n        self.fraction_of_params_changed = {}\n        self.history_data = {}\n        self.metric_weights = {}\n        self.reputation = {}\n        self.reputation_with_feedback = {}\n        self.reputation_with_all_feedback = {}\n        self.rejected_nodes = set()\n        self.round_timing_info = {}\n        self._messages_received_from_sources = {}\n        self.reputation_history = {}\n        self.number_message_history = {}\n        self.neighbor_reputation_history = {}\n        self.fraction_changed_history = {}\n        self.messages_number_message = []\n        self.previous_threshold_number_message = {}\n        self.previous_std_dev_number_message = {}\n        self.messages_model_arrival_latency = {}\n        self.model_arrival_latency_history = {}\n        self.previous_percentile_25_number_message = {}\n        self.previous_percentile_85_number_message = {}\n        self._addr = engine.addr\n        self._log_dir = engine.log_dir\n        self._idx = engine.idx\n        self.connection_metrics = []\n\n        neighbors: str = self._config.participant[\"network_args\"][\"neighbors\"]\n        self.connection_metrics = {}\n        for nei in neighbors.split():\n            self.connection_metrics[f\"{nei}\"] = Metrics()\n\n        self._with_reputation = self._config.participant[\"defense_args\"][\"with_reputation\"]\n        self._reputation_metrics = self._config.participant[\"defense_args\"][\"reputation_metrics\"]\n        self._initial_reputation = float(self._config.participant[\"defense_args\"][\"initial_reputation\"])\n        self._weighting_factor = self._config.participant[\"defense_args\"][\"weighting_factor\"]\n        self._weight_model_arrival_latency = float(self._config.participant[\"defense_args\"][\"weight_model_arrival_latency\"])\n        self._weight_model_similarity = float(self._config.participant[\"defense_args\"][\"weight_model_similarity\"])\n        self._weight_num_messages = float(self._config.participant[\"defense_args\"][\"weight_num_messages\"])\n        self._weight_fraction_params_changed = float(self._config.participant[\"defense_args\"][\"weight_fraction_params_changed\"])\n\n        msg = f\"Reputation system: {self._with_reputation}\"\n        msg += f\"\\nReputation metrics: {self._reputation_metrics}\"\n        msg += f\"\\nInitial reputation: {self._initial_reputation}\"\n        msg += f\"\\nWeighting factor: {self._weighting_factor}\"\n        if self._weighting_factor == \"static\":\n            msg += f\"\\nWeight model arrival latency: {self._weight_model_arrival_latency}\"\n            msg += f\"\\nWeight model similarity: {self._weight_model_similarity}\"\n            msg += f\"\\nWeight number of messages: {self._weight_num_messages}\"\n            msg += f\"\\nWeight fraction of parameters changed: {self._weight_fraction_params_changed}\"\n        print_msg_box(msg=msg, indent=2, title=\"Defense information\")\n\n    @property\n    def engine(self):\n        \"\"\"Return the engine instance.\"\"\"\n        return self._engine\n\n    def save_data(\n        self,\n        type_data,\n        nei,\n        addr,\n        num_round=None,\n        time=None,\n        current_round=None,\n        fraction_changed=None,\n        total_params=None,\n        changed_params=None,\n        threshold=None,\n        changes_record=None,\n        latency=None,\n    ):\n        \"\"\"\n        Save data received from nodes for further reputation calculations.\n\n        Args:\n            type_data (str): The type of data being saved (\"number_message\", \"fraction_of_params_changed\", or \"model_arrival_latency\").\n            nei (str): The neighbor node address.\n            addr (str): The current node address.\n            num_round (optional): The round number associated with the data.\n            time (optional): Timestamp or time value.\n            current_round (optional): The current round number.\n            fraction_changed (optional): Fraction of parameters changed.\n            total_params (optional): Total number of parameters.\n            changed_params (optional): Number of changed parameters.\n            threshold (optional): Threshold used for metrics.\n            changes_record (optional): Record of parameter changes.\n            latency (optional): Latency value.\n        \"\"\"\n        try:\n            if addr == nei:\n                return\n\n            combined_data = {}\n\n            if type_data == \"number_message\":\n                combined_data[\"number_message\"] = {\n                    \"time\": time,\n                    \"current_round\": current_round,\n                }\n            elif type_data == \"fraction_of_params_changed\":\n                combined_data[\"fraction_of_params_changed\"] = {\n                    \"fraction_changed\": fraction_changed,\n                    \"threshold\": threshold,\n                    \"round\": num_round,\n                }\n            elif type_data == \"model_arrival_latency\":\n                combined_data[\"model_arrival_latency\"] = {\n                    \"latency\": latency,\n                    \"round\": num_round,\n                    \"round_received\": current_round,\n                }\n\n            if nei in self.connection_metrics:\n                if type_data == \"number_message\":\n                    if not isinstance(self.connection_metrics[nei].messages, list):\n                        self.connection_metrics[nei].messages = []\n                    self.connection_metrics[nei].messages.append(combined_data[\"number_message\"])\n                elif type_data == \"fraction_of_params_changed\":\n                    self.connection_metrics[nei].fraction_of_params_changed.update(combined_data[\"fraction_of_params_changed\"])\n                elif type_data == \"model_arrival_latency\":\n                    self.connection_metrics[nei].model_arrival_latency.update(combined_data[\"model_arrival_latency\"])\n\n        except Exception:\n            logging.exception(\"Error saving data\")\n\n    async def setup(self):\n        \"\"\"\n        Setup the reputation system by subscribing to various events.\n\n        This function enables the reputation system and subscribes to events based on active metrics.\n        \"\"\"\n        if self._with_reputation:\n            logging.info(\"Reputation system enabled\")\n            await EventManager.get_instance().subscribe_node_event(RoundStartEvent, self.on_round_start)\n            await EventManager.get_instance().subscribe_node_event(AggregationEvent, self.calculate_reputation)\n            if self._reputation_metrics.get(\"model_similarity\", False):\n                await EventManager.get_instance().subscribe_node_event(UpdateReceivedEvent, self.recollect_similarity)\n            if self._reputation_metrics.get(\"fraction_parameters_changed\", False):\n                await EventManager.get_instance().subscribe_node_event(UpdateReceivedEvent, self.recollect_fraction_of_parameters_changed)\n            if self._reputation_metrics.get(\"num_messages\", False):\n                await EventManager.get_instance().subscribe((\"model\", \"update\"), self.recollect_number_message)\n                await EventManager.get_instance().subscribe((\"model\", \"initialization\"), self.recollect_number_message)\n                await EventManager.get_instance().subscribe((\"control\", \"alive\"), self.recollect_number_message)\n                await EventManager.get_instance().subscribe((\"federation\", \"federation_models_included\"), self.recollect_number_message)\n                await EventManager.get_instance().subscribe((\"reputation\", \"share\"), self.recollect_number_message)\n            if self._reputation_metrics.get(\"model_arrival_latency\", False):\n                await EventManager.get_instance().subscribe_node_event(UpdateReceivedEvent, self.recollect_model_arrival_latency)\n\n    def init_reputation(self, addr, federation_nodes=None, round_num=None, last_feedback_round=None, init_reputation=None):\n        \"\"\"\n        Initialize the reputation for each federation node.\n\n        Args:\n            addr (str): The address of the current node.\n            federation_nodes (list): List of federation nodes' addresses.\n            round_num (int): The current round number.\n            last_feedback_round (int): The last round in which feedback was provided.\n            init_reputation (float): The initial reputation value.\n        \"\"\"\n        if not federation_nodes:\n            logging.error(\"init_reputation | No federation nodes provided\")\n            return\n\n        if self._with_reputation:\n            neighbors = self.is_valid_ip(federation_nodes)\n\n            if not neighbors:\n                logging.error(\"init_reputation | No neighbors found\")\n                return\n\n            for nei in neighbors:\n                if nei not in self.reputation:\n                    self.reputation[nei] = {\n                        \"reputation\": init_reputation,\n                        \"round\": round_num,\n                        \"last_feedback_round\": last_feedback_round,\n                    }\n                elif self.reputation[nei].get(\"reputation\") is None:\n                    self.reputation[nei][\"reputation\"] = init_reputation\n                    self.reputation[nei][\"round\"] = round_num\n                    self.reputation[nei][\"last_feedback_round\"] = last_feedback_round\n\n                avg_reputation = self.save_reputation_history_in_memory(self._addr, nei, init_reputation)\n\n    def is_valid_ip(self, federation_nodes):\n        \"\"\"\n        Check if the IP addresses provided are valid.\n\n        Args:\n            federation_nodes (list): List of federation node addresses.\n\n        Returns:\n            list: A list of valid IP addresses.\n        \"\"\"\n        valid_ip = []\n        for i in federation_nodes:   \n            valid_ip.append(i)\n\n        return valid_ip\n\n    def _calculate_static_reputation(self, addr, nei, metric_messages_number, metric_similarity, metric_fraction, metric_model_arrival_latency,\n                                     weight_messages_number, weight_similarity, weight_fraction, weight_model_arrival_latency):\n        \"\"\"\n        Calculate the static reputation of a participant using fixed weights.\n\n        Args:\n            addr (str): The IP address of the current node.\n            nei (str): The neighbor node's IP address.\n            metric_messages_number (float): Metric value for number of messages.\n            metric_similarity (float): Metric value for model similarity.\n            metric_fraction (float): Metric value for fraction of parameters changed.\n            metric_model_arrival_latency (float): Metric value for model arrival latency.\n            weight_messages_number (float): Weight for number of messages.\n            weight_similarity (float): Weight for model similarity.\n            weight_fraction (float): Weight for fraction of parameters changed.\n            weight_model_arrival_latency (float): Weight for model arrival latency.\n        \"\"\"\n        static_weights = {\n            \"num_messages\": weight_messages_number,\n            \"model_similarity\": weight_similarity,\n            \"fraction_parameters_changed\": weight_fraction,\n            \"model_arrival_latency\": weight_model_arrival_latency,\n        }\n\n        metric_values = {\n            \"num_messages\": metric_messages_number,\n            \"model_similarity\": metric_similarity,\n            \"fraction_parameters_changed\": metric_fraction,\n            \"model_arrival_latency\": metric_model_arrival_latency,\n        }\n\n        reputation_static = sum(\n            metric_values[metric_name] * static_weights[metric_name] for metric_name in static_weights\n        )\n        logging.info(f\"Static reputation for node {nei} at round {self.engine.get_round()}: {reputation_static}\")\n\n        avg_reputation = self.save_reputation_history_in_memory(self.engine.addr, nei, reputation_static)\n\n        metrics_data = {\n            \"addr\": addr,\n            \"nei\": nei,\n            \"round\": self.engine.get_round(),\n            \"reputation_without_feedback\": avg_reputation,\n        }\n\n        for metric_name in metric_values:\n            metrics_data[f\"average_{metric_name}\"] = static_weights[metric_name]\n\n        self._update_reputation_record(nei, avg_reputation, metrics_data)\n\n    async def _calculate_dynamic_reputation(self, addr, neighbors):\n        \"\"\"\n        Calculate the dynamic reputation of a participant based on historical metric data.\n\n        Args:\n            addr (str): The address of the current node.\n            neighbors (list): List of neighbor node addresses.\n\n        Returns:\n            dict: Updated dynamic reputation values.\n        \"\"\"\n        average_weights = {}\n\n        for metric_name in self.history_data.keys():\n            if self._reputation_metrics.get(metric_name, False):\n                valid_entries = [\n                    entry for entry in self.history_data[metric_name]\n                    if entry[\"round\"] &gt;= self._engine.get_round() and entry.get(\"weight\") not in [None, -1]\n                ]\n\n                if valid_entries:\n                    average_weight = sum([entry[\"weight\"] for entry in valid_entries]) / len(valid_entries)\n                    average_weights[metric_name] = average_weight\n                else:\n                    average_weights[metric_name] = 0\n\n        for nei in neighbors:\n            metric_values = {}\n            for metric_name in self.history_data.keys():\n                if self._reputation_metrics.get(metric_name, False):\n                    for entry in self.history_data.get(metric_name, []):\n                        if entry[\"round\"] == self._engine.get_round() and entry[\"metric_name\"] == metric_name and entry[\"nei\"] == nei:\n                            metric_values[metric_name] = entry[\"metric_value\"]\n                            break\n\n            if all(metric_name in metric_values for metric_name in average_weights):\n                reputation_with_weights = sum(\n                    metric_values.get(metric_name, 0) * average_weights[metric_name]\n                    for metric_name in average_weights\n                )\n                logging.info(f\"Dynamic reputation with weights for {nei} at round {self.engine.get_round()}: {reputation_with_weights}\")\n\n                avg_reputation = self.save_reputation_history_in_memory(self.engine.addr, nei, reputation_with_weights)\n\n                metrics_data = {\n                    \"addr\": addr,\n                    \"nei\": nei,\n                    \"round\": self.engine.get_round(),\n                    \"reputation_without_feedback\": avg_reputation,\n                }\n\n                for metric_name in metric_values:\n                    metrics_data[f\"average_{metric_name}\"] = average_weights[metric_name]\n\n                self._update_reputation_record(nei, avg_reputation, metrics_data)\n\n    def _update_reputation_record(self, nei, reputation, data):\n        \"\"\"\n        Update the reputation record of a neighbor.\n\n        Args:\n            nei (str): The neighbor node's address.\n            reputation (float): The computed reputation value.\n            data (dict): Additional metrics data associated with the reputation.\n        \"\"\"\n        if nei not in self.reputation:\n            self.reputation[nei] = {\n                \"reputation\": reputation,\n                \"round\": self._engine.get_round(),\n                \"last_feedback_round\": -1,\n            }\n        else:\n            self.reputation[nei][\"reputation\"] = reputation\n            self.reputation[nei][\"round\"] = self._engine.get_round()\n\n        logging.info(f\"Reputation of node {nei}: {self.reputation[nei]['reputation']}\")\n        if self.reputation[nei][\"reputation\"] &lt; 0.6:\n            self.rejected_nodes.add(nei)\n            logging.info(f\"Rejected node {nei} at round {self._engine.get_round()}\")\n\n    def calculate_weighted_values(\n        self,\n        avg_messages_number_message_normalized,\n        similarity_reputation,\n        fraction_score_asign,\n        avg_model_arrival_latency,\n        history_data,\n        current_round,\n        addr,\n        nei,\n        reputation_metrics\n    ):\n        \"\"\"\n        Calculate the weighted values for each metric based on current measurements and historical data.\n\n        Args:\n            avg_messages_number_message_normalized (float): Normalized average message count.\n            similarity_reputation (float): Reputation score based on model similarity.\n            fraction_score_asign (float): Score assigned from fraction of parameters changed.\n            avg_model_arrival_latency (float): Average model arrival latency.\n            history_data (dict): Historical metrics data.\n            current_round (int): The current round number.\n            addr (str): The address of the current node.\n            nei (str): The neighbor node's address.\n            reputation_metrics (dict): Dictionary indicating which metrics are active.\n        \"\"\"\n        if current_round is not None:\n\n            normalized_weights = {}\n            required_keys = [\n                \"num_messages\",\n                \"model_similarity\",\n                \"fraction_parameters_changed\",\n                \"model_arrival_latency\",\n            ]\n\n            for key in required_keys:\n                if key not in history_data:\n                    history_data[key] = []\n\n            metrics = {\n                \"num_messages\": avg_messages_number_message_normalized,\n                \"model_similarity\": similarity_reputation,\n                \"fraction_parameters_changed\": fraction_score_asign,\n                \"model_arrival_latency\": avg_model_arrival_latency,                   \n            }\n\n            active_metrics = {k: v for k, v in metrics.items() if reputation_metrics.get(k, False)}\n            num_active_metrics = len(active_metrics)\n\n            for metric_name, current_value in active_metrics.items():\n                history_data[metric_name].append({\n                    \"round\": current_round,\n                    \"addr\": addr,\n                    \"nei\": nei,\n                    \"metric_name\": metric_name,\n                    \"metric_value\": current_value,\n                    \"weight\": None\n                })\n\n            adjusted_weights = {}\n\n            if current_round &gt;= 5 and num_active_metrics &gt; 0:\n                desviations = {}\n                for metric_name, current_value in active_metrics.items():\n                    historical_values = history_data[metric_name]\n\n                    metric_values = [entry['metric_value'] for entry in historical_values if 'metric_value' in entry and entry[\"metric_value\"] != 0]\n\n                    if metric_values:\n                        mean_value = np.mean(metric_values)\n                    else:\n                        mean_value = 0\n\n                    deviation = abs(current_value - mean_value)\n                    desviations[metric_name] = deviation\n\n                if all(deviation == 0.0 for deviation in desviations.values()):\n                    random_weights = [random.random() for _ in range(num_active_metrics)]\n                    total_random_weight = sum(random_weights)\n                    normalized_weights = {metric_name: weight / total_random_weight for metric_name, weight in zip(active_metrics, random_weights)}\n                else:\n                    max_desviation = max(desviations.values()) if desviations else 1\n                    normalized_weights = {\n                        metric_name: (desviation / max_desviation) for metric_name, desviation in desviations.items()\n                    }\n\n                    total_weight = sum(normalized_weights.values())\n                    if total_weight &gt; 0:\n                        normalized_weights = {\n                            metric_name: weight / total_weight for metric_name, weight in normalized_weights.items()\n                        }\n                    else:\n                        normalized_weights = {metric_name: 1 / num_active_metrics for metric_name in active_metrics}\n\n                mean_deviation = np.mean(list(desviations.values()))\n                dynamic_min_weight = max(0.1, mean_deviation / (mean_deviation + 1)) \n\n                total_adjusted_weight = 0\n\n                for metric_name, weight in normalized_weights.items():\n                    if weight &lt; dynamic_min_weight:\n                        adjusted_weights[metric_name] = dynamic_min_weight\n                    else:\n                        adjusted_weights[metric_name] = weight\n                    total_adjusted_weight += adjusted_weights[metric_name]\n\n                if total_adjusted_weight &gt; 1:\n                    for metric_name in adjusted_weights:\n                        adjusted_weights[metric_name] /= total_adjusted_weight\n                    total_adjusted_weight = 1\n            else:\n                adjusted_weights = {metric_name: 1 / num_active_metrics for metric_name in active_metrics}\n\n            for metric_name, current_value in active_metrics.items():\n                weight = adjusted_weights.get(metric_name, -1)\n                for entry in history_data[metric_name]:\n                    if entry[\"metric_name\"] == metric_name and entry[\"round\"] == current_round and entry[\"nei\"] == nei:\n                        entry[\"weight\"] = weight\n\n    async def calculate_value_metrics(self, log_dir, id_node, addr, nei, metrics_active=None):\n        \"\"\"\n        Calculate various metrics (message count, model similarity, fraction of parameters changed, and model arrival latency)\n        for a given neighbor node based on stored connection data.\n\n        Args:\n            log_dir (str): Directory for log files.\n            id_node (str): Identifier for the node.\n            addr (str): The address of the current node.\n            nei (str): The neighbor node's address.\n            metrics_active (dict): Dictionary indicating which metrics are active.\n\n        Returns:\n            tuple: A tuple containing:\n                - avg_messages_number_message_normalized (float)\n                - similarity_reputation (float)\n                - fraction_score_asign (float)\n                - avg_model_arrival_latency (float)\n        \"\"\"\n        messages_number_message_normalized = 0\n        messages_number_message_count = 0\n        avg_messages_number_message_normalized = 0\n        fraction_score_normalized = 0\n        fraction_score_asign = 0\n        messages_model_arrival_latency_normalized = 0\n        avg_model_arrival_latency = 0\n        similarity_reputation = 0\n        fraction_neighbors_scores = None\n\n        try:\n            current_round = self._engine.get_round()\n\n            metrics_instance = self.connection_metrics.get(nei)\n            if not metrics_instance:\n                logging.warning(f\"No metrics found for neighbor {nei}\")\n                return avg_messages_number_message_normalized, similarity_reputation, fraction_score_asign, avg_model_arrival_latency\n\n            if metrics_active.get(\"num_messages\", False):\n                filtered_messages = [msg for msg in metrics_instance.messages if msg.get(\"current_round\") == current_round]\n                for msg in filtered_messages:\n                    self.messages_number_message.append({\n                        \"number_message\": msg.get(\"time\"),\n                        \"current_round\": msg.get(\"current_round\"),\n                        \"key\": (addr, nei),\n                    })\n\n                messages_number_message_normalized, messages_number_message_count = self.manage_metric_number_message(\n                    self.messages_number_message, addr, nei, current_round, True\n                )\n                avg_messages_number_message_normalized = self.save_number_message_history(\n                    addr, nei, messages_number_message_normalized, current_round\n                )\n                if avg_messages_number_message_normalized is None and current_round &gt; 4:\n                    avg_messages_number_message_normalized = self.number_message_history[(addr, nei)][current_round - 1][\"avg_number_message\"]\n\n            if metrics_active.get(\"fraction_parameters_changed\", False):\n                if metrics_instance.fraction_of_params_changed.get(\"round\") == current_round:\n                    fraction_changed = metrics_instance.fraction_of_params_changed.get(\"fraction_changed\")\n                    threshold = metrics_instance.fraction_of_params_changed.get(\"threshold\")\n                    fraction_score_normalized = self.analyze_anomalies(\n                        addr,\n                        nei,\n                        current_round,\n                        current_round,  # Assumes round_received is the current round.\n                        fraction_changed,\n                        threshold,\n                    )\n\n            if metrics_active.get(\"model_arrival_latency\", False):\n                if metrics_instance.model_arrival_latency.get(\"round_received\") == current_round:\n                    round_latency = metrics_instance.model_arrival_latency.get(\"round\")\n                    latency = metrics_instance.model_arrival_latency.get(\"latency\")\n                    messages_model_arrival_latency_normalized = self.manage_model_arrival_latency(\n                        round_latency,\n                        addr,\n                        nei,\n                        latency,\n                        current_round\n                    )\n\n            if current_round &gt;= 5 and metrics_active.get(\"model_similarity\", False):\n                similarity_reputation = self.calculate_similarity_from_metrics(nei, current_round)\n            else:\n                similarity_reputation = 0\n\n            if messages_model_arrival_latency_normalized &gt;= 0:\n                avg_model_arrival_latency = self.save_model_arrival_latency_history(\n                    addr, nei, messages_model_arrival_latency_normalized, current_round\n                )\n                if avg_model_arrival_latency is None and current_round &gt; 4:\n                    avg_model_arrival_latency = self.model_arrival_latency_history[(addr, nei)][current_round - 1][\"score\"]\n\n            if self.messages_number_message is not None:\n                messages_number_message_normalized, messages_number_message_count = self.manage_metric_number_message(\n                    self.messages_number_message, addr, nei, current_round, metrics_active.get(\"num_messages\", False)\n                )\n                avg_messages_number_message_normalized = self.save_number_message_history(\n                    addr, nei, messages_number_message_normalized, current_round\n                )\n                if avg_messages_number_message_normalized is None and current_round &gt; 4:\n                    avg_messages_number_message_normalized = self.number_message_history[(addr, nei)][current_round - 1][\"avg_number_message\"]\n\n            if current_round &gt;= 5:\n                if fraction_score_normalized &gt; 0:\n                    key_previous_round = (addr, nei, current_round - 1) if current_round - 1 &gt; 0 else None\n                    fraction_previous_round = None\n\n                    if key_previous_round is not None and key_previous_round in self.fraction_changed_history:\n                        fraction_score_prev = self.fraction_changed_history[key_previous_round].get(\"fraction_score\")\n                        fraction_previous_round = fraction_score_prev if fraction_score_prev is not None else None\n\n                    if fraction_previous_round is not None:\n                        fraction_score_asign = fraction_score_normalized * 0.8 + fraction_previous_round * 0.2\n                        self.fraction_changed_history[(addr, nei, current_round)][\"fraction_score\"] = fraction_score_asign\n                    else:\n                        fraction_score_asign = fraction_score_normalized\n                        self.fraction_changed_history[(addr, nei, current_round)][\"fraction_score\"] = fraction_score_asign\n                else:\n                    fraction_previous_round = None\n                    key_previous_round = (addr, nei, current_round - 1) if current_round - 1 &gt; 0 else None\n                    if key_previous_round is not None and key_previous_round in self.fraction_changed_history:\n                        fraction_score_prev = self.fraction_changed_history[key_previous_round].get(\"fraction_score\")\n                        fraction_previous_round = fraction_score_prev if fraction_score_prev is not None else None\n\n                    if fraction_previous_round is not None:\n                        fraction_score_asign = fraction_previous_round - (fraction_previous_round * 0.5)\n                    else:\n                        if fraction_neighbors_scores is None:\n                            fraction_neighbors_scores = {}\n\n                        for key, value in self.fraction_changed_history.items():\n                            score = value.get(\"fraction_score\")\n                            if score is not None:\n                                fraction_neighbors_scores[key] = score\n\n                        if fraction_neighbors_scores:\n                            fraction_score_asign = np.mean(list(fraction_neighbors_scores.values()))\n                        else:\n                            fraction_score_asign = 0 \n            else:\n                fraction_score_asign = 0\n\n            self.create_graphics_to_metrics(\n                messages_number_message_count,\n                avg_messages_number_message_normalized,\n                similarity_reputation,\n                fraction_score_asign,\n                avg_model_arrival_latency,\n                addr,\n                nei,\n                current_round,\n                self.engine.total_rounds,\n            )\n\n            return avg_messages_number_message_normalized, similarity_reputation, fraction_score_asign, avg_model_arrival_latency\n\n        except Exception as e:\n            logging.exception(f\"Error calculating reputation. Type: {type(e).__name__}\")\n\n    def create_graphics_to_metrics(\n        self,\n        number_message_count,\n        number_message_norm,\n        similarity,\n        fraction,\n        model_arrival_latency,\n        addr,\n        nei,\n        current_round,\n        total_rounds,\n    ):\n        \"\"\"\n        Create and log graphics representing different metric values over the rounds.\n\n        Args:\n            number_message_count (int): Count of messages.\n            number_message_norm (float): Normalized number of messages.\n            similarity (float): Similarity metric value.\n            fraction (float): Fraction score metric.\n            model_arrival_latency (float): Latency metric score.\n            addr (str): The current node's address.\n            nei (str): The neighbor node's address.\n            current_round (int): The current round number.\n            total_rounds (int): Total rounds in the session.\n        \"\"\"\n        if current_round is not None and current_round &lt; total_rounds:\n            model_arrival_latency_dict = {f\"R-Model_arrival_latency_reputation/{addr}\": {nei: model_arrival_latency}}\n            messages_number_message_count_dict = {f\"R-Count_messages_number_message_reputation/{addr}\": {nei: number_message_count}}\n            messages_number_message_norm_dict = {f\"R-number_message_reputation/{addr}\": {nei: number_message_norm}}\n            similarity_dict = {f\"R-Similarity_reputation/{addr}\": {nei: similarity}}\n            fraction_dict = {f\"R-Fraction_reputation/{addr}\": {nei: fraction}}\n\n            if messages_number_message_count_dict is not None:\n                self.engine.trainer._logger.log_data(messages_number_message_count_dict, step=current_round)\n\n            if messages_number_message_norm_dict is not None:\n                self.engine.trainer._logger.log_data(messages_number_message_norm_dict, step=current_round)\n\n            if similarity_dict is not None:\n                self.engine.trainer._logger.log_data(similarity_dict, step=current_round)\n\n            if fraction_dict is not None:\n                self.engine.trainer._logger.log_data(fraction_dict, step=current_round)\n\n            if model_arrival_latency_dict is not None:\n                self.engine.trainer._logger.log_data(model_arrival_latency_dict, step=current_round)\n\n    def analyze_anomalies(\n        self,\n        addr,\n        nei,\n        round_num,\n        current_round,\n        fraction_changed,\n        threshold,\n    ):\n        \"\"\"\n        Analyze anomalies in the fraction of parameters changed and calculate a corresponding score.\n\n        Args:\n            addr (str): The source node's address.\n            nei (str): The neighbor node's address.\n            round_num (int): The round number for the metric.\n            current_round (int): The current round number.\n            fraction_changed (float): Fraction of parameters changed.\n            threshold (float): Threshold value for changes.\n\n        Returns:\n            float: A normalized fraction score between 0 and 1.\n        \"\"\"\n        try:\n            key = (addr, nei, round_num)\n\n            if key not in self.fraction_changed_history:\n                prev_key = (addr, nei, round_num - 1)\n                if round_num &gt; 0 and prev_key in self.fraction_changed_history:\n                    previous_data = self.fraction_changed_history[prev_key]\n                    fraction_changed = (\n                        fraction_changed if fraction_changed is not None else previous_data[\"fraction_changed\"]\n                    )\n                    threshold = threshold if threshold is not None else previous_data[\"threshold\"]\n                else:\n                    fraction_changed = fraction_changed if fraction_changed is not None else 0\n                    threshold = threshold if threshold is not None else 0\n\n                self.fraction_changed_history[key] = {\n                    \"fraction_changed\": fraction_changed,\n                    \"threshold\": threshold,\n                    \"fraction_score\": None,\n                    \"fraction_anomaly\": False,\n                    \"threshold_anomaly\": False,\n                    \"mean_fraction\": None,\n                    \"std_dev_fraction\": None,\n                    \"mean_threshold\": None,\n                    \"std_dev_threshold\": None,\n                }\n\n            if round_num &lt; 5:\n                past_fractions = []\n                past_thresholds = []\n\n                for r in range(round_num):\n                    past_key = (addr, nei, r)\n                    if past_key in self.fraction_changed_history:\n                        past_fractions.append(self.fraction_changed_history[past_key][\"fraction_changed\"])\n                        past_thresholds.append(self.fraction_changed_history[past_key][\"threshold\"])\n\n                if past_fractions:\n                    mean_fraction = np.mean(past_fractions)\n                    std_dev_fraction = np.std(past_fractions)\n                    self.fraction_changed_history[key][\"mean_fraction\"] = mean_fraction\n                    self.fraction_changed_history[key][\"std_dev_fraction\"] = std_dev_fraction\n\n                if past_thresholds:\n                    mean_threshold = np.mean(past_thresholds)\n                    std_dev_threshold = np.std(past_thresholds)\n                    self.fraction_changed_history[key][\"mean_threshold\"] = mean_threshold\n                    self.fraction_changed_history[key][\"std_dev_threshold\"] = std_dev_threshold\n\n                return 0\n            else:\n                fraction_value = 0\n                threshold_value = 0\n                prev_key = (addr, nei, round_num - 1)\n                if prev_key not in self.fraction_changed_history:\n                    for i in range(0, round_num + 1):\n                        potential_prev_key = (addr, nei, round_num - i)\n                        if potential_prev_key in self.fraction_changed_history:\n                            mean_fraction_prev = self.fraction_changed_history[potential_prev_key][\n                                \"mean_fraction\"\n                            ]\n                            if mean_fraction_prev is not None:\n                                prev_key = potential_prev_key\n                                break\n\n                if prev_key:\n                    mean_fraction_prev = self.fraction_changed_history[prev_key][\"mean_fraction\"]\n                    std_dev_fraction_prev = self.fraction_changed_history[prev_key][\"std_dev_fraction\"]\n                    mean_threshold_prev = self.fraction_changed_history[prev_key][\"mean_threshold\"]\n                    std_dev_threshold_prev = self.fraction_changed_history[prev_key][\"std_dev_threshold\"]\n\n                    current_fraction = self.fraction_changed_history[key][\"fraction_changed\"]\n                    current_threshold = self.fraction_changed_history[key][\"threshold\"]\n\n                    upper_mean_fraction_prev = (mean_fraction_prev + std_dev_fraction_prev) * 1.05\n                    upper_mean_threshold_prev = (mean_threshold_prev + std_dev_threshold_prev) * 1.10\n\n                    fraction_anomaly = current_fraction &gt; upper_mean_fraction_prev\n                    threshold_anomaly = current_threshold &gt; upper_mean_threshold_prev\n\n                    self.fraction_changed_history[key][\"fraction_anomaly\"] = fraction_anomaly\n                    self.fraction_changed_history[key][\"threshold_anomaly\"] = threshold_anomaly\n\n                    penalization_factor_fraction = abs(current_fraction - mean_fraction_prev) / mean_fraction_prev if mean_fraction_prev != 0 else 1\n                    penalization_factor_threshold = abs(current_threshold - mean_threshold_prev) / mean_threshold_prev if mean_threshold_prev != 0 else 1\n\n                    k_fraction = penalization_factor_fraction if penalization_factor_fraction != 0 else 1\n                    k_threshold = penalization_factor_threshold if penalization_factor_threshold != 0 else 1\n\n                    if fraction_anomaly:\n                        fraction_value = (\n                            1 - (1 / (1 + np.exp(-k_fraction)))\n                            if current_fraction is not None and mean_fraction_prev is not None\n                            else 0\n                        )\n                    else:\n                        fraction_value = (\n                            1 - (1 / (1 + np.exp(k_fraction)))\n                            if current_fraction is not None and mean_fraction_prev is not None\n                            else 0\n                        )\n\n                    if threshold_anomaly:\n                        threshold_value = (\n                            1 - (1 / (1 + np.exp(-k_threshold)))\n                            if current_threshold is not None and mean_threshold_prev is not None\n                            else 0\n                        )\n                    else:\n                        threshold_value = (\n                            1 - (1 / (1 + np.exp(k_threshold)))\n                            if current_threshold is not None and mean_threshold_prev is not None\n                            else 0\n                        )\n\n                    fraction_weight = 0.5\n                    threshold_weight = 0.5\n\n                    fraction_score = fraction_weight * fraction_value + threshold_weight * threshold_value\n\n                    self.fraction_changed_history[key][\"mean_fraction\"] = (current_fraction + mean_fraction_prev) / 2\n                    self.fraction_changed_history[key][\"std_dev_fraction\"] = np.sqrt(((current_fraction - mean_fraction_prev) ** 2 + std_dev_fraction_prev**2) / 2)\n                    self.fraction_changed_history[key][\"mean_threshold\"] = (current_threshold + mean_threshold_prev) / 2\n                    self.fraction_changed_history[key][\"std_dev_threshold\"] = np.sqrt(((0.1 * (current_threshold - mean_threshold_prev) ** 2) + std_dev_threshold_prev**2) / 2)\n\n                    return max(fraction_score, 0)\n                else:\n                    return -1\n        except Exception:\n            logging.exception(\"Error analyzing anomalies\")\n            return -1\n\n    def manage_model_arrival_latency(\n        self, round_num, addr, nei, latency, current_round\n    ):\n        \"\"\"\n        Manage the model arrival latency metric and normalize it based on historical latencies.\n\n        Args:\n            round_num (int): The round number when the model was sent.\n            addr (str): The address of the current node.\n            nei (str): The neighbor node's address.\n            latency (float): The measured latency.\n            current_round (int): The current round number.\n\n        Returns:\n            float: Normalized latency score between 0 and 1.\n        \"\"\"\n        try:\n            current_key = nei\n\n            if current_round not in self.model_arrival_latency_history:\n                self.model_arrival_latency_history[current_round] = {}\n\n            self.model_arrival_latency_history[current_round][current_key] = {\n                \"latency\": latency,\n                \"score\": 0.0,\n            }\n\n            prev_mean_latency = 0\n            prev_percentil_0 = 0\n            prev_percentil_25 = 0\n            difference = 0\n\n            if current_round &gt;= 5:\n                all_latencies = [\n                    data[\"latency\"]\n                    for r in self.model_arrival_latency_history\n                    for key, data in self.model_arrival_latency_history[r].items()\n                    if \"latency\" in data and data[\"latency\"] != 0\n                ]\n\n                prev_mean_latency = np.mean(all_latencies) if all_latencies else 0\n                prev_percentil_0 = np.percentile(all_latencies, 0) if all_latencies else 0\n                prev_percentil_25 = np.percentile(all_latencies, 25) if all_latencies else 0\n\n                k = 0.1\n                prev_mean_latency += k * (prev_percentil_25 - prev_percentil_0)\n\n                difference = latency - prev_mean_latency\n                if latency &lt;= prev_mean_latency:\n                    score = 1.0\n                else:\n                    score = 1 / (1 + np.exp(abs(difference) / prev_mean_latency))\n\n                if round_num &lt; current_round:\n                    round_diff = current_round - round_num\n                    penalty_factor = round_diff * 0.1\n                    penalty = penalty_factor * (1 - score)\n                    score -= penalty * score\n\n                self.model_arrival_latency_history[current_round][current_key].update({\n                    \"mean_latency\": prev_mean_latency,\n                    \"percentil_0\": prev_percentil_0,\n                    \"percentil_25\": prev_percentil_25,\n                    \"score\": score,\n                })\n            else:\n                score = 0\n\n            return score\n\n        except Exception as e:\n            logging.exception(f\"Error managing model_arrival_latency: {e}\")\n            return 0\n\n    def save_model_arrival_latency_history(self, addr, nei, model_arrival_latency, round_num):\n        \"\"\"\n        Save and update the model arrival latency history in memory.\n\n        Args:\n            addr (str): The current node's address.\n            nei (str): The neighbor node's address.\n            model_arrival_latency (float): The normalized latency score.\n            round_num (int): The current round number.\n\n        Returns:\n            float: The updated average model arrival latency.\n        \"\"\"\n        try:\n            current_key = nei\n\n            if round_num not in self.model_arrival_latency_history:\n                self.model_arrival_latency_history[round_num] = {}\n\n            if current_key not in self.model_arrival_latency_history[round_num]:\n                self.model_arrival_latency_history[round_num][current_key] = {}\n\n            self.model_arrival_latency_history[round_num][current_key].update({\n                \"score\": model_arrival_latency,\n            })\n\n            if model_arrival_latency &gt; 0 and round_num &gt; 5:\n                previous_avg = (\n                    self.model_arrival_latency_history.get(round_num - 1, {})\n                    .get(current_key, {})\n                    .get(\"avg_model_arrival_latency\", None)\n                )\n\n                if previous_avg is not None:\n                    avg_model_arrival_latency = (\n                        model_arrival_latency * 0.8 + previous_avg * 0.2\n                        if previous_avg is not None\n                        else model_arrival_latency\n                    )\n                else:\n                    avg_model_arrival_latency = model_arrival_latency - (model_arrival_latency * 0.05)\n            elif model_arrival_latency == 0 and round_num &gt; 5:\n                previous_avg = (\n                    self.model_arrival_latency_history.get(round_num - 1, {})\n                    .get(current_key, {})\n                    .get(\"avg_model_arrival_latency\", None)\n                )\n                avg_model_arrival_latency = previous_avg - (previous_avg * 0.05)\n            else:\n                avg_model_arrival_latency = model_arrival_latency\n\n            self.model_arrival_latency_history[round_num][current_key][\"avg_model_arrival_latency\"] = (\n                avg_model_arrival_latency\n            )\n\n            return avg_model_arrival_latency\n        except Exception:\n            logging.exception(\"Error saving model_arrival_latency history\")\n\n    def manage_metric_number_message(self, messages_number_message, addr, nei, current_round, metric_active=True):\n        \"\"\"\n        Manage and normalize the number of messages metric using percentiles.\n\n        Args:\n            messages_number_message (list): List containing message data.\n            addr (str): The current node's address.\n            nei (str): The neighbor node's address.\n            current_round (int): The current round number.\n            metric_active (bool): Flag indicating whether the metric is active.\n\n        Returns:\n            tuple: A tuple with the normalized number_message value (float) and the count of messages (int).\n        \"\"\"\n        try:\n            if current_round == 0:\n                return 0.0, 0\n\n            if not metric_active:\n                return 0.0, 0\n\n            previous_round = current_round\n\n            current_addr_nei = (addr, nei)\n            relevant_messages = [\n                msg\n                for msg in messages_number_message\n                if msg[\"key\"] == current_addr_nei and msg[\"current_round\"] == previous_round\n            ]\n            messages_count = len(relevant_messages) if relevant_messages else 0\n\n            rounds_to_consider = []\n            if previous_round &gt;= 4:\n                rounds_to_consider = [previous_round - 4, previous_round - 3, previous_round - 2, previous_round - 1]\n            elif previous_round == 3:\n                rounds_to_consider = [0, 1, 2, 3]\n            elif previous_round == 2:\n                rounds_to_consider = [0, 1, 2]\n            elif previous_round == 1:\n                rounds_to_consider = [0, 1]\n            elif previous_round == 0:\n                rounds_to_consider = [0]\n\n            previous_counts = [\n                len([m for m in messages_number_message if m[\"key\"] == current_addr_nei and m[\"current_round\"] == r])\n                for r in rounds_to_consider\n            ]\n\n            self.previous_percentile_25_number_message[current_addr_nei] = (\n                np.percentile(previous_counts, 25) if previous_counts else 0\n            )\n            self.previous_percentile_85_number_message[current_addr_nei] = (\n                np.percentile(previous_counts, 85) if previous_counts else 0\n            )\n\n            normalized_messages = 1.0\n            relative_position = 0\n\n            if previous_round &gt; 4:\n                percentile_25 = self.previous_percentile_25_number_message.get(current_addr_nei, 0)\n                percentile_85 = self.previous_percentile_85_number_message.get(current_addr_nei, 0)\n                if messages_count &gt; percentile_85:\n                    relative_position = (messages_count - percentile_85) / (percentile_85 - percentile_25)\n                    normalized_messages = np.exp(-relative_position)\n\n                normalized_messages = max(0.01, normalized_messages)\n\n            return normalized_messages, messages_count\n        except Exception:\n            logging.exception(\"Error managing metric number_message\")\n            return 0.0, 0\n\n    def save_number_message_history(self, addr, nei, messages_number_message_normalized, current_round):\n        \"\"\"\n        Save the normalized number_message history in memory and calculate a weighted average.\n\n        Args:\n            addr (str): The current node's address.\n            nei (str): The neighbor node's address.\n            messages_number_message_normalized (float): The normalized number_message value.\n            current_round (int): The current round number.\n\n        Returns:\n            float: The weighted average of the number_message metric.\n        \"\"\"\n        try:\n            key = (addr, nei)\n            avg_number_message = 0\n\n            if key not in self.number_message_history:\n                self.number_message_history[key] = {}\n\n            self.number_message_history[key][current_round] = {\"number_message\": messages_number_message_normalized}\n\n            if messages_number_message_normalized != 0 and current_round &gt; 4:\n                previous_avg = (\n                    self.number_message_history[key].get(current_round - 1, {}).get(\"avg_number_message\", None)\n                )\n                if previous_avg is not None:\n                    avg_number_message = messages_number_message_normalized * 0.8 + previous_avg * 0.2\n                else:\n                    avg_number_message = messages_number_message_normalized\n\n                self.number_message_history[key][current_round][\"avg_number_message\"] = avg_number_message\n            else:\n                avg_number_message = 0\n\n            return avg_number_message\n        except Exception:\n            logging.exception(\"Error saving number_message history\")\n            return -1\n\n        except Exception as e:\n            logging.exception(f\"Error managing model_arrival_latency latency: {e}\")\n            return 0.0\n\n    def save_reputation_history_in_memory(self, addr, nei, reputation):\n        \"\"\"\n        Save the reputation history for a neighbor and compute an average reputation.\n\n        Args:\n            addr (str): The current node's address.\n            nei (str): The neighbor node's address.\n            reputation (float): The computed reputation for the current round.\n\n        Returns:\n            float: The updated (weighted) reputation.\n        \"\"\"\n        try:\n            key = (addr, nei)\n\n            if key not in self.reputation_history:\n                self.reputation_history[key] = {}\n\n            self.reputation_history[key][self._engine.get_round()] = reputation\n\n            avg_reputation = 0\n            current_round = self._engine.get_round()\n            rounds = sorted(self.reputation_history[key].keys(), reverse=True)[:2]\n\n            if len(rounds) &gt;= 2:\n                current_round = rounds[0]\n                previous_round = rounds[1]\n\n                current_rep = self.reputation_history[key][current_round]\n                previous_rep = self.reputation_history[key][previous_round]\n                logging.info(f\"Current reputation: {current_rep}, Previous reputation: {previous_rep}\")\n\n                avg_reputation = (current_rep * 0.8) + (previous_rep * 0.2)\n                logging.info(f\"Reputation ponderated: {avg_reputation}\")\n            else:\n                avg_reputation = self.reputation_history[key][current_round]\n\n            return avg_reputation\n\n        except Exception:\n            logging.exception(\"Error saving reputation history\")\n            return -1\n\n    def calculate_decay_rate(self, reputation):\n        \"\"\"\n        Calculate the decay rate for a given reputation value.\n\n        Args:\n            reputation (float): The current reputation value.\n\n        Returns:\n            float: The decay rate.\n        \"\"\"\n        if reputation &gt; 0.8:\n            return 0.9  # Very low decay\n        elif reputation &gt; 0.7:\n            return 0.8  # Medium decay\n        elif reputation &gt; 0.6:\n            return 0.6  # Low decay\n        elif reputation &gt; 0.4:\n            return 0.2  # High decay\n        else:\n            return 0.1  # Very high decay\n\n    def calculate_similarity_from_metrics(self, nei, current_round):\n        \"\"\"\n        Calculate the similarity score based on stored similarity metrics.\n\n        Args:\n            nei (str): The neighbor node's address.\n            current_round (int): The current round number.\n\n        Returns:\n            float: The aggregated similarity score.\n        \"\"\"\n        similarity_value = 0.0\n\n        metrics_instance = self.connection_metrics.get(nei)\n        if metrics_instance is None:\n            logging.error(f\"No metrics instance found for neighbor {nei}\")\n            return similarity_value\n\n        for metric in metrics_instance.similarity:\n            source_ip = metric.get(\"nei\")\n            round_in_metric = metric.get(\"round\")\n\n            if source_ip == nei and round_in_metric == current_round:\n                weight_cosine = 0.25\n                weight_euclidean = 0.25\n                weight_manhattan = 0.25\n                weight_pearson = 0.25\n\n                cosine = float(metric.get(\"cosine\", 0))\n                euclidean = float(metric.get(\"euclidean\", 0))\n                manhattan = float(metric.get(\"manhattan\", 0))\n                pearson_correlation = float(metric.get(\"pearson_correlation\", 0))\n\n                similarity_value = (\n                    weight_cosine * cosine +\n                    weight_euclidean * euclidean +\n                    weight_manhattan * manhattan +\n                    weight_pearson * pearson_correlation\n                )\n\n        return similarity_value\n\n    async def calculate_reputation(self, ae: AggregationEvent):\n        \"\"\"\n        Calculate and update the reputation for all neighbor nodes based on active metrics.\n\n        This method processes the aggregated updates and then, based on the selected weighting factor (static or dynamic),\n        calculates the reputation. It also includes feedback and sends the reputation scores to neighbors.\n\n        Args:\n            ae (AggregationEvent): The event containing aggregated updates.\n        \"\"\"\n        (updates, _, _) = await ae.get_event_data()\n        if self._with_reputation:\n            logging.info(f\"Calculating reputation at round {self._engine.get_round()}\")\n            logging.info(f\"Active metrics: {self._reputation_metrics}\")\n            logging.info(f\"rejected nodes at round {self._engine.get_round()}: {self.rejected_nodes}\")\n\n            neighbors = set(await self._engine._cm.get_addrs_current_connections(only_direct=True))\n            history_data = self.history_data\n\n            for nei in neighbors:\n                metric_messages_number, metric_similarity, metric_fraction, metric_model_arrival_latency = (\n                    await self.calculate_value_metrics(\n                        self._log_dir,\n                        self._idx,\n                        self._addr,\n                        nei,\n                        metrics_active=self._reputation_metrics,\n                    )\n                )\n\n                if self._weighting_factor == \"dynamic\":\n                    self.calculate_weighted_values(\n                        metric_messages_number,\n                        metric_similarity,\n                        metric_fraction,\n                        metric_model_arrival_latency,\n                        history_data,\n                        self._engine.get_round(),\n                        self._addr,\n                        nei,\n                        self._reputation_metrics,\n                    )\n\n                if self._weighting_factor == \"static\" and self._engine.get_round() &gt;= 5:\n                    self._calculate_static_reputation(\n                        self._addr,\n                        nei,\n                        metric_messages_number,\n                        metric_similarity,\n                        metric_fraction,\n                        metric_model_arrival_latency,\n                        self._weight_num_messages,\n                        self._weight_model_similarity,\n                        self._weight_fraction_params_changed,\n                        self._weight_model_arrival_latency,\n                    )\n\n            if self._weighting_factor == \"dynamic\" and self._engine.get_round() &gt;= 5:\n                await self._calculate_dynamic_reputation(self._addr, neighbors)\n\n            if self._engine.get_round() &lt; 5 and self._with_reputation:\n                federation = self._engine.config.participant[\"network_args\"][\"neighbors\"].split()\n                self.init_reputation(\n                    self._addr,\n                    federation_nodes=federation,\n                    round_num=self._engine.get_round(),\n                    last_feedback_round=-1,\n                    init_reputation=self._initial_reputation,\n                )\n\n            status = await self.include_feedback_in_reputation()\n            if status:\n                logging.info(f\"Feedback included in reputation at round {self._engine.get_round()}\")\n            else:\n                logging.info(f\"Feedback not included in reputation at round {self._engine.get_round()}\")\n\n            if self.reputation is not None:\n                self.create_graphic_reputation(\n                    self._addr,\n                    self._engine.get_round(),\n                )\n\n                await self.update_process_aggregation(updates)\n                await self.send_reputation_to_neighbors(neighbors)\n\n    async def send_reputation_to_neighbors(self, neighbors):\n        \"\"\"\n        Send the calculated reputation scores to all neighbors.\n\n        Args:\n            neighbors (iterable): An iterable of neighbor node addresses.\n        \"\"\"\n        for nei, data in self.reputation.items():\n            if data[\"reputation\"] is not None:\n                neighbors_to_send = [neighbor for neighbor in neighbors if neighbor != nei]\n\n                for neighbor in neighbors_to_send:\n                    message = self._engine.cm.create_message(\n                        \"reputation\", \"share\", node_id=nei, score=float(data[\"reputation\"]), round=self._engine.get_round()\n                    )\n                    await self._engine.cm.send_message(neighbor, message)\n                    logging.info(\n                        f\"Sending reputation to node {nei} from node {neighbor} with reputation {data['reputation']}\"\n                    )\n\n    def create_graphic_reputation(self, addr, round_num):\n        \"\"\"\n        Create a graphical representation of the reputation scores and log the data.\n\n        Args:\n            addr (str): The current node's address.\n            round_num (int): The current round number.\n        \"\"\"\n        try:\n            reputation_dict_with_values = {\n                f\"Reputation/{addr}\": {\n                    node_id: float(data[\"reputation\"])\n                    for node_id, data in self.reputation.items()\n                    if data[\"reputation\"] is not None\n                }\n            }\n\n            logging.info(f\"Reputation dict: {reputation_dict_with_values}\")\n            self._engine.trainer._logger.log_data(reputation_dict_with_values, step=round_num)\n\n        except Exception:\n            logging.exception(\"Error creating reputation graphic\")\n\n    async def update_process_aggregation(self, updates):\n        \"\"\"\n        Update the aggregation process by removing nodes that have been rejected.\n\n        Args:\n            updates (dict): The dictionary of updates.\n        \"\"\"\n        for rn in self.rejected_nodes:\n            if rn in updates:\n                updates.pop(rn)\n\n        logging.info(f\"Updates after rejected nodes: {list(updates.keys())}\")\n        self.rejected_nodes.clear()\n        logging.info(f\"rejected nodes after clear at round {self._engine.get_round()}: {self.rejected_nodes}\")\n\n    async def include_feedback_in_reputation(self):\n        \"\"\"\n        Integrate feedback scores into the current reputation values.\n\n        The final reputation is computed as a weighted sum of the current reputation and the average feedback.\n\n        Returns:\n            bool: True if feedback was successfully included, False otherwise.\n        \"\"\"\n        weight_current_reputation = 0.9\n        weight_feedback = 0.1\n\n        if self.reputation_with_all_feedback is None:\n            logging.info(\"No feedback received.\")\n            return False\n\n        updated = False\n\n        for (current_node, node_ip, round_num), scores in self.reputation_with_all_feedback.items():\n            if not scores:\n                logging.info(f\"No feedback received for node {node_ip} in round {round_num}\")\n                continue\n\n            if node_ip not in self.reputation:\n                logging.info(f\"No reputation for node {node_ip}\")\n                continue\n\n            if \"last_feedback_round\" in self.reputation[node_ip] and self.reputation[node_ip][\"last_feedback_round\"] &gt;= round_num:\n                continue\n\n            avg_feedback = sum(scores) / len(scores)\n            logging.info(f\"Receive feedback to node {node_ip} with average score {avg_feedback}\")\n\n            current_reputation = self.reputation[node_ip][\"reputation\"]\n            if current_reputation is None:\n                logging.info(f\"No reputation calculate for node {node_ip}.\")\n                continue\n\n            combined_reputation = (current_reputation * weight_current_reputation) + (avg_feedback * weight_feedback)\n            logging.info(f\"Combined reputation for node {node_ip} in round {round_num}: {combined_reputation}\")\n\n            self.reputation[node_ip] = {\n                \"reputation\": combined_reputation,\n                \"round\": self._engine.get_round(),\n                \"last_feedback_round\": round_num,\n            }\n            updated = True\n            logging.info(f\"Updated self.reputation for {node_ip}: {self.reputation[node_ip]}\")\n\n        if updated:\n            return True\n        else:\n            return False\n\n    async def on_round_start(self, rse: RoundStartEvent):\n        \"\"\"\n        Event handler for the start of a round. It stores the start time and updates the expected nodes.\n\n        Args:\n            rse (RoundStartEvent): The event data containing round start information.\n        \"\"\"\n        (round_id, start_time, expected_nodes) = await rse.get_event_data()\n        if round_id not in self.round_timing_info:\n            self.round_timing_info[round_id] = {}\n        self.round_timing_info[round_id][\"start_time\"] = start_time\n        expected_nodes.difference_update(self.rejected_nodes)\n\n    async def recollect_model_arrival_latency(self, ure: UpdateReceivedEvent):\n        \"\"\"\n        Event handler to record the model arrival latency when an update is received.\n\n        Args:\n            ure (UpdateReceivedEvent): The event data for a model update.\n        \"\"\"\n        (decoded_model, weight, source, round_num, local) = await ure.get_event_data()\n        current_time = time.time()\n        current_round = round_num\n\n        if current_round not in self.round_timing_info:\n            self.round_timing_info[current_round] = {}\n\n        if \"model_received_time\" not in self.round_timing_info[current_round]:\n            self.round_timing_info[current_round][\"model_received_time\"] = {}\n\n        if source not in self.round_timing_info[current_round][\"model_received_time\"]:\n            self.round_timing_info[current_round][\"model_received_time\"][source] = current_time\n\n            if \"start_time\" in self.round_timing_info[current_round]:\n                start = self.round_timing_info[current_round][\"start_time\"]\n                received_time = self.round_timing_info[current_round][\"model_received_time\"][source]\n                duration = received_time - start\n                self.round_timing_info[current_round][\"duration\"] = duration\n                logging.info(f\"Source {source} , round {current_round}, duration: {duration:.4f} seconds\")\n\n                self.save_data(\n                    \"model_arrival_latency\",\n                    source,\n                    self._addr,\n                    num_round=current_round,\n                    current_round=self._engine.get_round(),\n                    latency=duration,\n                )\n        else:\n            logging.info(f\"Model arrival latency already calculated for node {source} in round {current_round}\")\n\n    async def recollect_similarity(self, ure: UpdateReceivedEvent):\n        \"\"\"\n        Event handler to recollect and store model similarity metrics when an update is received.\n\n        Args:\n            ure (UpdateReceivedEvent): The event data containing model update information.\n        \"\"\"\n        (decoded_model, weight, nei, round_num, local) = await ure.get_event_data()\n        if self._with_reputation and self._reputation_metrics.get(\"model_similarity\"):\n            if self._engine.config.participant[\"adaptive_args\"][\"model_similarity\"]:\n                if nei != self._addr:\n                    logging.info(\"\ud83e\udd16  handle_model_message | Checking model similarity\")\n                    cosine_value = cosine_metric(\n                        self._engine.trainer.get_model_parameters(),\n                        decoded_model,\n                        similarity=True,\n                    )\n                    euclidean_value = euclidean_metric(\n                        self._engine.trainer.get_model_parameters(),\n                        decoded_model,\n                        similarity=True,\n                    )\n                    minkowski_value = minkowski_metric(\n                        self._engine.trainer.get_model_parameters(),\n                        decoded_model,\n                        p=2,\n                        similarity=True,\n                    )\n                    manhattan_value = manhattan_metric(\n                        self._engine.trainer.get_model_parameters(),\n                        decoded_model,\n                        similarity=True,\n                    )\n                    pearson_correlation_value = pearson_correlation_metric(\n                        self._engine.trainer.get_model_parameters(),\n                        decoded_model,\n                        similarity=True,\n                    )\n                    jaccard_value = jaccard_metric(\n                        self._engine.trainer.get_model_parameters(),\n                        decoded_model,\n                        similarity=True,\n                    )\n\n                    similarity_metrics = {\n                        \"timestamp\": datetime.now(),\n                        \"nei\": nei,\n                        \"round\": round_num,\n                        \"current_round\": self._engine.get_round(),\n                        \"cosine\": cosine_value,\n                        \"euclidean\": euclidean_value,\n                        \"minkowski\": minkowski_value,\n                        \"manhattan\": manhattan_value,\n                        \"pearson_correlation\": pearson_correlation_value,\n                        \"jaccard\": jaccard_value,\n                    }\n\n                    if nei in self.connection_metrics:\n                        self.connection_metrics[nei].similarity.append(similarity_metrics)\n                        logging.info(f\"Stored similarity metrics for {nei}: {similarity_metrics}\")\n                    else:\n                        logging.warning(f\"No metrics instance found for neighbor {nei}\")\n\n                    if cosine_value &lt; 0.6:\n                        logging.info(\"\ud83e\udd16  handle_model_message | Model similarity is less than 0.6\")\n                        self.rejected_nodes.add(nei)\n\n    async def recollect_number_message(self, source, message):\n        \"\"\"\n        Event handler to collect message count metrics when a message is received.\n\n        Args:\n            source (str): The source node's address.\n            message: The received message (content not used directly).\n        \"\"\"\n        if source != self._addr:\n            current_time = time.time()\n            if current_time:\n                self.save_data(\n                    \"number_message\",\n                    source,\n                    self._addr,\n                    time=current_time,\n                    current_round=self._engine.get_round(),\n                )\n\n    async def recollect_fraction_of_parameters_changed(self, ure: UpdateReceivedEvent):\n        \"\"\"\n        Event handler to recollect the fraction of parameters changed when an update is received.\n\n        Args:\n            ure (UpdateReceivedEvent): The event data containing model update information.\n        \"\"\"\n        (decoded_model, weight, source, round_num, local) = await ure.get_event_data()\n        current_round = self._engine.get_round()\n        parameters_local = self._engine.trainer.get_model_parameters()\n        parameters_received = decoded_model\n        differences = []\n        total_params = 0\n        changed_params = 0\n        changes_record = {}\n        prev_threshold = None\n\n        if source in self.fraction_of_params_changed and current_round - 1 in self.fraction_of_params_changed[source]:\n            prev_threshold = self.fraction_of_params_changed[source][current_round - 1][-1][\"threshold\"]\n\n        for key in parameters_local.keys():\n            if key in parameters_received:\n                diff = torch.abs(parameters_local[key] - parameters_received[key])\n                differences.extend(diff.flatten().tolist())\n                total_params += diff.numel()\n\n        if differences:\n            mean_threshold = torch.mean(torch.tensor(differences)).item()\n            current_threshold = (prev_threshold + mean_threshold) / 2 if prev_threshold is not None else mean_threshold\n        else:\n            current_threshold = 0\n\n        for key in parameters_local.keys():\n            if key in parameters_received:\n                diff = torch.abs(parameters_local[key] - parameters_received[key])\n                num_changed = torch.sum(diff &gt; current_threshold).item()\n                changed_params += num_changed\n                if num_changed &gt; 0:\n                    changes_record[key] = num_changed\n\n        fraction_changed = changed_params / total_params if total_params &gt; 0 else 0.0\n\n        if source not in self.fraction_of_params_changed:\n            self.fraction_of_params_changed[source] = {}\n        if current_round not in self.fraction_of_params_changed[source]:\n            self.fraction_of_params_changed[source][current_round] = []\n\n        self.fraction_of_params_changed[source][current_round].append({\n            \"fraction_changed\": fraction_changed,\n            \"total_params\": total_params,\n            \"changed_params\": changed_params,\n            \"threshold\": current_threshold,\n            \"changes_record\": changes_record,\n        })\n\n        self.save_data(\n            \"fraction_of_params_changed\",\n            source,\n            self._addr,\n            current_round,\n            fraction_changed=fraction_changed,\n            total_params=total_params,\n            changed_params=changed_params,\n            threshold=current_threshold,\n            changes_record=changes_record,\n        )\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.engine","title":"<code>engine</code>  <code>property</code>","text":"<p>Return the engine instance.</p>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.__init__","title":"<code>__init__(engine, config)</code>","text":"<p>Initialize the Reputation system.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>The engine instance providing the runtime context.</p> required <code>config</code> <code>Config</code> <p>The configuration object with participant settings.</p> required Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>def __init__(self, engine: \"Engine\", config: \"Config\"):\n    \"\"\"\n    Initialize the Reputation system.\n\n    Args:\n        engine (Engine): The engine instance providing the runtime context.\n        config (Config): The configuration object with participant settings.\n    \"\"\"\n    self._engine = engine\n    self._config = config\n    self.fraction_of_params_changed = {}\n    self.history_data = {}\n    self.metric_weights = {}\n    self.reputation = {}\n    self.reputation_with_feedback = {}\n    self.reputation_with_all_feedback = {}\n    self.rejected_nodes = set()\n    self.round_timing_info = {}\n    self._messages_received_from_sources = {}\n    self.reputation_history = {}\n    self.number_message_history = {}\n    self.neighbor_reputation_history = {}\n    self.fraction_changed_history = {}\n    self.messages_number_message = []\n    self.previous_threshold_number_message = {}\n    self.previous_std_dev_number_message = {}\n    self.messages_model_arrival_latency = {}\n    self.model_arrival_latency_history = {}\n    self.previous_percentile_25_number_message = {}\n    self.previous_percentile_85_number_message = {}\n    self._addr = engine.addr\n    self._log_dir = engine.log_dir\n    self._idx = engine.idx\n    self.connection_metrics = []\n\n    neighbors: str = self._config.participant[\"network_args\"][\"neighbors\"]\n    self.connection_metrics = {}\n    for nei in neighbors.split():\n        self.connection_metrics[f\"{nei}\"] = Metrics()\n\n    self._with_reputation = self._config.participant[\"defense_args\"][\"with_reputation\"]\n    self._reputation_metrics = self._config.participant[\"defense_args\"][\"reputation_metrics\"]\n    self._initial_reputation = float(self._config.participant[\"defense_args\"][\"initial_reputation\"])\n    self._weighting_factor = self._config.participant[\"defense_args\"][\"weighting_factor\"]\n    self._weight_model_arrival_latency = float(self._config.participant[\"defense_args\"][\"weight_model_arrival_latency\"])\n    self._weight_model_similarity = float(self._config.participant[\"defense_args\"][\"weight_model_similarity\"])\n    self._weight_num_messages = float(self._config.participant[\"defense_args\"][\"weight_num_messages\"])\n    self._weight_fraction_params_changed = float(self._config.participant[\"defense_args\"][\"weight_fraction_params_changed\"])\n\n    msg = f\"Reputation system: {self._with_reputation}\"\n    msg += f\"\\nReputation metrics: {self._reputation_metrics}\"\n    msg += f\"\\nInitial reputation: {self._initial_reputation}\"\n    msg += f\"\\nWeighting factor: {self._weighting_factor}\"\n    if self._weighting_factor == \"static\":\n        msg += f\"\\nWeight model arrival latency: {self._weight_model_arrival_latency}\"\n        msg += f\"\\nWeight model similarity: {self._weight_model_similarity}\"\n        msg += f\"\\nWeight number of messages: {self._weight_num_messages}\"\n        msg += f\"\\nWeight fraction of parameters changed: {self._weight_fraction_params_changed}\"\n    print_msg_box(msg=msg, indent=2, title=\"Defense information\")\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.analyze_anomalies","title":"<code>analyze_anomalies(addr, nei, round_num, current_round, fraction_changed, threshold)</code>","text":"<p>Analyze anomalies in the fraction of parameters changed and calculate a corresponding score.</p> <p>Parameters:</p> Name Type Description Default <code>addr</code> <code>str</code> <p>The source node's address.</p> required <code>nei</code> <code>str</code> <p>The neighbor node's address.</p> required <code>round_num</code> <code>int</code> <p>The round number for the metric.</p> required <code>current_round</code> <code>int</code> <p>The current round number.</p> required <code>fraction_changed</code> <code>float</code> <p>Fraction of parameters changed.</p> required <code>threshold</code> <code>float</code> <p>Threshold value for changes.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>A normalized fraction score between 0 and 1.</p> Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>def analyze_anomalies(\n    self,\n    addr,\n    nei,\n    round_num,\n    current_round,\n    fraction_changed,\n    threshold,\n):\n    \"\"\"\n    Analyze anomalies in the fraction of parameters changed and calculate a corresponding score.\n\n    Args:\n        addr (str): The source node's address.\n        nei (str): The neighbor node's address.\n        round_num (int): The round number for the metric.\n        current_round (int): The current round number.\n        fraction_changed (float): Fraction of parameters changed.\n        threshold (float): Threshold value for changes.\n\n    Returns:\n        float: A normalized fraction score between 0 and 1.\n    \"\"\"\n    try:\n        key = (addr, nei, round_num)\n\n        if key not in self.fraction_changed_history:\n            prev_key = (addr, nei, round_num - 1)\n            if round_num &gt; 0 and prev_key in self.fraction_changed_history:\n                previous_data = self.fraction_changed_history[prev_key]\n                fraction_changed = (\n                    fraction_changed if fraction_changed is not None else previous_data[\"fraction_changed\"]\n                )\n                threshold = threshold if threshold is not None else previous_data[\"threshold\"]\n            else:\n                fraction_changed = fraction_changed if fraction_changed is not None else 0\n                threshold = threshold if threshold is not None else 0\n\n            self.fraction_changed_history[key] = {\n                \"fraction_changed\": fraction_changed,\n                \"threshold\": threshold,\n                \"fraction_score\": None,\n                \"fraction_anomaly\": False,\n                \"threshold_anomaly\": False,\n                \"mean_fraction\": None,\n                \"std_dev_fraction\": None,\n                \"mean_threshold\": None,\n                \"std_dev_threshold\": None,\n            }\n\n        if round_num &lt; 5:\n            past_fractions = []\n            past_thresholds = []\n\n            for r in range(round_num):\n                past_key = (addr, nei, r)\n                if past_key in self.fraction_changed_history:\n                    past_fractions.append(self.fraction_changed_history[past_key][\"fraction_changed\"])\n                    past_thresholds.append(self.fraction_changed_history[past_key][\"threshold\"])\n\n            if past_fractions:\n                mean_fraction = np.mean(past_fractions)\n                std_dev_fraction = np.std(past_fractions)\n                self.fraction_changed_history[key][\"mean_fraction\"] = mean_fraction\n                self.fraction_changed_history[key][\"std_dev_fraction\"] = std_dev_fraction\n\n            if past_thresholds:\n                mean_threshold = np.mean(past_thresholds)\n                std_dev_threshold = np.std(past_thresholds)\n                self.fraction_changed_history[key][\"mean_threshold\"] = mean_threshold\n                self.fraction_changed_history[key][\"std_dev_threshold\"] = std_dev_threshold\n\n            return 0\n        else:\n            fraction_value = 0\n            threshold_value = 0\n            prev_key = (addr, nei, round_num - 1)\n            if prev_key not in self.fraction_changed_history:\n                for i in range(0, round_num + 1):\n                    potential_prev_key = (addr, nei, round_num - i)\n                    if potential_prev_key in self.fraction_changed_history:\n                        mean_fraction_prev = self.fraction_changed_history[potential_prev_key][\n                            \"mean_fraction\"\n                        ]\n                        if mean_fraction_prev is not None:\n                            prev_key = potential_prev_key\n                            break\n\n            if prev_key:\n                mean_fraction_prev = self.fraction_changed_history[prev_key][\"mean_fraction\"]\n                std_dev_fraction_prev = self.fraction_changed_history[prev_key][\"std_dev_fraction\"]\n                mean_threshold_prev = self.fraction_changed_history[prev_key][\"mean_threshold\"]\n                std_dev_threshold_prev = self.fraction_changed_history[prev_key][\"std_dev_threshold\"]\n\n                current_fraction = self.fraction_changed_history[key][\"fraction_changed\"]\n                current_threshold = self.fraction_changed_history[key][\"threshold\"]\n\n                upper_mean_fraction_prev = (mean_fraction_prev + std_dev_fraction_prev) * 1.05\n                upper_mean_threshold_prev = (mean_threshold_prev + std_dev_threshold_prev) * 1.10\n\n                fraction_anomaly = current_fraction &gt; upper_mean_fraction_prev\n                threshold_anomaly = current_threshold &gt; upper_mean_threshold_prev\n\n                self.fraction_changed_history[key][\"fraction_anomaly\"] = fraction_anomaly\n                self.fraction_changed_history[key][\"threshold_anomaly\"] = threshold_anomaly\n\n                penalization_factor_fraction = abs(current_fraction - mean_fraction_prev) / mean_fraction_prev if mean_fraction_prev != 0 else 1\n                penalization_factor_threshold = abs(current_threshold - mean_threshold_prev) / mean_threshold_prev if mean_threshold_prev != 0 else 1\n\n                k_fraction = penalization_factor_fraction if penalization_factor_fraction != 0 else 1\n                k_threshold = penalization_factor_threshold if penalization_factor_threshold != 0 else 1\n\n                if fraction_anomaly:\n                    fraction_value = (\n                        1 - (1 / (1 + np.exp(-k_fraction)))\n                        if current_fraction is not None and mean_fraction_prev is not None\n                        else 0\n                    )\n                else:\n                    fraction_value = (\n                        1 - (1 / (1 + np.exp(k_fraction)))\n                        if current_fraction is not None and mean_fraction_prev is not None\n                        else 0\n                    )\n\n                if threshold_anomaly:\n                    threshold_value = (\n                        1 - (1 / (1 + np.exp(-k_threshold)))\n                        if current_threshold is not None and mean_threshold_prev is not None\n                        else 0\n                    )\n                else:\n                    threshold_value = (\n                        1 - (1 / (1 + np.exp(k_threshold)))\n                        if current_threshold is not None and mean_threshold_prev is not None\n                        else 0\n                    )\n\n                fraction_weight = 0.5\n                threshold_weight = 0.5\n\n                fraction_score = fraction_weight * fraction_value + threshold_weight * threshold_value\n\n                self.fraction_changed_history[key][\"mean_fraction\"] = (current_fraction + mean_fraction_prev) / 2\n                self.fraction_changed_history[key][\"std_dev_fraction\"] = np.sqrt(((current_fraction - mean_fraction_prev) ** 2 + std_dev_fraction_prev**2) / 2)\n                self.fraction_changed_history[key][\"mean_threshold\"] = (current_threshold + mean_threshold_prev) / 2\n                self.fraction_changed_history[key][\"std_dev_threshold\"] = np.sqrt(((0.1 * (current_threshold - mean_threshold_prev) ** 2) + std_dev_threshold_prev**2) / 2)\n\n                return max(fraction_score, 0)\n            else:\n                return -1\n    except Exception:\n        logging.exception(\"Error analyzing anomalies\")\n        return -1\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.calculate_decay_rate","title":"<code>calculate_decay_rate(reputation)</code>","text":"<p>Calculate the decay rate for a given reputation value.</p> <p>Parameters:</p> Name Type Description Default <code>reputation</code> <code>float</code> <p>The current reputation value.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The decay rate.</p> Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>def calculate_decay_rate(self, reputation):\n    \"\"\"\n    Calculate the decay rate for a given reputation value.\n\n    Args:\n        reputation (float): The current reputation value.\n\n    Returns:\n        float: The decay rate.\n    \"\"\"\n    if reputation &gt; 0.8:\n        return 0.9  # Very low decay\n    elif reputation &gt; 0.7:\n        return 0.8  # Medium decay\n    elif reputation &gt; 0.6:\n        return 0.6  # Low decay\n    elif reputation &gt; 0.4:\n        return 0.2  # High decay\n    else:\n        return 0.1  # Very high decay\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.calculate_reputation","title":"<code>calculate_reputation(ae)</code>  <code>async</code>","text":"<p>Calculate and update the reputation for all neighbor nodes based on active metrics.</p> <p>This method processes the aggregated updates and then, based on the selected weighting factor (static or dynamic), calculates the reputation. It also includes feedback and sends the reputation scores to neighbors.</p> <p>Parameters:</p> Name Type Description Default <code>ae</code> <code>AggregationEvent</code> <p>The event containing aggregated updates.</p> required Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>async def calculate_reputation(self, ae: AggregationEvent):\n    \"\"\"\n    Calculate and update the reputation for all neighbor nodes based on active metrics.\n\n    This method processes the aggregated updates and then, based on the selected weighting factor (static or dynamic),\n    calculates the reputation. It also includes feedback and sends the reputation scores to neighbors.\n\n    Args:\n        ae (AggregationEvent): The event containing aggregated updates.\n    \"\"\"\n    (updates, _, _) = await ae.get_event_data()\n    if self._with_reputation:\n        logging.info(f\"Calculating reputation at round {self._engine.get_round()}\")\n        logging.info(f\"Active metrics: {self._reputation_metrics}\")\n        logging.info(f\"rejected nodes at round {self._engine.get_round()}: {self.rejected_nodes}\")\n\n        neighbors = set(await self._engine._cm.get_addrs_current_connections(only_direct=True))\n        history_data = self.history_data\n\n        for nei in neighbors:\n            metric_messages_number, metric_similarity, metric_fraction, metric_model_arrival_latency = (\n                await self.calculate_value_metrics(\n                    self._log_dir,\n                    self._idx,\n                    self._addr,\n                    nei,\n                    metrics_active=self._reputation_metrics,\n                )\n            )\n\n            if self._weighting_factor == \"dynamic\":\n                self.calculate_weighted_values(\n                    metric_messages_number,\n                    metric_similarity,\n                    metric_fraction,\n                    metric_model_arrival_latency,\n                    history_data,\n                    self._engine.get_round(),\n                    self._addr,\n                    nei,\n                    self._reputation_metrics,\n                )\n\n            if self._weighting_factor == \"static\" and self._engine.get_round() &gt;= 5:\n                self._calculate_static_reputation(\n                    self._addr,\n                    nei,\n                    metric_messages_number,\n                    metric_similarity,\n                    metric_fraction,\n                    metric_model_arrival_latency,\n                    self._weight_num_messages,\n                    self._weight_model_similarity,\n                    self._weight_fraction_params_changed,\n                    self._weight_model_arrival_latency,\n                )\n\n        if self._weighting_factor == \"dynamic\" and self._engine.get_round() &gt;= 5:\n            await self._calculate_dynamic_reputation(self._addr, neighbors)\n\n        if self._engine.get_round() &lt; 5 and self._with_reputation:\n            federation = self._engine.config.participant[\"network_args\"][\"neighbors\"].split()\n            self.init_reputation(\n                self._addr,\n                federation_nodes=federation,\n                round_num=self._engine.get_round(),\n                last_feedback_round=-1,\n                init_reputation=self._initial_reputation,\n            )\n\n        status = await self.include_feedback_in_reputation()\n        if status:\n            logging.info(f\"Feedback included in reputation at round {self._engine.get_round()}\")\n        else:\n            logging.info(f\"Feedback not included in reputation at round {self._engine.get_round()}\")\n\n        if self.reputation is not None:\n            self.create_graphic_reputation(\n                self._addr,\n                self._engine.get_round(),\n            )\n\n            await self.update_process_aggregation(updates)\n            await self.send_reputation_to_neighbors(neighbors)\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.calculate_similarity_from_metrics","title":"<code>calculate_similarity_from_metrics(nei, current_round)</code>","text":"<p>Calculate the similarity score based on stored similarity metrics.</p> <p>Parameters:</p> Name Type Description Default <code>nei</code> <code>str</code> <p>The neighbor node's address.</p> required <code>current_round</code> <code>int</code> <p>The current round number.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The aggregated similarity score.</p> Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>def calculate_similarity_from_metrics(self, nei, current_round):\n    \"\"\"\n    Calculate the similarity score based on stored similarity metrics.\n\n    Args:\n        nei (str): The neighbor node's address.\n        current_round (int): The current round number.\n\n    Returns:\n        float: The aggregated similarity score.\n    \"\"\"\n    similarity_value = 0.0\n\n    metrics_instance = self.connection_metrics.get(nei)\n    if metrics_instance is None:\n        logging.error(f\"No metrics instance found for neighbor {nei}\")\n        return similarity_value\n\n    for metric in metrics_instance.similarity:\n        source_ip = metric.get(\"nei\")\n        round_in_metric = metric.get(\"round\")\n\n        if source_ip == nei and round_in_metric == current_round:\n            weight_cosine = 0.25\n            weight_euclidean = 0.25\n            weight_manhattan = 0.25\n            weight_pearson = 0.25\n\n            cosine = float(metric.get(\"cosine\", 0))\n            euclidean = float(metric.get(\"euclidean\", 0))\n            manhattan = float(metric.get(\"manhattan\", 0))\n            pearson_correlation = float(metric.get(\"pearson_correlation\", 0))\n\n            similarity_value = (\n                weight_cosine * cosine +\n                weight_euclidean * euclidean +\n                weight_manhattan * manhattan +\n                weight_pearson * pearson_correlation\n            )\n\n    return similarity_value\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.calculate_value_metrics","title":"<code>calculate_value_metrics(log_dir, id_node, addr, nei, metrics_active=None)</code>  <code>async</code>","text":"<p>Calculate various metrics (message count, model similarity, fraction of parameters changed, and model arrival latency) for a given neighbor node based on stored connection data.</p> <p>Parameters:</p> Name Type Description Default <code>log_dir</code> <code>str</code> <p>Directory for log files.</p> required <code>id_node</code> <code>str</code> <p>Identifier for the node.</p> required <code>addr</code> <code>str</code> <p>The address of the current node.</p> required <code>nei</code> <code>str</code> <p>The neighbor node's address.</p> required <code>metrics_active</code> <code>dict</code> <p>Dictionary indicating which metrics are active.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing: - avg_messages_number_message_normalized (float) - similarity_reputation (float) - fraction_score_asign (float) - avg_model_arrival_latency (float)</p> Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>async def calculate_value_metrics(self, log_dir, id_node, addr, nei, metrics_active=None):\n    \"\"\"\n    Calculate various metrics (message count, model similarity, fraction of parameters changed, and model arrival latency)\n    for a given neighbor node based on stored connection data.\n\n    Args:\n        log_dir (str): Directory for log files.\n        id_node (str): Identifier for the node.\n        addr (str): The address of the current node.\n        nei (str): The neighbor node's address.\n        metrics_active (dict): Dictionary indicating which metrics are active.\n\n    Returns:\n        tuple: A tuple containing:\n            - avg_messages_number_message_normalized (float)\n            - similarity_reputation (float)\n            - fraction_score_asign (float)\n            - avg_model_arrival_latency (float)\n    \"\"\"\n    messages_number_message_normalized = 0\n    messages_number_message_count = 0\n    avg_messages_number_message_normalized = 0\n    fraction_score_normalized = 0\n    fraction_score_asign = 0\n    messages_model_arrival_latency_normalized = 0\n    avg_model_arrival_latency = 0\n    similarity_reputation = 0\n    fraction_neighbors_scores = None\n\n    try:\n        current_round = self._engine.get_round()\n\n        metrics_instance = self.connection_metrics.get(nei)\n        if not metrics_instance:\n            logging.warning(f\"No metrics found for neighbor {nei}\")\n            return avg_messages_number_message_normalized, similarity_reputation, fraction_score_asign, avg_model_arrival_latency\n\n        if metrics_active.get(\"num_messages\", False):\n            filtered_messages = [msg for msg in metrics_instance.messages if msg.get(\"current_round\") == current_round]\n            for msg in filtered_messages:\n                self.messages_number_message.append({\n                    \"number_message\": msg.get(\"time\"),\n                    \"current_round\": msg.get(\"current_round\"),\n                    \"key\": (addr, nei),\n                })\n\n            messages_number_message_normalized, messages_number_message_count = self.manage_metric_number_message(\n                self.messages_number_message, addr, nei, current_round, True\n            )\n            avg_messages_number_message_normalized = self.save_number_message_history(\n                addr, nei, messages_number_message_normalized, current_round\n            )\n            if avg_messages_number_message_normalized is None and current_round &gt; 4:\n                avg_messages_number_message_normalized = self.number_message_history[(addr, nei)][current_round - 1][\"avg_number_message\"]\n\n        if metrics_active.get(\"fraction_parameters_changed\", False):\n            if metrics_instance.fraction_of_params_changed.get(\"round\") == current_round:\n                fraction_changed = metrics_instance.fraction_of_params_changed.get(\"fraction_changed\")\n                threshold = metrics_instance.fraction_of_params_changed.get(\"threshold\")\n                fraction_score_normalized = self.analyze_anomalies(\n                    addr,\n                    nei,\n                    current_round,\n                    current_round,  # Assumes round_received is the current round.\n                    fraction_changed,\n                    threshold,\n                )\n\n        if metrics_active.get(\"model_arrival_latency\", False):\n            if metrics_instance.model_arrival_latency.get(\"round_received\") == current_round:\n                round_latency = metrics_instance.model_arrival_latency.get(\"round\")\n                latency = metrics_instance.model_arrival_latency.get(\"latency\")\n                messages_model_arrival_latency_normalized = self.manage_model_arrival_latency(\n                    round_latency,\n                    addr,\n                    nei,\n                    latency,\n                    current_round\n                )\n\n        if current_round &gt;= 5 and metrics_active.get(\"model_similarity\", False):\n            similarity_reputation = self.calculate_similarity_from_metrics(nei, current_round)\n        else:\n            similarity_reputation = 0\n\n        if messages_model_arrival_latency_normalized &gt;= 0:\n            avg_model_arrival_latency = self.save_model_arrival_latency_history(\n                addr, nei, messages_model_arrival_latency_normalized, current_round\n            )\n            if avg_model_arrival_latency is None and current_round &gt; 4:\n                avg_model_arrival_latency = self.model_arrival_latency_history[(addr, nei)][current_round - 1][\"score\"]\n\n        if self.messages_number_message is not None:\n            messages_number_message_normalized, messages_number_message_count = self.manage_metric_number_message(\n                self.messages_number_message, addr, nei, current_round, metrics_active.get(\"num_messages\", False)\n            )\n            avg_messages_number_message_normalized = self.save_number_message_history(\n                addr, nei, messages_number_message_normalized, current_round\n            )\n            if avg_messages_number_message_normalized is None and current_round &gt; 4:\n                avg_messages_number_message_normalized = self.number_message_history[(addr, nei)][current_round - 1][\"avg_number_message\"]\n\n        if current_round &gt;= 5:\n            if fraction_score_normalized &gt; 0:\n                key_previous_round = (addr, nei, current_round - 1) if current_round - 1 &gt; 0 else None\n                fraction_previous_round = None\n\n                if key_previous_round is not None and key_previous_round in self.fraction_changed_history:\n                    fraction_score_prev = self.fraction_changed_history[key_previous_round].get(\"fraction_score\")\n                    fraction_previous_round = fraction_score_prev if fraction_score_prev is not None else None\n\n                if fraction_previous_round is not None:\n                    fraction_score_asign = fraction_score_normalized * 0.8 + fraction_previous_round * 0.2\n                    self.fraction_changed_history[(addr, nei, current_round)][\"fraction_score\"] = fraction_score_asign\n                else:\n                    fraction_score_asign = fraction_score_normalized\n                    self.fraction_changed_history[(addr, nei, current_round)][\"fraction_score\"] = fraction_score_asign\n            else:\n                fraction_previous_round = None\n                key_previous_round = (addr, nei, current_round - 1) if current_round - 1 &gt; 0 else None\n                if key_previous_round is not None and key_previous_round in self.fraction_changed_history:\n                    fraction_score_prev = self.fraction_changed_history[key_previous_round].get(\"fraction_score\")\n                    fraction_previous_round = fraction_score_prev if fraction_score_prev is not None else None\n\n                if fraction_previous_round is not None:\n                    fraction_score_asign = fraction_previous_round - (fraction_previous_round * 0.5)\n                else:\n                    if fraction_neighbors_scores is None:\n                        fraction_neighbors_scores = {}\n\n                    for key, value in self.fraction_changed_history.items():\n                        score = value.get(\"fraction_score\")\n                        if score is not None:\n                            fraction_neighbors_scores[key] = score\n\n                    if fraction_neighbors_scores:\n                        fraction_score_asign = np.mean(list(fraction_neighbors_scores.values()))\n                    else:\n                        fraction_score_asign = 0 \n        else:\n            fraction_score_asign = 0\n\n        self.create_graphics_to_metrics(\n            messages_number_message_count,\n            avg_messages_number_message_normalized,\n            similarity_reputation,\n            fraction_score_asign,\n            avg_model_arrival_latency,\n            addr,\n            nei,\n            current_round,\n            self.engine.total_rounds,\n        )\n\n        return avg_messages_number_message_normalized, similarity_reputation, fraction_score_asign, avg_model_arrival_latency\n\n    except Exception as e:\n        logging.exception(f\"Error calculating reputation. Type: {type(e).__name__}\")\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.calculate_weighted_values","title":"<code>calculate_weighted_values(avg_messages_number_message_normalized, similarity_reputation, fraction_score_asign, avg_model_arrival_latency, history_data, current_round, addr, nei, reputation_metrics)</code>","text":"<p>Calculate the weighted values for each metric based on current measurements and historical data.</p> <p>Parameters:</p> Name Type Description Default <code>avg_messages_number_message_normalized</code> <code>float</code> <p>Normalized average message count.</p> required <code>similarity_reputation</code> <code>float</code> <p>Reputation score based on model similarity.</p> required <code>fraction_score_asign</code> <code>float</code> <p>Score assigned from fraction of parameters changed.</p> required <code>avg_model_arrival_latency</code> <code>float</code> <p>Average model arrival latency.</p> required <code>history_data</code> <code>dict</code> <p>Historical metrics data.</p> required <code>current_round</code> <code>int</code> <p>The current round number.</p> required <code>addr</code> <code>str</code> <p>The address of the current node.</p> required <code>nei</code> <code>str</code> <p>The neighbor node's address.</p> required <code>reputation_metrics</code> <code>dict</code> <p>Dictionary indicating which metrics are active.</p> required Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>def calculate_weighted_values(\n    self,\n    avg_messages_number_message_normalized,\n    similarity_reputation,\n    fraction_score_asign,\n    avg_model_arrival_latency,\n    history_data,\n    current_round,\n    addr,\n    nei,\n    reputation_metrics\n):\n    \"\"\"\n    Calculate the weighted values for each metric based on current measurements and historical data.\n\n    Args:\n        avg_messages_number_message_normalized (float): Normalized average message count.\n        similarity_reputation (float): Reputation score based on model similarity.\n        fraction_score_asign (float): Score assigned from fraction of parameters changed.\n        avg_model_arrival_latency (float): Average model arrival latency.\n        history_data (dict): Historical metrics data.\n        current_round (int): The current round number.\n        addr (str): The address of the current node.\n        nei (str): The neighbor node's address.\n        reputation_metrics (dict): Dictionary indicating which metrics are active.\n    \"\"\"\n    if current_round is not None:\n\n        normalized_weights = {}\n        required_keys = [\n            \"num_messages\",\n            \"model_similarity\",\n            \"fraction_parameters_changed\",\n            \"model_arrival_latency\",\n        ]\n\n        for key in required_keys:\n            if key not in history_data:\n                history_data[key] = []\n\n        metrics = {\n            \"num_messages\": avg_messages_number_message_normalized,\n            \"model_similarity\": similarity_reputation,\n            \"fraction_parameters_changed\": fraction_score_asign,\n            \"model_arrival_latency\": avg_model_arrival_latency,                   \n        }\n\n        active_metrics = {k: v for k, v in metrics.items() if reputation_metrics.get(k, False)}\n        num_active_metrics = len(active_metrics)\n\n        for metric_name, current_value in active_metrics.items():\n            history_data[metric_name].append({\n                \"round\": current_round,\n                \"addr\": addr,\n                \"nei\": nei,\n                \"metric_name\": metric_name,\n                \"metric_value\": current_value,\n                \"weight\": None\n            })\n\n        adjusted_weights = {}\n\n        if current_round &gt;= 5 and num_active_metrics &gt; 0:\n            desviations = {}\n            for metric_name, current_value in active_metrics.items():\n                historical_values = history_data[metric_name]\n\n                metric_values = [entry['metric_value'] for entry in historical_values if 'metric_value' in entry and entry[\"metric_value\"] != 0]\n\n                if metric_values:\n                    mean_value = np.mean(metric_values)\n                else:\n                    mean_value = 0\n\n                deviation = abs(current_value - mean_value)\n                desviations[metric_name] = deviation\n\n            if all(deviation == 0.0 for deviation in desviations.values()):\n                random_weights = [random.random() for _ in range(num_active_metrics)]\n                total_random_weight = sum(random_weights)\n                normalized_weights = {metric_name: weight / total_random_weight for metric_name, weight in zip(active_metrics, random_weights)}\n            else:\n                max_desviation = max(desviations.values()) if desviations else 1\n                normalized_weights = {\n                    metric_name: (desviation / max_desviation) for metric_name, desviation in desviations.items()\n                }\n\n                total_weight = sum(normalized_weights.values())\n                if total_weight &gt; 0:\n                    normalized_weights = {\n                        metric_name: weight / total_weight for metric_name, weight in normalized_weights.items()\n                    }\n                else:\n                    normalized_weights = {metric_name: 1 / num_active_metrics for metric_name in active_metrics}\n\n            mean_deviation = np.mean(list(desviations.values()))\n            dynamic_min_weight = max(0.1, mean_deviation / (mean_deviation + 1)) \n\n            total_adjusted_weight = 0\n\n            for metric_name, weight in normalized_weights.items():\n                if weight &lt; dynamic_min_weight:\n                    adjusted_weights[metric_name] = dynamic_min_weight\n                else:\n                    adjusted_weights[metric_name] = weight\n                total_adjusted_weight += adjusted_weights[metric_name]\n\n            if total_adjusted_weight &gt; 1:\n                for metric_name in adjusted_weights:\n                    adjusted_weights[metric_name] /= total_adjusted_weight\n                total_adjusted_weight = 1\n        else:\n            adjusted_weights = {metric_name: 1 / num_active_metrics for metric_name in active_metrics}\n\n        for metric_name, current_value in active_metrics.items():\n            weight = adjusted_weights.get(metric_name, -1)\n            for entry in history_data[metric_name]:\n                if entry[\"metric_name\"] == metric_name and entry[\"round\"] == current_round and entry[\"nei\"] == nei:\n                    entry[\"weight\"] = weight\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.create_graphic_reputation","title":"<code>create_graphic_reputation(addr, round_num)</code>","text":"<p>Create a graphical representation of the reputation scores and log the data.</p> <p>Parameters:</p> Name Type Description Default <code>addr</code> <code>str</code> <p>The current node's address.</p> required <code>round_num</code> <code>int</code> <p>The current round number.</p> required Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>def create_graphic_reputation(self, addr, round_num):\n    \"\"\"\n    Create a graphical representation of the reputation scores and log the data.\n\n    Args:\n        addr (str): The current node's address.\n        round_num (int): The current round number.\n    \"\"\"\n    try:\n        reputation_dict_with_values = {\n            f\"Reputation/{addr}\": {\n                node_id: float(data[\"reputation\"])\n                for node_id, data in self.reputation.items()\n                if data[\"reputation\"] is not None\n            }\n        }\n\n        logging.info(f\"Reputation dict: {reputation_dict_with_values}\")\n        self._engine.trainer._logger.log_data(reputation_dict_with_values, step=round_num)\n\n    except Exception:\n        logging.exception(\"Error creating reputation graphic\")\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.create_graphics_to_metrics","title":"<code>create_graphics_to_metrics(number_message_count, number_message_norm, similarity, fraction, model_arrival_latency, addr, nei, current_round, total_rounds)</code>","text":"<p>Create and log graphics representing different metric values over the rounds.</p> <p>Parameters:</p> Name Type Description Default <code>number_message_count</code> <code>int</code> <p>Count of messages.</p> required <code>number_message_norm</code> <code>float</code> <p>Normalized number of messages.</p> required <code>similarity</code> <code>float</code> <p>Similarity metric value.</p> required <code>fraction</code> <code>float</code> <p>Fraction score metric.</p> required <code>model_arrival_latency</code> <code>float</code> <p>Latency metric score.</p> required <code>addr</code> <code>str</code> <p>The current node's address.</p> required <code>nei</code> <code>str</code> <p>The neighbor node's address.</p> required <code>current_round</code> <code>int</code> <p>The current round number.</p> required <code>total_rounds</code> <code>int</code> <p>Total rounds in the session.</p> required Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>def create_graphics_to_metrics(\n    self,\n    number_message_count,\n    number_message_norm,\n    similarity,\n    fraction,\n    model_arrival_latency,\n    addr,\n    nei,\n    current_round,\n    total_rounds,\n):\n    \"\"\"\n    Create and log graphics representing different metric values over the rounds.\n\n    Args:\n        number_message_count (int): Count of messages.\n        number_message_norm (float): Normalized number of messages.\n        similarity (float): Similarity metric value.\n        fraction (float): Fraction score metric.\n        model_arrival_latency (float): Latency metric score.\n        addr (str): The current node's address.\n        nei (str): The neighbor node's address.\n        current_round (int): The current round number.\n        total_rounds (int): Total rounds in the session.\n    \"\"\"\n    if current_round is not None and current_round &lt; total_rounds:\n        model_arrival_latency_dict = {f\"R-Model_arrival_latency_reputation/{addr}\": {nei: model_arrival_latency}}\n        messages_number_message_count_dict = {f\"R-Count_messages_number_message_reputation/{addr}\": {nei: number_message_count}}\n        messages_number_message_norm_dict = {f\"R-number_message_reputation/{addr}\": {nei: number_message_norm}}\n        similarity_dict = {f\"R-Similarity_reputation/{addr}\": {nei: similarity}}\n        fraction_dict = {f\"R-Fraction_reputation/{addr}\": {nei: fraction}}\n\n        if messages_number_message_count_dict is not None:\n            self.engine.trainer._logger.log_data(messages_number_message_count_dict, step=current_round)\n\n        if messages_number_message_norm_dict is not None:\n            self.engine.trainer._logger.log_data(messages_number_message_norm_dict, step=current_round)\n\n        if similarity_dict is not None:\n            self.engine.trainer._logger.log_data(similarity_dict, step=current_round)\n\n        if fraction_dict is not None:\n            self.engine.trainer._logger.log_data(fraction_dict, step=current_round)\n\n        if model_arrival_latency_dict is not None:\n            self.engine.trainer._logger.log_data(model_arrival_latency_dict, step=current_round)\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.include_feedback_in_reputation","title":"<code>include_feedback_in_reputation()</code>  <code>async</code>","text":"<p>Integrate feedback scores into the current reputation values.</p> <p>The final reputation is computed as a weighted sum of the current reputation and the average feedback.</p> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if feedback was successfully included, False otherwise.</p> Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>async def include_feedback_in_reputation(self):\n    \"\"\"\n    Integrate feedback scores into the current reputation values.\n\n    The final reputation is computed as a weighted sum of the current reputation and the average feedback.\n\n    Returns:\n        bool: True if feedback was successfully included, False otherwise.\n    \"\"\"\n    weight_current_reputation = 0.9\n    weight_feedback = 0.1\n\n    if self.reputation_with_all_feedback is None:\n        logging.info(\"No feedback received.\")\n        return False\n\n    updated = False\n\n    for (current_node, node_ip, round_num), scores in self.reputation_with_all_feedback.items():\n        if not scores:\n            logging.info(f\"No feedback received for node {node_ip} in round {round_num}\")\n            continue\n\n        if node_ip not in self.reputation:\n            logging.info(f\"No reputation for node {node_ip}\")\n            continue\n\n        if \"last_feedback_round\" in self.reputation[node_ip] and self.reputation[node_ip][\"last_feedback_round\"] &gt;= round_num:\n            continue\n\n        avg_feedback = sum(scores) / len(scores)\n        logging.info(f\"Receive feedback to node {node_ip} with average score {avg_feedback}\")\n\n        current_reputation = self.reputation[node_ip][\"reputation\"]\n        if current_reputation is None:\n            logging.info(f\"No reputation calculate for node {node_ip}.\")\n            continue\n\n        combined_reputation = (current_reputation * weight_current_reputation) + (avg_feedback * weight_feedback)\n        logging.info(f\"Combined reputation for node {node_ip} in round {round_num}: {combined_reputation}\")\n\n        self.reputation[node_ip] = {\n            \"reputation\": combined_reputation,\n            \"round\": self._engine.get_round(),\n            \"last_feedback_round\": round_num,\n        }\n        updated = True\n        logging.info(f\"Updated self.reputation for {node_ip}: {self.reputation[node_ip]}\")\n\n    if updated:\n        return True\n    else:\n        return False\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.init_reputation","title":"<code>init_reputation(addr, federation_nodes=None, round_num=None, last_feedback_round=None, init_reputation=None)</code>","text":"<p>Initialize the reputation for each federation node.</p> <p>Parameters:</p> Name Type Description Default <code>addr</code> <code>str</code> <p>The address of the current node.</p> required <code>federation_nodes</code> <code>list</code> <p>List of federation nodes' addresses.</p> <code>None</code> <code>round_num</code> <code>int</code> <p>The current round number.</p> <code>None</code> <code>last_feedback_round</code> <code>int</code> <p>The last round in which feedback was provided.</p> <code>None</code> <code>init_reputation</code> <code>float</code> <p>The initial reputation value.</p> <code>None</code> Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>def init_reputation(self, addr, federation_nodes=None, round_num=None, last_feedback_round=None, init_reputation=None):\n    \"\"\"\n    Initialize the reputation for each federation node.\n\n    Args:\n        addr (str): The address of the current node.\n        federation_nodes (list): List of federation nodes' addresses.\n        round_num (int): The current round number.\n        last_feedback_round (int): The last round in which feedback was provided.\n        init_reputation (float): The initial reputation value.\n    \"\"\"\n    if not federation_nodes:\n        logging.error(\"init_reputation | No federation nodes provided\")\n        return\n\n    if self._with_reputation:\n        neighbors = self.is_valid_ip(federation_nodes)\n\n        if not neighbors:\n            logging.error(\"init_reputation | No neighbors found\")\n            return\n\n        for nei in neighbors:\n            if nei not in self.reputation:\n                self.reputation[nei] = {\n                    \"reputation\": init_reputation,\n                    \"round\": round_num,\n                    \"last_feedback_round\": last_feedback_round,\n                }\n            elif self.reputation[nei].get(\"reputation\") is None:\n                self.reputation[nei][\"reputation\"] = init_reputation\n                self.reputation[nei][\"round\"] = round_num\n                self.reputation[nei][\"last_feedback_round\"] = last_feedback_round\n\n            avg_reputation = self.save_reputation_history_in_memory(self._addr, nei, init_reputation)\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.is_valid_ip","title":"<code>is_valid_ip(federation_nodes)</code>","text":"<p>Check if the IP addresses provided are valid.</p> <p>Parameters:</p> Name Type Description Default <code>federation_nodes</code> <code>list</code> <p>List of federation node addresses.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>A list of valid IP addresses.</p> Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>def is_valid_ip(self, federation_nodes):\n    \"\"\"\n    Check if the IP addresses provided are valid.\n\n    Args:\n        federation_nodes (list): List of federation node addresses.\n\n    Returns:\n        list: A list of valid IP addresses.\n    \"\"\"\n    valid_ip = []\n    for i in federation_nodes:   \n        valid_ip.append(i)\n\n    return valid_ip\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.manage_metric_number_message","title":"<code>manage_metric_number_message(messages_number_message, addr, nei, current_round, metric_active=True)</code>","text":"<p>Manage and normalize the number of messages metric using percentiles.</p> <p>Parameters:</p> Name Type Description Default <code>messages_number_message</code> <code>list</code> <p>List containing message data.</p> required <code>addr</code> <code>str</code> <p>The current node's address.</p> required <code>nei</code> <code>str</code> <p>The neighbor node's address.</p> required <code>current_round</code> <code>int</code> <p>The current round number.</p> required <code>metric_active</code> <code>bool</code> <p>Flag indicating whether the metric is active.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple with the normalized number_message value (float) and the count of messages (int).</p> Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>def manage_metric_number_message(self, messages_number_message, addr, nei, current_round, metric_active=True):\n    \"\"\"\n    Manage and normalize the number of messages metric using percentiles.\n\n    Args:\n        messages_number_message (list): List containing message data.\n        addr (str): The current node's address.\n        nei (str): The neighbor node's address.\n        current_round (int): The current round number.\n        metric_active (bool): Flag indicating whether the metric is active.\n\n    Returns:\n        tuple: A tuple with the normalized number_message value (float) and the count of messages (int).\n    \"\"\"\n    try:\n        if current_round == 0:\n            return 0.0, 0\n\n        if not metric_active:\n            return 0.0, 0\n\n        previous_round = current_round\n\n        current_addr_nei = (addr, nei)\n        relevant_messages = [\n            msg\n            for msg in messages_number_message\n            if msg[\"key\"] == current_addr_nei and msg[\"current_round\"] == previous_round\n        ]\n        messages_count = len(relevant_messages) if relevant_messages else 0\n\n        rounds_to_consider = []\n        if previous_round &gt;= 4:\n            rounds_to_consider = [previous_round - 4, previous_round - 3, previous_round - 2, previous_round - 1]\n        elif previous_round == 3:\n            rounds_to_consider = [0, 1, 2, 3]\n        elif previous_round == 2:\n            rounds_to_consider = [0, 1, 2]\n        elif previous_round == 1:\n            rounds_to_consider = [0, 1]\n        elif previous_round == 0:\n            rounds_to_consider = [0]\n\n        previous_counts = [\n            len([m for m in messages_number_message if m[\"key\"] == current_addr_nei and m[\"current_round\"] == r])\n            for r in rounds_to_consider\n        ]\n\n        self.previous_percentile_25_number_message[current_addr_nei] = (\n            np.percentile(previous_counts, 25) if previous_counts else 0\n        )\n        self.previous_percentile_85_number_message[current_addr_nei] = (\n            np.percentile(previous_counts, 85) if previous_counts else 0\n        )\n\n        normalized_messages = 1.0\n        relative_position = 0\n\n        if previous_round &gt; 4:\n            percentile_25 = self.previous_percentile_25_number_message.get(current_addr_nei, 0)\n            percentile_85 = self.previous_percentile_85_number_message.get(current_addr_nei, 0)\n            if messages_count &gt; percentile_85:\n                relative_position = (messages_count - percentile_85) / (percentile_85 - percentile_25)\n                normalized_messages = np.exp(-relative_position)\n\n            normalized_messages = max(0.01, normalized_messages)\n\n        return normalized_messages, messages_count\n    except Exception:\n        logging.exception(\"Error managing metric number_message\")\n        return 0.0, 0\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.manage_model_arrival_latency","title":"<code>manage_model_arrival_latency(round_num, addr, nei, latency, current_round)</code>","text":"<p>Manage the model arrival latency metric and normalize it based on historical latencies.</p> <p>Parameters:</p> Name Type Description Default <code>round_num</code> <code>int</code> <p>The round number when the model was sent.</p> required <code>addr</code> <code>str</code> <p>The address of the current node.</p> required <code>nei</code> <code>str</code> <p>The neighbor node's address.</p> required <code>latency</code> <code>float</code> <p>The measured latency.</p> required <code>current_round</code> <code>int</code> <p>The current round number.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>Normalized latency score between 0 and 1.</p> Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>def manage_model_arrival_latency(\n    self, round_num, addr, nei, latency, current_round\n):\n    \"\"\"\n    Manage the model arrival latency metric and normalize it based on historical latencies.\n\n    Args:\n        round_num (int): The round number when the model was sent.\n        addr (str): The address of the current node.\n        nei (str): The neighbor node's address.\n        latency (float): The measured latency.\n        current_round (int): The current round number.\n\n    Returns:\n        float: Normalized latency score between 0 and 1.\n    \"\"\"\n    try:\n        current_key = nei\n\n        if current_round not in self.model_arrival_latency_history:\n            self.model_arrival_latency_history[current_round] = {}\n\n        self.model_arrival_latency_history[current_round][current_key] = {\n            \"latency\": latency,\n            \"score\": 0.0,\n        }\n\n        prev_mean_latency = 0\n        prev_percentil_0 = 0\n        prev_percentil_25 = 0\n        difference = 0\n\n        if current_round &gt;= 5:\n            all_latencies = [\n                data[\"latency\"]\n                for r in self.model_arrival_latency_history\n                for key, data in self.model_arrival_latency_history[r].items()\n                if \"latency\" in data and data[\"latency\"] != 0\n            ]\n\n            prev_mean_latency = np.mean(all_latencies) if all_latencies else 0\n            prev_percentil_0 = np.percentile(all_latencies, 0) if all_latencies else 0\n            prev_percentil_25 = np.percentile(all_latencies, 25) if all_latencies else 0\n\n            k = 0.1\n            prev_mean_latency += k * (prev_percentil_25 - prev_percentil_0)\n\n            difference = latency - prev_mean_latency\n            if latency &lt;= prev_mean_latency:\n                score = 1.0\n            else:\n                score = 1 / (1 + np.exp(abs(difference) / prev_mean_latency))\n\n            if round_num &lt; current_round:\n                round_diff = current_round - round_num\n                penalty_factor = round_diff * 0.1\n                penalty = penalty_factor * (1 - score)\n                score -= penalty * score\n\n            self.model_arrival_latency_history[current_round][current_key].update({\n                \"mean_latency\": prev_mean_latency,\n                \"percentil_0\": prev_percentil_0,\n                \"percentil_25\": prev_percentil_25,\n                \"score\": score,\n            })\n        else:\n            score = 0\n\n        return score\n\n    except Exception as e:\n        logging.exception(f\"Error managing model_arrival_latency: {e}\")\n        return 0\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.on_round_start","title":"<code>on_round_start(rse)</code>  <code>async</code>","text":"<p>Event handler for the start of a round. It stores the start time and updates the expected nodes.</p> <p>Parameters:</p> Name Type Description Default <code>rse</code> <code>RoundStartEvent</code> <p>The event data containing round start information.</p> required Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>async def on_round_start(self, rse: RoundStartEvent):\n    \"\"\"\n    Event handler for the start of a round. It stores the start time and updates the expected nodes.\n\n    Args:\n        rse (RoundStartEvent): The event data containing round start information.\n    \"\"\"\n    (round_id, start_time, expected_nodes) = await rse.get_event_data()\n    if round_id not in self.round_timing_info:\n        self.round_timing_info[round_id] = {}\n    self.round_timing_info[round_id][\"start_time\"] = start_time\n    expected_nodes.difference_update(self.rejected_nodes)\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.recollect_fraction_of_parameters_changed","title":"<code>recollect_fraction_of_parameters_changed(ure)</code>  <code>async</code>","text":"<p>Event handler to recollect the fraction of parameters changed when an update is received.</p> <p>Parameters:</p> Name Type Description Default <code>ure</code> <code>UpdateReceivedEvent</code> <p>The event data containing model update information.</p> required Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>async def recollect_fraction_of_parameters_changed(self, ure: UpdateReceivedEvent):\n    \"\"\"\n    Event handler to recollect the fraction of parameters changed when an update is received.\n\n    Args:\n        ure (UpdateReceivedEvent): The event data containing model update information.\n    \"\"\"\n    (decoded_model, weight, source, round_num, local) = await ure.get_event_data()\n    current_round = self._engine.get_round()\n    parameters_local = self._engine.trainer.get_model_parameters()\n    parameters_received = decoded_model\n    differences = []\n    total_params = 0\n    changed_params = 0\n    changes_record = {}\n    prev_threshold = None\n\n    if source in self.fraction_of_params_changed and current_round - 1 in self.fraction_of_params_changed[source]:\n        prev_threshold = self.fraction_of_params_changed[source][current_round - 1][-1][\"threshold\"]\n\n    for key in parameters_local.keys():\n        if key in parameters_received:\n            diff = torch.abs(parameters_local[key] - parameters_received[key])\n            differences.extend(diff.flatten().tolist())\n            total_params += diff.numel()\n\n    if differences:\n        mean_threshold = torch.mean(torch.tensor(differences)).item()\n        current_threshold = (prev_threshold + mean_threshold) / 2 if prev_threshold is not None else mean_threshold\n    else:\n        current_threshold = 0\n\n    for key in parameters_local.keys():\n        if key in parameters_received:\n            diff = torch.abs(parameters_local[key] - parameters_received[key])\n            num_changed = torch.sum(diff &gt; current_threshold).item()\n            changed_params += num_changed\n            if num_changed &gt; 0:\n                changes_record[key] = num_changed\n\n    fraction_changed = changed_params / total_params if total_params &gt; 0 else 0.0\n\n    if source not in self.fraction_of_params_changed:\n        self.fraction_of_params_changed[source] = {}\n    if current_round not in self.fraction_of_params_changed[source]:\n        self.fraction_of_params_changed[source][current_round] = []\n\n    self.fraction_of_params_changed[source][current_round].append({\n        \"fraction_changed\": fraction_changed,\n        \"total_params\": total_params,\n        \"changed_params\": changed_params,\n        \"threshold\": current_threshold,\n        \"changes_record\": changes_record,\n    })\n\n    self.save_data(\n        \"fraction_of_params_changed\",\n        source,\n        self._addr,\n        current_round,\n        fraction_changed=fraction_changed,\n        total_params=total_params,\n        changed_params=changed_params,\n        threshold=current_threshold,\n        changes_record=changes_record,\n    )\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.recollect_model_arrival_latency","title":"<code>recollect_model_arrival_latency(ure)</code>  <code>async</code>","text":"<p>Event handler to record the model arrival latency when an update is received.</p> <p>Parameters:</p> Name Type Description Default <code>ure</code> <code>UpdateReceivedEvent</code> <p>The event data for a model update.</p> required Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>async def recollect_model_arrival_latency(self, ure: UpdateReceivedEvent):\n    \"\"\"\n    Event handler to record the model arrival latency when an update is received.\n\n    Args:\n        ure (UpdateReceivedEvent): The event data for a model update.\n    \"\"\"\n    (decoded_model, weight, source, round_num, local) = await ure.get_event_data()\n    current_time = time.time()\n    current_round = round_num\n\n    if current_round not in self.round_timing_info:\n        self.round_timing_info[current_round] = {}\n\n    if \"model_received_time\" not in self.round_timing_info[current_round]:\n        self.round_timing_info[current_round][\"model_received_time\"] = {}\n\n    if source not in self.round_timing_info[current_round][\"model_received_time\"]:\n        self.round_timing_info[current_round][\"model_received_time\"][source] = current_time\n\n        if \"start_time\" in self.round_timing_info[current_round]:\n            start = self.round_timing_info[current_round][\"start_time\"]\n            received_time = self.round_timing_info[current_round][\"model_received_time\"][source]\n            duration = received_time - start\n            self.round_timing_info[current_round][\"duration\"] = duration\n            logging.info(f\"Source {source} , round {current_round}, duration: {duration:.4f} seconds\")\n\n            self.save_data(\n                \"model_arrival_latency\",\n                source,\n                self._addr,\n                num_round=current_round,\n                current_round=self._engine.get_round(),\n                latency=duration,\n            )\n    else:\n        logging.info(f\"Model arrival latency already calculated for node {source} in round {current_round}\")\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.recollect_number_message","title":"<code>recollect_number_message(source, message)</code>  <code>async</code>","text":"<p>Event handler to collect message count metrics when a message is received.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The source node's address.</p> required <code>message</code> <p>The received message (content not used directly).</p> required Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>async def recollect_number_message(self, source, message):\n    \"\"\"\n    Event handler to collect message count metrics when a message is received.\n\n    Args:\n        source (str): The source node's address.\n        message: The received message (content not used directly).\n    \"\"\"\n    if source != self._addr:\n        current_time = time.time()\n        if current_time:\n            self.save_data(\n                \"number_message\",\n                source,\n                self._addr,\n                time=current_time,\n                current_round=self._engine.get_round(),\n            )\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.recollect_similarity","title":"<code>recollect_similarity(ure)</code>  <code>async</code>","text":"<p>Event handler to recollect and store model similarity metrics when an update is received.</p> <p>Parameters:</p> Name Type Description Default <code>ure</code> <code>UpdateReceivedEvent</code> <p>The event data containing model update information.</p> required Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>async def recollect_similarity(self, ure: UpdateReceivedEvent):\n    \"\"\"\n    Event handler to recollect and store model similarity metrics when an update is received.\n\n    Args:\n        ure (UpdateReceivedEvent): The event data containing model update information.\n    \"\"\"\n    (decoded_model, weight, nei, round_num, local) = await ure.get_event_data()\n    if self._with_reputation and self._reputation_metrics.get(\"model_similarity\"):\n        if self._engine.config.participant[\"adaptive_args\"][\"model_similarity\"]:\n            if nei != self._addr:\n                logging.info(\"\ud83e\udd16  handle_model_message | Checking model similarity\")\n                cosine_value = cosine_metric(\n                    self._engine.trainer.get_model_parameters(),\n                    decoded_model,\n                    similarity=True,\n                )\n                euclidean_value = euclidean_metric(\n                    self._engine.trainer.get_model_parameters(),\n                    decoded_model,\n                    similarity=True,\n                )\n                minkowski_value = minkowski_metric(\n                    self._engine.trainer.get_model_parameters(),\n                    decoded_model,\n                    p=2,\n                    similarity=True,\n                )\n                manhattan_value = manhattan_metric(\n                    self._engine.trainer.get_model_parameters(),\n                    decoded_model,\n                    similarity=True,\n                )\n                pearson_correlation_value = pearson_correlation_metric(\n                    self._engine.trainer.get_model_parameters(),\n                    decoded_model,\n                    similarity=True,\n                )\n                jaccard_value = jaccard_metric(\n                    self._engine.trainer.get_model_parameters(),\n                    decoded_model,\n                    similarity=True,\n                )\n\n                similarity_metrics = {\n                    \"timestamp\": datetime.now(),\n                    \"nei\": nei,\n                    \"round\": round_num,\n                    \"current_round\": self._engine.get_round(),\n                    \"cosine\": cosine_value,\n                    \"euclidean\": euclidean_value,\n                    \"minkowski\": minkowski_value,\n                    \"manhattan\": manhattan_value,\n                    \"pearson_correlation\": pearson_correlation_value,\n                    \"jaccard\": jaccard_value,\n                }\n\n                if nei in self.connection_metrics:\n                    self.connection_metrics[nei].similarity.append(similarity_metrics)\n                    logging.info(f\"Stored similarity metrics for {nei}: {similarity_metrics}\")\n                else:\n                    logging.warning(f\"No metrics instance found for neighbor {nei}\")\n\n                if cosine_value &lt; 0.6:\n                    logging.info(\"\ud83e\udd16  handle_model_message | Model similarity is less than 0.6\")\n                    self.rejected_nodes.add(nei)\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.save_data","title":"<code>save_data(type_data, nei, addr, num_round=None, time=None, current_round=None, fraction_changed=None, total_params=None, changed_params=None, threshold=None, changes_record=None, latency=None)</code>","text":"<p>Save data received from nodes for further reputation calculations.</p> <p>Parameters:</p> Name Type Description Default <code>type_data</code> <code>str</code> <p>The type of data being saved (\"number_message\", \"fraction_of_params_changed\", or \"model_arrival_latency\").</p> required <code>nei</code> <code>str</code> <p>The neighbor node address.</p> required <code>addr</code> <code>str</code> <p>The current node address.</p> required <code>num_round</code> <code>optional</code> <p>The round number associated with the data.</p> <code>None</code> <code>time</code> <code>optional</code> <p>Timestamp or time value.</p> <code>None</code> <code>current_round</code> <code>optional</code> <p>The current round number.</p> <code>None</code> <code>fraction_changed</code> <code>optional</code> <p>Fraction of parameters changed.</p> <code>None</code> <code>total_params</code> <code>optional</code> <p>Total number of parameters.</p> <code>None</code> <code>changed_params</code> <code>optional</code> <p>Number of changed parameters.</p> <code>None</code> <code>threshold</code> <code>optional</code> <p>Threshold used for metrics.</p> <code>None</code> <code>changes_record</code> <code>optional</code> <p>Record of parameter changes.</p> <code>None</code> <code>latency</code> <code>optional</code> <p>Latency value.</p> <code>None</code> Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>def save_data(\n    self,\n    type_data,\n    nei,\n    addr,\n    num_round=None,\n    time=None,\n    current_round=None,\n    fraction_changed=None,\n    total_params=None,\n    changed_params=None,\n    threshold=None,\n    changes_record=None,\n    latency=None,\n):\n    \"\"\"\n    Save data received from nodes for further reputation calculations.\n\n    Args:\n        type_data (str): The type of data being saved (\"number_message\", \"fraction_of_params_changed\", or \"model_arrival_latency\").\n        nei (str): The neighbor node address.\n        addr (str): The current node address.\n        num_round (optional): The round number associated with the data.\n        time (optional): Timestamp or time value.\n        current_round (optional): The current round number.\n        fraction_changed (optional): Fraction of parameters changed.\n        total_params (optional): Total number of parameters.\n        changed_params (optional): Number of changed parameters.\n        threshold (optional): Threshold used for metrics.\n        changes_record (optional): Record of parameter changes.\n        latency (optional): Latency value.\n    \"\"\"\n    try:\n        if addr == nei:\n            return\n\n        combined_data = {}\n\n        if type_data == \"number_message\":\n            combined_data[\"number_message\"] = {\n                \"time\": time,\n                \"current_round\": current_round,\n            }\n        elif type_data == \"fraction_of_params_changed\":\n            combined_data[\"fraction_of_params_changed\"] = {\n                \"fraction_changed\": fraction_changed,\n                \"threshold\": threshold,\n                \"round\": num_round,\n            }\n        elif type_data == \"model_arrival_latency\":\n            combined_data[\"model_arrival_latency\"] = {\n                \"latency\": latency,\n                \"round\": num_round,\n                \"round_received\": current_round,\n            }\n\n        if nei in self.connection_metrics:\n            if type_data == \"number_message\":\n                if not isinstance(self.connection_metrics[nei].messages, list):\n                    self.connection_metrics[nei].messages = []\n                self.connection_metrics[nei].messages.append(combined_data[\"number_message\"])\n            elif type_data == \"fraction_of_params_changed\":\n                self.connection_metrics[nei].fraction_of_params_changed.update(combined_data[\"fraction_of_params_changed\"])\n            elif type_data == \"model_arrival_latency\":\n                self.connection_metrics[nei].model_arrival_latency.update(combined_data[\"model_arrival_latency\"])\n\n    except Exception:\n        logging.exception(\"Error saving data\")\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.save_model_arrival_latency_history","title":"<code>save_model_arrival_latency_history(addr, nei, model_arrival_latency, round_num)</code>","text":"<p>Save and update the model arrival latency history in memory.</p> <p>Parameters:</p> Name Type Description Default <code>addr</code> <code>str</code> <p>The current node's address.</p> required <code>nei</code> <code>str</code> <p>The neighbor node's address.</p> required <code>model_arrival_latency</code> <code>float</code> <p>The normalized latency score.</p> required <code>round_num</code> <code>int</code> <p>The current round number.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The updated average model arrival latency.</p> Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>def save_model_arrival_latency_history(self, addr, nei, model_arrival_latency, round_num):\n    \"\"\"\n    Save and update the model arrival latency history in memory.\n\n    Args:\n        addr (str): The current node's address.\n        nei (str): The neighbor node's address.\n        model_arrival_latency (float): The normalized latency score.\n        round_num (int): The current round number.\n\n    Returns:\n        float: The updated average model arrival latency.\n    \"\"\"\n    try:\n        current_key = nei\n\n        if round_num not in self.model_arrival_latency_history:\n            self.model_arrival_latency_history[round_num] = {}\n\n        if current_key not in self.model_arrival_latency_history[round_num]:\n            self.model_arrival_latency_history[round_num][current_key] = {}\n\n        self.model_arrival_latency_history[round_num][current_key].update({\n            \"score\": model_arrival_latency,\n        })\n\n        if model_arrival_latency &gt; 0 and round_num &gt; 5:\n            previous_avg = (\n                self.model_arrival_latency_history.get(round_num - 1, {})\n                .get(current_key, {})\n                .get(\"avg_model_arrival_latency\", None)\n            )\n\n            if previous_avg is not None:\n                avg_model_arrival_latency = (\n                    model_arrival_latency * 0.8 + previous_avg * 0.2\n                    if previous_avg is not None\n                    else model_arrival_latency\n                )\n            else:\n                avg_model_arrival_latency = model_arrival_latency - (model_arrival_latency * 0.05)\n        elif model_arrival_latency == 0 and round_num &gt; 5:\n            previous_avg = (\n                self.model_arrival_latency_history.get(round_num - 1, {})\n                .get(current_key, {})\n                .get(\"avg_model_arrival_latency\", None)\n            )\n            avg_model_arrival_latency = previous_avg - (previous_avg * 0.05)\n        else:\n            avg_model_arrival_latency = model_arrival_latency\n\n        self.model_arrival_latency_history[round_num][current_key][\"avg_model_arrival_latency\"] = (\n            avg_model_arrival_latency\n        )\n\n        return avg_model_arrival_latency\n    except Exception:\n        logging.exception(\"Error saving model_arrival_latency history\")\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.save_number_message_history","title":"<code>save_number_message_history(addr, nei, messages_number_message_normalized, current_round)</code>","text":"<p>Save the normalized number_message history in memory and calculate a weighted average.</p> <p>Parameters:</p> Name Type Description Default <code>addr</code> <code>str</code> <p>The current node's address.</p> required <code>nei</code> <code>str</code> <p>The neighbor node's address.</p> required <code>messages_number_message_normalized</code> <code>float</code> <p>The normalized number_message value.</p> required <code>current_round</code> <code>int</code> <p>The current round number.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The weighted average of the number_message metric.</p> Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>def save_number_message_history(self, addr, nei, messages_number_message_normalized, current_round):\n    \"\"\"\n    Save the normalized number_message history in memory and calculate a weighted average.\n\n    Args:\n        addr (str): The current node's address.\n        nei (str): The neighbor node's address.\n        messages_number_message_normalized (float): The normalized number_message value.\n        current_round (int): The current round number.\n\n    Returns:\n        float: The weighted average of the number_message metric.\n    \"\"\"\n    try:\n        key = (addr, nei)\n        avg_number_message = 0\n\n        if key not in self.number_message_history:\n            self.number_message_history[key] = {}\n\n        self.number_message_history[key][current_round] = {\"number_message\": messages_number_message_normalized}\n\n        if messages_number_message_normalized != 0 and current_round &gt; 4:\n            previous_avg = (\n                self.number_message_history[key].get(current_round - 1, {}).get(\"avg_number_message\", None)\n            )\n            if previous_avg is not None:\n                avg_number_message = messages_number_message_normalized * 0.8 + previous_avg * 0.2\n            else:\n                avg_number_message = messages_number_message_normalized\n\n            self.number_message_history[key][current_round][\"avg_number_message\"] = avg_number_message\n        else:\n            avg_number_message = 0\n\n        return avg_number_message\n    except Exception:\n        logging.exception(\"Error saving number_message history\")\n        return -1\n\n    except Exception as e:\n        logging.exception(f\"Error managing model_arrival_latency latency: {e}\")\n        return 0.0\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.save_reputation_history_in_memory","title":"<code>save_reputation_history_in_memory(addr, nei, reputation)</code>","text":"<p>Save the reputation history for a neighbor and compute an average reputation.</p> <p>Parameters:</p> Name Type Description Default <code>addr</code> <code>str</code> <p>The current node's address.</p> required <code>nei</code> <code>str</code> <p>The neighbor node's address.</p> required <code>reputation</code> <code>float</code> <p>The computed reputation for the current round.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The updated (weighted) reputation.</p> Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>def save_reputation_history_in_memory(self, addr, nei, reputation):\n    \"\"\"\n    Save the reputation history for a neighbor and compute an average reputation.\n\n    Args:\n        addr (str): The current node's address.\n        nei (str): The neighbor node's address.\n        reputation (float): The computed reputation for the current round.\n\n    Returns:\n        float: The updated (weighted) reputation.\n    \"\"\"\n    try:\n        key = (addr, nei)\n\n        if key not in self.reputation_history:\n            self.reputation_history[key] = {}\n\n        self.reputation_history[key][self._engine.get_round()] = reputation\n\n        avg_reputation = 0\n        current_round = self._engine.get_round()\n        rounds = sorted(self.reputation_history[key].keys(), reverse=True)[:2]\n\n        if len(rounds) &gt;= 2:\n            current_round = rounds[0]\n            previous_round = rounds[1]\n\n            current_rep = self.reputation_history[key][current_round]\n            previous_rep = self.reputation_history[key][previous_round]\n            logging.info(f\"Current reputation: {current_rep}, Previous reputation: {previous_rep}\")\n\n            avg_reputation = (current_rep * 0.8) + (previous_rep * 0.2)\n            logging.info(f\"Reputation ponderated: {avg_reputation}\")\n        else:\n            avg_reputation = self.reputation_history[key][current_round]\n\n        return avg_reputation\n\n    except Exception:\n        logging.exception(\"Error saving reputation history\")\n        return -1\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.send_reputation_to_neighbors","title":"<code>send_reputation_to_neighbors(neighbors)</code>  <code>async</code>","text":"<p>Send the calculated reputation scores to all neighbors.</p> <p>Parameters:</p> Name Type Description Default <code>neighbors</code> <code>iterable</code> <p>An iterable of neighbor node addresses.</p> required Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>async def send_reputation_to_neighbors(self, neighbors):\n    \"\"\"\n    Send the calculated reputation scores to all neighbors.\n\n    Args:\n        neighbors (iterable): An iterable of neighbor node addresses.\n    \"\"\"\n    for nei, data in self.reputation.items():\n        if data[\"reputation\"] is not None:\n            neighbors_to_send = [neighbor for neighbor in neighbors if neighbor != nei]\n\n            for neighbor in neighbors_to_send:\n                message = self._engine.cm.create_message(\n                    \"reputation\", \"share\", node_id=nei, score=float(data[\"reputation\"]), round=self._engine.get_round()\n                )\n                await self._engine.cm.send_message(neighbor, message)\n                logging.info(\n                    f\"Sending reputation to node {nei} from node {neighbor} with reputation {data['reputation']}\"\n                )\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.setup","title":"<code>setup()</code>  <code>async</code>","text":"<p>Setup the reputation system by subscribing to various events.</p> <p>This function enables the reputation system and subscribes to events based on active metrics.</p> Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>async def setup(self):\n    \"\"\"\n    Setup the reputation system by subscribing to various events.\n\n    This function enables the reputation system and subscribes to events based on active metrics.\n    \"\"\"\n    if self._with_reputation:\n        logging.info(\"Reputation system enabled\")\n        await EventManager.get_instance().subscribe_node_event(RoundStartEvent, self.on_round_start)\n        await EventManager.get_instance().subscribe_node_event(AggregationEvent, self.calculate_reputation)\n        if self._reputation_metrics.get(\"model_similarity\", False):\n            await EventManager.get_instance().subscribe_node_event(UpdateReceivedEvent, self.recollect_similarity)\n        if self._reputation_metrics.get(\"fraction_parameters_changed\", False):\n            await EventManager.get_instance().subscribe_node_event(UpdateReceivedEvent, self.recollect_fraction_of_parameters_changed)\n        if self._reputation_metrics.get(\"num_messages\", False):\n            await EventManager.get_instance().subscribe((\"model\", \"update\"), self.recollect_number_message)\n            await EventManager.get_instance().subscribe((\"model\", \"initialization\"), self.recollect_number_message)\n            await EventManager.get_instance().subscribe((\"control\", \"alive\"), self.recollect_number_message)\n            await EventManager.get_instance().subscribe((\"federation\", \"federation_models_included\"), self.recollect_number_message)\n            await EventManager.get_instance().subscribe((\"reputation\", \"share\"), self.recollect_number_message)\n        if self._reputation_metrics.get(\"model_arrival_latency\", False):\n            await EventManager.get_instance().subscribe_node_event(UpdateReceivedEvent, self.recollect_model_arrival_latency)\n</code></pre>"},{"location":"api/addons/reputation/reputation/#nebula.addons.reputation.reputation.Reputation.update_process_aggregation","title":"<code>update_process_aggregation(updates)</code>  <code>async</code>","text":"<p>Update the aggregation process by removing nodes that have been rejected.</p> <p>Parameters:</p> Name Type Description Default <code>updates</code> <code>dict</code> <p>The dictionary of updates.</p> required Source code in <code>nebula/addons/reputation/reputation.py</code> <pre><code>async def update_process_aggregation(self, updates):\n    \"\"\"\n    Update the aggregation process by removing nodes that have been rejected.\n\n    Args:\n        updates (dict): The dictionary of updates.\n    \"\"\"\n    for rn in self.rejected_nodes:\n        if rn in updates:\n            updates.pop(rn)\n\n    logging.info(f\"Updates after rejected nodes: {list(updates.keys())}\")\n    self.rejected_nodes.clear()\n    logging.info(f\"rejected nodes after clear at round {self._engine.get_round()}: {self.rejected_nodes}\")\n</code></pre>"},{"location":"api/addons/trustworthiness/","title":"Documentation for Trustworthiness Module","text":""},{"location":"api/addons/trustworthiness/calculation/","title":"Documentation for Calculation Module","text":""},{"location":"api/addons/trustworthiness/calculation/#nebula.addons.trustworthiness.calculation.check_properties","title":"<code>check_properties(*args)</code>","text":"<p>Check if all the arguments have values.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>list</code> <p>All the arguments.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>float</code> <p>The mean of arguments that have values.</p> Source code in <code>nebula/addons/trustworthiness/calculation.py</code> <pre><code>def check_properties(*args):\n    \"\"\"\n    Check if all the arguments have values.\n\n    Args:\n        args (list): All the arguments.\n\n    Returns:\n        float: The mean of arguments that have values.\n    \"\"\"\n\n    result = map(lambda x: x is not None and x != \"\", args)\n    return np.mean(list(result))\n</code></pre>"},{"location":"api/addons/trustworthiness/calculation/#nebula.addons.trustworthiness.calculation.get_avg_loss_accuracy","title":"<code>get_avg_loss_accuracy(loss_files, accuracy_files)</code>","text":"<p>Calculates the mean accuracy and loss models of the nodes.</p> <p>Parameters:</p> Name Type Description Default <code>loss_files</code> <code>list</code> <p>Files that contain the loss of the models of the nodes.</p> required <code>accuracy_files</code> <code>list</code> <p>Files that contain the acurracies of the models of the nodes.</p> required <p>Returns:</p> Type Description <p>3-tupla: The mean loss of the models, the mean accuracies of the models, the standard deviation of the accuracies of the models.</p> Source code in <code>nebula/addons/trustworthiness/calculation.py</code> <pre><code>def get_avg_loss_accuracy(loss_files, accuracy_files):\n    \"\"\"\n    Calculates the mean accuracy and loss models of the nodes.\n\n    Args:\n        loss_files (list): Files that contain the loss of the models of the nodes.\n        accuracy_files (list): Files that contain the acurracies of the models of the nodes.\n\n    Returns:\n        3-tupla: The mean loss of the models, the mean accuracies of the models, the standard deviation of the accuracies of the models.\n    \"\"\"\n    total_accuracy = 0\n    total_loss = 0\n    number_files = len(loss_files)\n    accuracies = []\n\n    for file_loss, file_accuracy in zip(loss_files, accuracy_files, strict=False):\n        with open(file_loss) as f:\n            loss = f.read()\n\n        with open(file_accuracy) as f:\n            accuracy = f.read()\n\n        total_loss += float(loss)\n        total_accuracy += float(accuracy)\n        accuracies.append(float(accuracy))\n\n    avg_loss = total_loss / number_files\n    avg_accuracy = total_accuracy / number_files\n\n    std_accuracy = statistics.stdev(accuracies)\n\n    return avg_loss, avg_accuracy, std_accuracy\n</code></pre>"},{"location":"api/addons/trustworthiness/calculation/#nebula.addons.trustworthiness.calculation.get_bytes_models","title":"<code>get_bytes_models(models_files)</code>","text":"<p>Calculates the mean bytes of the final models of the nodes.</p> <p>Parameters:</p> Name Type Description Default <code>models_files</code> <code>list</code> <p>List of final models.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The mean bytes of the models.</p> Source code in <code>nebula/addons/trustworthiness/calculation.py</code> <pre><code>def get_bytes_models(models_files):\n    \"\"\"\n    Calculates the mean bytes of the final models of the nodes.\n\n    Args:\n        models_files (list): List of final models.\n\n    Returns:\n        float: The mean bytes of the models.\n    \"\"\"\n\n    total_models_size = 0\n    number_models = len(models_files)\n\n    for file in models_files:\n        model_size = os.path.getsize(file)\n        total_models_size += model_size\n\n    avg_model_size = total_models_size / number_models\n\n    return avg_model_size\n</code></pre>"},{"location":"api/addons/trustworthiness/calculation/#nebula.addons.trustworthiness.calculation.get_bytes_sent_recv","title":"<code>get_bytes_sent_recv(bytes_sent_files, bytes_recv_files)</code>","text":"<p>Calculates the mean bytes sent and received of the nodes.</p> <p>Parameters:</p> Name Type Description Default <code>bytes_sent_files</code> <code>list</code> <p>Files that contain the bytes sent of the nodes.</p> required <code>bytes_recv_files</code> <code>list</code> <p>Files that contain the bytes received of the nodes.</p> required <p>Returns:</p> Type Description <p>4-tupla: The total bytes sent, the total bytes received, the mean bytes sent and the mean bytes received of the nodes.</p> Source code in <code>nebula/addons/trustworthiness/calculation.py</code> <pre><code>def get_bytes_sent_recv(bytes_sent_files, bytes_recv_files):\n    \"\"\"\n    Calculates the mean bytes sent and received of the nodes.\n\n    Args:\n        bytes_sent_files (list): Files that contain the bytes sent of the nodes.\n        bytes_recv_files (list): Files that contain the bytes received of the nodes.\n\n    Returns:\n        4-tupla: The total bytes sent, the total bytes received, the mean bytes sent and the mean bytes received of the nodes.\n    \"\"\"\n    total_upload_bytes = 0\n    total_download_bytes = 0\n    number_files = len(bytes_sent_files)\n\n    for file_bytes_sent, file_bytes_recv in zip(bytes_sent_files, bytes_recv_files, strict=False):\n        with open(file_bytes_sent) as f:\n            bytes_sent = f.read()\n\n        with open(file_bytes_recv) as f:\n            bytes_recv = f.read()\n\n        total_upload_bytes += int(bytes_sent)\n        total_download_bytes += int(bytes_recv)\n\n    avg_upload_bytes = total_upload_bytes / number_files\n    avg_download_bytes = total_download_bytes / number_files\n    return (\n        total_upload_bytes,\n        total_download_bytes,\n        avg_upload_bytes,\n        avg_download_bytes,\n    )\n</code></pre>"},{"location":"api/addons/trustworthiness/calculation/#nebula.addons.trustworthiness.calculation.get_clever_score","title":"<code>get_clever_score(model, test_sample, nb_classes, learning_rate)</code>","text":"<p>Calculates the CLEVER score.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>object</code> <p>The model.</p> required <code>test_sample</code> <code>object</code> <p>One test sample to calculate the CLEVER score.</p> required <code>nb_classes</code> <code>int</code> <p>The nb_classes of the model.</p> required <code>learning_rate</code> <code>float</code> <p>The learning rate of the model.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The CLEVER score.</p> Source code in <code>nebula/addons/trustworthiness/calculation.py</code> <pre><code>def get_clever_score(model, test_sample, nb_classes, learning_rate):\n    \"\"\"\n    Calculates the CLEVER score.\n\n    Args:\n        model (object): The model.\n        test_sample (object): One test sample to calculate the CLEVER score.\n        nb_classes (int): The nb_classes of the model.\n        learning_rate (float): The learning rate of the model.\n\n    Returns:\n        float: The CLEVER score.\n    \"\"\"\n\n    images, _ = test_sample\n    background = images[-1]\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), learning_rate)\n\n    # Create the ART classifier\n    classifier = PyTorchClassifier(\n        model=model,\n        loss=criterion,\n        optimizer=optimizer,\n        input_shape=(1, 28, 28),\n        nb_classes=nb_classes,\n    )\n\n    score_untargeted = clever_u(\n        classifier,\n        background.numpy(),\n        10,\n        5,\n        R_L2,\n        norm=2,\n        pool_factor=3,\n        verbose=False,\n    )\n    return score_untargeted\n</code></pre>"},{"location":"api/addons/trustworthiness/calculation/#nebula.addons.trustworthiness.calculation.get_cv","title":"<code>get_cv(list=None, std=None, mean=None)</code>","text":"<p>Get the coefficient of variation.</p> <p>Parameters:</p> Name Type Description Default <code>list</code> <code>list</code> <p>List in which the coefficient of variation will be calculated.</p> <code>None</code> <code>std</code> <code>float</code> <p>Standard deviation of a list.</p> <code>None</code> <code>mean</code> <code>float</code> <p>Mean of a list.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>float</code> <p>The coefficient of variation calculated.</p> Source code in <code>nebula/addons/trustworthiness/calculation.py</code> <pre><code>def get_cv(list=None, std=None, mean=None):\n    \"\"\"\n    Get the coefficient of variation.\n\n    Args:\n        list (list): List in which the coefficient of variation will be calculated.\n        std (float): Standard deviation of a list.\n        mean (float): Mean of a list.\n\n    Returns:\n        float: The coefficient of variation calculated.\n    \"\"\"\n    if std is not None and mean is not None:\n        return std / mean\n\n    if list is not None:\n        return np.std(list) / np.mean(list)\n\n    return 0\n</code></pre>"},{"location":"api/addons/trustworthiness/calculation/#nebula.addons.trustworthiness.calculation.get_elapsed_time","title":"<code>get_elapsed_time(scenario)</code>","text":"<p>Calculates the elapsed time during the execution of the scenario.</p> <p>Parameters:</p> Name Type Description Default <code>scenario</code> <code>object</code> <p>Scenario required.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The elapsed time.</p> Source code in <code>nebula/addons/trustworthiness/calculation.py</code> <pre><code>def get_elapsed_time(scenario):\n    \"\"\"\n    Calculates the elapsed time during the execution of the scenario.\n\n    Args:\n        scenario (object): Scenario required.\n\n    Returns:\n        float: The elapsed time.\n    \"\"\"\n    start_time = scenario[1]\n    end_time = scenario[2]\n\n    start_date = datetime.strptime(start_time, \"%d/%m/%Y %H:%M:%S\")\n    end_date = datetime.strptime(end_time, \"%d/%m/%Y %H:%M:%S\")\n\n    elapsed_time = (end_date - start_date).total_seconds() / 60\n\n    return elapsed_time\n</code></pre>"},{"location":"api/addons/trustworthiness/calculation/#nebula.addons.trustworthiness.calculation.get_feature_importance_cv","title":"<code>get_feature_importance_cv(model, test_sample)</code>","text":"<p>Calculates the coefficient of variation of the feature importance.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>object</code> <p>The model.</p> required <code>test_sample</code> <code>object</code> <p>One test sample to calculate the feature importance.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The coefficient of variation of the feature importance.</p> Source code in <code>nebula/addons/trustworthiness/calculation.py</code> <pre><code>def get_feature_importance_cv(model, test_sample):\n    \"\"\"\n    Calculates the coefficient of variation of the feature importance.\n\n    Args:\n        model (object): The model.\n        test_sample (object): One test sample to calculate the feature importance.\n\n    Returns:\n        float: The coefficient of variation of the feature importance.\n    \"\"\"\n\n    try:\n        cv = 0\n        batch_size = 10\n        device = \"cpu\"\n\n        if isinstance(model, torch.nn.Module):\n            batched_data, _ = test_sample\n\n            n = batch_size\n            m = math.floor(0.8 * n)\n\n            background = batched_data[:m].to(device)\n            test_data = batched_data[m:n].to(device)\n\n            e = shap.DeepExplainer(model, background)\n            shap_values = e.shap_values(test_data)\n            if shap_values is not None and len(shap_values) &gt; 0:\n                sums = np.array([shap_values[i].sum() for i in range(len(shap_values))])\n                abs_sums = np.absolute(sums)\n                cv = variation(abs_sums)\n    except Exception as e:\n        logger.warning(\"Could not compute feature importance CV with shap\")\n        cv = 1\n    if math.isnan(cv):\n        cv = 1\n    return cv\n</code></pre>"},{"location":"api/addons/trustworthiness/calculation/#nebula.addons.trustworthiness.calculation.get_global_privacy_risk","title":"<code>get_global_privacy_risk(dp, epsilon, n)</code>","text":"<p>Calculates the global privacy risk by epsilon and the number of clients.</p> <p>Parameters:</p> Name Type Description Default <code>dp</code> <code>bool</code> <p>Indicates if differential privacy is used or not.</p> required <code>epsilon</code> <code>int</code> <p>The epsilon value.</p> required <code>n</code> <code>int</code> <p>The number of clients in the scenario.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The global privacy risk.</p> Source code in <code>nebula/addons/trustworthiness/calculation.py</code> <pre><code>def get_global_privacy_risk(dp, epsilon, n):\n    \"\"\"\n    Calculates the global privacy risk by epsilon and the number of clients.\n\n    Args:\n        dp (bool): Indicates if differential privacy is used or not.\n        epsilon (int): The epsilon value.\n        n (int): The number of clients in the scenario.\n\n    Returns:\n        float: The global privacy risk.\n    \"\"\"\n\n    if dp is True and isinstance(epsilon, numbers.Number):\n        return 1 / (1 + (n - 1) * math.pow(e, -epsilon))\n    else:\n        return 1\n</code></pre>"},{"location":"api/addons/trustworthiness/calculation/#nebula.addons.trustworthiness.calculation.get_map_value_score","title":"<code>get_map_value_score(score_key, score_map)</code>","text":"<p>Finds the score by the score_key in the score_map and returns the value.</p> <p>Parameters:</p> Name Type Description Default <code>score_key</code> <code>string</code> <p>The key to look up in the score_map.</p> required <code>score_map</code> <code>dict</code> <p>The score map defined in the eval_metrics.json file.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The score obtained in the score_map.</p> Source code in <code>nebula/addons/trustworthiness/calculation.py</code> <pre><code>def get_map_value_score(score_key, score_map):\n    \"\"\"\n    Finds the score by the score_key in the score_map and returns the value.\n\n    Args:\n        score_key (string): The key to look up in the score_map.\n        score_map (dict): The score map defined in the eval_metrics.json file.\n\n    Returns:\n        float: The score obtained in the score_map.\n    \"\"\"\n    score = 0\n    if score_map is None:\n        logger.warning(\"Score map is missing\")\n    else:\n        score = score_map[score_key]\n    return score\n</code></pre>"},{"location":"api/addons/trustworthiness/calculation/#nebula.addons.trustworthiness.calculation.get_mapped_score","title":"<code>get_mapped_score(score_key, score_map)</code>","text":"<p>Finds the score by the score_key in the score_map.</p> <p>Parameters:</p> Name Type Description Default <code>score_key</code> <code>string</code> <p>The key to look up in the score_map.</p> required <code>score_map</code> <code>dict</code> <p>The score map defined in the eval_metrics.json file.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The normalized score of [0, 1].</p> Source code in <code>nebula/addons/trustworthiness/calculation.py</code> <pre><code>def get_mapped_score(score_key, score_map):\n    \"\"\"\n    Finds the score by the score_key in the score_map.\n\n    Args:\n        score_key (string): The key to look up in the score_map.\n        score_map (dict): The score map defined in the eval_metrics.json file.\n\n    Returns:\n        float: The normalized score of [0, 1].\n    \"\"\"\n    score = 0\n    if score_map is None:\n        logger.warning(\"Score map is missing\")\n    else:\n        keys = [key for key, value in score_map.items()]\n        scores = [value for key, value in score_map.items()]\n        normalized_scores = get_normalized_scores(scores)\n        normalized_score_map = dict(zip(keys, normalized_scores, strict=False))\n        score = normalized_score_map.get(score_key, np.nan)\n\n    return score\n</code></pre>"},{"location":"api/addons/trustworthiness/calculation/#nebula.addons.trustworthiness.calculation.get_normalized_scores","title":"<code>get_normalized_scores(scores)</code>","text":"<p>Calculates the normalized scores of a list.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>list</code> <p>The values that will be normalized.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>The normalized list.</p> Source code in <code>nebula/addons/trustworthiness/calculation.py</code> <pre><code>def get_normalized_scores(scores):\n    \"\"\"\n    Calculates the normalized scores of a list.\n\n    Args:\n        scores (list): The values that will be normalized.\n\n    Returns:\n        list: The normalized list.\n    \"\"\"\n    normalized = [(x - np.min(scores)) / (np.max(scores) - np.min(scores)) for x in scores]\n    return normalized\n</code></pre>"},{"location":"api/addons/trustworthiness/calculation/#nebula.addons.trustworthiness.calculation.get_range_score","title":"<code>get_range_score(value, ranges, direction='asc')</code>","text":"<p>Maps the value to a range and gets the score by the range and direction.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>int</code> <p>The input score.</p> required <code>ranges</code> <code>list</code> <p>The ranges defined.</p> required <code>direction</code> <code>string</code> <p>Asc means the higher the range the higher the score, desc means otherwise.</p> <code>'asc'</code> <p>Returns:</p> Name Type Description <code>float</code> <p>The normalized score of [0, 1].</p> Source code in <code>nebula/addons/trustworthiness/calculation.py</code> <pre><code>def get_range_score(value, ranges, direction=\"asc\"):\n    \"\"\"\n    Maps the value to a range and gets the score by the range and direction.\n\n    Args:\n        value (int): The input score.\n        ranges (list): The ranges defined.\n        direction (string): Asc means the higher the range the higher the score, desc means otherwise.\n\n    Returns:\n        float: The normalized score of [0, 1].\n    \"\"\"\n\n    if not (type(value) == int or type(value) == float):\n        logger.warning(\"Input value is not a number\")\n        logger.warning(f\"{value}\")\n        return 0\n    else:\n        score = 0\n        if ranges is None:\n            logger.warning(\"Score ranges are missing\")\n        else:\n            total_bins = len(ranges) + 1\n            bin = np.digitize(value, ranges, right=True)\n            score = 1 - (bin / total_bins) if direction == \"desc\" else bin / total_bins\n        return score\n</code></pre>"},{"location":"api/addons/trustworthiness/calculation/#nebula.addons.trustworthiness.calculation.get_scaled_score","title":"<code>get_scaled_score(value, scale, direction)</code>","text":"<p>Maps a score of a specific scale into the scale between zero and one.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>int or float</code> <p>The raw value of the metric.</p> required <code>scale</code> <code>list</code> <p>List containing the minimum and maximum value the value can fall in between.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The normalized score of [0, 1].</p> Source code in <code>nebula/addons/trustworthiness/calculation.py</code> <pre><code>def get_scaled_score(value, scale: list, direction: str):\n    \"\"\"\n    Maps a score of a specific scale into the scale between zero and one.\n\n    Args:\n        value (int or float): The raw value of the metric.\n        scale (list): List containing the minimum and maximum value the value can fall in between.\n\n    Returns:\n        float: The normalized score of [0, 1].\n    \"\"\"\n\n    score = 0\n    try:\n        value_min, value_max = scale[0], scale[1]\n    except Exception:\n        logger.warning(\"Score minimum or score maximum is missing. The minimum has been set to 0 and the maximum to 1\")\n        value_min, value_max = 0, 1\n    if not value:\n        logger.warning(\"Score value is missing. Set value to zero\")\n    else:\n        low, high = 0, 1\n        if value &gt;= value_max:\n            score = 1\n        elif value &lt;= value_min:\n            score = 0\n        else:\n            diff = value_max - value_min\n            diffScale = high - low\n            score = (float(value) - value_min) * (float(diffScale) / diff) + low\n        if direction == \"desc\":\n            score = high - score\n\n    return score\n</code></pre>"},{"location":"api/addons/trustworthiness/calculation/#nebula.addons.trustworthiness.calculation.get_true_score","title":"<code>get_true_score(value, direction)</code>","text":"<p>Returns the negative of the value if direction is 'desc', otherwise returns value.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>int</code> <p>The input score.</p> required <code>direction</code> <code>string</code> <p>Asc means the higher the range the higher the score, desc means otherwise.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The score obtained.</p> Source code in <code>nebula/addons/trustworthiness/calculation.py</code> <pre><code>def get_true_score(value, direction):\n    \"\"\"\n    Returns the negative of the value if direction is 'desc', otherwise returns value.\n\n    Args:\n        value (int): The input score.\n        direction (string): Asc means the higher the range the higher the score, desc means otherwise.\n\n    Returns:\n        float: The score obtained.\n    \"\"\"\n\n    if value is True:\n        return 1\n    elif value is False:\n        return 0\n    else:\n        if not (type(value) == int or type(value) == float):\n            logger.warning(\"Input value is not a number\")\n            logger.warning(f\"{value}.\")\n            return 0\n        else:\n            if direction == \"desc\":\n                return 1 - value\n            else:\n                return value\n</code></pre>"},{"location":"api/addons/trustworthiness/calculation/#nebula.addons.trustworthiness.calculation.get_value","title":"<code>get_value(value)</code>","text":"<p>Get the value of a metric.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>The value of the metric.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The value of the metric.</p> Source code in <code>nebula/addons/trustworthiness/calculation.py</code> <pre><code>def get_value(value):\n    \"\"\"\n    Get the value of a metric.\n\n    Args:\n        value (float): The value of the metric.\n\n    Returns:\n        float: The value of the metric.\n    \"\"\"\n\n    return value\n</code></pre>"},{"location":"api/addons/trustworthiness/calculation/#nebula.addons.trustworthiness.calculation.stop_emissions_tracking_and_save","title":"<code>stop_emissions_tracking_and_save(tracker, outdir, emissions_file, role, workload, sample_size=0)</code>","text":"<p>Stops emissions tracking object from CodeCarbon and saves relevant information to emissions.csv file.</p> <p>Parameters:</p> Name Type Description Default <code>tracker</code> <code>object</code> <p>The emissions tracker object holding information.</p> required <code>outdir</code> <code>str</code> <p>The path of the output directory of the experiment.</p> required <code>emissions_file</code> <code>str</code> <p>The path to the emissions file.</p> required <code>role</code> <code>str</code> <p>Either client or server depending on the role.</p> required <code>workload</code> <code>str</code> <p>Either aggregation or training depending on the workload.</p> required <code>sample_size</code> <code>int</code> <p>The number of samples used for training, if aggregation 0.</p> <code>0</code> Source code in <code>nebula/addons/trustworthiness/calculation.py</code> <pre><code>def stop_emissions_tracking_and_save(\n    tracker: EmissionsTracker,\n    outdir: str,\n    emissions_file: str,\n    role: str,\n    workload: str,\n    sample_size: int = 0,\n):\n    \"\"\"\n    Stops emissions tracking object from CodeCarbon and saves relevant information to emissions.csv file.\n\n    Args:\n        tracker (object): The emissions tracker object holding information.\n        outdir (str): The path of the output directory of the experiment.\n        emissions_file (str): The path to the emissions file.\n        role (str): Either client or server depending on the role.\n        workload (str): Either aggregation or training depending on the workload.\n        sample_size (int): The number of samples used for training, if aggregation 0.\n    \"\"\"\n\n    tracker.stop()\n\n    emissions_file = os.path.join(outdir, emissions_file)\n\n    if exists(emissions_file):\n        df = pd.read_csv(emissions_file)\n    else:\n        df = pd.DataFrame(\n            columns=[\n                \"role\",\n                \"energy_grid\",\n                \"emissions\",\n                \"workload\",\n                \"CPU_model\",\n                \"GPU_model\",\n            ]\n        )\n    try:\n        energy_grid = (tracker.final_emissions_data.emissions / tracker.final_emissions_data.energy_consumed) * 1000\n        df = pd.concat(\n            [\n                df,\n                pd.DataFrame({\n                    \"role\": role,\n                    \"energy_grid\": [energy_grid],\n                    \"emissions\": [tracker.final_emissions_data.emissions],\n                    \"workload\": workload,\n                    \"CPU_model\": tracker.final_emissions_data.cpu_model\n                    if tracker.final_emissions_data.cpu_model\n                    else \"None\",\n                    \"GPU_model\": tracker.final_emissions_data.gpu_model\n                    if tracker.final_emissions_data.gpu_model\n                    else \"None\",\n                    \"CPU_used\": True if tracker.final_emissions_data.cpu_energy else False,\n                    \"GPU_used\": True if tracker.final_emissions_data.gpu_energy else False,\n                    \"energy_consumed\": tracker.final_emissions_data.energy_consumed,\n                    \"sample_size\": sample_size,\n                }),\n            ],\n            ignore_index=True,\n        )\n        df.to_csv(emissions_file, encoding=\"utf-8\", index=False)\n    except Exception as e:\n        logger.warning(e)\n</code></pre>"},{"location":"api/addons/trustworthiness/factsheet/","title":"Documentation for Factsheet Module","text":""},{"location":"api/addons/trustworthiness/factsheet/#nebula.addons.trustworthiness.factsheet.Factsheet","title":"<code>Factsheet</code>","text":"Source code in <code>nebula/addons/trustworthiness/factsheet.py</code> <pre><code>class Factsheet:\n    def __init__(self):\n        \"\"\"\n        Manager class to populate the FactSheet\n        \"\"\"\n        self.factsheet_file_nm = \"factsheet.json\"\n        self.factsheet_template_file_nm = \"factsheet_template.json\"\n\n    def populate_factsheet_pre_train(self, data, scenario_name):\n        \"\"\"\n        Populates the factsheet with values before the training.\n\n        Args:\n            data (dict): Contains the data from the scenario.\n            scenario_name (string): The name of the scenario.\n        \"\"\"\n\n        factsheet_file = os.path.join(dirname, f\"files/{scenario_name}/{self.factsheet_file_nm}\")\n\n        factsheet_template = os.path.join(dirname, f\"configs/{self.factsheet_template_file_nm}\")\n\n        if not os.path.exists(factsheet_file):\n            shutil.copyfile(factsheet_template, factsheet_file)\n\n        with open(factsheet_file, \"r+\") as f:\n            factsheet = {}\n\n            try:\n                factsheet = json.load(f)\n\n                if data is not None:\n                    logger.info(\"FactSheet: Populating factsheet with pre training metrics\")\n\n                    federation = data[\"federation\"]\n                    n_nodes = int(data[\"n_nodes\"])\n                    dataset = data[\"dataset\"]\n                    algorithm = data[\"model\"]\n                    aggregation_algorithm = data[\"agg_algorithm\"]\n                    n_rounds = int(data[\"rounds\"])\n                    attack = data[\"attacks\"]\n                    poisoned_node_percent = int(data[\"poisoned_node_percent\"])\n                    poisoned_sample_percent = int(data[\"poisoned_sample_percent\"])\n                    poisoned_noise_percent = int(data[\"poisoned_noise_percent\"])\n                    with_reputation = data[\"with_reputation\"]\n                    is_dynamic_topology = data[\"is_dynamic_topology\"]\n                    is_dynamic_aggregation = data[\"is_dynamic_aggregation\"]\n                    target_aggregation = data[\"target_aggregation\"]\n\n                    if attack != \"No Attack\" and with_reputation == True and is_dynamic_aggregation == True:\n                        background = f\"For the project setup, the most important aspects are the following: The federation architecture is {federation}, involving {n_nodes} clients, the dataset used is {dataset}, the learning algorithm is {algorithm}, the aggregation algorithm is {aggregation_algorithm} and the number of rounds is {n_rounds}. In addition, the type of attack used against the clients is {attack}, where the percentage of attacked nodes is {poisoned_node_percent}, the percentage of attacked samples of each node is {poisoned_sample_percent}, and the percent of poisoned noise is {poisoned_noise_percent}. A reputation-based defence with a dynamic aggregation based on the aggregation algorithm {target_aggregation} is used, and the trustworthiness of the project is desired.\"\n\n                    elif attack != \"No Attack\" and with_reputation == True and is_dynamic_topology == True:\n                        background = f\"For the project setup, the most important aspects are the following: The federation architecture is {federation}, involving {n_nodes} clients, the dataset used is {dataset}, the learning algorithm is {algorithm}, the aggregation algorithm is {aggregation_algorithm} and the number of rounds is {n_rounds}. In addition, the type of attack used against the clients is {attack}, where the percentage of attacked nodes is {poisoned_node_percent}, the percentage of attacked samples of each node is {poisoned_sample_percent}, and the percent of poisoned noise is {poisoned_noise_percent}. A reputation-based defence with a dynamic topology is used, and the trustworthiness of the project is desired.\"\n\n                    elif attack != \"No Attack\" and with_reputation == False:\n                        background = f\"For the project setup, the most important aspects are the following: The federation architecture is {federation}, involving {n_nodes} clients, the dataset used is {dataset}, the learning algorithm is {algorithm}, the aggregation algorithm is {aggregation_algorithm} and the number of rounds is {n_rounds}. In addition, the type of attack used against the clients is {attack}, where the percentage of attacked nodes is {poisoned_node_percent}, the percentage of attacked samples of each node is {poisoned_sample_percent}, and the percent of poisoned noise is {poisoned_noise_percent}. No defence mechanism is used, and the trustworthiness of the project is desired.\"\n\n                    elif attack == \"No Attack\":\n                        background = f\"For the project setup, the most important aspects are the following: The federation architecture is {federation}, involving {n_nodes} clients, the dataset used is {dataset}, the learning algorithm is {algorithm}, the aggregation algorithm is {aggregation_algorithm} and the number of rounds is {n_rounds}. No attacks against clients are used, and the trustworthiness of the project is desired.\"\n\n                    # Set project specifications\n                    factsheet[\"project\"][\"overview\"] = data[\"scenario_title\"]\n                    factsheet[\"project\"][\"purpose\"] = data[\"scenario_description\"]\n                    factsheet[\"project\"][\"background\"] = background\n\n                    # Set data specifications\n                    factsheet[\"data\"][\"provenance\"] = data[\"dataset\"]\n                    factsheet[\"data\"][\"preprocessing\"] = data[\"topology\"]\n\n                    # Set participants\n                    factsheet[\"participants\"][\"client_num\"] = data[\"n_nodes\"] or \"\"\n                    factsheet[\"participants\"][\"sample_client_rate\"] = 1\n                    factsheet[\"participants\"][\"client_selector\"] = \"\"\n\n                    # Set configuration\n                    factsheet[\"configuration\"][\"aggregation_algorithm\"] = data[\"agg_algorithm\"] or \"\"\n                    factsheet[\"configuration\"][\"training_model\"] = data[\"model\"] or \"\"\n                    factsheet[\"configuration\"][\"personalization\"] = False\n                    factsheet[\"configuration\"][\"visualization\"] = True\n                    factsheet[\"configuration\"][\"total_round_num\"] = n_rounds\n\n                    if poisoned_noise_percent != 0:\n                        factsheet[\"configuration\"][\"differential_privacy\"] = True\n                        factsheet[\"configuration\"][\"dp_epsilon\"] = poisoned_noise_percent\n                    else:\n                        factsheet[\"configuration\"][\"differential_privacy\"] = False\n                        factsheet[\"configuration\"][\"dp_epsilon\"] = \"\"\n\n                    if dataset == \"MNIST\" and algorithm == \"MLP\":\n                        model = MNISTModelMLP()\n                    elif dataset == \"MNIST\" and algorithm == \"CNN\":\n                        model = MNISTModelCNN()\n                    else:\n                        model = MNISTModelCNN()\n\n                    factsheet[\"configuration\"][\"learning_rate\"] = model.get_learning_rate()\n                    factsheet[\"configuration\"][\"trainable_param_num\"] = model.count_parameters()\n                    factsheet[\"configuration\"][\"local_update_steps\"] = 1\n\n            except JSONDecodeError as e:\n                logger.warning(f\"{factsheet_file} is invalid\")\n                logger.error(e)\n\n            f.seek(0)\n            f.truncate()\n            json.dump(factsheet, f, indent=4)\n            f.close()\n\n    def populate_factsheet_post_train(self, scenario):\n        \"\"\"\n        Populates the factsheet with values after the training.\n\n        Args:\n            scenario (object): The scenario object.\n        \"\"\"\n        scenario_name = scenario[0]\n\n        factsheet_file = os.path.join(dirname, f\"files/{scenario_name}/{self.factsheet_file_nm}\")\n\n        logger.info(\"FactSheet: Populating factsheet with post training metrics\")\n\n        with open(factsheet_file, \"r+\") as f:\n            factsheet = {}\n            try:\n                factsheet = json.load(f)\n\n                dataset = factsheet[\"data\"][\"provenance\"]\n                model = factsheet[\"configuration\"][\"training_model\"]\n\n                actual_dir = os.getcwd()\n                files_dir = f\"{actual_dir}/trustworthiness/files/{scenario_name}\"\n                data_dir = f\"{actual_dir}/trustworthiness/data/\"\n\n                models_files = glob.glob(os.path.join(files_dir, \"*final_model*\"))\n                bytes_sent_files = glob.glob(os.path.join(files_dir, \"*bytes_sent*\"))\n                bytes_recv_files = glob.glob(os.path.join(files_dir, \"*bytes_recv*\"))\n                loss_files = glob.glob(os.path.join(files_dir, \"*loss*\"))\n                accuracy_files = glob.glob(os.path.join(files_dir, \"*accuracy*\"))\n                dataloaders_files = glob.glob(os.path.join(files_dir, \"*train_loader*\"))\n                test_dataloader_file = f\"{files_dir}/participant_1_test_loader.pk\"\n                train_model_file = f\"{files_dir}/participant_1_train_model.pk\"\n                emissions_file = os.path.join(files_dir, \"emissions.csv\")\n\n                # Entropy\n                i = 0\n                for file in dataloaders_files:\n                    with open(file, \"rb\") as file:\n                        dataloader = pickle.load(file)\n                    get_entropy(i, scenario_name, dataloader)\n                    i += 1\n\n                with open(f\"{files_dir}/entropy.json\") as file:\n                    entropy_distribution = json.load(file)\n\n                values = np.array(list(entropy_distribution.values()))\n\n                normalized_values = (values - np.min(values)) / (np.max(values) - np.min(values))\n\n                avg_entropy = np.mean(normalized_values)\n\n                factsheet[\"data\"][\"avg_entropy\"] = avg_entropy\n\n                # Set performance data\n                result_avg_loss_accuracy = get_avg_loss_accuracy(loss_files, accuracy_files)\n                factsheet[\"performance\"][\"test_loss_avg\"] = result_avg_loss_accuracy[0]\n                factsheet[\"performance\"][\"test_acc_avg\"] = result_avg_loss_accuracy[1]\n                test_acc_cv = get_cv(std=result_avg_loss_accuracy[2], mean=result_avg_loss_accuracy[1])\n                factsheet[\"fairness\"][\"test_acc_cv\"] = 1 if test_acc_cv &gt; 1 else test_acc_cv\n\n                factsheet[\"system\"][\"avg_time_minutes\"] = get_elapsed_time(scenario)\n                factsheet[\"system\"][\"avg_model_size\"] = get_bytes_models(models_files)\n\n                result_bytes_sent_recv = get_bytes_sent_recv(bytes_sent_files, bytes_recv_files)\n                factsheet[\"system\"][\"total_upload_bytes\"] = result_bytes_sent_recv[0]\n                factsheet[\"system\"][\"total_download_bytes\"] = result_bytes_sent_recv[1]\n                factsheet[\"system\"][\"avg_upload_bytes\"] = result_bytes_sent_recv[2]\n                factsheet[\"system\"][\"avg_download_bytes\"] = result_bytes_sent_recv[3]\n\n                factsheet[\"fairness\"][\"selection_cv\"] = 1\n\n                count_class_samples(scenario_name, dataloaders_files)\n\n                with open(f\"{files_dir}/count_class.json\") as file:\n                    class_distribution = json.load(file)\n\n                class_samples_sizes = [x for x in class_distribution.values()]\n                class_imbalance = get_cv(list=class_samples_sizes)\n                factsheet[\"fairness\"][\"class_imbalance\"] = 1 if class_imbalance &gt; 1 else class_imbalance\n\n                with open(train_model_file, \"rb\") as file:\n                    lightning_model = pickle.load(file)\n\n                if dataset == \"MNIST\" and model == \"MLP\":\n                    pytorch_model = MNISTTorchModelMLP()\n                elif dataset == \"MNIST\" and model == \"CNN\":\n                    pytorch_model = MNISTTorchModelCNN()\n                else:\n                    pytorch_model = CIFAR10TorchModelCNN()\n\n                pytorch_model.load_state_dict(lightning_model.state_dict())\n\n                with open(test_dataloader_file, \"rb\") as file:\n                    test_dataloader = pickle.load(file)\n\n                test_sample = next(iter(test_dataloader))\n\n                lr = factsheet[\"configuration\"][\"learning_rate\"]\n                value_clever = get_clever_score(pytorch_model, test_sample, 10, lr)\n\n                factsheet[\"performance\"][\"test_clever\"] = 1 if value_clever &gt; 1 else value_clever\n\n                feature_importance = get_feature_importance_cv(pytorch_model, test_sample)\n\n                factsheet[\"performance\"][\"test_feature_importance_cv\"] = (\n                    1 if feature_importance &gt; 1 else feature_importance\n                )\n\n                # Set emissions metrics\n                emissions = None if emissions_file is None else read_csv(emissions_file)\n                if emissions is not None:\n                    logger.info(\"FactSheet: Populating emissions\")\n                    cpu_spez_df = pd.read_csv(os.path.join(data_dir, \"CPU_benchmarks_v4.csv\"), header=0)\n                    emissions[\"CPU_model\"] = (\n                        emissions[\"CPU_model\"].astype(str).str.replace(r\"\\([^)]*\\)\", \"\", regex=True)\n                    )\n                    emissions[\"CPU_model\"] = emissions[\"CPU_model\"].astype(str).str.replace(r\" CPU\", \"\", regex=True)\n                    emissions[\"GPU_model\"] = emissions[\"GPU_model\"].astype(str).str.replace(r\"[0-9] x \", \"\", regex=True)\n                    emissions = pd.merge(\n                        emissions,\n                        cpu_spez_df[[\"cpuName\", \"powerPerf\"]],\n                        left_on=\"CPU_model\",\n                        right_on=\"cpuName\",\n                        how=\"left\",\n                    )\n                    gpu_spez_df = pd.read_csv(os.path.join(data_dir, \"GPU_benchmarks_v7.csv\"), header=0)\n                    emissions = pd.merge(\n                        emissions,\n                        gpu_spez_df[[\"gpuName\", \"powerPerformance\"]],\n                        left_on=\"GPU_model\",\n                        right_on=\"gpuName\",\n                        how=\"left\",\n                    )\n\n                    emissions.drop(\"cpuName\", axis=1, inplace=True)\n                    emissions.drop(\"gpuName\", axis=1, inplace=True)\n                    emissions[\"powerPerf\"] = emissions[\"powerPerf\"].astype(float)\n                    emissions[\"powerPerformance\"] = emissions[\"powerPerformance\"].astype(float)\n                    client_emissions = emissions.loc[emissions[\"role\"] == \"client\"]\n                    client_avg_carbon_intensity = round(client_emissions[\"energy_grid\"].mean(), 2)\n                    factsheet[\"sustainability\"][\"avg_carbon_intensity_clients\"] = check_field_filled(\n                        factsheet,\n                        [\"sustainability\", \"avg_carbon_intensity_clients\"],\n                        client_avg_carbon_intensity,\n                        \"\",\n                    )\n                    factsheet[\"sustainability\"][\"emissions_training\"] = check_field_filled(\n                        factsheet,\n                        [\"sustainability\", \"emissions_training\"],\n                        client_emissions[\"emissions\"].sum(),\n                        \"\",\n                    )\n                    factsheet[\"participants\"][\"avg_dataset_size\"] = check_field_filled(\n                        factsheet,\n                        [\"participants\", \"avg_dataset_size\"],\n                        client_emissions[\"sample_size\"].mean(),\n                        \"\",\n                    )\n\n                    server_emissions = emissions.loc[emissions[\"role\"] == \"server\"]\n                    server_avg_carbon_intensity = round(server_emissions[\"energy_grid\"].mean(), 2)\n                    factsheet[\"sustainability\"][\"avg_carbon_intensity_server\"] = check_field_filled(\n                        factsheet,\n                        [\"sustainability\", \"avg_carbon_intensity_server\"],\n                        server_avg_carbon_intensity,\n                        \"\",\n                    )\n                    factsheet[\"sustainability\"][\"emissions_aggregation\"] = check_field_filled(\n                        factsheet,\n                        [\"sustainability\", \"emissions_aggregation\"],\n                        server_emissions[\"emissions\"].sum(),\n                        \"\",\n                    )\n                    GPU_powerperf = (server_emissions.loc[server_emissions[\"GPU_used\"] == True])[\"powerPerformance\"]\n                    CPU_powerperf = (server_emissions.loc[server_emissions[\"CPU_used\"] == True])[\"powerPerf\"]\n                    server_power_performance = round(pd.concat([GPU_powerperf, CPU_powerperf]).mean(), 2)\n                    factsheet[\"sustainability\"][\"avg_power_performance_server\"] = check_field_filled(\n                        factsheet,\n                        [\"sustainability\", \"avg_power_performance_server\"],\n                        server_power_performance,\n                        \"\",\n                    )\n\n                    GPU_powerperf = (client_emissions.loc[client_emissions[\"GPU_used\"] == True])[\"powerPerformance\"]\n                    CPU_powerperf = (client_emissions.loc[client_emissions[\"CPU_used\"] == True])[\"powerPerf\"]\n                    clients_power_performance = round(pd.concat([GPU_powerperf, CPU_powerperf]).mean(), 2)\n                    factsheet[\"sustainability\"][\"avg_power_performance_clients\"] = clients_power_performance\n\n                    factsheet[\"sustainability\"][\"emissions_communication_uplink\"] = check_field_filled(\n                        factsheet,\n                        [\"sustainability\", \"emissions_communication_uplink\"],\n                        factsheet[\"system\"][\"total_upload_bytes\"]\n                        * 2.24e-10\n                        * factsheet[\"sustainability\"][\"avg_carbon_intensity_clients\"],\n                        \"\",\n                    )\n                    factsheet[\"sustainability\"][\"emissions_communication_downlink\"] = check_field_filled(\n                        factsheet,\n                        [\"sustainability\", \"emissions_communication_downlink\"],\n                        factsheet[\"system\"][\"total_download_bytes\"]\n                        * 2.24e-10\n                        * factsheet[\"sustainability\"][\"avg_carbon_intensity_server\"],\n                        \"\",\n                    )\n\n            except JSONDecodeError as e:\n                logger.warning(f\"{factsheet_file} is invalid\")\n                logger.error(e)\n\n            f.seek(0)\n            f.truncate()\n            json.dump(factsheet, f, indent=4)\n            f.close()\n</code></pre>"},{"location":"api/addons/trustworthiness/factsheet/#nebula.addons.trustworthiness.factsheet.Factsheet.__init__","title":"<code>__init__()</code>","text":"<p>Manager class to populate the FactSheet</p> Source code in <code>nebula/addons/trustworthiness/factsheet.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Manager class to populate the FactSheet\n    \"\"\"\n    self.factsheet_file_nm = \"factsheet.json\"\n    self.factsheet_template_file_nm = \"factsheet_template.json\"\n</code></pre>"},{"location":"api/addons/trustworthiness/factsheet/#nebula.addons.trustworthiness.factsheet.Factsheet.populate_factsheet_post_train","title":"<code>populate_factsheet_post_train(scenario)</code>","text":"<p>Populates the factsheet with values after the training.</p> <p>Parameters:</p> Name Type Description Default <code>scenario</code> <code>object</code> <p>The scenario object.</p> required Source code in <code>nebula/addons/trustworthiness/factsheet.py</code> <pre><code>def populate_factsheet_post_train(self, scenario):\n    \"\"\"\n    Populates the factsheet with values after the training.\n\n    Args:\n        scenario (object): The scenario object.\n    \"\"\"\n    scenario_name = scenario[0]\n\n    factsheet_file = os.path.join(dirname, f\"files/{scenario_name}/{self.factsheet_file_nm}\")\n\n    logger.info(\"FactSheet: Populating factsheet with post training metrics\")\n\n    with open(factsheet_file, \"r+\") as f:\n        factsheet = {}\n        try:\n            factsheet = json.load(f)\n\n            dataset = factsheet[\"data\"][\"provenance\"]\n            model = factsheet[\"configuration\"][\"training_model\"]\n\n            actual_dir = os.getcwd()\n            files_dir = f\"{actual_dir}/trustworthiness/files/{scenario_name}\"\n            data_dir = f\"{actual_dir}/trustworthiness/data/\"\n\n            models_files = glob.glob(os.path.join(files_dir, \"*final_model*\"))\n            bytes_sent_files = glob.glob(os.path.join(files_dir, \"*bytes_sent*\"))\n            bytes_recv_files = glob.glob(os.path.join(files_dir, \"*bytes_recv*\"))\n            loss_files = glob.glob(os.path.join(files_dir, \"*loss*\"))\n            accuracy_files = glob.glob(os.path.join(files_dir, \"*accuracy*\"))\n            dataloaders_files = glob.glob(os.path.join(files_dir, \"*train_loader*\"))\n            test_dataloader_file = f\"{files_dir}/participant_1_test_loader.pk\"\n            train_model_file = f\"{files_dir}/participant_1_train_model.pk\"\n            emissions_file = os.path.join(files_dir, \"emissions.csv\")\n\n            # Entropy\n            i = 0\n            for file in dataloaders_files:\n                with open(file, \"rb\") as file:\n                    dataloader = pickle.load(file)\n                get_entropy(i, scenario_name, dataloader)\n                i += 1\n\n            with open(f\"{files_dir}/entropy.json\") as file:\n                entropy_distribution = json.load(file)\n\n            values = np.array(list(entropy_distribution.values()))\n\n            normalized_values = (values - np.min(values)) / (np.max(values) - np.min(values))\n\n            avg_entropy = np.mean(normalized_values)\n\n            factsheet[\"data\"][\"avg_entropy\"] = avg_entropy\n\n            # Set performance data\n            result_avg_loss_accuracy = get_avg_loss_accuracy(loss_files, accuracy_files)\n            factsheet[\"performance\"][\"test_loss_avg\"] = result_avg_loss_accuracy[0]\n            factsheet[\"performance\"][\"test_acc_avg\"] = result_avg_loss_accuracy[1]\n            test_acc_cv = get_cv(std=result_avg_loss_accuracy[2], mean=result_avg_loss_accuracy[1])\n            factsheet[\"fairness\"][\"test_acc_cv\"] = 1 if test_acc_cv &gt; 1 else test_acc_cv\n\n            factsheet[\"system\"][\"avg_time_minutes\"] = get_elapsed_time(scenario)\n            factsheet[\"system\"][\"avg_model_size\"] = get_bytes_models(models_files)\n\n            result_bytes_sent_recv = get_bytes_sent_recv(bytes_sent_files, bytes_recv_files)\n            factsheet[\"system\"][\"total_upload_bytes\"] = result_bytes_sent_recv[0]\n            factsheet[\"system\"][\"total_download_bytes\"] = result_bytes_sent_recv[1]\n            factsheet[\"system\"][\"avg_upload_bytes\"] = result_bytes_sent_recv[2]\n            factsheet[\"system\"][\"avg_download_bytes\"] = result_bytes_sent_recv[3]\n\n            factsheet[\"fairness\"][\"selection_cv\"] = 1\n\n            count_class_samples(scenario_name, dataloaders_files)\n\n            with open(f\"{files_dir}/count_class.json\") as file:\n                class_distribution = json.load(file)\n\n            class_samples_sizes = [x for x in class_distribution.values()]\n            class_imbalance = get_cv(list=class_samples_sizes)\n            factsheet[\"fairness\"][\"class_imbalance\"] = 1 if class_imbalance &gt; 1 else class_imbalance\n\n            with open(train_model_file, \"rb\") as file:\n                lightning_model = pickle.load(file)\n\n            if dataset == \"MNIST\" and model == \"MLP\":\n                pytorch_model = MNISTTorchModelMLP()\n            elif dataset == \"MNIST\" and model == \"CNN\":\n                pytorch_model = MNISTTorchModelCNN()\n            else:\n                pytorch_model = CIFAR10TorchModelCNN()\n\n            pytorch_model.load_state_dict(lightning_model.state_dict())\n\n            with open(test_dataloader_file, \"rb\") as file:\n                test_dataloader = pickle.load(file)\n\n            test_sample = next(iter(test_dataloader))\n\n            lr = factsheet[\"configuration\"][\"learning_rate\"]\n            value_clever = get_clever_score(pytorch_model, test_sample, 10, lr)\n\n            factsheet[\"performance\"][\"test_clever\"] = 1 if value_clever &gt; 1 else value_clever\n\n            feature_importance = get_feature_importance_cv(pytorch_model, test_sample)\n\n            factsheet[\"performance\"][\"test_feature_importance_cv\"] = (\n                1 if feature_importance &gt; 1 else feature_importance\n            )\n\n            # Set emissions metrics\n            emissions = None if emissions_file is None else read_csv(emissions_file)\n            if emissions is not None:\n                logger.info(\"FactSheet: Populating emissions\")\n                cpu_spez_df = pd.read_csv(os.path.join(data_dir, \"CPU_benchmarks_v4.csv\"), header=0)\n                emissions[\"CPU_model\"] = (\n                    emissions[\"CPU_model\"].astype(str).str.replace(r\"\\([^)]*\\)\", \"\", regex=True)\n                )\n                emissions[\"CPU_model\"] = emissions[\"CPU_model\"].astype(str).str.replace(r\" CPU\", \"\", regex=True)\n                emissions[\"GPU_model\"] = emissions[\"GPU_model\"].astype(str).str.replace(r\"[0-9] x \", \"\", regex=True)\n                emissions = pd.merge(\n                    emissions,\n                    cpu_spez_df[[\"cpuName\", \"powerPerf\"]],\n                    left_on=\"CPU_model\",\n                    right_on=\"cpuName\",\n                    how=\"left\",\n                )\n                gpu_spez_df = pd.read_csv(os.path.join(data_dir, \"GPU_benchmarks_v7.csv\"), header=0)\n                emissions = pd.merge(\n                    emissions,\n                    gpu_spez_df[[\"gpuName\", \"powerPerformance\"]],\n                    left_on=\"GPU_model\",\n                    right_on=\"gpuName\",\n                    how=\"left\",\n                )\n\n                emissions.drop(\"cpuName\", axis=1, inplace=True)\n                emissions.drop(\"gpuName\", axis=1, inplace=True)\n                emissions[\"powerPerf\"] = emissions[\"powerPerf\"].astype(float)\n                emissions[\"powerPerformance\"] = emissions[\"powerPerformance\"].astype(float)\n                client_emissions = emissions.loc[emissions[\"role\"] == \"client\"]\n                client_avg_carbon_intensity = round(client_emissions[\"energy_grid\"].mean(), 2)\n                factsheet[\"sustainability\"][\"avg_carbon_intensity_clients\"] = check_field_filled(\n                    factsheet,\n                    [\"sustainability\", \"avg_carbon_intensity_clients\"],\n                    client_avg_carbon_intensity,\n                    \"\",\n                )\n                factsheet[\"sustainability\"][\"emissions_training\"] = check_field_filled(\n                    factsheet,\n                    [\"sustainability\", \"emissions_training\"],\n                    client_emissions[\"emissions\"].sum(),\n                    \"\",\n                )\n                factsheet[\"participants\"][\"avg_dataset_size\"] = check_field_filled(\n                    factsheet,\n                    [\"participants\", \"avg_dataset_size\"],\n                    client_emissions[\"sample_size\"].mean(),\n                    \"\",\n                )\n\n                server_emissions = emissions.loc[emissions[\"role\"] == \"server\"]\n                server_avg_carbon_intensity = round(server_emissions[\"energy_grid\"].mean(), 2)\n                factsheet[\"sustainability\"][\"avg_carbon_intensity_server\"] = check_field_filled(\n                    factsheet,\n                    [\"sustainability\", \"avg_carbon_intensity_server\"],\n                    server_avg_carbon_intensity,\n                    \"\",\n                )\n                factsheet[\"sustainability\"][\"emissions_aggregation\"] = check_field_filled(\n                    factsheet,\n                    [\"sustainability\", \"emissions_aggregation\"],\n                    server_emissions[\"emissions\"].sum(),\n                    \"\",\n                )\n                GPU_powerperf = (server_emissions.loc[server_emissions[\"GPU_used\"] == True])[\"powerPerformance\"]\n                CPU_powerperf = (server_emissions.loc[server_emissions[\"CPU_used\"] == True])[\"powerPerf\"]\n                server_power_performance = round(pd.concat([GPU_powerperf, CPU_powerperf]).mean(), 2)\n                factsheet[\"sustainability\"][\"avg_power_performance_server\"] = check_field_filled(\n                    factsheet,\n                    [\"sustainability\", \"avg_power_performance_server\"],\n                    server_power_performance,\n                    \"\",\n                )\n\n                GPU_powerperf = (client_emissions.loc[client_emissions[\"GPU_used\"] == True])[\"powerPerformance\"]\n                CPU_powerperf = (client_emissions.loc[client_emissions[\"CPU_used\"] == True])[\"powerPerf\"]\n                clients_power_performance = round(pd.concat([GPU_powerperf, CPU_powerperf]).mean(), 2)\n                factsheet[\"sustainability\"][\"avg_power_performance_clients\"] = clients_power_performance\n\n                factsheet[\"sustainability\"][\"emissions_communication_uplink\"] = check_field_filled(\n                    factsheet,\n                    [\"sustainability\", \"emissions_communication_uplink\"],\n                    factsheet[\"system\"][\"total_upload_bytes\"]\n                    * 2.24e-10\n                    * factsheet[\"sustainability\"][\"avg_carbon_intensity_clients\"],\n                    \"\",\n                )\n                factsheet[\"sustainability\"][\"emissions_communication_downlink\"] = check_field_filled(\n                    factsheet,\n                    [\"sustainability\", \"emissions_communication_downlink\"],\n                    factsheet[\"system\"][\"total_download_bytes\"]\n                    * 2.24e-10\n                    * factsheet[\"sustainability\"][\"avg_carbon_intensity_server\"],\n                    \"\",\n                )\n\n        except JSONDecodeError as e:\n            logger.warning(f\"{factsheet_file} is invalid\")\n            logger.error(e)\n\n        f.seek(0)\n        f.truncate()\n        json.dump(factsheet, f, indent=4)\n        f.close()\n</code></pre>"},{"location":"api/addons/trustworthiness/factsheet/#nebula.addons.trustworthiness.factsheet.Factsheet.populate_factsheet_pre_train","title":"<code>populate_factsheet_pre_train(data, scenario_name)</code>","text":"<p>Populates the factsheet with values before the training.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>Contains the data from the scenario.</p> required <code>scenario_name</code> <code>string</code> <p>The name of the scenario.</p> required Source code in <code>nebula/addons/trustworthiness/factsheet.py</code> <pre><code>def populate_factsheet_pre_train(self, data, scenario_name):\n    \"\"\"\n    Populates the factsheet with values before the training.\n\n    Args:\n        data (dict): Contains the data from the scenario.\n        scenario_name (string): The name of the scenario.\n    \"\"\"\n\n    factsheet_file = os.path.join(dirname, f\"files/{scenario_name}/{self.factsheet_file_nm}\")\n\n    factsheet_template = os.path.join(dirname, f\"configs/{self.factsheet_template_file_nm}\")\n\n    if not os.path.exists(factsheet_file):\n        shutil.copyfile(factsheet_template, factsheet_file)\n\n    with open(factsheet_file, \"r+\") as f:\n        factsheet = {}\n\n        try:\n            factsheet = json.load(f)\n\n            if data is not None:\n                logger.info(\"FactSheet: Populating factsheet with pre training metrics\")\n\n                federation = data[\"federation\"]\n                n_nodes = int(data[\"n_nodes\"])\n                dataset = data[\"dataset\"]\n                algorithm = data[\"model\"]\n                aggregation_algorithm = data[\"agg_algorithm\"]\n                n_rounds = int(data[\"rounds\"])\n                attack = data[\"attacks\"]\n                poisoned_node_percent = int(data[\"poisoned_node_percent\"])\n                poisoned_sample_percent = int(data[\"poisoned_sample_percent\"])\n                poisoned_noise_percent = int(data[\"poisoned_noise_percent\"])\n                with_reputation = data[\"with_reputation\"]\n                is_dynamic_topology = data[\"is_dynamic_topology\"]\n                is_dynamic_aggregation = data[\"is_dynamic_aggregation\"]\n                target_aggregation = data[\"target_aggregation\"]\n\n                if attack != \"No Attack\" and with_reputation == True and is_dynamic_aggregation == True:\n                    background = f\"For the project setup, the most important aspects are the following: The federation architecture is {federation}, involving {n_nodes} clients, the dataset used is {dataset}, the learning algorithm is {algorithm}, the aggregation algorithm is {aggregation_algorithm} and the number of rounds is {n_rounds}. In addition, the type of attack used against the clients is {attack}, where the percentage of attacked nodes is {poisoned_node_percent}, the percentage of attacked samples of each node is {poisoned_sample_percent}, and the percent of poisoned noise is {poisoned_noise_percent}. A reputation-based defence with a dynamic aggregation based on the aggregation algorithm {target_aggregation} is used, and the trustworthiness of the project is desired.\"\n\n                elif attack != \"No Attack\" and with_reputation == True and is_dynamic_topology == True:\n                    background = f\"For the project setup, the most important aspects are the following: The federation architecture is {federation}, involving {n_nodes} clients, the dataset used is {dataset}, the learning algorithm is {algorithm}, the aggregation algorithm is {aggregation_algorithm} and the number of rounds is {n_rounds}. In addition, the type of attack used against the clients is {attack}, where the percentage of attacked nodes is {poisoned_node_percent}, the percentage of attacked samples of each node is {poisoned_sample_percent}, and the percent of poisoned noise is {poisoned_noise_percent}. A reputation-based defence with a dynamic topology is used, and the trustworthiness of the project is desired.\"\n\n                elif attack != \"No Attack\" and with_reputation == False:\n                    background = f\"For the project setup, the most important aspects are the following: The federation architecture is {federation}, involving {n_nodes} clients, the dataset used is {dataset}, the learning algorithm is {algorithm}, the aggregation algorithm is {aggregation_algorithm} and the number of rounds is {n_rounds}. In addition, the type of attack used against the clients is {attack}, where the percentage of attacked nodes is {poisoned_node_percent}, the percentage of attacked samples of each node is {poisoned_sample_percent}, and the percent of poisoned noise is {poisoned_noise_percent}. No defence mechanism is used, and the trustworthiness of the project is desired.\"\n\n                elif attack == \"No Attack\":\n                    background = f\"For the project setup, the most important aspects are the following: The federation architecture is {federation}, involving {n_nodes} clients, the dataset used is {dataset}, the learning algorithm is {algorithm}, the aggregation algorithm is {aggregation_algorithm} and the number of rounds is {n_rounds}. No attacks against clients are used, and the trustworthiness of the project is desired.\"\n\n                # Set project specifications\n                factsheet[\"project\"][\"overview\"] = data[\"scenario_title\"]\n                factsheet[\"project\"][\"purpose\"] = data[\"scenario_description\"]\n                factsheet[\"project\"][\"background\"] = background\n\n                # Set data specifications\n                factsheet[\"data\"][\"provenance\"] = data[\"dataset\"]\n                factsheet[\"data\"][\"preprocessing\"] = data[\"topology\"]\n\n                # Set participants\n                factsheet[\"participants\"][\"client_num\"] = data[\"n_nodes\"] or \"\"\n                factsheet[\"participants\"][\"sample_client_rate\"] = 1\n                factsheet[\"participants\"][\"client_selector\"] = \"\"\n\n                # Set configuration\n                factsheet[\"configuration\"][\"aggregation_algorithm\"] = data[\"agg_algorithm\"] or \"\"\n                factsheet[\"configuration\"][\"training_model\"] = data[\"model\"] or \"\"\n                factsheet[\"configuration\"][\"personalization\"] = False\n                factsheet[\"configuration\"][\"visualization\"] = True\n                factsheet[\"configuration\"][\"total_round_num\"] = n_rounds\n\n                if poisoned_noise_percent != 0:\n                    factsheet[\"configuration\"][\"differential_privacy\"] = True\n                    factsheet[\"configuration\"][\"dp_epsilon\"] = poisoned_noise_percent\n                else:\n                    factsheet[\"configuration\"][\"differential_privacy\"] = False\n                    factsheet[\"configuration\"][\"dp_epsilon\"] = \"\"\n\n                if dataset == \"MNIST\" and algorithm == \"MLP\":\n                    model = MNISTModelMLP()\n                elif dataset == \"MNIST\" and algorithm == \"CNN\":\n                    model = MNISTModelCNN()\n                else:\n                    model = MNISTModelCNN()\n\n                factsheet[\"configuration\"][\"learning_rate\"] = model.get_learning_rate()\n                factsheet[\"configuration\"][\"trainable_param_num\"] = model.count_parameters()\n                factsheet[\"configuration\"][\"local_update_steps\"] = 1\n\n        except JSONDecodeError as e:\n            logger.warning(f\"{factsheet_file} is invalid\")\n            logger.error(e)\n\n        f.seek(0)\n        f.truncate()\n        json.dump(factsheet, f, indent=4)\n        f.close()\n</code></pre>"},{"location":"api/addons/trustworthiness/metric/","title":"Documentation for Metric Module","text":""},{"location":"api/addons/trustworthiness/metric/#nebula.addons.trustworthiness.metric.TrustMetricManager","title":"<code>TrustMetricManager</code>","text":"<p>Manager class to help store the output directory and handle calls from the FL framework.</p> Source code in <code>nebula/addons/trustworthiness/metric.py</code> <pre><code>class TrustMetricManager:\n    \"\"\"\n    Manager class to help store the output directory and handle calls from the FL framework.\n    \"\"\"\n\n    def __init__(self):\n        self.factsheet_file_nm = \"factsheet.json\"\n        self.eval_metrics_file_nm = \"eval_metrics.json\"\n        self.nebula_trust_results_nm = \"nebula_trust_results.json\"\n\n    def evaluate(self, scenario, weights, use_weights=False):\n        \"\"\"\n        Evaluates the trustworthiness score.\n\n        Args:\n            scenario (object): The scenario in whith the trustworthiness will be calculated.\n            weights (dict): The desired weghts of the pillars.\n            use_weights (bool): True to turn on the weights in the metric config file, default to False.\n        \"\"\"\n        # Get scenario name\n        scenario_name = scenario[0]\n        factsheet_file = os.path.join(dirname, f\"files/{scenario_name}/{self.factsheet_file_nm}\")\n        metrics_cfg_file = os.path.join(dirname, f\"configs/{self.eval_metrics_file_nm}\")\n        results_file = os.path.join(dirname, f\"files/{scenario_name}/{self.nebula_trust_results_nm}\")\n\n        if not os.path.exists(factsheet_file):\n            logger.error(f\"{factsheet_file} is missing! Please check documentation.\")\n            return\n\n        if not os.path.exists(metrics_cfg_file):\n            logger.error(f\"{metrics_cfg_file} is missing! Please check documentation.\")\n            return\n\n        with open(factsheet_file) as f, open(metrics_cfg_file) as m:\n            factsheet = json.load(f)\n            metrics_cfg = json.load(m)\n            metrics = metrics_cfg.items()\n            input_docs = {\"factsheet\": factsheet}\n\n            result_json = {\"trust_score\": 0, \"pillars\": []}\n            final_score = 0\n            result_print = []\n            for key, value in metrics:\n                pillar = TrustPillar(key, value, input_docs, use_weights)\n                score, result = pillar.evaluate()\n                weight = weights.get(key)\n                final_score += weight * score\n                result_print.append([key, score])\n                result_json[\"pillars\"].append(result)\n            final_score = round(final_score, 2)\n            result_json[\"trust_score\"] = final_score\n            write_results_json(results_file, result_json)\n</code></pre>"},{"location":"api/addons/trustworthiness/metric/#nebula.addons.trustworthiness.metric.TrustMetricManager.evaluate","title":"<code>evaluate(scenario, weights, use_weights=False)</code>","text":"<p>Evaluates the trustworthiness score.</p> <p>Parameters:</p> Name Type Description Default <code>scenario</code> <code>object</code> <p>The scenario in whith the trustworthiness will be calculated.</p> required <code>weights</code> <code>dict</code> <p>The desired weghts of the pillars.</p> required <code>use_weights</code> <code>bool</code> <p>True to turn on the weights in the metric config file, default to False.</p> <code>False</code> Source code in <code>nebula/addons/trustworthiness/metric.py</code> <pre><code>def evaluate(self, scenario, weights, use_weights=False):\n    \"\"\"\n    Evaluates the trustworthiness score.\n\n    Args:\n        scenario (object): The scenario in whith the trustworthiness will be calculated.\n        weights (dict): The desired weghts of the pillars.\n        use_weights (bool): True to turn on the weights in the metric config file, default to False.\n    \"\"\"\n    # Get scenario name\n    scenario_name = scenario[0]\n    factsheet_file = os.path.join(dirname, f\"files/{scenario_name}/{self.factsheet_file_nm}\")\n    metrics_cfg_file = os.path.join(dirname, f\"configs/{self.eval_metrics_file_nm}\")\n    results_file = os.path.join(dirname, f\"files/{scenario_name}/{self.nebula_trust_results_nm}\")\n\n    if not os.path.exists(factsheet_file):\n        logger.error(f\"{factsheet_file} is missing! Please check documentation.\")\n        return\n\n    if not os.path.exists(metrics_cfg_file):\n        logger.error(f\"{metrics_cfg_file} is missing! Please check documentation.\")\n        return\n\n    with open(factsheet_file) as f, open(metrics_cfg_file) as m:\n        factsheet = json.load(f)\n        metrics_cfg = json.load(m)\n        metrics = metrics_cfg.items()\n        input_docs = {\"factsheet\": factsheet}\n\n        result_json = {\"trust_score\": 0, \"pillars\": []}\n        final_score = 0\n        result_print = []\n        for key, value in metrics:\n            pillar = TrustPillar(key, value, input_docs, use_weights)\n            score, result = pillar.evaluate()\n            weight = weights.get(key)\n            final_score += weight * score\n            result_print.append([key, score])\n            result_json[\"pillars\"].append(result)\n        final_score = round(final_score, 2)\n        result_json[\"trust_score\"] = final_score\n        write_results_json(results_file, result_json)\n</code></pre>"},{"location":"api/addons/trustworthiness/pillar/","title":"Documentation for Pillar Module","text":""},{"location":"api/addons/trustworthiness/pillar/#nebula.addons.trustworthiness.pillar.TrustPillar","title":"<code>TrustPillar</code>","text":"<p>Class to represent a trust pillar.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>string</code> <p>Name of the pillar.</p> required <code>metrics</code> <code>dict</code> <p>Metric definitions for the pillar.</p> required <code>input_docs</code> <code>dict</code> <p>Input documents.</p> required <code>use_weights</code> <code>bool</code> <p>True to turn on the weights in the metric config file.</p> <code>False</code> Source code in <code>nebula/addons/trustworthiness/pillar.py</code> <pre><code>class TrustPillar:\n    \"\"\"\n    Class to represent a trust pillar.\n\n    Args:\n        name (string): Name of the pillar.\n        metrics (dict): Metric definitions for the pillar.\n        input_docs (dict): Input documents.\n        use_weights (bool): True to turn on the weights in the metric config file.\n\n    \"\"\"\n\n    def __init__(self, name, metrics, input_docs, use_weights=False):\n        self.name = name\n        self.input_docs = input_docs\n        self.metrics = metrics\n        self.result = []\n        self.use_weights = use_weights\n\n    def evaluate(self):\n        \"\"\"\n        Evaluate the trust score for the pillar.\n\n        Returns:\n            float: Score of [0, 1].\n        \"\"\"\n        score = 0\n        avg_weight = 1 / len(self.metrics)\n        for key, value in self.metrics.items():\n            weight = value.get(\"weight\", avg_weight) if self.use_weights else avg_weight\n            score += weight * self.get_notion_score(key, value.get(\"metrics\"))\n        score = round(score, 2)\n        return score, {self.name: {\"score\": score, \"notions\": self.result}}\n\n    def get_notion_score(self, name, metrics):\n        \"\"\"\n        Evaluate the trust score for the notion.\n\n        Args:\n            name (string): Name of the notion.\n            metrics (list): Metrics definitions of the notion.\n\n        Returns:\n            float: Score of [0, 1].\n        \"\"\"\n\n        notion_score = 0\n        avg_weight = 1 / len(metrics)\n        metrics_result = []\n        for key, value in metrics.items():\n            metric_score = self.get_metric_score(metrics_result, key, value)\n            weight = value.get(\"weight\", avg_weight) if self.use_weights else avg_weight\n            notion_score += weight * float(metric_score)\n        self.result.append({name: {\"score\": notion_score, \"metrics\": metrics_result}})\n        return notion_score\n\n    def get_metric_score(self, result, name, metric):\n        \"\"\"\n        Evaluate the trust score for the metric.\n\n        Args:\n            result (object): The result object\n            name (string): Name of the metric.\n            metrics (dict): The metric definition.\n\n        Returns:\n            float: Score of [0, 1].\n        \"\"\"\n\n        score = 0\n        try:\n            input_value = get_input_value(self.input_docs, metric.get(\"inputs\"), metric.get(\"operation\"))\n\n            score_type = metric.get(\"type\")\n            if input_value is None:\n                logger.warning(f\"{name} input value is null\")\n            else:\n                if score_type == \"true_score\":\n                    score = calculation.get_true_score(input_value, metric.get(\"direction\"))\n                elif score_type == \"score_mapping\":\n                    score = calculation.get_mapped_score(input_value, metric.get(\"score_map\"))\n                elif score_type == \"ranges\":\n                    score = calculation.get_range_score(input_value, metric.get(\"ranges\"), metric.get(\"direction\"))\n                elif score_type == \"score_map_value\":\n                    score = calculation.get_map_value_score(input_value, metric.get(\"score_map\"))\n                elif score_type == \"scaled_score\":\n                    score = calculation.get_scaled_score(input_value, metric.get(\"scale\"), metric.get(\"direction\"))\n                elif score_type == \"property_check\":\n                    score = 0 if input_value is None else input_value\n\n                else:\n                    logger.warning(f\"The score type {score_type} is not yet implemented.\")\n\n        except KeyError:\n            logger.warning(f\"Null input for {name} metric\")\n        score = round(score, 2)\n        result.append({name: {\"score\": score}})\n        return score\n</code></pre>"},{"location":"api/addons/trustworthiness/pillar/#nebula.addons.trustworthiness.pillar.TrustPillar.evaluate","title":"<code>evaluate()</code>","text":"<p>Evaluate the trust score for the pillar.</p> <p>Returns:</p> Name Type Description <code>float</code> <p>Score of [0, 1].</p> Source code in <code>nebula/addons/trustworthiness/pillar.py</code> <pre><code>def evaluate(self):\n    \"\"\"\n    Evaluate the trust score for the pillar.\n\n    Returns:\n        float: Score of [0, 1].\n    \"\"\"\n    score = 0\n    avg_weight = 1 / len(self.metrics)\n    for key, value in self.metrics.items():\n        weight = value.get(\"weight\", avg_weight) if self.use_weights else avg_weight\n        score += weight * self.get_notion_score(key, value.get(\"metrics\"))\n    score = round(score, 2)\n    return score, {self.name: {\"score\": score, \"notions\": self.result}}\n</code></pre>"},{"location":"api/addons/trustworthiness/pillar/#nebula.addons.trustworthiness.pillar.TrustPillar.get_metric_score","title":"<code>get_metric_score(result, name, metric)</code>","text":"<p>Evaluate the trust score for the metric.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>object</code> <p>The result object</p> required <code>name</code> <code>string</code> <p>Name of the metric.</p> required <code>metrics</code> <code>dict</code> <p>The metric definition.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>Score of [0, 1].</p> Source code in <code>nebula/addons/trustworthiness/pillar.py</code> <pre><code>def get_metric_score(self, result, name, metric):\n    \"\"\"\n    Evaluate the trust score for the metric.\n\n    Args:\n        result (object): The result object\n        name (string): Name of the metric.\n        metrics (dict): The metric definition.\n\n    Returns:\n        float: Score of [0, 1].\n    \"\"\"\n\n    score = 0\n    try:\n        input_value = get_input_value(self.input_docs, metric.get(\"inputs\"), metric.get(\"operation\"))\n\n        score_type = metric.get(\"type\")\n        if input_value is None:\n            logger.warning(f\"{name} input value is null\")\n        else:\n            if score_type == \"true_score\":\n                score = calculation.get_true_score(input_value, metric.get(\"direction\"))\n            elif score_type == \"score_mapping\":\n                score = calculation.get_mapped_score(input_value, metric.get(\"score_map\"))\n            elif score_type == \"ranges\":\n                score = calculation.get_range_score(input_value, metric.get(\"ranges\"), metric.get(\"direction\"))\n            elif score_type == \"score_map_value\":\n                score = calculation.get_map_value_score(input_value, metric.get(\"score_map\"))\n            elif score_type == \"scaled_score\":\n                score = calculation.get_scaled_score(input_value, metric.get(\"scale\"), metric.get(\"direction\"))\n            elif score_type == \"property_check\":\n                score = 0 if input_value is None else input_value\n\n            else:\n                logger.warning(f\"The score type {score_type} is not yet implemented.\")\n\n    except KeyError:\n        logger.warning(f\"Null input for {name} metric\")\n    score = round(score, 2)\n    result.append({name: {\"score\": score}})\n    return score\n</code></pre>"},{"location":"api/addons/trustworthiness/pillar/#nebula.addons.trustworthiness.pillar.TrustPillar.get_notion_score","title":"<code>get_notion_score(name, metrics)</code>","text":"<p>Evaluate the trust score for the notion.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>string</code> <p>Name of the notion.</p> required <code>metrics</code> <code>list</code> <p>Metrics definitions of the notion.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>Score of [0, 1].</p> Source code in <code>nebula/addons/trustworthiness/pillar.py</code> <pre><code>def get_notion_score(self, name, metrics):\n    \"\"\"\n    Evaluate the trust score for the notion.\n\n    Args:\n        name (string): Name of the notion.\n        metrics (list): Metrics definitions of the notion.\n\n    Returns:\n        float: Score of [0, 1].\n    \"\"\"\n\n    notion_score = 0\n    avg_weight = 1 / len(metrics)\n    metrics_result = []\n    for key, value in metrics.items():\n        metric_score = self.get_metric_score(metrics_result, key, value)\n        weight = value.get(\"weight\", avg_weight) if self.use_weights else avg_weight\n        notion_score += weight * float(metric_score)\n    self.result.append({name: {\"score\": notion_score, \"metrics\": metrics_result}})\n    return notion_score\n</code></pre>"},{"location":"api/addons/trustworthiness/utils/","title":"Documentation for Utils Module","text":""},{"location":"api/addons/trustworthiness/utils/#nebula.addons.trustworthiness.utils.check_field_filled","title":"<code>check_field_filled(factsheet_dict, factsheet_path, value, empty='')</code>","text":"<p>Check if the field in the factsheet file is filled or not.</p> <p>Parameters:</p> Name Type Description Default <code>factsheet_dict</code> <code>dict</code> <p>The factshett dict.</p> required <code>factsheet_path</code> <code>list</code> <p>The factsheet field to check.</p> required <code>value</code> <code>float</code> <p>The value to add in the field.</p> required <code>empty</code> <code>string</code> <p>If the value could not be appended, the empty string is returned.</p> <code>''</code> <p>Returns:</p> Name Type Description <code>float</code> <p>The value added in the factsheet or empty if the value could not be appened</p> Source code in <code>nebula/addons/trustworthiness/utils.py</code> <pre><code>def check_field_filled(factsheet_dict, factsheet_path, value, empty=\"\"):\n    \"\"\"\n    Check if the field in the factsheet file is filled or not.\n\n    Args:\n        factsheet_dict (dict): The factshett dict.\n        factsheet_path (list): The factsheet field to check.\n        value (float): The value to add in the field.\n        empty (string): If the value could not be appended, the empty string is returned.\n\n    Returns:\n        float: The value added in the factsheet or empty if the value could not be appened\n\n    \"\"\"\n    if factsheet_dict[factsheet_path[0]][factsheet_path[1]]:\n        return factsheet_dict[factsheet_path[0]][factsheet_path[1]]\n    elif value != \"\" and value != \"nan\":\n        if type(value) != str and type(value) != list:\n            if math.isnan(value):\n                return 0\n            else:\n                return value\n        else:\n            return value\n    else:\n        return empty\n</code></pre>"},{"location":"api/addons/trustworthiness/utils/#nebula.addons.trustworthiness.utils.count_class_samples","title":"<code>count_class_samples(scenario_name, dataloaders_files)</code>","text":"<p>Counts the number of samples by class.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_name</code> <code>string</code> <p>Name of the scenario.</p> required <code>dataloaders_files</code> <code>list</code> <p>Files that contain the dataloaders.</p> required Source code in <code>nebula/addons/trustworthiness/utils.py</code> <pre><code>def count_class_samples(scenario_name, dataloaders_files):\n    \"\"\"\n    Counts the number of samples by class.\n\n    Args:\n        scenario_name (string): Name of the scenario.\n        dataloaders_files (list): Files that contain the dataloaders.\n\n    \"\"\"\n\n    result = {}\n    dataloaders = []\n\n    for file in dataloaders_files:\n        with open(file, \"rb\") as f:\n            dataloader = pickle.load(f)\n            dataloaders.append(dataloader)\n\n    for dataloader in dataloaders:\n        for batch, labels in dataloader:\n            for b, label in zip(batch, labels, strict=False):\n                l = hashids.encode(label.item())\n                if l in result:\n                    result[l] += 1\n                else:\n                    result[l] = 1\n\n    name_file = f\"{dirname}/files/{scenario_name}/count_class.json\"\n    with open(name_file, \"w\") as f:\n        json.dump(result, f)\n</code></pre>"},{"location":"api/addons/trustworthiness/utils/#nebula.addons.trustworthiness.utils.get_entropy","title":"<code>get_entropy(client_id, scenario_name, dataloader)</code>","text":"<p>Get the entropy of each client in the scenario.</p> <p>Parameters:</p> Name Type Description Default <code>client_id</code> <code>int</code> <p>The client id.</p> required <code>scenario_name</code> <code>string</code> <p>Name of the scenario.</p> required <code>dataloaders_files</code> <code>list</code> <p>Files that contain the dataloaders.</p> required Source code in <code>nebula/addons/trustworthiness/utils.py</code> <pre><code>def get_entropy(client_id, scenario_name, dataloader):\n    \"\"\"\n    Get the entropy of each client in the scenario.\n\n    Args:\n        client_id (int): The client id.\n        scenario_name (string): Name of the scenario.\n        dataloaders_files (list): Files that contain the dataloaders.\n\n    \"\"\"\n    result = {}\n    client_entropy = {}\n\n    name_file = f\"{dirname}/files/{scenario_name}/entropy.json\"\n    if os.path.exists(name_file):\n        with open(name_file) as f:\n            client_entropy = json.load(f)\n\n    client_id_hash = hashids.encode(client_id)\n\n    for batch, labels in dataloader:\n        for b, label in zip(batch, labels, strict=False):\n            l = hashids.encode(label.item())\n            if l in result:\n                result[l] += 1\n            else:\n                result[l] = 1\n\n    n = len(dataloader)\n    entropy_value = entropy([x / n for x in result.values()], base=2)\n    client_entropy[client_id_hash] = entropy_value\n    with open(name_file, \"w\") as f:\n        json.dump(client_entropy, f)\n</code></pre>"},{"location":"api/addons/trustworthiness/utils/#nebula.addons.trustworthiness.utils.get_input_value","title":"<code>get_input_value(input_docs, inputs, operation)</code>","text":"<p>Gets the input value from input document and apply the metric operation on the value.</p> <p>Parameters:</p> Name Type Description Default <code>inputs_docs</code> <code>map</code> <p>The input document map.</p> required <code>inputs</code> <code>list</code> <p>All the inputs.</p> required <code>operation</code> <code>string</code> <p>The metric operation.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The metric value</p> Source code in <code>nebula/addons/trustworthiness/utils.py</code> <pre><code>def get_input_value(input_docs, inputs, operation):\n    \"\"\"\n    Gets the input value from input document and apply the metric operation on the value.\n\n    Args:\n        inputs_docs (map): The input document map.\n        inputs (list): All the inputs.\n        operation (string): The metric operation.\n\n    Returns:\n        float: The metric value\n\n    \"\"\"\n\n    input_value = None\n    args = []\n    for i in inputs:\n        source = i.get(\"source\", \"\")\n        field = i.get(\"field_path\", \"\")\n        input_doc = input_docs.get(source, None)\n        if input_doc is None:\n            logger.warning(f\"{source} is null\")\n        else:\n            input = get_value_from_path(input_doc, field)\n            args.append(input)\n    try:\n        operationFn = getattr(calculation, operation)\n        input_value = operationFn(*args)\n    except TypeError:\n        logger.warning(f\"{operation} is not valid\")\n\n    return input_value\n</code></pre>"},{"location":"api/addons/trustworthiness/utils/#nebula.addons.trustworthiness.utils.get_value_from_path","title":"<code>get_value_from_path(input_doc, path)</code>","text":"<p>Gets the input value from input document by path.</p> <p>Parameters:</p> Name Type Description Default <code>inputs_doc</code> <code>map</code> <p>The input document map.</p> required <code>path</code> <code>string</code> <p>The field name of the input value of interest.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The input value from the input document</p> Source code in <code>nebula/addons/trustworthiness/utils.py</code> <pre><code>def get_value_from_path(input_doc, path):\n    \"\"\"\n    Gets the input value from input document by path.\n\n    Args:\n        inputs_doc (map): The input document map.\n        path (string): The field name of the input value of interest.\n\n    Returns:\n        float: The input value from the input document\n\n    \"\"\"\n\n    d = input_doc\n    for nested_key in path.split(\"/\"):\n        temp = d.get(nested_key)\n        if isinstance(temp, dict):\n            d = d.get(nested_key)\n        else:\n            return temp\n    return None\n</code></pre>"},{"location":"api/addons/trustworthiness/utils/#nebula.addons.trustworthiness.utils.read_csv","title":"<code>read_csv(filename)</code>","text":"<p>Read a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>string</code> <p>Name of the file.</p> required <p>Returns:</p> Name Type Description <code>object</code> <p>The CSV readed.</p> Source code in <code>nebula/addons/trustworthiness/utils.py</code> <pre><code>def read_csv(filename):\n    \"\"\"\n    Read a CSV file.\n\n    Args:\n        filename (string): Name of the file.\n\n    Returns:\n        object: The CSV readed.\n\n    \"\"\"\n    if exists(filename):\n        return pd.read_csv(filename)\n</code></pre>"},{"location":"api/addons/trustworthiness/utils/#nebula.addons.trustworthiness.utils.write_results_json","title":"<code>write_results_json(out_file, dict)</code>","text":"<p>Writes the result to JSON.</p> <p>Parameters:</p> Name Type Description Default <code>out_file</code> <code>string</code> <p>The output file.</p> required <code>dict</code> <code>dict</code> <p>The object to be witten into JSON.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The input value from the input document</p> Source code in <code>nebula/addons/trustworthiness/utils.py</code> <pre><code>def write_results_json(out_file, dict):\n    \"\"\"\n    Writes the result to JSON.\n\n    Args:\n        out_file (string): The output file.\n        dict (dict): The object to be witten into JSON.\n\n    Returns:\n        float: The input value from the input document\n\n    \"\"\"\n\n    with open(out_file, \"a\") as f:\n        json.dump(dict, f, indent=4)\n</code></pre>"},{"location":"api/addons/waf/","title":"Documentation for Waf Module","text":""},{"location":"api/core/","title":"Documentation for Core Module","text":""},{"location":"api/core/addonmanager/","title":"Documentation for Addonmanager Module","text":""},{"location":"api/core/engine/","title":"Documentation for Engine Module","text":""},{"location":"api/core/engine/#nebula.core.engine.Engine","title":"<code>Engine</code>","text":"Source code in <code>nebula/core/engine.py</code> <pre><code>class Engine:\n    def __init__(\n        self,\n        model,\n        datamodule,\n        config=Config,\n        trainer=Lightning,\n        security=False,\n    ):\n        self.config = config\n        self.idx = config.participant[\"device_args\"][\"idx\"]\n        self.experiment_name = config.participant[\"scenario_args\"][\"name\"]\n        self.ip = config.participant[\"network_args\"][\"ip\"]\n        self.port = config.participant[\"network_args\"][\"port\"]\n        self.addr = config.participant[\"network_args\"][\"addr\"]\n        self.role = config.participant[\"device_args\"][\"role\"]\n        self.name = config.participant[\"device_args\"][\"name\"]\n        self.docker_id = config.participant[\"device_args\"][\"docker_id\"]\n        self.client = docker.from_env()\n\n        print_banner()\n\n        print_msg_box(\n            msg=f\"Name {self.name}\\nRole: {self.role}\",\n            indent=2,\n            title=\"Node information\",\n        )\n\n        self._trainer = None\n        self._aggregator = None\n        self.round = None\n        self.total_rounds = None\n        self.federation_nodes = set()\n        self.initialized = False\n        self.log_dir = os.path.join(config.participant[\"tracking_args\"][\"log_dir\"], self.experiment_name)\n\n        self.security = security\n\n        self._trainer = trainer(model, datamodule, config=self.config)\n        self._aggregator = create_aggregator(config=self.config, engine=self)\n\n        self._secure_neighbors = []\n        self._is_malicious = self.config.participant[\"adversarial_args\"][\"attacks\"] != \"No Attack\"\n\n        msg = f\"Trainer: {self._trainer.__class__.__name__}\"\n        msg += f\"\\nDataset: {self.config.participant['data_args']['dataset']}\"\n        msg += f\"\\nIID: {self.config.participant['data_args']['iid']}\"\n        msg += f\"\\nModel: {model.__class__.__name__}\"\n        msg += f\"\\nAggregation algorithm: {self._aggregator.__class__.__name__}\"\n        msg += f\"\\nNode behavior: {'malicious' if self._is_malicious else 'benign'}\"\n        print_msg_box(msg=msg, indent=2, title=\"Scenario information\")\n        print_msg_box(\n            msg=f\"Logging type: {self._trainer.logger.__class__.__name__}\",\n            indent=2,\n            title=\"Logging information\",\n        )\n\n        self.learning_cycle_lock = Locker(name=\"learning_cycle_lock\", async_lock=True)\n        self.federation_setup_lock = Locker(name=\"federation_setup_lock\", async_lock=True)\n        self.federation_ready_lock = Locker(name=\"federation_ready_lock\", async_lock=True)\n        self.round_lock = Locker(name=\"round_lock\", async_lock=True)\n        self.config.reload_config_file()\n\n        self._cm = CommunicationsManager(engine=self)\n        # Set the communication manager in the model (send messages from there)\n        self.trainer.model.set_communication_manager(self._cm)\n        self._reporter = Reporter(config=self.config, trainer=self.trainer, cm=self.cm)\n        self._addon_manager = AddonManager(self, self.config)\n        self._reputation = Reputation(self, self.config)\n\n    @property\n    def cm(self):\n        return self._cm\n\n    @property\n    def reporter(self):\n        return self._reporter\n\n    @property\n    def aggregator(self):\n        return self._aggregator\n\n    def get_aggregator_type(self):\n        return type(self.aggregator)\n\n    @property\n    def trainer(self):\n        return self._trainer\n\n    def get_addr(self):\n        return self.addr\n\n    def get_config(self):\n        return self.config\n\n    def get_federation_nodes(self):\n        return self.federation_nodes\n\n    def get_initialization_status(self):\n        return self.initialized\n\n    def set_initialization_status(self, status):\n        self.initialized = status\n\n    def get_round(self):\n        return self.round\n\n    def get_federation_ready_lock(self):\n        return self.federation_ready_lock\n\n    def get_federation_setup_lock(self):\n        return self.federation_setup_lock\n\n    def get_round_lock(self):\n        return self.round_lock\n\n    def set_round(self, new_round):\n        logging.info(f\"\ud83e\udd16  Update round count | from: {self.round} | to round: {new_round}\")\n        self.round = new_round\n        self.trainer.set_current_round(new_round)\n\n    \"\"\"\n    ##############################\n    #       MODEL CALLBACKS      #\n    ##############################\n    \"\"\"\n\n    async def model_initialization_callback(self, source, message):\n        logging.info(f\"\ud83e\udd16  handle_model_message | Received model initialization from {source}\")\n        try:\n            model = self.trainer.deserialize_model(message.parameters)\n            self.trainer.set_model_parameters(model, initialize=True)\n            logging.info(\"\ud83e\udd16  Init Model | Model Parameters Initialized\")\n            self.set_initialization_status(True)\n            await (\n                self.get_federation_ready_lock().release_async()\n            )  # Enable learning cycle once the initialization is done\n            try:\n                await (\n                    self.get_federation_ready_lock().release_async()\n                )  # Release the lock acquired at the beginning of the engine\n            except RuntimeError:\n                pass\n        except RuntimeError:\n            pass\n\n    async def model_update_callback(self, source, message):\n        logging.info(f\"\ud83e\udd16  handle_model_message | Received model update from {source} with round {message.round}\")\n        if not self.get_federation_ready_lock().locked() and len(self.get_federation_nodes()) == 0:\n            logging.info(\"\ud83e\udd16  handle_model_message | There are no defined federation nodes\")\n            return\n        decoded_model = self.trainer.deserialize_model(message.parameters)\n        updt_received_event = UpdateReceivedEvent(decoded_model, message.weight, source, message.round)\n        await EventManager.get_instance().publish_node_event(updt_received_event)\n\n    \"\"\"\n    ##############################\n    #      General callbacks     #\n    ##############################\n    \"\"\"\n\n    async def _discovery_discover_callback(self, source, message):\n        logging.info(\n            f\"\ud83d\udd0d  handle_discovery_message | Trigger | Received discovery message from {source} (network propagation)\"\n        )\n        current_connections = await self.cm.get_addrs_current_connections(myself=True)\n        if source not in current_connections:\n            logging.info(f\"\ud83d\udd0d  handle_discovery_message | Trigger | Connecting to {source} indirectly\")\n            await self.cm.connect(source, direct=False)\n        async with self.cm.get_connections_lock():\n            if source in self.cm.connections:\n                # Update the latitude and longitude of the node (if already connected)\n                if (\n                    message.latitude is not None\n                    and -90 &lt;= message.latitude &lt;= 90\n                    and message.longitude is not None\n                    and -180 &lt;= message.longitude &lt;= 180\n                ):\n                    self.cm.connections[source].update_geolocation(message.latitude, message.longitude)\n                else:\n                    logging.warning(\n                        f\"\ud83d\udd0d  Invalid geolocation received from {source}: latitude={message.latitude}, longitude={message.longitude}\"\n                    )\n\n    async def _control_alive_callback(self, source, message):\n        logging.info(f\"\ud83d\udd27  handle_control_message | Trigger | Received alive message from {source}\")\n        current_connections = await self.cm.get_addrs_current_connections(myself=True)\n        if source in current_connections:\n            try:\n                await self.cm.health.alive(source)\n            except Exception as e:\n                logging.exception(f\"Error updating alive status in connection: {e}\")\n        else:\n            logging.error(f\"\u2757\ufe0f  Connection {source} not found in connections...\")\n\n    async def _connection_connect_callback(self, source, message):\n        logging.info(f\"\ud83d\udd17  handle_connection_message | Trigger | Received connection message from {source}\")\n        current_connections = await self.cm.get_addrs_current_connections(myself=True)\n        if source not in current_connections:\n            logging.info(f\"\ud83d\udd17  handle_connection_message | Trigger | Connecting to {source}\")\n            await self.cm.connect(source, direct=True)\n\n    async def _connection_disconnect_callback(self, source, message):\n        logging.info(f\"\ud83d\udd17  handle_connection_message | Trigger | Received disconnection message from {source}\")\n        if self.mobility:\n            if await self.nm.waiting_confirmation_from(source):\n                await self.nm.confirmation_received(source, confirmation=False)\n            # if source in await self.cm.get_all_addrs_current_connections(only_direct=True):\n            await self.nm.update_neighbors(source, remove=True)\n        await self.cm.disconnect(source, mutual_disconnection=False)\n\n    async def _federation_federation_ready_callback(self, source, message):\n        logging.info(f\"\ud83d\udcdd  handle_federation_message | Trigger | Received ready federation message from {source}\")\n        if self.config.participant[\"device_args\"][\"start\"]:\n            logging.info(f\"\ud83d\udcdd  handle_federation_message | Trigger | Adding ready connection {source}\")\n            await self.cm.add_ready_connection(source)\n\n    async def _federation_federation_start_callback(self, source, message):\n        logging.info(f\"\ud83d\udcdd  handle_federation_message | Trigger | Received start federation message from {source}\")\n        await self.create_trainer_module()\n\n    async def _reputation_share_callback(self, source, message):\n        try:\n            logging.info(f\"handle_reputation_message | Trigger | Received reputation message from {source} | Node: {message.node_id} | Score: {message.score} | Round: {message.round}\")\n\n            current_node = self.addr\n            nei = message.node_id\n\n            # Manage reputation\n            if current_node != nei:\n                key = (current_node, nei, message.round)\n\n                if key not in self._reputation.reputation_with_all_feedback:\n                    self._reputation.reputation_with_all_feedback[key] = []\n\n                self._reputation.reputation_with_all_feedback[key].append(message.score)\n                #logging.info(f\"Reputation with all feedback: {self.reputation_with_all_feedback}\")\n\n        except Exception as e:\n            logging.exception(f\"Error handling reputation message: {e}\")\n\n    async def _federation_federation_models_included_callback(self, source, message):\n        logging.info(f\"\ud83d\udcdd  handle_federation_message | Trigger | Received aggregation finished message from {source}\")\n        try:\n            await self.cm.get_connections_lock().acquire_async()\n            if self.round is not None and source in self.cm.connections:\n                try:\n                    if message is not None and len(message.arguments) &gt; 0:\n                        self.cm.connections[source].update_round(int(message.arguments[0])) if message.round in [\n                            self.round - 1,\n                            self.round,\n                        ] else None\n                except Exception as e:\n                    logging.exception(f\"Error updating round in connection: {e}\")\n            else:\n                logging.error(f\"Connection not found for {source}\")\n        except Exception as e:\n            logging.exception(f\"Error updating round in connection: {e}\")\n        finally:\n            await self.cm.get_connections_lock().release_async()\n\n    \"\"\"\n    ##############################\n    #    REGISTERING CALLBACKS   #\n    ##############################\n    \"\"\"\n\n    async def register_events_callbacks(self):\n        await self.init_message_callbacks()\n        await EventManager.get_instance().subscribe_node_event(AggregationEvent, self.broadcast_models_include)\n\n    async def init_message_callbacks(self):\n        logging.info(\"Registering callbacks for MessageEvents...\")\n        await self.register_message_events_callbacks()\n        # Additional callbacks not registered automatically\n        await self.register_message_callback((\"model\", \"initialization\"), \"model_initialization_callback\")\n        await self.register_message_callback((\"model\", \"update\"), \"model_update_callback\")\n\n    async def register_message_events_callbacks(self):\n        me_dict = self.cm.get_messages_events()\n        message_events = [\n            (message_name, message_action)\n            for (message_name, message_actions) in me_dict.items()\n            for message_action in message_actions\n        ]\n        # logging.info(f\"{message_events}\")\n        for event_type, action in message_events:\n            callback_name = f\"_{event_type}_{action}_callback\"\n            # logging.info(f\"Searching callback named: {callback_name}\")\n            method = getattr(self, callback_name, None)\n\n            if callable(method):\n                await EventManager.get_instance().subscribe((event_type, action), method)\n\n    async def register_message_callback(self, message_event: tuple[str, str], callback: str):\n        event_type, action = message_event\n        method = getattr(self, callback, None)\n        if callable(method):\n            await EventManager.get_instance().subscribe((event_type, action), method)\n\n    \"\"\"\n    ##############################\n    #    ENGINE FUNCTIONALITY    #\n    ##############################\n    \"\"\"\n\n    async def update_neighbors(self, removed_neighbor_addr, neighbors, remove=False):\n        self.federation_nodes = neighbors\n        updt_nei_event = UpdateNeighborEvent(removed_neighbor_addr, remove)\n        asyncio.create_task(EventManager.get_instance().publish_node_event(updt_nei_event))\n\n    async def broadcast_models_include(self, aggregation_event: AggregationEvent):\n        logging.info(f\"\ud83d\udd04  Broadcasting MODELS_INCLUDED for round {self.get_round()}\")\n        message = self.cm.create_message(\n            \"federation\", \"federation_models_included\", [str(arg) for arg in [self.get_round()]]\n        )\n        asyncio.create_task(self.cm.send_message_to_neighbors(message))\n\n    async def create_trainer_module(self):\n        asyncio.create_task(self._start_learning())\n        logging.info(\"Started trainer module...\")\n\n    async def start_communications(self):\n        await self.register_events_callbacks()\n        await self.aggregator.init()\n        logging.info(f\"Neighbors: {self.config.participant['network_args']['neighbors']}\")\n        logging.info(\n            f\"\ud83d\udca4  Cold start time: {self.config.participant['misc_args']['grace_time_connection']} seconds before connecting to the network\"\n        )\n        await asyncio.sleep(self.config.participant[\"misc_args\"][\"grace_time_connection\"])\n        await self.cm.start()\n        initial_neighbors = self.config.participant[\"network_args\"][\"neighbors\"].split()\n        for i in initial_neighbors:\n            addr = f\"{i.split(':')[0]}:{i.split(':')[1]}\"\n            await self.cm.connect(addr, direct=True)\n            await asyncio.sleep(1)\n        while not self.cm.verify_connections(initial_neighbors):\n            await asyncio.sleep(1)\n        current_connections = await self.cm.get_addrs_current_connections()\n        logging.info(f\"Connections verified: {current_connections}\")\n        await self._reporter.start()\n        await self.cm.deploy_additional_services()\n        await self._addon_manager.deploy_additional_services()\n        await self._reputation.setup()\n        await asyncio.sleep(self.config.participant[\"misc_args\"][\"grace_time_connection\"] // 2)\n\n    async def deploy_federation(self):\n        await self.federation_ready_lock.acquire_async()\n        if self.config.participant[\"device_args\"][\"start\"]:\n            logging.info(\n                f\"\ud83d\udca4  Waiting for {self.config.participant['misc_args']['grace_time_start_federation']} seconds to start the federation\"\n            )\n            await asyncio.sleep(self.config.participant[\"misc_args\"][\"grace_time_start_federation\"])\n            if self.round is None:\n                while not await self.cm.check_federation_ready():\n                    await asyncio.sleep(1)\n                logging.info(\"Sending FEDERATION_START to neighbors...\")\n                message = self.cm.create_message(\"federation\", \"federation_start\")\n                await self.cm.send_message_to_neighbors(message)\n                await self.get_federation_ready_lock().release_async()\n                await self.create_trainer_module()\n                self.set_initialization_status(True)\n            else:\n                logging.info(\"Federation already started\")\n\n        else:\n            logging.info(\"Sending FEDERATION_READY to neighbors...\")\n            message = self.cm.create_message(\"federation\", \"federation_ready\")\n            await self.cm.send_message_to_neighbors(message)\n            logging.info(\"\ud83d\udca4  Waiting until receiving the start signal from the start node\")\n\n    async def _start_learning(self):\n        await self.learning_cycle_lock.acquire_async()\n        try:\n            if self.round is None:\n                self.total_rounds = self.config.participant[\"scenario_args\"][\"rounds\"]\n                epochs = self.config.participant[\"training_args\"][\"epochs\"]\n                await self.get_round_lock().acquire_async()\n                self.round = 0\n                await self.get_round_lock().release_async()\n                await self.learning_cycle_lock.release_async()\n                print_msg_box(\n                    msg=\"Starting Federated Learning process...\",\n                    indent=2,\n                    title=\"Start of the experiment\",\n                )\n                direct_connections = await self.cm.get_addrs_current_connections(only_direct=True)\n                undirected_connections = await self.cm.get_addrs_current_connections(only_undirected=True)\n                logging.info(\n                    f\"Initial DIRECT connections: {direct_connections} | Initial UNDIRECT participants: {undirected_connections}\"\n                )\n                logging.info(\"\ud83d\udca4  Waiting initialization of the federation...\")\n                # Lock to wait for the federation to be ready (only affects the first round, when the learning starts)\n                # Only applies to non-start nodes --&gt; start node does not wait for the federation to be ready\n                await self.get_federation_ready_lock().acquire_async()\n                if self.config.participant[\"device_args\"][\"start\"]:\n                    logging.info(\"Propagate initial model updates.\")\n                    await self.cm.propagator.propagate(\"initialization\")\n                    await self.get_federation_ready_lock().release_async()\n\n                self.trainer.set_epochs(epochs)\n                self.trainer.create_trainer()\n\n                await self._learning_cycle()\n            else:\n                if await self.learning_cycle_lock.locked_async():\n                    await self.learning_cycle_lock.release_async()\n        finally:\n            if await self.learning_cycle_lock.locked_async():\n                await self.learning_cycle_lock.release_async()\n\n    async def _disrupt_connection_using_reputation(self, malicious_nodes):\n        malicious_nodes = list(set(malicious_nodes) &amp; set(self.get_current_connections()))\n        logging.info(f\"Disrupting connection with malicious nodes at round {self.round}\")\n        logging.info(f\"Removing {malicious_nodes} from {self.get_current_connections()}\")\n        logging.info(f\"Current connections before aggregation at round {self.round}: {self.get_current_connections()}\")\n        for malicious_node in malicious_nodes:\n            if (self.get_name() != malicious_node) and (malicious_node not in self._secure_neighbors):\n                await self.cm.disconnect(malicious_node)\n        logging.info(f\"Current connections after aggregation at round {self.round}: {self.get_current_connections()}\")\n\n        await self._connect_with_benign(malicious_nodes)\n\n    async def _connect_with_benign(self, malicious_nodes):\n        lower_threshold = 1\n        higher_threshold = len(self.federation_nodes) - 1\n        if higher_threshold &lt; lower_threshold:\n            higher_threshold = lower_threshold\n\n        benign_nodes = [i for i in self.federation_nodes if i not in malicious_nodes]\n        logging.info(f\"_reputation_callback benign_nodes at round {self.round}: {benign_nodes}\")\n        if len(self.get_current_connections()) &lt;= lower_threshold:\n            for node in benign_nodes:\n                if len(self.get_current_connections()) &lt;= higher_threshold and self.get_name() != node:\n                    connected = await self.cm.connect(node)\n                    if connected:\n                        logging.info(f\"Connect new connection with at round {self.round}: {connected}\")\n\n    async def _dynamic_aggregator(self, aggregated_models_weights, malicious_nodes):\n        logging.info(f\"malicious detected at round {self.round}, change aggergation protocol!\")\n        if self.aggregator != self.target_aggregation:\n            logging.info(f\"Current aggregator is: {self.aggregator}\")\n            self.aggregator = self.target_aggregation\n            await self.aggregator.update_federation_nodes(self.federation_nodes)\n\n            for subnodes in aggregated_models_weights:\n                sublist = subnodes.split()\n                (submodel, weights) = aggregated_models_weights[subnodes]\n                for node in sublist:\n                    if node not in malicious_nodes:\n                        await self.aggregator.include_model_in_buffer(\n                            submodel, weights, source=self.get_name(), round=self.round\n                        )\n            logging.info(f\"Current aggregator is: {self.aggregator}\")\n\n    async def _waiting_model_updates(self):\n        logging.info(f\"\ud83d\udca4  Waiting convergence in round {self.round}.\")\n        params = await self.aggregator.get_aggregation()\n        if params is not None:\n            logging.info(\n                f\"_waiting_model_updates | Aggregation done for round {self.round}, including parameters in local model.\"\n            )\n            self.trainer.set_model_parameters(params)\n        else:\n            logging.error(\"Aggregation finished with no parameters\")\n\n    def learning_cycle_finished(self):\n        return not (self.round &lt; self.total_rounds)\n\n    async def _learning_cycle(self):\n        while self.round is not None and self.round &lt; self.total_rounds:\n            current_time = time.time()\n            print_msg_box(\n                msg=f\"Round {self.round} of {self.total_rounds - 1} started (max. {self.total_rounds} rounds)\",\n                indent=2,\n                title=\"Round information\",\n            )\n            logging.info(f\"Federation nodes: {self.federation_nodes}\")\n            self.federation_nodes = await self.cm.get_addrs_current_connections(only_direct=True, myself=True)\n            expected_nodes = self.federation_nodes.copy()\n            rse = RoundStartEvent(self.round, current_time, expected_nodes)\n            await EventManager.get_instance().publish_node_event(rse)\n            self.trainer.on_round_start()   \n            logging.info(f\"Expected nodes: {expected_nodes}\")\n            direct_connections = await self.cm.get_addrs_current_connections(only_direct=True)\n            undirected_connections = await self.cm.get_addrs_current_connections(only_undirected=True)\n            logging.info(f\"Direct connections: {direct_connections} | Undirected connections: {undirected_connections}\")\n            logging.info(f\"[Role {self.role}] Starting learning cycle...\")\n            await self.aggregator.update_federation_nodes(expected_nodes)\n            await self._extended_learning_cycle()\n            await self.get_round_lock().acquire_async()\n\n            print_msg_box(\n                msg=f\"Round {self.round} of {self.total_rounds - 1} finished (max. {self.total_rounds} rounds)\",\n                indent=2,\n                title=\"Round information\",\n            )\n            # await self.aggregator.reset()\n            self.trainer.on_round_end()\n            self.round += 1\n            self.config.participant[\"federation_args\"][\"round\"] = (\n                self.round\n            )  # Set current round in config (send to the controller)\n            await self.get_round_lock().release_async()\n\n        # End of the learning cycle\n        self.trainer.on_learning_cycle_end()\n        await self.trainer.test()\n        print_msg_box(\n            msg=f\"FL process has been completed successfully (max. {self.total_rounds} rounds reached)\",\n            indent=2,\n            title=\"End of the experiment\",\n        )\n        # Report\n        if self.config.participant[\"scenario_args\"][\"controller\"] != \"nebula-test\":\n            result = await self.reporter.report_scenario_finished()\n            if result:\n                pass\n            else:\n                logging.error(\"Error reporting scenario finished\")\n\n        logging.info(\"Checking if all my connections reached the total rounds...\")\n        while not self.cm.check_finished_experiment():\n            await asyncio.sleep(1)\n\n        await asyncio.sleep(5)\n\n        # Kill itself\n        if self.config.participant[\"scenario_args\"][\"deployment\"] == \"docker\":\n            try:\n                self.client.containers.get(self.docker_id).stop()\n            except Exception as e:\n                print(f\"Error stopping Docker container with ID {self.docker_id}: {e}\")\n\n    async def _extended_learning_cycle(self):\n        \"\"\"\n        This method is called in each round of the learning cycle. It is used to extend the learning cycle with additional\n        functionalities. The method is called in the _learning_cycle method.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/core/eventmanager/","title":"Documentation for Eventmanager Module","text":""},{"location":"api/core/eventmanager/#nebula.core.eventmanager.EventManager","title":"<code>EventManager</code>","text":"Source code in <code>nebula/core/eventmanager.py</code> <pre><code>class EventManager:\n    _instance = None\n    _lock = Locker(\"event_manager\")  # To avoid race conditions in multithreaded environments\n\n    def __new__(cls, *args, **kwargs):\n        \"\"\"Implementation of the Singleton pattern.\"\"\"\n        with cls._lock:\n            if cls._instance is None:\n                cls._instance = super().__new__(cls)\n                cls._instance._initialize(*args, **kwargs)\n        return cls._instance\n\n    def _initialize(self, verbose=False):\n        \"\"\"Initializes the single instance (runs only once).\"\"\"\n        if hasattr(self, \"_initialized\"):  # Prevents resetting\n            return\n        self._subscribers: dict[tuple[str, str], list] = {}\n        self._message_events_lock = Locker(\"message_events_lock\", async_lock=True)\n        self._addons_events_subs: dict[type, list] = {}\n        self._addons_event_lock = Locker(\"addons_event_lock\", async_lock=True)\n        self._node_events_subs: dict[type, list] = {}\n        self._node_events_lock = Locker(\"node_events_lock\", async_lock=True)\n        self._verbose = verbose\n        self._initialized = True  # Mark already initialized\n\n    @staticmethod\n    def get_instance(verbose=False):\n        \"\"\"Static method to get the unique instance.\"\"\"\n        if EventManager._instance is None:\n            EventManager(verbose=verbose)\n        return EventManager._instance\n\n    async def subscribe(self, event_type: tuple[str, str], callback: callable):\n        \"\"\"Register a callback for a specific event type.\"\"\"\n        async with self._message_events_lock:\n            if event_type not in self._subscribers:\n                self._subscribers[event_type] = []\n            self._subscribers[event_type].append(callback)\n        logging.info(f\"EventManager | Subscribed callback for event: {event_type}\")\n\n    async def publish(self, message_event: MessageEvent):\n        \"\"\"Trigger all callbacks registered for a specific event type.\"\"\"\n        if self._verbose:\n            logging.info(f\"Publishing MessageEvent: {message_event.message_type}\")\n        async with self._message_events_lock:\n            event_type = message_event.message_type\n            callbacks = self._subscribers.get(event_type, [])\n        if not callbacks:\n            logging.error(f\"EventManager | No subscribers for event: {event_type}\")\n            return\n\n        for callback in self._subscribers[event_type]:\n            try:\n                if self._verbose:\n                    logging.info(\n                        f\"EventManager | Triggering callback for event: {event_type}, from source: {message_event.source}\"\n                    )\n                if asyncio.iscoroutinefunction(callback) or inspect.iscoroutine(callback):\n                    await callback(message_event.source, message_event.message)\n                else:\n                    callback(message_event.source, message_event.message)\n            except Exception as e:\n                logging.exception(f\"EventManager | Error in callback for event {event_type}: {e}\")\n\n    async def subscribe_addonevent(self, addonEventType: type[AddonEvent], callback: callable):\n        \"\"\"Register a callback for a specific type of AddonEvent.\"\"\"\n        async with self._addons_event_lock:\n            if addonEventType not in self._addons_events_subs:\n                self._addons_events_subs[addonEventType] = []\n            self._addons_events_subs[addonEventType].append(callback)\n        logging.info(f\"EventManager | Subscribed callback for AddonEvent type: {addonEventType.__name__}\")\n\n    async def publish_addonevent(self, addonevent: AddonEvent):\n        \"\"\"Trigger all callbacks registered for a specific type of AddonEvent.\"\"\"\n        if self._verbose:\n            logging.info(f\"Publishing AddonEvent: {addonevent}\")\n        async with self._addons_event_lock:\n            event_type = type(addonevent)\n            callbacks = self._addons_events_subs.get(event_type, [])\n\n        if not callbacks:\n            logging.error(f\"EventManager | No subscribers for AddonEvent type: {event_type.__name__}\")\n            return\n\n        for callback in self._addons_events_subs[event_type]:\n            try:\n                if self._verbose:\n                    logging.info(f\"EventManager | Triggering callback for event type: {event_type.__name__}\")\n                if asyncio.iscoroutinefunction(callback) or inspect.iscoroutine(callback):\n                    await callback(addonevent)\n                else:\n                    callback(addonevent)\n            except Exception as e:\n                logging.exception(f\"EventManager | Error in callback for AddonEvent {event_type.__name__}: {e}\")\n\n    async def subscribe_node_event(self, nodeEventType: type[NodeEvent], callback: callable):\n        \"\"\"Register a callback for a specific type of AddonEvent.\"\"\"\n        async with self._node_events_lock:\n            if nodeEventType not in self._node_events_subs:\n                self._node_events_subs[nodeEventType] = []\n            self._node_events_subs[nodeEventType].append(callback)\n        logging.info(f\"EventManager | Subscribed callback for NodeEvent type: {nodeEventType.__name__}\")\n\n    async def publish_node_event(self, nodeevent: NodeEvent):\n        \"\"\"Trigger all callbacks registered for a specific type of AddonEvent.\"\"\"\n        if self._verbose:\n            logging.info(f\"Publishing NodeEvent: {nodeevent}\")\n        async with self._node_events_lock:\n            event_type = type(nodeevent)\n            callbacks = self._node_events_subs.get(event_type, [])  # Extraer la lista de callbacks\n\n        if not callbacks:\n            if self._verbose:\n                logging.error(f\"EventManager | No subscribers for NodeEvent type: {event_type.__name__}\")\n            return\n\n        for callback in self._node_events_subs[event_type]:\n            try:\n                if self._verbose:\n                    logging.info(f\"EventManager | Triggering callback for event type: {event_type.__name__}\")\n                if asyncio.iscoroutinefunction(callback) or inspect.iscoroutine(callback):\n                    if await nodeevent.is_concurrent():\n                        asyncio.create_task(callback(nodeevent))\n                    else:\n                        await callback(nodeevent)\n                else:\n                    callback(nodeevent)\n            except Exception as e:\n                logging.exception(f\"EventManager | Error in callback for NodeEvent {event_type.__name__}: {e}\")\n\n    async def unsubscribe_event(self, event_type, callback):\n        \"\"\"Unsubscribe a callback from a given event type (MessageEvent, AddonEvent, or NodeEvent).\"\"\"\n        if isinstance(event_type, tuple):  # MessageEvent\n            async with self._message_events_lock:\n                if event_type in self._subscribers and callback in self._subscribers[event_type]:\n                    self._subscribers[event_type].remove(callback)\n                    logging.info(f\"EventManager | Unsubscribed callback for MessageEvent: {event_type}\")\n        elif issubclass(event_type, AddonEvent):  # AddonEvent\n            async with self._addons_event_lock:\n                if event_type in self._addons_events_subs and callback in self._addons_events_subs[event_type]:\n                    self._addons_events_subs[event_type].remove(callback)\n                    logging.info(f\"EventManager | Unsubscribed callback for AddonEvent: {event_type.__name__}\")\n        elif issubclass(event_type, NodeEvent):  # NodeEvent\n            async with self._node_events_lock:\n                if event_type in self._node_events_subs and callback in self._node_events_subs[event_type]:\n                    self._node_events_subs[event_type].remove(callback)\n                    logging.info(f\"EventManager | Unsubscribed callback for NodeEvent: {event_type.__name__}\")\n</code></pre>"},{"location":"api/core/eventmanager/#nebula.core.eventmanager.EventManager.__new__","title":"<code>__new__(*args, **kwargs)</code>","text":"<p>Implementation of the Singleton pattern.</p> Source code in <code>nebula/core/eventmanager.py</code> <pre><code>def __new__(cls, *args, **kwargs):\n    \"\"\"Implementation of the Singleton pattern.\"\"\"\n    with cls._lock:\n        if cls._instance is None:\n            cls._instance = super().__new__(cls)\n            cls._instance._initialize(*args, **kwargs)\n    return cls._instance\n</code></pre>"},{"location":"api/core/eventmanager/#nebula.core.eventmanager.EventManager.get_instance","title":"<code>get_instance(verbose=False)</code>  <code>staticmethod</code>","text":"<p>Static method to get the unique instance.</p> Source code in <code>nebula/core/eventmanager.py</code> <pre><code>@staticmethod\ndef get_instance(verbose=False):\n    \"\"\"Static method to get the unique instance.\"\"\"\n    if EventManager._instance is None:\n        EventManager(verbose=verbose)\n    return EventManager._instance\n</code></pre>"},{"location":"api/core/eventmanager/#nebula.core.eventmanager.EventManager.publish","title":"<code>publish(message_event)</code>  <code>async</code>","text":"<p>Trigger all callbacks registered for a specific event type.</p> Source code in <code>nebula/core/eventmanager.py</code> <pre><code>async def publish(self, message_event: MessageEvent):\n    \"\"\"Trigger all callbacks registered for a specific event type.\"\"\"\n    if self._verbose:\n        logging.info(f\"Publishing MessageEvent: {message_event.message_type}\")\n    async with self._message_events_lock:\n        event_type = message_event.message_type\n        callbacks = self._subscribers.get(event_type, [])\n    if not callbacks:\n        logging.error(f\"EventManager | No subscribers for event: {event_type}\")\n        return\n\n    for callback in self._subscribers[event_type]:\n        try:\n            if self._verbose:\n                logging.info(\n                    f\"EventManager | Triggering callback for event: {event_type}, from source: {message_event.source}\"\n                )\n            if asyncio.iscoroutinefunction(callback) or inspect.iscoroutine(callback):\n                await callback(message_event.source, message_event.message)\n            else:\n                callback(message_event.source, message_event.message)\n        except Exception as e:\n            logging.exception(f\"EventManager | Error in callback for event {event_type}: {e}\")\n</code></pre>"},{"location":"api/core/eventmanager/#nebula.core.eventmanager.EventManager.publish_addonevent","title":"<code>publish_addonevent(addonevent)</code>  <code>async</code>","text":"<p>Trigger all callbacks registered for a specific type of AddonEvent.</p> Source code in <code>nebula/core/eventmanager.py</code> <pre><code>async def publish_addonevent(self, addonevent: AddonEvent):\n    \"\"\"Trigger all callbacks registered for a specific type of AddonEvent.\"\"\"\n    if self._verbose:\n        logging.info(f\"Publishing AddonEvent: {addonevent}\")\n    async with self._addons_event_lock:\n        event_type = type(addonevent)\n        callbacks = self._addons_events_subs.get(event_type, [])\n\n    if not callbacks:\n        logging.error(f\"EventManager | No subscribers for AddonEvent type: {event_type.__name__}\")\n        return\n\n    for callback in self._addons_events_subs[event_type]:\n        try:\n            if self._verbose:\n                logging.info(f\"EventManager | Triggering callback for event type: {event_type.__name__}\")\n            if asyncio.iscoroutinefunction(callback) or inspect.iscoroutine(callback):\n                await callback(addonevent)\n            else:\n                callback(addonevent)\n        except Exception as e:\n            logging.exception(f\"EventManager | Error in callback for AddonEvent {event_type.__name__}: {e}\")\n</code></pre>"},{"location":"api/core/eventmanager/#nebula.core.eventmanager.EventManager.publish_node_event","title":"<code>publish_node_event(nodeevent)</code>  <code>async</code>","text":"<p>Trigger all callbacks registered for a specific type of AddonEvent.</p> Source code in <code>nebula/core/eventmanager.py</code> <pre><code>async def publish_node_event(self, nodeevent: NodeEvent):\n    \"\"\"Trigger all callbacks registered for a specific type of AddonEvent.\"\"\"\n    if self._verbose:\n        logging.info(f\"Publishing NodeEvent: {nodeevent}\")\n    async with self._node_events_lock:\n        event_type = type(nodeevent)\n        callbacks = self._node_events_subs.get(event_type, [])  # Extraer la lista de callbacks\n\n    if not callbacks:\n        if self._verbose:\n            logging.error(f\"EventManager | No subscribers for NodeEvent type: {event_type.__name__}\")\n        return\n\n    for callback in self._node_events_subs[event_type]:\n        try:\n            if self._verbose:\n                logging.info(f\"EventManager | Triggering callback for event type: {event_type.__name__}\")\n            if asyncio.iscoroutinefunction(callback) or inspect.iscoroutine(callback):\n                if await nodeevent.is_concurrent():\n                    asyncio.create_task(callback(nodeevent))\n                else:\n                    await callback(nodeevent)\n            else:\n                callback(nodeevent)\n        except Exception as e:\n            logging.exception(f\"EventManager | Error in callback for NodeEvent {event_type.__name__}: {e}\")\n</code></pre>"},{"location":"api/core/eventmanager/#nebula.core.eventmanager.EventManager.subscribe","title":"<code>subscribe(event_type, callback)</code>  <code>async</code>","text":"<p>Register a callback for a specific event type.</p> Source code in <code>nebula/core/eventmanager.py</code> <pre><code>async def subscribe(self, event_type: tuple[str, str], callback: callable):\n    \"\"\"Register a callback for a specific event type.\"\"\"\n    async with self._message_events_lock:\n        if event_type not in self._subscribers:\n            self._subscribers[event_type] = []\n        self._subscribers[event_type].append(callback)\n    logging.info(f\"EventManager | Subscribed callback for event: {event_type}\")\n</code></pre>"},{"location":"api/core/eventmanager/#nebula.core.eventmanager.EventManager.subscribe_addonevent","title":"<code>subscribe_addonevent(addonEventType, callback)</code>  <code>async</code>","text":"<p>Register a callback for a specific type of AddonEvent.</p> Source code in <code>nebula/core/eventmanager.py</code> <pre><code>async def subscribe_addonevent(self, addonEventType: type[AddonEvent], callback: callable):\n    \"\"\"Register a callback for a specific type of AddonEvent.\"\"\"\n    async with self._addons_event_lock:\n        if addonEventType not in self._addons_events_subs:\n            self._addons_events_subs[addonEventType] = []\n        self._addons_events_subs[addonEventType].append(callback)\n    logging.info(f\"EventManager | Subscribed callback for AddonEvent type: {addonEventType.__name__}\")\n</code></pre>"},{"location":"api/core/eventmanager/#nebula.core.eventmanager.EventManager.subscribe_node_event","title":"<code>subscribe_node_event(nodeEventType, callback)</code>  <code>async</code>","text":"<p>Register a callback for a specific type of AddonEvent.</p> Source code in <code>nebula/core/eventmanager.py</code> <pre><code>async def subscribe_node_event(self, nodeEventType: type[NodeEvent], callback: callable):\n    \"\"\"Register a callback for a specific type of AddonEvent.\"\"\"\n    async with self._node_events_lock:\n        if nodeEventType not in self._node_events_subs:\n            self._node_events_subs[nodeEventType] = []\n        self._node_events_subs[nodeEventType].append(callback)\n    logging.info(f\"EventManager | Subscribed callback for NodeEvent type: {nodeEventType.__name__}\")\n</code></pre>"},{"location":"api/core/eventmanager/#nebula.core.eventmanager.EventManager.unsubscribe_event","title":"<code>unsubscribe_event(event_type, callback)</code>  <code>async</code>","text":"<p>Unsubscribe a callback from a given event type (MessageEvent, AddonEvent, or NodeEvent).</p> Source code in <code>nebula/core/eventmanager.py</code> <pre><code>async def unsubscribe_event(self, event_type, callback):\n    \"\"\"Unsubscribe a callback from a given event type (MessageEvent, AddonEvent, or NodeEvent).\"\"\"\n    if isinstance(event_type, tuple):  # MessageEvent\n        async with self._message_events_lock:\n            if event_type in self._subscribers and callback in self._subscribers[event_type]:\n                self._subscribers[event_type].remove(callback)\n                logging.info(f\"EventManager | Unsubscribed callback for MessageEvent: {event_type}\")\n    elif issubclass(event_type, AddonEvent):  # AddonEvent\n        async with self._addons_event_lock:\n            if event_type in self._addons_events_subs and callback in self._addons_events_subs[event_type]:\n                self._addons_events_subs[event_type].remove(callback)\n                logging.info(f\"EventManager | Unsubscribed callback for AddonEvent: {event_type.__name__}\")\n    elif issubclass(event_type, NodeEvent):  # NodeEvent\n        async with self._node_events_lock:\n            if event_type in self._node_events_subs and callback in self._node_events_subs[event_type]:\n                self._node_events_subs[event_type].remove(callback)\n                logging.info(f\"EventManager | Unsubscribed callback for NodeEvent: {event_type.__name__}\")\n</code></pre>"},{"location":"api/core/nebulaevents/","title":"Documentation for Nebulaevents Module","text":""},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.AggregationEvent","title":"<code>AggregationEvent</code>","text":"<p>               Bases: <code>NodeEvent</code></p> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>class AggregationEvent(NodeEvent):\n    def __init__(self, updates: dict, expected_nodes: set, missing_nodes: set):\n        \"\"\"Event triggered when model aggregation is ready.\n\n        Args:\n            updates (dict): Dictionary containing model updates.\n            expected_nodes (set): Set of nodes expected to participate in aggregation.\n            missing_nodes (set): Set of nodes that did not send their update.\n        \"\"\"\n        self._updates = updates\n        self._expected_nodes = expected_nodes\n        self._missing_nodes = missing_nodes\n\n    def __str__(self):\n        return \"Aggregation Ready\"\n\n    async def get_event_data(self) -&gt; tuple[dict, set, set]:\n        \"\"\"Retrieves the aggregation event data.\n\n        Returns:\n            tuple[dict, set, set]:\n                - updates (dict): Model updates.\n                - expected_nodes (set): Expected nodes.\n                - missing_nodes (set): Missing nodes.\n        \"\"\"\n        return (self._updates, self._expected_nodes, self._missing_nodes)\n\n    async def is_concurrent(self) -&gt; bool:\n        return False\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.AggregationEvent.__init__","title":"<code>__init__(updates, expected_nodes, missing_nodes)</code>","text":"<p>Event triggered when model aggregation is ready.</p> <p>Parameters:</p> Name Type Description Default <code>updates</code> <code>dict</code> <p>Dictionary containing model updates.</p> required <code>expected_nodes</code> <code>set</code> <p>Set of nodes expected to participate in aggregation.</p> required <code>missing_nodes</code> <code>set</code> <p>Set of nodes that did not send their update.</p> required Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>def __init__(self, updates: dict, expected_nodes: set, missing_nodes: set):\n    \"\"\"Event triggered when model aggregation is ready.\n\n    Args:\n        updates (dict): Dictionary containing model updates.\n        expected_nodes (set): Set of nodes expected to participate in aggregation.\n        missing_nodes (set): Set of nodes that did not send their update.\n    \"\"\"\n    self._updates = updates\n    self._expected_nodes = expected_nodes\n    self._missing_nodes = missing_nodes\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.AggregationEvent.get_event_data","title":"<code>get_event_data()</code>  <code>async</code>","text":"<p>Retrieves the aggregation event data.</p> <p>Returns:</p> Type Description <code>tuple[dict, set, set]</code> <p>tuple[dict, set, set]: - updates (dict): Model updates. - expected_nodes (set): Expected nodes. - missing_nodes (set): Missing nodes.</p> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>async def get_event_data(self) -&gt; tuple[dict, set, set]:\n    \"\"\"Retrieves the aggregation event data.\n\n    Returns:\n        tuple[dict, set, set]:\n            - updates (dict): Model updates.\n            - expected_nodes (set): Expected nodes.\n            - missing_nodes (set): Missing nodes.\n    \"\"\"\n    return (self._updates, self._expected_nodes, self._missing_nodes)\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.RoundStartEvent","title":"<code>RoundStartEvent</code>","text":"<p>               Bases: <code>NodeEvent</code></p> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>class RoundStartEvent(NodeEvent):\n    def __init__(self, round, start_time, expected_nodes):\n        \"\"\"Event triggered when round is going to start.\n\n        Args:\n            round (int): Round number.\n            start_time (time): Current time when round is going to start.\n            rejected_nodes (set): Set of nodes that were rejected in the previous round.\n        \"\"\"\n        self._round_start_time = start_time\n        self._round = round\n        self._expected_nodes = expected_nodes\n\n    def __str__(self):\n        return \"Round starting\"\n\n    async def get_event_data(self):\n        return (self._round, self._round_start_time, self._expected_nodes)\n\n    async def is_concurrent(self):\n        return False\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.RoundStartEvent.__init__","title":"<code>__init__(round, start_time, expected_nodes)</code>","text":"<p>Event triggered when round is going to start.</p> <p>Parameters:</p> Name Type Description Default <code>round</code> <code>int</code> <p>Round number.</p> required <code>start_time</code> <code>time</code> <p>Current time when round is going to start.</p> required <code>rejected_nodes</code> <code>set</code> <p>Set of nodes that were rejected in the previous round.</p> required Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>def __init__(self, round, start_time, expected_nodes):\n    \"\"\"Event triggered when round is going to start.\n\n    Args:\n        round (int): Round number.\n        start_time (time): Current time when round is going to start.\n        rejected_nodes (set): Set of nodes that were rejected in the previous round.\n    \"\"\"\n    self._round_start_time = start_time\n    self._round = round\n    self._expected_nodes = expected_nodes\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.UpdateNeighborEvent","title":"<code>UpdateNeighborEvent</code>","text":"<p>               Bases: <code>NodeEvent</code></p> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>class UpdateNeighborEvent(NodeEvent):\n    def __init__(self, node_addr, removed=False):\n        \"\"\"Event triggered when a neighboring node is updated.\n\n        Args:\n            node_addr (str): Address of the neighboring node.\n            removed (bool, optional): Indicates whether the node was removed.\n                                      Defaults to False.\n        \"\"\"\n        self._node_addr = node_addr\n        self._removed = removed\n\n    def __str__(self):\n        return f\"Node addr: {self._node_addr}, removed: {self._removed}\"\n\n    async def get_event_data(self) -&gt; tuple[str, bool]:\n        \"\"\"Retrieves the neighbor update event data.\n\n        Returns:\n            tuple[str, bool]:\n                - node_addr (str): Address of the neighboring node.\n                - removed (bool): Whether the node was removed.\n        \"\"\"\n        return (self._node_addr, self._removed)\n\n    async def is_concurrent(self) -&gt; bool:\n        return False\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.UpdateNeighborEvent.__init__","title":"<code>__init__(node_addr, removed=False)</code>","text":"<p>Event triggered when a neighboring node is updated.</p> <p>Parameters:</p> Name Type Description Default <code>node_addr</code> <code>str</code> <p>Address of the neighboring node.</p> required <code>removed</code> <code>bool</code> <p>Indicates whether the node was removed.                       Defaults to False.</p> <code>False</code> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>def __init__(self, node_addr, removed=False):\n    \"\"\"Event triggered when a neighboring node is updated.\n\n    Args:\n        node_addr (str): Address of the neighboring node.\n        removed (bool, optional): Indicates whether the node was removed.\n                                  Defaults to False.\n    \"\"\"\n    self._node_addr = node_addr\n    self._removed = removed\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.UpdateNeighborEvent.get_event_data","title":"<code>get_event_data()</code>  <code>async</code>","text":"<p>Retrieves the neighbor update event data.</p> <p>Returns:</p> Type Description <code>tuple[str, bool]</code> <p>tuple[str, bool]: - node_addr (str): Address of the neighboring node. - removed (bool): Whether the node was removed.</p> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>async def get_event_data(self) -&gt; tuple[str, bool]:\n    \"\"\"Retrieves the neighbor update event data.\n\n    Returns:\n        tuple[str, bool]:\n            - node_addr (str): Address of the neighboring node.\n            - removed (bool): Whether the node was removed.\n    \"\"\"\n    return (self._node_addr, self._removed)\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.UpdateReceivedEvent","title":"<code>UpdateReceivedEvent</code>","text":"<p>               Bases: <code>NodeEvent</code></p> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>class UpdateReceivedEvent(NodeEvent):\n    def __init__(self, decoded_model, weight, source, round, local=False):\n        \"\"\"\n        Initializes an UpdateReceivedEvent.\n\n        Args:\n            decoded_model (Any): The received model update.\n            weight (float): The weight associated with the received update.\n            source (str): The identifier or address of the node that sent the update.\n            round (int): The round number in which the update was received.\n            local (bool): Local update\n        \"\"\"\n        self._source = source\n        self._round = round\n        self._model = decoded_model\n        self._weight = weight\n        self._local = local\n\n    def __str__(self):\n        return f\"Update received from source: {self._source}, round: {self._round}\"\n\n    async def get_event_data(self) -&gt; tuple[str, bool]:\n        \"\"\"\n        Retrieves the event data.\n\n        Returns:\n            tuple[Any, float, str, int, bool]: A tuple containing:\n                - The received model update.\n                - The weight associated with the update.\n                - The source node identifier.\n                - The round number of the update.\n                - If the update is local\n        \"\"\"\n        return (self._model, self._weight, self._source, self._round, self._local)\n\n    async def is_concurrent(self) -&gt; bool:\n        return False\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.UpdateReceivedEvent.__init__","title":"<code>__init__(decoded_model, weight, source, round, local=False)</code>","text":"<p>Initializes an UpdateReceivedEvent.</p> <p>Parameters:</p> Name Type Description Default <code>decoded_model</code> <code>Any</code> <p>The received model update.</p> required <code>weight</code> <code>float</code> <p>The weight associated with the received update.</p> required <code>source</code> <code>str</code> <p>The identifier or address of the node that sent the update.</p> required <code>round</code> <code>int</code> <p>The round number in which the update was received.</p> required <code>local</code> <code>bool</code> <p>Local update</p> <code>False</code> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>def __init__(self, decoded_model, weight, source, round, local=False):\n    \"\"\"\n    Initializes an UpdateReceivedEvent.\n\n    Args:\n        decoded_model (Any): The received model update.\n        weight (float): The weight associated with the received update.\n        source (str): The identifier or address of the node that sent the update.\n        round (int): The round number in which the update was received.\n        local (bool): Local update\n    \"\"\"\n    self._source = source\n    self._round = round\n    self._model = decoded_model\n    self._weight = weight\n    self._local = local\n</code></pre>"},{"location":"api/core/nebulaevents/#nebula.core.nebulaevents.UpdateReceivedEvent.get_event_data","title":"<code>get_event_data()</code>  <code>async</code>","text":"<p>Retrieves the event data.</p> <p>Returns:</p> Type Description <code>tuple[str, bool]</code> <p>tuple[Any, float, str, int, bool]: A tuple containing: - The received model update. - The weight associated with the update. - The source node identifier. - The round number of the update. - If the update is local</p> Source code in <code>nebula/core/nebulaevents.py</code> <pre><code>async def get_event_data(self) -&gt; tuple[str, bool]:\n    \"\"\"\n    Retrieves the event data.\n\n    Returns:\n        tuple[Any, float, str, int, bool]: A tuple containing:\n            - The received model update.\n            - The weight associated with the update.\n            - The source node identifier.\n            - The round number of the update.\n            - If the update is local\n    \"\"\"\n    return (self._model, self._weight, self._source, self._round, self._local)\n</code></pre>"},{"location":"api/core/role/","title":"Documentation for Role Module","text":""},{"location":"api/core/role/#nebula.core.role.Role","title":"<code>Role</code>","text":"<p>This class defines the participant roles of the platform.</p> Source code in <code>nebula/core/role.py</code> <pre><code>class Role:\n    \"\"\"\n    This class defines the participant roles of the platform.\n    \"\"\"\n\n    TRAINER = \"trainer\"\n    AGGREGATOR = \"aggregator\"\n    PROXY = \"proxy\"\n    IDLE = \"idle\"\n    SERVER = \"server\"\n</code></pre>"},{"location":"api/core/aggregation/","title":"Documentation for Aggregation Module","text":""},{"location":"api/core/aggregation/aggregator/","title":"Documentation for Aggregator Module","text":""},{"location":"api/core/aggregation/blockchainReputation/","title":"Documentation for Blockchainreputation Module","text":""},{"location":"api/core/aggregation/blockchainReputation/#nebula.core.aggregation.blockchainReputation.BlockchainHandler","title":"<code>BlockchainHandler</code>","text":"<p>Handles interaction with Oracle and Non-Validator Node of Blockchain Network</p> Source code in <code>nebula/core/aggregation/blockchainReputation.py</code> <pre><code>class BlockchainHandler:\n    \"\"\"\n    Handles interaction with Oracle and Non-Validator Node of Blockchain Network\n    \"\"\"\n\n    # static ip address of non-validator node with RPC-API\n    __rpc_url = \"http://172.25.0.104:8545\"\n\n    # static ip address of oracle with REST-API\n    __oracle_url = \"http://172.25.0.105:8081\"\n\n    # default REST header for interacting with oracle\n    __rest_header = {\"Content-type\": \"application/json\", \"Accept\": \"application/json\"}\n\n    def __init__(self, home_address):\n        print_with_frame(\"BLOCKCHAIN INITIALIZATION: START\")\n\n        # local NEBULA name, needed for reputation system\n        self.__home_ip = home_address\n\n        # randomly generated private key, needed to sign transaction\n        self.__private_key = \"\"\n\n        # public wallet address generated from the private key\n        self.__acc_address = \"\"\n\n        # variables for experiment, not required for aggregation\n        self.__gas_used = 0\n        self.__gas_price = 27.3\n        self.round = 1\n\n        # generate randomized primary key\n        self.__acc = self.__create_account()\n\n        # configure web3 objects for using Proof-of-Authority\n        self.__web3 = self.__initialize_web3()\n\n        # call Oracle to sense if blockchain is ready\n        print(f\"{'-' * 25} CONNECT TO ORACLE {'-' * 25}\", flush=True)\n        self.__wait_for_blockchain()\n\n        # request ETH funds for creating transactions, paying gas\n        self.__request_funds_from_oracle()\n\n        # check if funds were assigned by checking directly with blockchain\n        self.verify_balance()\n\n        # request contract address and header from Oracle\n        self.__contract_obj = self.__get_contract_from_oracle()\n\n        # register public wallet address at reputation system\n        print(f\"{'-' * 25} CONNECT TO REPUTATION SYSTEM {'-' * 25}\", flush=True)\n        self.__register()\n        print(\"BLOCKCHAIN: Registered to reputation system\", flush=True)\n\n        # check if registration was successful\n        self.verify_registration()\n        print(\"BLOCKCHAIN: Verified registration\", flush=True)\n\n        print_with_frame(\"BLOCKCHAIN INITIALIZATION: FINISHED\")\n\n    @classmethod\n    @property\n    def oracle_url(cls) -&gt; str:\n        return cls.__oracle_url\n\n    @classmethod\n    @property\n    def rest_header(cls) -&gt; Mapping[str, str]:\n        return cls.__rest_header\n\n    def __create_account(self):\n        \"\"\"\n        Generates randomized primary key and derives public account from it\n        Returns: None\n\n        \"\"\"\n        print(f\"{'-' * 25} REGISTER WORKING NODE {'-' * 25}\", flush=True)\n\n        # generate random private key, address, public address\n        acc = Account.create()\n\n        # initialize web3 utility object\n        web3 = Web3()\n\n        # convert private key to hex, used in raw transactions\n        self.__private_key = web3.to_hex(acc.key)\n\n        # convert address type, used in raw transactions\n        self.__acc_address = web3.to_checksum_address(acc.address)\n\n        print(f\"WORKER NODE: Registered account: {self.__home_ip}\", flush=True)\n        print(f\"WORKER NODE: Account address: {self.__acc_address}\", flush=True)\n\n        # return generated account\n        return acc\n\n    def __initialize_web3(self):\n        \"\"\"\n        Initializes Web3 object and configures it for PoA protocol\n        Returns: Web3 object\n\n        \"\"\"\n\n        # initialize Web3 object with ip of non-validator node\n        web3 = Web3(Web3.HTTPProvider(self.__rpc_url, request_kwargs={\"timeout\": 20}))  # 10\n\n        # inject Proof-of-Authority settings to object\n        web3.middleware_onion.inject(geth_poa_middleware, layer=0)\n\n        # automatically sign transactions if available for execution\n        web3.middleware_onion.add(construct_sign_and_send_raw_middleware(self.__acc))\n\n        # inject local account as default\n        web3.eth.default_account = self.__acc_address\n\n        # return initialized object for executing transaction\n        return web3\n\n    @retry((Exception, requests.exceptions.HTTPError), tries=20, delay=4)\n    def __wait_for_blockchain(self) -&gt; None:\n        \"\"\"\n        Request state of blockchain from Oracle by periodic calls and sleep\n        Returns: None\n\n        \"\"\"\n\n        # check with oracle if blockchain is ready for requests\n        response = requests.get(\n            url=f\"{self.__oracle_url}/status\",\n            headers=self.__rest_header,\n            timeout=20,  # 10\n        )\n\n        # raise Exception if status is not successful\n        response.raise_for_status()\n\n        return print(\"ORACLE: Blockchain is ready\", flush=True)\n\n    @retry((Exception, requests.exceptions.HTTPError), tries=3, delay=4)\n    def __request_funds_from_oracle(self) -&gt; None:\n        \"\"\"\n        Requests funds from Oracle by sending public address\n        Returns: None\n\n        \"\"\"\n\n        # call oracle's faucet by Http post request\n        response = requests.post(\n            url=f\"{self.__oracle_url}/faucet\",\n            json={\"address\": self.__acc_address},\n            headers=self.__rest_header,\n            timeout=20,  # 10\n        )\n\n        # raise Exception if status is not successful\n        response.raise_for_status()\n\n        return print(\"ORACLE: Received 500 ETH\", flush=True)\n\n    def verify_balance(self) -&gt; None:\n        \"\"\"\n        Calls blockchain directly for requesting current balance\n        Returns: None\n\n        \"\"\"\n\n        # directly call view method from non-validator node\n        balance = self.__web3.eth.get_balance(self.__acc_address, \"latest\")\n\n        # convert wei to ether\n        balance_eth = self.__web3.from_wei(balance, \"ether\")\n        print(\n            f\"BLOCKCHAIN: Successfully verified balance of {balance_eth} ETH\",\n            flush=True,\n        )\n\n        # if node ran out of funds, it requests ether from the oracle\n        if balance_eth &lt;= 1:\n            self.__request_funds_from_oracle()\n\n        return None\n\n    @retry((Exception, requests.exceptions.HTTPError), tries=3, delay=4)\n    def __get_contract_from_oracle(self):\n        \"\"\"\n        Requests header file and contract address, generates Web3 Contract object with it\n        Returns: Web3 Contract object\n\n        \"\"\"\n\n        response = requests.get(\n            url=f\"{self.__oracle_url}/contract\",\n            headers=self.__rest_header,\n            timeout=20,  # 10\n        )\n\n        # raise Exception if status is not successful\n        response.raise_for_status()\n\n        # convert response to json to extract the abi and address\n        json_response = response.json()\n\n        print(\n            f\"ORACLE: Initialized chain code: {json_response.get('address')}\",\n            flush=True,\n        )\n\n        # return an initialized web3 contract object\n        return self.__web3.eth.contract(abi=json_response.get(\"abi\"), address=json_response.get(\"address\"))\n\n    @retry((Exception, requests.exceptions.HTTPError), tries=3, delay=4)\n    def report_gas_oracle(self) -&gt; list:\n        \"\"\"\n        Reports accumulated gas costs of all transactions made to the blockchain\n        Returns: List of all accumulated gas costs per registered node\n\n        \"\"\"\n\n        # method used for experiments, not needed for aggregation\n        response = requests.post(\n            url=f\"{self.__oracle_url}/gas\",\n            json={\"amount\": self.__gas_used, \"round\": self.round},\n            headers=self.__rest_header,\n            timeout=20,  # 10\n        )\n\n        # raise Exception if status is not successful\n        response.raise_for_status()\n\n        # reset local gas accumulation\n        self.__gas_used = 0\n\n        # return list with gas usage for logging\n        return list(response.json().items())\n\n    @retry((Exception, requests.exceptions.HTTPError), tries=3, delay=4)\n    def report_reputation_oracle(self, records: list) -&gt; None:\n        \"\"\"\n        Reports reputations used for aggregation\n        Returns: None\n\n        \"\"\"\n\n        # method used for experiments, not needed for aggregation\n        response = requests.post(\n            url=f\"{self.__oracle_url}/reputation\",\n            json={\"records\": records, \"round\": self.round, \"sender\": self.__home_ip},\n            headers=self.__rest_header,\n            timeout=20,  # 10\n        )\n\n        # raise Exception if status is not successful\n        response.raise_for_status()\n\n        return None\n\n    def __sign_and_deploy(self, trx_hash):\n        \"\"\"\n        Signs a function call to the chain code with the primary key and awaits the receipt\n        Args:\n            trx_hash: Transformed dictionary of all properties relevant for call to chain code\n\n        Returns: transaction receipt confirming the successful write to the ledger\n\n        \"\"\"\n\n        # transaction is signed with private key\n        signed_transaction = self.__web3.eth.account.sign_transaction(trx_hash, private_key=self.__private_key)\n\n        # confirmation that transaction was passed from non-validator node to validator nodes\n        executed_transaction = self.__web3.eth.send_raw_transaction(signed_transaction.rawTransaction)\n\n        # non-validator node awaited the successful validation by validation nodes and returns receipt\n        transaction_receipt = self.__web3.eth.wait_for_transaction_receipt(executed_transaction)\n\n        # accumulate used gas\n        self.__gas_used += transaction_receipt.gasUsed\n\n        return transaction_receipt\n\n    @retry(Exception, tries=3, delay=4)\n    def push_opinions(self, opinion_dict: dict):\n        \"\"\"\n        Pushes all locally computed opinions of models to aggregate to the reputation system\n        Args:\n            opinion_dict: Dict of all names:opinions for writing to the reputation system\n\n        Returns: Json of transaction receipt\n\n        \"\"\"\n\n        # create raw transaction object to call rate_neighbors() from the reputation system\n        unsigned_trx = self.__contract_obj.functions.rate_neighbours(list(opinion_dict.items())).build_transaction({\n            \"chainId\": self.__web3.eth.chain_id,\n            \"from\": self.__acc_address,\n            \"nonce\": self.__web3.eth.get_transaction_count(\n                self.__web3.to_checksum_address(self.__acc_address), \"pending\"\n            ),\n            \"gasPrice\": self.__web3.to_wei(self.__gas_price, \"gwei\"),\n        })\n\n        # sign and execute the transaction\n        conf = self.__sign_and_deploy(unsigned_trx)\n\n        self.report_reputation_oracle(list(opinion_dict.items()))\n        # return the receipt as json\n        return self.__web3.to_json(conf)\n\n    @retry(Exception, tries=3, delay=4)\n    def get_reputations(self, ip_addresses: list) -&gt; dict:\n        \"\"\"\n        Requests globally aggregated opinions values from reputation system for computing aggregation weights\n        Args:\n            ip_addresses: Names of nodes of which the reputation values should be generated\n\n        Returns: Dictionary of name:reputation from the reputation system\n\n        \"\"\"\n\n        final_reputations = dict()\n        stats_to_print = list()\n\n        # call get_reputations() from reputation system\n        raw_reputation = self.__contract_obj.functions.get_reputations(ip_addresses).call({\"from\": self.__acc_address})\n\n        # loop list with tuples from reputation system\n        for (\n            name,\n            reputation,\n            weighted_reputation,\n            stddev_count,\n            divisor,\n            final_reputation,\n            avg,\n            median,\n            stddev,\n            index,\n            avg_deviation,\n            avg_avg_deviation,\n            malicious_opinions,\n        ) in raw_reputation:\n            # list elements with an empty name can be ignored\n            if not name:\n                continue\n\n            # print statistical values\n            stats_to_print.append([\n                name,\n                reputation / 10,\n                weighted_reputation / 10,\n                stddev_count / 10,\n                divisor / 10,\n                final_reputation / 10,\n                avg / 10,\n                median / 10,\n                stddev / 10,\n                avg_deviation / 10,\n                avg_avg_deviation / 10,\n                malicious_opinions,\n            ])\n\n            # assign the final reputation to a dict for later aggregation\n            final_reputations[name] = final_reputation / 10\n\n        print_table(\n            \"REPUTATION SYSTEM STATE\",\n            stats_to_print,\n            [\n                \"Name\",\n                \"Reputation\",\n                \"Weighted Rep. by local Node\",\n                \"Stddev Count\",\n                \"Divisor\",\n                \"Final Reputation\",\n                \"Mean\",\n                \"Median\",\n                \"Stddev\",\n                \"Avg Deviation in Opinion\",\n                \"Avg of all Avg Deviations in Opinions\",\n                \"Malicious Opinions\",\n            ],\n        )\n\n        # if sum(final_reputations.values()):\n        #     self.report_reputation_oracle(list(final_reputations.items()))\n\n        return final_reputations\n\n    @retry(Exception, tries=3, delay=4)\n    def __register(self) -&gt; str:\n        \"\"\"\n        Registers a node's name with its public address, signed with private key\n        Returns: Json of transaction receipt\n\n        \"\"\"\n\n        # build raw transaction object to call public method register() from reputation system\n        unsigned_trx = self.__contract_obj.functions.register(self.__home_ip).build_transaction({\n            \"chainId\": self.__web3.eth.chain_id,\n            \"from\": self.__acc_address,\n            \"nonce\": self.__web3.eth.get_transaction_count(\n                self.__web3.to_checksum_address(self.__acc_address), \"pending\"\n            ),\n            \"gasPrice\": self.__web3.to_wei(self.__gas_price, \"gwei\"),\n        })\n\n        # sign and execute created transaction\n        conf = self.__sign_and_deploy(unsigned_trx)\n\n        # return the receipt as json\n        return self.__web3.to_json(conf)\n\n    @retry(Exception, tries=3, delay=4)\n    def verify_registration(self) -&gt; None:\n        \"\"\"\n        Verifies the successful registration of the node itself,\n        executes registration again if reputation system returns false\n        Returns: None\n\n        \"\"\"\n\n        # call view function of reputation system to check if registration was not abandoned by hard fork\n        confirmation = self.__contract_obj.functions.confirm_registration().call({\"from\": self.__acc_address})\n\n        # function returns boolean\n        if not confirmation:\n            # register again if not successful\n            self.__register()\n\n            # raise Exception to check again\n            raise Exception(\"EXCEPTION: _verify_registration() =&gt; Could not be confirmed)\")\n\n        return None\n\n    @retry((Exception, requests.exceptions.HTTPError), tries=3, delay=4)\n    def report_time_oracle(self, start: float) -&gt; None:\n        \"\"\"\n        Reports time used for aggregation\n        Returns: None\n\n        \"\"\"\n        # method used for experiments, not needed for aggregation\n        # report aggregation time and round to oracle\n        response = requests.post(\n            url=f\"{BlockchainHandler.oracle_url}/time\",\n            json={\"time\": (time.time_ns() - start) / (10**9), \"round\": self.round},\n            headers=self.__rest_header,\n            timeout=20,  # 10\n        )\n\n        # raise Exception if status is not successful\n        response.raise_for_status()\n\n        # increase aggregation round counter after reporting time\n        self.round += 1\n        return None\n</code></pre>"},{"location":"api/core/aggregation/blockchainReputation/#nebula.core.aggregation.blockchainReputation.BlockchainHandler.__create_account","title":"<code>__create_account()</code>","text":"<p>Generates randomized primary key and derives public account from it Returns: None</p> Source code in <code>nebula/core/aggregation/blockchainReputation.py</code> <pre><code>def __create_account(self):\n    \"\"\"\n    Generates randomized primary key and derives public account from it\n    Returns: None\n\n    \"\"\"\n    print(f\"{'-' * 25} REGISTER WORKING NODE {'-' * 25}\", flush=True)\n\n    # generate random private key, address, public address\n    acc = Account.create()\n\n    # initialize web3 utility object\n    web3 = Web3()\n\n    # convert private key to hex, used in raw transactions\n    self.__private_key = web3.to_hex(acc.key)\n\n    # convert address type, used in raw transactions\n    self.__acc_address = web3.to_checksum_address(acc.address)\n\n    print(f\"WORKER NODE: Registered account: {self.__home_ip}\", flush=True)\n    print(f\"WORKER NODE: Account address: {self.__acc_address}\", flush=True)\n\n    # return generated account\n    return acc\n</code></pre>"},{"location":"api/core/aggregation/blockchainReputation/#nebula.core.aggregation.blockchainReputation.BlockchainHandler.__get_contract_from_oracle","title":"<code>__get_contract_from_oracle()</code>","text":"<p>Requests header file and contract address, generates Web3 Contract object with it Returns: Web3 Contract object</p> Source code in <code>nebula/core/aggregation/blockchainReputation.py</code> <pre><code>@retry((Exception, requests.exceptions.HTTPError), tries=3, delay=4)\ndef __get_contract_from_oracle(self):\n    \"\"\"\n    Requests header file and contract address, generates Web3 Contract object with it\n    Returns: Web3 Contract object\n\n    \"\"\"\n\n    response = requests.get(\n        url=f\"{self.__oracle_url}/contract\",\n        headers=self.__rest_header,\n        timeout=20,  # 10\n    )\n\n    # raise Exception if status is not successful\n    response.raise_for_status()\n\n    # convert response to json to extract the abi and address\n    json_response = response.json()\n\n    print(\n        f\"ORACLE: Initialized chain code: {json_response.get('address')}\",\n        flush=True,\n    )\n\n    # return an initialized web3 contract object\n    return self.__web3.eth.contract(abi=json_response.get(\"abi\"), address=json_response.get(\"address\"))\n</code></pre>"},{"location":"api/core/aggregation/blockchainReputation/#nebula.core.aggregation.blockchainReputation.BlockchainHandler.__initialize_web3","title":"<code>__initialize_web3()</code>","text":"<p>Initializes Web3 object and configures it for PoA protocol Returns: Web3 object</p> Source code in <code>nebula/core/aggregation/blockchainReputation.py</code> <pre><code>def __initialize_web3(self):\n    \"\"\"\n    Initializes Web3 object and configures it for PoA protocol\n    Returns: Web3 object\n\n    \"\"\"\n\n    # initialize Web3 object with ip of non-validator node\n    web3 = Web3(Web3.HTTPProvider(self.__rpc_url, request_kwargs={\"timeout\": 20}))  # 10\n\n    # inject Proof-of-Authority settings to object\n    web3.middleware_onion.inject(geth_poa_middleware, layer=0)\n\n    # automatically sign transactions if available for execution\n    web3.middleware_onion.add(construct_sign_and_send_raw_middleware(self.__acc))\n\n    # inject local account as default\n    web3.eth.default_account = self.__acc_address\n\n    # return initialized object for executing transaction\n    return web3\n</code></pre>"},{"location":"api/core/aggregation/blockchainReputation/#nebula.core.aggregation.blockchainReputation.BlockchainHandler.__register","title":"<code>__register()</code>","text":"<p>Registers a node's name with its public address, signed with private key Returns: Json of transaction receipt</p> Source code in <code>nebula/core/aggregation/blockchainReputation.py</code> <pre><code>@retry(Exception, tries=3, delay=4)\ndef __register(self) -&gt; str:\n    \"\"\"\n    Registers a node's name with its public address, signed with private key\n    Returns: Json of transaction receipt\n\n    \"\"\"\n\n    # build raw transaction object to call public method register() from reputation system\n    unsigned_trx = self.__contract_obj.functions.register(self.__home_ip).build_transaction({\n        \"chainId\": self.__web3.eth.chain_id,\n        \"from\": self.__acc_address,\n        \"nonce\": self.__web3.eth.get_transaction_count(\n            self.__web3.to_checksum_address(self.__acc_address), \"pending\"\n        ),\n        \"gasPrice\": self.__web3.to_wei(self.__gas_price, \"gwei\"),\n    })\n\n    # sign and execute created transaction\n    conf = self.__sign_and_deploy(unsigned_trx)\n\n    # return the receipt as json\n    return self.__web3.to_json(conf)\n</code></pre>"},{"location":"api/core/aggregation/blockchainReputation/#nebula.core.aggregation.blockchainReputation.BlockchainHandler.__request_funds_from_oracle","title":"<code>__request_funds_from_oracle()</code>","text":"<p>Requests funds from Oracle by sending public address Returns: None</p> Source code in <code>nebula/core/aggregation/blockchainReputation.py</code> <pre><code>@retry((Exception, requests.exceptions.HTTPError), tries=3, delay=4)\ndef __request_funds_from_oracle(self) -&gt; None:\n    \"\"\"\n    Requests funds from Oracle by sending public address\n    Returns: None\n\n    \"\"\"\n\n    # call oracle's faucet by Http post request\n    response = requests.post(\n        url=f\"{self.__oracle_url}/faucet\",\n        json={\"address\": self.__acc_address},\n        headers=self.__rest_header,\n        timeout=20,  # 10\n    )\n\n    # raise Exception if status is not successful\n    response.raise_for_status()\n\n    return print(\"ORACLE: Received 500 ETH\", flush=True)\n</code></pre>"},{"location":"api/core/aggregation/blockchainReputation/#nebula.core.aggregation.blockchainReputation.BlockchainHandler.__sign_and_deploy","title":"<code>__sign_and_deploy(trx_hash)</code>","text":"<p>Signs a function call to the chain code with the primary key and awaits the receipt Args:     trx_hash: Transformed dictionary of all properties relevant for call to chain code</p> <p>Returns: transaction receipt confirming the successful write to the ledger</p> Source code in <code>nebula/core/aggregation/blockchainReputation.py</code> <pre><code>def __sign_and_deploy(self, trx_hash):\n    \"\"\"\n    Signs a function call to the chain code with the primary key and awaits the receipt\n    Args:\n        trx_hash: Transformed dictionary of all properties relevant for call to chain code\n\n    Returns: transaction receipt confirming the successful write to the ledger\n\n    \"\"\"\n\n    # transaction is signed with private key\n    signed_transaction = self.__web3.eth.account.sign_transaction(trx_hash, private_key=self.__private_key)\n\n    # confirmation that transaction was passed from non-validator node to validator nodes\n    executed_transaction = self.__web3.eth.send_raw_transaction(signed_transaction.rawTransaction)\n\n    # non-validator node awaited the successful validation by validation nodes and returns receipt\n    transaction_receipt = self.__web3.eth.wait_for_transaction_receipt(executed_transaction)\n\n    # accumulate used gas\n    self.__gas_used += transaction_receipt.gasUsed\n\n    return transaction_receipt\n</code></pre>"},{"location":"api/core/aggregation/blockchainReputation/#nebula.core.aggregation.blockchainReputation.BlockchainHandler.__wait_for_blockchain","title":"<code>__wait_for_blockchain()</code>","text":"<p>Request state of blockchain from Oracle by periodic calls and sleep Returns: None</p> Source code in <code>nebula/core/aggregation/blockchainReputation.py</code> <pre><code>@retry((Exception, requests.exceptions.HTTPError), tries=20, delay=4)\ndef __wait_for_blockchain(self) -&gt; None:\n    \"\"\"\n    Request state of blockchain from Oracle by periodic calls and sleep\n    Returns: None\n\n    \"\"\"\n\n    # check with oracle if blockchain is ready for requests\n    response = requests.get(\n        url=f\"{self.__oracle_url}/status\",\n        headers=self.__rest_header,\n        timeout=20,  # 10\n    )\n\n    # raise Exception if status is not successful\n    response.raise_for_status()\n\n    return print(\"ORACLE: Blockchain is ready\", flush=True)\n</code></pre>"},{"location":"api/core/aggregation/blockchainReputation/#nebula.core.aggregation.blockchainReputation.BlockchainHandler.get_reputations","title":"<code>get_reputations(ip_addresses)</code>","text":"<p>Requests globally aggregated opinions values from reputation system for computing aggregation weights Args:     ip_addresses: Names of nodes of which the reputation values should be generated</p> <p>Returns: Dictionary of name:reputation from the reputation system</p> Source code in <code>nebula/core/aggregation/blockchainReputation.py</code> <pre><code>@retry(Exception, tries=3, delay=4)\ndef get_reputations(self, ip_addresses: list) -&gt; dict:\n    \"\"\"\n    Requests globally aggregated opinions values from reputation system for computing aggregation weights\n    Args:\n        ip_addresses: Names of nodes of which the reputation values should be generated\n\n    Returns: Dictionary of name:reputation from the reputation system\n\n    \"\"\"\n\n    final_reputations = dict()\n    stats_to_print = list()\n\n    # call get_reputations() from reputation system\n    raw_reputation = self.__contract_obj.functions.get_reputations(ip_addresses).call({\"from\": self.__acc_address})\n\n    # loop list with tuples from reputation system\n    for (\n        name,\n        reputation,\n        weighted_reputation,\n        stddev_count,\n        divisor,\n        final_reputation,\n        avg,\n        median,\n        stddev,\n        index,\n        avg_deviation,\n        avg_avg_deviation,\n        malicious_opinions,\n    ) in raw_reputation:\n        # list elements with an empty name can be ignored\n        if not name:\n            continue\n\n        # print statistical values\n        stats_to_print.append([\n            name,\n            reputation / 10,\n            weighted_reputation / 10,\n            stddev_count / 10,\n            divisor / 10,\n            final_reputation / 10,\n            avg / 10,\n            median / 10,\n            stddev / 10,\n            avg_deviation / 10,\n            avg_avg_deviation / 10,\n            malicious_opinions,\n        ])\n\n        # assign the final reputation to a dict for later aggregation\n        final_reputations[name] = final_reputation / 10\n\n    print_table(\n        \"REPUTATION SYSTEM STATE\",\n        stats_to_print,\n        [\n            \"Name\",\n            \"Reputation\",\n            \"Weighted Rep. by local Node\",\n            \"Stddev Count\",\n            \"Divisor\",\n            \"Final Reputation\",\n            \"Mean\",\n            \"Median\",\n            \"Stddev\",\n            \"Avg Deviation in Opinion\",\n            \"Avg of all Avg Deviations in Opinions\",\n            \"Malicious Opinions\",\n        ],\n    )\n\n    # if sum(final_reputations.values()):\n    #     self.report_reputation_oracle(list(final_reputations.items()))\n\n    return final_reputations\n</code></pre>"},{"location":"api/core/aggregation/blockchainReputation/#nebula.core.aggregation.blockchainReputation.BlockchainHandler.push_opinions","title":"<code>push_opinions(opinion_dict)</code>","text":"<p>Pushes all locally computed opinions of models to aggregate to the reputation system Args:     opinion_dict: Dict of all names:opinions for writing to the reputation system</p> <p>Returns: Json of transaction receipt</p> Source code in <code>nebula/core/aggregation/blockchainReputation.py</code> <pre><code>@retry(Exception, tries=3, delay=4)\ndef push_opinions(self, opinion_dict: dict):\n    \"\"\"\n    Pushes all locally computed opinions of models to aggregate to the reputation system\n    Args:\n        opinion_dict: Dict of all names:opinions for writing to the reputation system\n\n    Returns: Json of transaction receipt\n\n    \"\"\"\n\n    # create raw transaction object to call rate_neighbors() from the reputation system\n    unsigned_trx = self.__contract_obj.functions.rate_neighbours(list(opinion_dict.items())).build_transaction({\n        \"chainId\": self.__web3.eth.chain_id,\n        \"from\": self.__acc_address,\n        \"nonce\": self.__web3.eth.get_transaction_count(\n            self.__web3.to_checksum_address(self.__acc_address), \"pending\"\n        ),\n        \"gasPrice\": self.__web3.to_wei(self.__gas_price, \"gwei\"),\n    })\n\n    # sign and execute the transaction\n    conf = self.__sign_and_deploy(unsigned_trx)\n\n    self.report_reputation_oracle(list(opinion_dict.items()))\n    # return the receipt as json\n    return self.__web3.to_json(conf)\n</code></pre>"},{"location":"api/core/aggregation/blockchainReputation/#nebula.core.aggregation.blockchainReputation.BlockchainHandler.report_gas_oracle","title":"<code>report_gas_oracle()</code>","text":"<p>Reports accumulated gas costs of all transactions made to the blockchain Returns: List of all accumulated gas costs per registered node</p> Source code in <code>nebula/core/aggregation/blockchainReputation.py</code> <pre><code>@retry((Exception, requests.exceptions.HTTPError), tries=3, delay=4)\ndef report_gas_oracle(self) -&gt; list:\n    \"\"\"\n    Reports accumulated gas costs of all transactions made to the blockchain\n    Returns: List of all accumulated gas costs per registered node\n\n    \"\"\"\n\n    # method used for experiments, not needed for aggregation\n    response = requests.post(\n        url=f\"{self.__oracle_url}/gas\",\n        json={\"amount\": self.__gas_used, \"round\": self.round},\n        headers=self.__rest_header,\n        timeout=20,  # 10\n    )\n\n    # raise Exception if status is not successful\n    response.raise_for_status()\n\n    # reset local gas accumulation\n    self.__gas_used = 0\n\n    # return list with gas usage for logging\n    return list(response.json().items())\n</code></pre>"},{"location":"api/core/aggregation/blockchainReputation/#nebula.core.aggregation.blockchainReputation.BlockchainHandler.report_reputation_oracle","title":"<code>report_reputation_oracle(records)</code>","text":"<p>Reports reputations used for aggregation Returns: None</p> Source code in <code>nebula/core/aggregation/blockchainReputation.py</code> <pre><code>@retry((Exception, requests.exceptions.HTTPError), tries=3, delay=4)\ndef report_reputation_oracle(self, records: list) -&gt; None:\n    \"\"\"\n    Reports reputations used for aggregation\n    Returns: None\n\n    \"\"\"\n\n    # method used for experiments, not needed for aggregation\n    response = requests.post(\n        url=f\"{self.__oracle_url}/reputation\",\n        json={\"records\": records, \"round\": self.round, \"sender\": self.__home_ip},\n        headers=self.__rest_header,\n        timeout=20,  # 10\n    )\n\n    # raise Exception if status is not successful\n    response.raise_for_status()\n\n    return None\n</code></pre>"},{"location":"api/core/aggregation/blockchainReputation/#nebula.core.aggregation.blockchainReputation.BlockchainHandler.report_time_oracle","title":"<code>report_time_oracle(start)</code>","text":"<p>Reports time used for aggregation Returns: None</p> Source code in <code>nebula/core/aggregation/blockchainReputation.py</code> <pre><code>@retry((Exception, requests.exceptions.HTTPError), tries=3, delay=4)\ndef report_time_oracle(self, start: float) -&gt; None:\n    \"\"\"\n    Reports time used for aggregation\n    Returns: None\n\n    \"\"\"\n    # method used for experiments, not needed for aggregation\n    # report aggregation time and round to oracle\n    response = requests.post(\n        url=f\"{BlockchainHandler.oracle_url}/time\",\n        json={\"time\": (time.time_ns() - start) / (10**9), \"round\": self.round},\n        headers=self.__rest_header,\n        timeout=20,  # 10\n    )\n\n    # raise Exception if status is not successful\n    response.raise_for_status()\n\n    # increase aggregation round counter after reporting time\n    self.round += 1\n    return None\n</code></pre>"},{"location":"api/core/aggregation/blockchainReputation/#nebula.core.aggregation.blockchainReputation.BlockchainHandler.verify_balance","title":"<code>verify_balance()</code>","text":"<p>Calls blockchain directly for requesting current balance Returns: None</p> Source code in <code>nebula/core/aggregation/blockchainReputation.py</code> <pre><code>def verify_balance(self) -&gt; None:\n    \"\"\"\n    Calls blockchain directly for requesting current balance\n    Returns: None\n\n    \"\"\"\n\n    # directly call view method from non-validator node\n    balance = self.__web3.eth.get_balance(self.__acc_address, \"latest\")\n\n    # convert wei to ether\n    balance_eth = self.__web3.from_wei(balance, \"ether\")\n    print(\n        f\"BLOCKCHAIN: Successfully verified balance of {balance_eth} ETH\",\n        flush=True,\n    )\n\n    # if node ran out of funds, it requests ether from the oracle\n    if balance_eth &lt;= 1:\n        self.__request_funds_from_oracle()\n\n    return None\n</code></pre>"},{"location":"api/core/aggregation/blockchainReputation/#nebula.core.aggregation.blockchainReputation.BlockchainHandler.verify_registration","title":"<code>verify_registration()</code>","text":"<p>Verifies the successful registration of the node itself, executes registration again if reputation system returns false Returns: None</p> Source code in <code>nebula/core/aggregation/blockchainReputation.py</code> <pre><code>@retry(Exception, tries=3, delay=4)\ndef verify_registration(self) -&gt; None:\n    \"\"\"\n    Verifies the successful registration of the node itself,\n    executes registration again if reputation system returns false\n    Returns: None\n\n    \"\"\"\n\n    # call view function of reputation system to check if registration was not abandoned by hard fork\n    confirmation = self.__contract_obj.functions.confirm_registration().call({\"from\": self.__acc_address})\n\n    # function returns boolean\n    if not confirmation:\n        # register again if not successful\n        self.__register()\n\n        # raise Exception to check again\n        raise Exception(\"EXCEPTION: _verify_registration() =&gt; Could not be confirmed)\")\n\n    return None\n</code></pre>"},{"location":"api/core/aggregation/blockchainReputation/#nebula.core.aggregation.blockchainReputation.BlockchainReputation","title":"<code>BlockchainReputation</code>","text":"<p>               Bases: <code>Aggregator</code></p>"},{"location":"api/core/aggregation/blockchainReputation/#nebula.core.aggregation.blockchainReputation.BlockchainReputation--bat-sandrinhunkeler-blockchainreputation","title":"BAT-SandrinHunkeler (BlockchainReputation)","text":"<p>Weighted FedAvg by using relative reputation of each model's trainer Returns: aggregated model</p> Source code in <code>nebula/core/aggregation/blockchainReputation.py</code> <pre><code>class BlockchainReputation(Aggregator):\n    \"\"\"\n    # BAT-SandrinHunkeler (BlockchainReputation)\n    Weighted FedAvg by using relative reputation of each model's trainer\n    Returns: aggregated model\n    \"\"\"\n\n    ALGORITHM_MAP = {\n        \"Cossim\": cosine_metric,\n        \"Pearson\": pearson_correlation_metric,\n        \"Euclidean\": euclidean_metric,\n        \"Minkowski\": minkowski_metric,\n        \"Manhattan\": manhattan_metric,\n        \"Jaccard\": jaccard_metric,\n        \"CossimEuclid\": cossim_euclidean,\n    }\n\n    def __init__(self, similarity_metric: str = \"CossimEuclid\", config=None, **kwargs):\n        # initialize parent class\n        super().__init__(config, **kwargs)\n\n        self.config = config\n\n        # extract local NEBULA name\n        self.node_name = self.config.participant[\"network_args\"][\"addr\"]\n\n        # initialize BlockchainHandler for interacting with oracle and non-validator node\n        self.__blockchain = BlockchainHandler(self.node_name)\n\n        # check if node is malicious for debugging\n        self.__malicious = self.config.participant[\"device_args\"][\"malicious\"]\n\n        self.__opinion_algo = BlockchainReputation.ALGORITHM_MAP.get(similarity_metric)\n        self.__similarity_metric = similarity_metric\n\n    def run_aggregation(self, model_buffer: OrderedDict[str, OrderedDict[torch.Tensor, int]]) -&gt; torch.Tensor:\n        print_with_frame(\"BLOCKCHAIN AGGREGATION: START\")\n\n        # track aggregation time for experiments\n        start = time.time_ns()\n\n        # verify the registration process during initialization of the BlockchainHandler\n        self.__blockchain.verify_registration()\n\n        # verify if ether balance is still sufficient for aggregating, request more otherwise\n        self.__blockchain.verify_balance()\n\n        # create dict&lt;sender, model&gt;\n        current_models = {sender: model for sender, (model, weight) in model_buffer.items()}\n\n        print(f\"Node: {self.node_name}\", flush=True)\n        print(f\"self.__malicious: {self.__malicious}\", flush=True)\n\n        # extract local model from models to aggregate\n        local_model = model_buffer[self.node_name][0]\n\n        # compute similarity between local model and all buffered models\n        metric_values = {\n            sender: max(\n                min(\n                    round(\n                        self.__opinion_algo(local_model, current_models[sender], similarity=True),\n                        5,\n                    ),\n                    1,\n                ),\n                0,\n            )\n            for sender in current_models\n            if sender != self.node_name\n        }\n\n        # log similarity metric values\n        print_table(\n            \"SIMILARITY METRIC\",\n            list(metric_values.items()),\n            [\"neighbour Node\", f\"{self.__similarity_metric} Similarity\"],\n        )\n\n        # increase resolution of metric in upper half of interval\n        opinion_values = {sender: round(metric**3 * 100) for sender, metric in metric_values.items()}\n\n        # DEBUG\n        if int(self.node_name[-7]) &lt;= 1 and self.__blockchain.round &gt;= 5:\n            opinion_values = {sender: int(torch.randint(0, 101, (1,))[0]) for sender, metric in metric_values.items()}\n\n        # push local opinions to reputation system\n        self.__blockchain.push_opinions(opinion_values)\n\n        # log pushed opinion values\n        print_table(\n            \"REPORT LOCAL OPINION\",\n            list(opinion_values.items()),\n            [\"Node\", f\"Transformed {self.__similarity_metric} Similarity\"],\n        )\n\n        # request global reputation values from reputation system\n        reputation_values = self.__blockchain.get_reputations([sender for sender in current_models])\n\n        # log received global reputations\n        print_table(\n            \"GLOBAL REPUTATION\",\n            list(reputation_values.items()),\n            [\"Node\", \"Global Reputation\"],\n        )\n\n        # normalize all reputation values to sum() == 1\n        sum_reputations = sum(reputation_values.values())\n        if sum_reputations &gt; 0:\n            normalized_reputation_values = {\n                name: round(reputation_values[name] / sum_reputations, 3) for name in reputation_values\n            }\n        else:\n            normalized_reputation_values = reputation_values\n\n        # log normalized aggregation weights\n        print_table(\n            \"AGGREGATION WEIGHTS\",\n            list(normalized_reputation_values.items()),\n            [\"Node\", \"Aggregation Weight\"],\n        )\n\n        # initialize empty model\n        final_model = {layer: torch.zeros_like(param).float() for layer, param in local_model.items()}\n\n        # cover rare case where no models were added or reputation is 0 to return the local model\n        if sum_reputations &gt; 0:\n            for sender in normalized_reputation_values.keys():\n                for layer in final_model:\n                    final_model[layer] += current_models[sender][layer].float() * normalized_reputation_values[sender]\n\n        # otherwise, skip aggregation\n        else:\n            final_model = local_model\n\n        # report used gas to oracle and log cumulative gas used\n        print_table(\n            \"TOTAL GAS USED\",\n            self.__blockchain.report_gas_oracle(),\n            [\"Node\", \"Cumulative Gas used\"],\n        )\n        self.__blockchain.report_time_oracle(start)\n\n        print_with_frame(\"BLOCKCHAIN AGGREGATION: FINISHED\")\n\n        # return newly aggregated model\n        return final_model\n</code></pre>"},{"location":"api/core/aggregation/blockchainReputation/#nebula.core.aggregation.blockchainReputation.print_table","title":"<code>print_table(title, values, headers)</code>","text":"<p>Prints a title, all values ordered in a table, with the headers as column titles. Args:     title: Title of the table     values: Rows of table     headers: Column headers of table</p> <p>Returns: None, prints output</p> Source code in <code>nebula/core/aggregation/blockchainReputation.py</code> <pre><code>def print_table(title: str, values: list[tuple | list], headers: list[str]) -&gt; None:\n    \"\"\"\n    Prints a title, all values ordered in a table, with the headers as column titles.\n    Args:\n        title: Title of the table\n        values: Rows of table\n        headers: Column headers of table\n\n    Returns: None, prints output\n\n    \"\"\"\n    print(f\"\\n{'-' * 25} {title.upper()} {'-' * 25}\", flush=True)\n    print(tabulate(sorted(values), headers=headers, tablefmt=\"grid\"), flush=True)\n</code></pre>"},{"location":"api/core/aggregation/blockchainReputation/#nebula.core.aggregation.blockchainReputation.print_with_frame","title":"<code>print_with_frame(message)</code>","text":"<p>Prints a large frame with a title inside Args:     message: Title to put into the frame</p> <p>Returns: None</p> Source code in <code>nebula/core/aggregation/blockchainReputation.py</code> <pre><code>def print_with_frame(message) -&gt; None:\n    \"\"\"\n    Prints a large frame with a title inside\n    Args:\n        message: Title to put into the frame\n\n    Returns: None\n\n    \"\"\"\n    message_length = len(message)\n    print(f\"{' ' * 20}+{'-' * (message_length + 2)}+\", flush=True)\n    print(f\"{'*' * 20}| {message.upper()} |{'*' * 20}\", flush=True)\n    print(f\"{' ' * 20}+{'-' * (message_length + 2)}+\", flush=True)\n</code></pre>"},{"location":"api/core/aggregation/fedavg/","title":"Documentation for Fedavg Module","text":""},{"location":"api/core/aggregation/fedavg/#nebula.core.aggregation.fedavg.FedAvg","title":"<code>FedAvg</code>","text":"<p>               Bases: <code>Aggregator</code></p> <p>Aggregator: Federated Averaging (FedAvg) Authors: McMahan et al. Year: 2016</p> Source code in <code>nebula/core/aggregation/fedavg.py</code> <pre><code>class FedAvg(Aggregator):\n    \"\"\"\n    Aggregator: Federated Averaging (FedAvg)\n    Authors: McMahan et al.\n    Year: 2016\n    \"\"\"\n\n    def __init__(self, config=None, **kwargs):\n        super().__init__(config, **kwargs)\n\n    def run_aggregation(self, models):\n        super().run_aggregation(models)\n\n        models = list(models.values())\n\n        total_samples = float(sum(weight for _, weight in models))\n\n        if total_samples == 0:\n            raise ValueError(\"Total number of samples must be greater than zero.\")\n\n        last_model_params = models[-1][0]\n        accum = {layer: torch.zeros_like(param, dtype=torch.float32) for layer, param in last_model_params.items()}\n\n        with torch.no_grad():\n            for model_parameters, weight in models:\n                normalized_weight = weight / total_samples\n                for layer in accum:\n                    accum[layer].add_(\n                        model_parameters[layer].to(accum[layer].dtype),\n                        alpha=normalized_weight,\n                    )\n\n        del models\n        gc.collect()\n\n        # self.print_model_size(accum)\n        return accum\n</code></pre>"},{"location":"api/core/aggregation/krum/","title":"Documentation for Krum Module","text":""},{"location":"api/core/aggregation/krum/#nebula.core.aggregation.krum.Krum","title":"<code>Krum</code>","text":"<p>               Bases: <code>Aggregator</code></p> <p>Aggregator: Krum Authors: Peva Blanchard et al. Year: 2017 Note: https://papers.nips.cc/paper/2017/hash/f4b9ec30ad9f68f89b29639786cb62ef-Abstract.html</p> Source code in <code>nebula/core/aggregation/krum.py</code> <pre><code>class Krum(Aggregator):\n    \"\"\"\n    Aggregator: Krum\n    Authors: Peva Blanchard et al.\n    Year: 2017\n    Note: https://papers.nips.cc/paper/2017/hash/f4b9ec30ad9f68f89b29639786cb62ef-Abstract.html\n    \"\"\"\n\n    def __init__(self, config=None, **kwargs):\n        super().__init__(config, **kwargs)\n\n    def run_aggregation(self, models):\n        super().run_aggregation(models)\n\n        models = list(models.values())\n\n        accum = {layer: torch.zeros_like(param).float() for layer, param in models[-1][0].items()}\n        total_models = len(models)\n        distance_list = [0 for i in range(0, total_models)]\n        min_index = 0\n        min_distance_sum = float(\"inf\")\n\n        for i in range(0, total_models):\n            m1, _ = models[i]\n            for j in range(0, total_models):\n                m2, _ = models[j]\n                distance = 0\n                if i == j:\n                    distance = 0\n                else:\n                    for layer in m1:\n                        l1 = m1[layer]\n\n                        l2 = m2[layer]\n                        distance += numpy.linalg.norm(l1 - l2)\n                distance_list[i] += distance\n\n            if min_distance_sum &gt; distance_list[i]:\n                min_distance_sum = distance_list[i]\n                min_index = i\n        m, _ = models[min_index]\n        for layer in m:\n            accum[layer] = accum[layer] + m[layer]\n\n        return accum\n</code></pre>"},{"location":"api/core/aggregation/median/","title":"Documentation for Median Module","text":""},{"location":"api/core/aggregation/median/#nebula.core.aggregation.median.Median","title":"<code>Median</code>","text":"<p>               Bases: <code>Aggregator</code></p> <p>Aggregator: Median Authors: Dong Yin et al et al. Year: 2021 Note: https://arxiv.org/pdf/1803.01498.pdf</p> Source code in <code>nebula/core/aggregation/median.py</code> <pre><code>class Median(Aggregator):\n    \"\"\"\n    Aggregator: Median\n    Authors: Dong Yin et al et al.\n    Year: 2021\n    Note: https://arxiv.org/pdf/1803.01498.pdf\n    \"\"\"\n\n    def __init__(self, config=None, **kwargs):\n        super().__init__(config, **kwargs)\n\n    def get_median(self, weights):\n        # check if the weight tensor has enough space\n        weight_len = len(weights)\n\n        median = 0\n        if weight_len % 2 == 1:\n            # odd number, return the median\n            median, _ = torch.median(weights, 0)\n        else:\n            # even number, return the mean of median two numbers\n            # sort the tensor\n            arr_weights = np.asarray(weights)\n            nobs = arr_weights.shape[0]\n            start = int(nobs / 2) - 1\n            end = int(nobs / 2) + 1\n            atmp = np.partition(arr_weights, (start, end - 1), 0)\n            sl = [slice(None)] * atmp.ndim\n            sl[0] = slice(start, end)\n            arr_median = np.mean(atmp[tuple(sl)], axis=0)\n            median = torch.tensor(arr_median)\n        return median\n\n    def run_aggregation(self, models):\n        super().run_aggregation(models)\n\n        models = list(models.values())\n        models_params = [m for m, _ in models]\n\n        total_models = len(models)\n\n        accum = {layer: torch.zeros_like(param).float() for layer, param in models[-1][0].items()}\n\n        # Calculate the trimmedmean for each parameter\n        for layer in accum:\n            weight_layer = accum[layer]\n            # get the shape of layer tensor\n            l_shape = list(weight_layer.shape)\n\n            # get the number of elements of layer tensor\n            number_layer_weights = torch.numel(weight_layer)\n            # if its 0-d tensor\n            if l_shape == []:\n                weights = torch.tensor([models_params[j][layer] for j in range(0, total_models)])\n                weights = weights.double()\n                w = self.get_median(weights)\n                accum[layer] = w\n\n            else:\n                # flatten the tensor\n                weight_layer_flatten = weight_layer.view(number_layer_weights)\n\n                # flatten the tensor of each model\n                models_layer_weight_flatten = torch.stack(\n                    [models_params[j][layer].view(number_layer_weights) for j in range(0, total_models)],\n                    0,\n                )\n\n                # get the weight list [w1j,w2j,\u00b7\u00b7\u00b7 ,wmj], where wij is the jth parameter of the ith local model\n                median = self.get_median(models_layer_weight_flatten)\n                accum[layer] = median.view(l_shape)\n        return accum\n</code></pre>"},{"location":"api/core/aggregation/trimmedmean/","title":"Documentation for Trimmedmean Module","text":""},{"location":"api/core/aggregation/trimmedmean/#nebula.core.aggregation.trimmedmean.TrimmedMean","title":"<code>TrimmedMean</code>","text":"<p>               Bases: <code>Aggregator</code></p> <p>Aggregator: TrimmedMean Authors: Dong Yin et al et al. Year: 2021 Note: https://arxiv.org/pdf/1803.01498.pdf</p> Source code in <code>nebula/core/aggregation/trimmedmean.py</code> <pre><code>class TrimmedMean(Aggregator):\n    \"\"\"\n    Aggregator: TrimmedMean\n    Authors: Dong Yin et al et al.\n    Year: 2021\n    Note: https://arxiv.org/pdf/1803.01498.pdf\n    \"\"\"\n\n    def __init__(self, config=None, beta=0, **kwargs):\n        super().__init__(config, **kwargs)\n        self.beta = beta\n\n    def get_trimmedmean(self, weights):\n        # check if the weight tensor has enough space\n        weight_len = len(weights)\n\n        if weight_len &lt;= 2 * self.beta:\n            remaining_wrights = weights\n            res = torch.mean(remaining_wrights, 0)\n\n        else:\n            # remove the largest and smallest \u03b2 items\n            arr_weights = np.asarray(weights)\n            nobs = arr_weights.shape[0]\n            start = self.beta\n            end = nobs - self.beta\n            atmp = np.partition(arr_weights, (start, end - 1), 0)\n            sl = [slice(None)] * atmp.ndim\n            sl[0] = slice(start, end)\n            print(atmp[tuple(sl)])\n            arr_median = np.mean(atmp[tuple(sl)], axis=0)\n            res = torch.tensor(arr_median)\n\n        # get the mean of the remaining weights\n\n        return res\n\n    def run_aggregation(self, models):\n        super().run_aggregation(models)\n\n        models = list(models.values())\n        models_params = [m for m, _ in models]\n\n        total_models = len(models)\n\n        accum = {layer: torch.zeros_like(param).float() for layer, param in models[-1][0].items()}\n\n        for layer in accum:\n            weight_layer = accum[layer]\n            # get the shape of layer tensor\n            l_shape = list(weight_layer.shape)\n\n            # get the number of elements of layer tensor\n            number_layer_weights = torch.numel(weight_layer)\n            # if its 0-d tensor\n            if l_shape == []:\n                weights = torch.tensor([models_params[j][layer] for j in range(0, total_models)])\n                weights = weights.double()\n                w = self.get_trimmedmean(weights)\n                accum[layer] = w\n\n            else:\n                # flatten the tensor\n                weight_layer_flatten = weight_layer.view(number_layer_weights)\n\n                # flatten the tensor of each model\n                models_layer_weight_flatten = torch.stack(\n                    [models_params[j][layer].view(number_layer_weights) for j in range(0, total_models)],\n                    0,\n                )\n\n                # get the weight list [w1j,w2j,\u00b7\u00b7\u00b7 ,wmj], where wij is the jth parameter of the ith local model\n                trimmedmean = self.get_trimmedmean(models_layer_weight_flatten)\n                accum[layer] = trimmedmean.view(l_shape)\n\n        return accum\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/","title":"Documentation for Updatehandlers Module","text":""},{"location":"api/core/aggregation/updatehandlers/cflupdatehandler/","title":"Documentation for Cflupdatehandler Module","text":""},{"location":"api/core/aggregation/updatehandlers/dflupdatehandler/","title":"Documentation for Dflupdatehandler Module","text":""},{"location":"api/core/aggregation/updatehandlers/sdflupdatehandler/","title":"Documentation for Sdflupdatehandler Module","text":""},{"location":"api/core/aggregation/updatehandlers/updatehandler/","title":"Documentation for Updatehandler Module","text":""},{"location":"api/core/aggregation/updatehandlers/updatehandler/#nebula.core.aggregation.updatehandlers.updatehandler.UpdateHandler","title":"<code>UpdateHandler</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for managing update storage and retrieval in a federated learning setting.</p> <p>This class defines the required methods for handling updates from multiple sources, ensuring they are properly stored, retrieved, and processed during the aggregation process.</p> Source code in <code>nebula/core/aggregation/updatehandlers/updatehandler.py</code> <pre><code>class UpdateHandler(ABC):\n    \"\"\"\n    Abstract base class for managing update storage and retrieval in a federated learning setting.\n\n    This class defines the required methods for handling updates from multiple sources,\n    ensuring they are properly stored, retrieved, and processed during the aggregation process.\n    \"\"\"\n\n    @abstractmethod\n    async def init(self, config: dict):\n        raise NotImplementedError\n\n    @abstractmethod\n    async def round_expected_updates(self, federation_nodes: set):\n        \"\"\"\n        Initializes the expected updates for the current round.\n\n        This method sets up the expected sources (`federation_nodes`) that should provide updates\n        in the current training round. It ensures that each source has an entry in the storage\n        and resets any previous tracking of received updates.\n\n        Args:\n            federation_nodes (set): A set of node identifiers expected to provide updates.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    async def storage_update(self, updt_received_event: UpdateReceivedEvent):\n        \"\"\"\n        Stores an update from a source in the update storage.\n\n        This method ensures that an update received from a source is properly stored in the buffer,\n        avoiding duplicates and managing update history if necessary.\n\n        Args:\n            model: The model associated with the update.\n            weight: The weight assigned to the update (e.g., based on the amount of data used in training).\n            source (str): The identifier of the node sending the update.\n            round (int): The current device local training round when the update was done.\n            local (boolean): Local update\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    async def get_round_updates(self) -&gt; dict[str, tuple[object, float]]:\n        \"\"\"\n        Retrieves the latest updates from all received sources in the current round.\n\n        This method collects updates from all sources that have sent updates,\n        prioritizing the most recent update available in the buffer.\n\n        Returns:\n            dict: A dictionary where keys are source identifiers and values are tuples `(model, weight)`,\n                  representing the latest updates received from each source.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    async def notify_federation_update(self, updt_nei_event: UpdateNeighborEvent):\n        \"\"\"\n        Notifies the system of a change in the federation regarding a specific source.\n\n        If a source leaves the federation, it is removed from the list of expected updates.\n        If a source is newly added, it is registered for future updates.\n\n        Args:\n            source (str): The identifier of the source node.\n            remove (bool, optional): Whether to remove the source from the federation. Defaults to `False`.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    async def get_round_missing_nodes(self) -&gt; set[str]:\n        \"\"\"\n        Identifies sources that have not yet provided updates in the current round.\n\n        Returns:\n            set: A set of source identifiers that are expected to send updates but have not yet been received.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    async def notify_if_all_updates_received(self):\n        \"\"\"\n        Notifies the system when all expected updates for the current round have been received.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    async def stop_notifying_updates(self):\n        \"\"\"\n        Stops notifications related to update reception.\n\n        This method can be used to reset any notification mechanisms or stop tracking updates\n        if the aggregation process is halted.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/updatehandler/#nebula.core.aggregation.updatehandlers.updatehandler.UpdateHandler.get_round_missing_nodes","title":"<code>get_round_missing_nodes()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Identifies sources that have not yet provided updates in the current round.</p> <p>Returns:</p> Name Type Description <code>set</code> <code>set[str]</code> <p>A set of source identifiers that are expected to send updates but have not yet been received.</p> Source code in <code>nebula/core/aggregation/updatehandlers/updatehandler.py</code> <pre><code>@abstractmethod\nasync def get_round_missing_nodes(self) -&gt; set[str]:\n    \"\"\"\n    Identifies sources that have not yet provided updates in the current round.\n\n    Returns:\n        set: A set of source identifiers that are expected to send updates but have not yet been received.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/updatehandler/#nebula.core.aggregation.updatehandlers.updatehandler.UpdateHandler.get_round_updates","title":"<code>get_round_updates()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Retrieves the latest updates from all received sources in the current round.</p> <p>This method collects updates from all sources that have sent updates, prioritizing the most recent update available in the buffer.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, tuple[object, float]]</code> <p>A dictionary where keys are source identifiers and values are tuples <code>(model, weight)</code>,   representing the latest updates received from each source.</p> Source code in <code>nebula/core/aggregation/updatehandlers/updatehandler.py</code> <pre><code>@abstractmethod\nasync def get_round_updates(self) -&gt; dict[str, tuple[object, float]]:\n    \"\"\"\n    Retrieves the latest updates from all received sources in the current round.\n\n    This method collects updates from all sources that have sent updates,\n    prioritizing the most recent update available in the buffer.\n\n    Returns:\n        dict: A dictionary where keys are source identifiers and values are tuples `(model, weight)`,\n              representing the latest updates received from each source.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/updatehandler/#nebula.core.aggregation.updatehandlers.updatehandler.UpdateHandler.notify_federation_update","title":"<code>notify_federation_update(updt_nei_event)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Notifies the system of a change in the federation regarding a specific source.</p> <p>If a source leaves the federation, it is removed from the list of expected updates. If a source is newly added, it is registered for future updates.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The identifier of the source node.</p> required <code>remove</code> <code>bool</code> <p>Whether to remove the source from the federation. Defaults to <code>False</code>.</p> required Source code in <code>nebula/core/aggregation/updatehandlers/updatehandler.py</code> <pre><code>@abstractmethod\nasync def notify_federation_update(self, updt_nei_event: UpdateNeighborEvent):\n    \"\"\"\n    Notifies the system of a change in the federation regarding a specific source.\n\n    If a source leaves the federation, it is removed from the list of expected updates.\n    If a source is newly added, it is registered for future updates.\n\n    Args:\n        source (str): The identifier of the source node.\n        remove (bool, optional): Whether to remove the source from the federation. Defaults to `False`.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/updatehandler/#nebula.core.aggregation.updatehandlers.updatehandler.UpdateHandler.notify_if_all_updates_received","title":"<code>notify_if_all_updates_received()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Notifies the system when all expected updates for the current round have been received.</p> Source code in <code>nebula/core/aggregation/updatehandlers/updatehandler.py</code> <pre><code>@abstractmethod\nasync def notify_if_all_updates_received(self):\n    \"\"\"\n    Notifies the system when all expected updates for the current round have been received.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/updatehandler/#nebula.core.aggregation.updatehandlers.updatehandler.UpdateHandler.round_expected_updates","title":"<code>round_expected_updates(federation_nodes)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Initializes the expected updates for the current round.</p> <p>This method sets up the expected sources (<code>federation_nodes</code>) that should provide updates in the current training round. It ensures that each source has an entry in the storage and resets any previous tracking of received updates.</p> <p>Parameters:</p> Name Type Description Default <code>federation_nodes</code> <code>set</code> <p>A set of node identifiers expected to provide updates.</p> required Source code in <code>nebula/core/aggregation/updatehandlers/updatehandler.py</code> <pre><code>@abstractmethod\nasync def round_expected_updates(self, federation_nodes: set):\n    \"\"\"\n    Initializes the expected updates for the current round.\n\n    This method sets up the expected sources (`federation_nodes`) that should provide updates\n    in the current training round. It ensures that each source has an entry in the storage\n    and resets any previous tracking of received updates.\n\n    Args:\n        federation_nodes (set): A set of node identifiers expected to provide updates.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/updatehandler/#nebula.core.aggregation.updatehandlers.updatehandler.UpdateHandler.stop_notifying_updates","title":"<code>stop_notifying_updates()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Stops notifications related to update reception.</p> <p>This method can be used to reset any notification mechanisms or stop tracking updates if the aggregation process is halted.</p> Source code in <code>nebula/core/aggregation/updatehandlers/updatehandler.py</code> <pre><code>@abstractmethod\nasync def stop_notifying_updates(self):\n    \"\"\"\n    Stops notifications related to update reception.\n\n    This method can be used to reset any notification mechanisms or stop tracking updates\n    if the aggregation process is halted.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/core/aggregation/updatehandlers/updatehandler/#nebula.core.aggregation.updatehandlers.updatehandler.UpdateHandler.storage_update","title":"<code>storage_update(updt_received_event)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Stores an update from a source in the update storage.</p> <p>This method ensures that an update received from a source is properly stored in the buffer, avoiding duplicates and managing update history if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The model associated with the update.</p> required <code>weight</code> <p>The weight assigned to the update (e.g., based on the amount of data used in training).</p> required <code>source</code> <code>str</code> <p>The identifier of the node sending the update.</p> required <code>round</code> <code>int</code> <p>The current device local training round when the update was done.</p> required <code>local</code> <code>boolean</code> <p>Local update</p> required Source code in <code>nebula/core/aggregation/updatehandlers/updatehandler.py</code> <pre><code>@abstractmethod\nasync def storage_update(self, updt_received_event: UpdateReceivedEvent):\n    \"\"\"\n    Stores an update from a source in the update storage.\n\n    This method ensures that an update received from a source is properly stored in the buffer,\n    avoiding duplicates and managing update history if necessary.\n\n    Args:\n        model: The model associated with the update.\n        weight: The weight assigned to the update (e.g., based on the amount of data used in training).\n        source (str): The identifier of the node sending the update.\n        round (int): The current device local training round when the update was done.\n        local (boolean): Local update\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/core/datasets/","title":"Documentation for Datasets Module","text":""},{"location":"api/core/datasets/changeablesubset/","title":"Documentation for Changeablesubset Module","text":""},{"location":"api/core/datasets/datamodule/","title":"Documentation for Datamodule Module","text":""},{"location":"api/core/datasets/nebuladataset/","title":"Documentation for Nebuladataset Module","text":""},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset","title":"<code>NebulaDataset</code>","text":"Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>class NebulaDataset:\n    def __init__(\n        self,\n        num_classes=10,\n        partitions_number=1,\n        batch_size=32,\n        num_workers=4,\n        iid=True,\n        partition=\"dirichlet\",\n        partition_parameter=0.5,\n        seed=42,\n        config_dir=None,\n    ):\n        self.num_classes = num_classes\n        self.partitions_number = partitions_number\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.iid = iid\n        self.partition = partition\n        self.partition_parameter = partition_parameter\n        self.seed = seed\n        self.config_dir = config_dir\n\n        logging.info(\n            f\"Dataset {self.__class__.__name__} initialized | Partitions: {self.partitions_number} | IID: {self.iid} | Partition: {self.partition} | Partition parameter: {self.partition_parameter}\"\n        )\n\n        # Dataset\n        self.train_set = None\n        self.train_indices_map = None\n        self.test_set = None\n        self.test_indices_map = None\n        self.local_test_indices_map = None\n\n        enable_deterministic(self.seed)\n\n    @abstractmethod\n    def initialize_dataset(self):\n        \"\"\"\n        Initialize the dataset. This should load or create the dataset.\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement this method.\")\n\n    def clear(self):\n        \"\"\"\n        Clear the dataset. This should remove or reset the dataset.\n        \"\"\"\n        if self.train_set is not None and hasattr(self.train_set, \"close\"):\n            self.train_set.close()\n        if self.test_set is not None and hasattr(self.test_set, \"close\"):\n            self.test_set.close()\n\n        self.train_set = None\n        self.train_indices_map = None\n        self.test_set = None\n        self.test_indices_map = None\n        self.local_test_indices_map = None\n\n    def data_partitioning(self, plot=False):\n        \"\"\"\n        Perform the data partitioning.\n        \"\"\"\n\n        logging.info(\n            f\"Partitioning data for {self.__class__.__name__} | Partitions: {self.partitions_number} | IID: {self.iid} | Partition: {self.partition} | Partition parameter: {self.partition_parameter}\"\n        )\n\n        self.train_indices_map = (\n            self.generate_iid_map(self.train_set)\n            if self.iid\n            else self.generate_non_iid_map(self.train_set, self.partition, self.partition_parameter)\n        )\n        self.test_indices_map = self.get_test_indices_map()\n        self.local_test_indices_map = self.get_local_test_indices_map()\n\n        if plot:\n            self.plot_data_distribution(\"train\", self.train_set, self.train_indices_map)\n            self.plot_all_data_distribution(\"train\", self.train_set, self.train_indices_map)\n            self.plot_data_distribution(\"local_test\", self.test_set, self.local_test_indices_map)\n            self.plot_all_data_distribution(\"local_test\", self.test_set, self.local_test_indices_map)\n\n        self.save_partitions()\n\n    def get_test_indices_map(self):\n        \"\"\"\n        Get the indices of the test set for each participant.\n\n        Returns:\n            A dictionary mapping participant_id to a list of indices.\n        \"\"\"\n        try:\n            test_indices_map = {}\n            for participant_id in range(self.partitions_number):\n                test_indices_map[participant_id] = list(range(len(self.test_set)))\n            return test_indices_map\n        except Exception as e:\n            logging.exception(f\"Error in get_test_indices_map: {e}\")\n\n    def get_local_test_indices_map(self):\n        \"\"\"\n        Get the indices of the local test set for each participant.\n        Indices whose labels are the same as the training set are selected.\n\n        Returns:\n            A dictionary mapping participant_id to a list of indices.\n        \"\"\"\n        try:\n            local_test_indices_map = {}\n            test_targets = np.array(self.test_set.targets)\n            for participant_id in range(self.partitions_number):\n                train_labels = np.array([self.train_set.targets[idx] for idx in self.train_indices_map[participant_id]])\n                indices = np.where(np.isin(test_targets, train_labels))[0].tolist()\n                local_test_indices_map[participant_id] = indices\n            return local_test_indices_map\n        except Exception as e:\n            logging.exception(f\"Error in get_local_test_indices_map: {e}\")\n            raise\n\n    def save_partition(self, obj, file, name):\n        try:\n            logging.info(f\"Saving pickled object of type {type(obj)}\")\n            pickled = pickle.dumps(obj)\n            ds = file.create_dataset(name, data=np.void(pickled))\n            ds.attrs[\"__type__\"] = \"pickle\"\n            logging.info(f\"Saved pickled object of type {type(obj)} to {name}\")\n        except Exception as e:\n            logging.exception(f\"Error saving object to HDF5: {e}\")\n            raise\n\n    def save_partitions(self):\n        \"\"\"\n        Save each partition data (train, test, and local test) to separate pickle files.\n        The controller saves one file per partition for each data split.\n        \"\"\"\n        try:\n            logging.info(f\"Saving partitions data for ALL participants ({self.partitions_number}) in {self.config_dir}\")\n            path = self.config_dir\n            if not os.path.exists(path):\n                raise FileNotFoundError(f\"Path {path} does not exist\")\n            # Check that the partition maps have the expected number of partitions\n            if not (\n                len(self.train_indices_map)\n                == len(self.test_indices_map)\n                == len(self.local_test_indices_map)\n                == self.partitions_number\n            ):\n                raise ValueError(\"One of the partition maps has an unexpected length.\")\n\n            # Save global test data\n            file_name = os.path.join(path, \"global_test.h5\")\n            with h5py.File(file_name, \"w\") as f:\n                indices = list(range(len(self.test_set)))\n                test_data = [self.test_set[i] for i in indices]\n                self.save_partition(test_data, f, \"test_data\")\n                f[\"test_data\"].attrs[\"num_classes\"] = self.num_classes\n                test_targets = np.array(self.test_set.targets)\n                f.create_dataset(\"test_targets\", data=test_targets, compression=\"gzip\")\n\n            for participant in range(self.partitions_number):\n                file_name = os.path.join(path, f\"participant_{participant}_train.h5\")\n                with h5py.File(file_name, \"w\") as f:\n                    logging.info(f\"Saving training data for participant {participant} in {file_name}\")\n                    indices = self.train_indices_map[participant]\n                    train_data = [self.train_set[i] for i in indices]\n                    self.save_partition(train_data, f, \"train_data\")\n                    f[\"train_data\"].attrs[\"num_classes\"] = self.num_classes\n                    train_targets = np.array([self.train_set.targets[i] for i in indices])\n                    f.create_dataset(\"train_targets\", data=train_targets, compression=\"gzip\")\n                    logging.info(f\"Partition saved for participant {participant}.\")\n\n            logging.info(\"Successfully saved all partition files.\")\n\n        except Exception as e:\n            logging.exception(f\"Error in save_partitions: {e}\")\n\n        finally:\n            self.clear()\n            logging.info(\"Cleared dataset after saving partitions.\")\n\n    @abstractmethod\n    def generate_non_iid_map(self, dataset, partition=\"dirichlet\", plot=False):\n        \"\"\"\n        Create a non-iid map of the dataset.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def generate_iid_map(self, dataset, plot=False):\n        \"\"\"\n        Create an iid map of the dataset.\n        \"\"\"\n        pass\n\n    def plot_data_distribution(self, phase, dataset, partitions_map):\n        \"\"\"\n        Plot the data distribution of the dataset.\n\n        Plot the data distribution of the dataset according to the partitions map provided.\n\n        Args:\n            phase: The phase of the dataset (train, test, local_test).\n            dataset: The dataset to plot (torch.utils.data.Dataset).\n            partitions_map: The map of the dataset partitions.\n        \"\"\"\n        logging_training.info(f\"Plotting data distribution for {phase} dataset\")\n        # Plot the data distribution of the dataset, one graph per partition\n        sns.set()\n        sns.set_style(\"whitegrid\", {\"axes.grid\": False})\n        sns.set_context(\"paper\", font_scale=1.5)\n        sns.set_palette(\"Set2\")\n\n        # Plot bar charts for each partition\n        partition_index = 0\n        for partition_index in range(self.partitions_number):\n            indices = partitions_map[partition_index]\n            class_counts = [0] * self.num_classes\n            for idx in indices:\n                label = dataset.targets[idx]\n                class_counts[label] += 1\n\n            logging_training.info(f\"[{phase}] Participant {partition_index} total samples: {len(indices)}\")\n            logging_training.info(f\"[{phase}] Participant {partition_index} data distribution: {class_counts}\")\n\n            plt.figure(figsize=(12, 8))\n            plt.bar(range(self.num_classes), class_counts)\n            for j, count in enumerate(class_counts):\n                plt.text(j, count, str(count), ha=\"center\", va=\"bottom\", fontsize=12)\n            plt.xlabel(\"Class\")\n            plt.ylabel(\"Number of samples\")\n            plt.xticks(range(self.num_classes))\n            plt.title(\n                f\"[{phase}] Participant {partition_index} data distribution ({'IID' if self.iid else f'Non-IID - {self.partition}'} - {self.partition_parameter})\"\n            )\n            plt.tight_layout()\n            path_to_save = f\"{self.config_dir}/participant_{partition_index}_data_distribution_{'iid' if self.iid else 'non_iid'}{'_' + self.partition if not self.iid else ''}_{phase}.pdf\"\n            logging_training.info(f\"Saving data distribution for participant {partition_index} to {path_to_save}\")\n            plt.savefig(path_to_save, dpi=300, bbox_inches=\"tight\")\n            plt.close()\n\n        if hasattr(self, \"tsne\") and self.tsne:\n            self.visualize_tsne(dataset)\n\n    def visualize_tsne(self, dataset):\n        X = []  # List for storing the characteristics of the samples\n        y = []  # Ready to store the labels of the samples\n        for idx in range(len(dataset)):  # Assuming that 'dataset' is a list or array of your samples\n            sample, label = dataset[idx]\n            X.append(sample.flatten())\n            y.append(label)\n\n        X = np.array(X)\n        y = np.array(y)\n\n        tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n        tsne_results = tsne.fit_transform(X)\n\n        plt.figure(figsize=(16, 10))\n        sns.scatterplot(\n            x=tsne_results[:, 0],\n            y=tsne_results[:, 1],\n            hue=y,\n            palette=sns.color_palette(\"hsv\", self.num_classes),\n            legend=\"full\",\n            alpha=0.7,\n        )\n\n        plt.title(\"t-SNE visualization of the dataset\")\n        plt.xlabel(\"t-SNE axis 1\")\n        plt.ylabel(\"t-SNE axis 2\")\n        plt.legend(title=\"Class\")\n        plt.tight_layout()\n\n        path_to_save_tsne = f\"{self.config_dir}/tsne_visualization.png\"\n        plt.savefig(path_to_save_tsne, dpi=300, bbox_inches=\"tight\")\n        plt.close()\n\n    def dirichlet_partition(\n        self,\n        dataset: Any,\n        alpha: float = 0.5,\n        min_samples_size: int = 50,\n        balanced: bool = False,\n        max_iter: int = 100,\n        verbose: bool = True,\n    ) -&gt; dict[int, list[int]]:\n        \"\"\"\n        Partition the dataset among clients using a Dirichlet distribution.\n        This function ensures each client gets at least min_samples_size samples.\n\n        Parameters\n        ----------\n        dataset : Dataset\n            The dataset to partition. Must have a 'targets' attribute.\n        alpha : float, default=0.5\n            Concentration parameter for the Dirichlet distribution.\n        min_samples_size : int, default=50\n            Minimum number of samples required in each partition.\n        balanced : bool, default=False\n            If True, distribute each class evenly among clients.\n            Otherwise, allocate according to a Dirichlet distribution.\n        max_iter : int, default=100\n            Maximum number of iterations to try finding a valid partition.\n        verbose : bool, default=True\n            If True, print debug information per iteration.\n\n        Returns\n        -------\n        partitions : dict[int, list[int]]\n            Dictionary mapping each client index to a list of sample indices.\n        \"\"\"\n        # Extract targets and unique labels.\n        y_data = self._get_targets(dataset)\n        unique_labels = np.unique(y_data)\n\n        # For each class, get a shuffled list of indices.\n        class_indices = {}\n        base_rng = np.random.default_rng(self.seed)\n        for label in unique_labels:\n            idx = np.where(y_data == label)[0]\n            base_rng.shuffle(idx)\n            class_indices[label] = idx\n\n        # Prepare container for client indices.\n        indices_per_partition = [[] for _ in range(self.partitions_number)]\n\n        def allocate_for_label(label_idx: np.ndarray, rng: np.random.Generator) -&gt; np.ndarray:\n            num_label_samples = len(label_idx)\n            if balanced:\n                proportions = np.full(self.partitions_number, 1.0 / self.partitions_number)\n            else:\n                proportions = rng.dirichlet([alpha] * self.partitions_number)\n            sample_counts = (proportions * num_label_samples).astype(int)\n            remainder = num_label_samples - sample_counts.sum()\n            if remainder &gt; 0:\n                extra_indices = rng.choice(self.partitions_number, size=remainder, replace=False)\n                for idx in extra_indices:\n                    sample_counts[idx] += 1\n            return sample_counts\n\n        for iteration in range(1, max_iter + 1):\n            rng = np.random.default_rng(self.seed + iteration)\n            temp_indices_per_partition = [[] for _ in range(self.partitions_number)]\n            for label in unique_labels:\n                label_idx = class_indices[label]\n                counts = allocate_for_label(label_idx, rng)\n                start = 0\n                for client_idx, count in enumerate(counts):\n                    end = start + count\n                    temp_indices_per_partition[client_idx].extend(label_idx[start:end])\n                    start = end\n\n            client_sizes = [len(indices) for indices in temp_indices_per_partition]\n            if min(client_sizes) &gt;= min_samples_size:\n                indices_per_partition = temp_indices_per_partition\n                if verbose:\n                    print(f\"Partition successful at iteration {iteration}. Client sizes: {client_sizes}\")\n                break\n            if verbose:\n                print(f\"Iteration {iteration}: client sizes {client_sizes}\")\n\n        else:\n            raise ValueError(\n                f\"Could not create partitions with at least {min_samples_size} samples per client after {max_iter} iterations.\"\n            )\n\n        initial_partition = {i: indices for i, indices in enumerate(indices_per_partition)}\n\n        final_partition = self.postprocess_partition(initial_partition, y_data)\n\n        return final_partition\n\n    @staticmethod\n    def _get_targets(dataset) -&gt; np.ndarray:\n        if isinstance(dataset.targets, np.ndarray):\n            return dataset.targets\n        elif hasattr(dataset.targets, \"numpy\"):\n            return dataset.targets.numpy()\n        else:\n            return np.asarray(dataset.targets)\n\n    def postprocess_partition(\n        self, partition: dict[int, list[int]], y_data: np.ndarray, min_samples_per_class: int = 10\n    ) -&gt; dict[int, list[int]]:\n        \"\"\"\n        Post-process a partition to remove (and reassign) classes with too few samples per client.\n\n        For each class:\n        - For clients with a count &gt; 0 but below min_samples_per_class, remove those samples.\n        - Reassign the removed samples to the client that already has the maximum count for that class.\n\n        Parameters\n        ----------\n        partition : dict[int, list[int]]\n            The initial partition mapping client indices to sample indices.\n        y_data : np.ndarray\n            The array of labels corresponding to the dataset samples.\n        min_samples_per_class : int, default=10\n            The minimum acceptable number of samples per class for each client.\n\n        Returns\n        -------\n        new_partition : dict[int, list[int]]\n            The updated partition.\n        \"\"\"\n        # Copy partition so we can modify it.\n        new_partition = {client: list(indices) for client, indices in partition.items()}\n\n        # Iterate over each class.\n        for label in np.unique(y_data):\n            # For each client, count how many samples of this label exist.\n            client_counts = {}\n            for client, indices in new_partition.items():\n                client_counts[client] = np.sum(np.array(y_data)[indices] == label)\n\n            # Identify clients with fewer than min_samples_per_class but nonzero counts.\n            donors = [client for client, count in client_counts.items() if 0 &lt; count &lt; min_samples_per_class]\n            # Identify potential recipients: those with at least min_samples_per_class.\n            recipients = [client for client, count in client_counts.items() if count &gt;= min_samples_per_class]\n            # If no client meets the threshold, choose the one with the highest count.\n            if not recipients:\n                best_recipient = max(client_counts, key=client_counts.get)\n                recipients = [best_recipient]\n            # Choose the recipient with the maximum count.\n            best_recipient = max(recipients, key=lambda c: client_counts[c])\n\n            # For each donor, remove samples of this label and reassign them.\n            for donor in donors:\n                donor_indices = new_partition[donor]\n                # Identify indices corresponding to this label.\n                donor_label_indices = [idx for idx in donor_indices if y_data[idx] == label]\n                # Remove these from the donor.\n                new_partition[donor] = [idx for idx in donor_indices if y_data[idx] != label]\n                # Add these to the best recipient.\n                new_partition[best_recipient].extend(donor_label_indices)\n\n        return new_partition\n\n    def homo_partition(self, dataset):\n        \"\"\"\n        Homogeneously partition the dataset into multiple subsets.\n\n        This function divides a dataset into a specified number of subsets, where each subset\n        is intended to have a roughly equal number of samples. This method aims to ensure a\n        homogeneous distribution of data across all subsets. It's particularly useful in\n        scenarios where a uniform distribution of data is desired among all federated learning\n        clients.\n\n        Args:\n            dataset (torch.utils.data.Dataset): The dataset to partition. It should have\n                                                'data' and 'targets' attributes.\n\n        Returns:\n            dict: A dictionary where keys are subset indices (ranging from 0 to partitions_number-1)\n                and values are lists of indices corresponding to the samples in each subset.\n\n        The function randomly shuffles the entire dataset and then splits it into the number\n        of subsets specified by `partitions_number`. It ensures that each subset has a similar number\n        of samples. The function also prints the class distribution in each subset for reference.\n\n        Example usage:\n            federated_data = homo_partition(my_dataset)\n            # This creates federated data subsets with homogeneous distribution.\n        \"\"\"\n        n_nets = self.partitions_number\n\n        n_train = len(dataset.targets)\n        np.random.seed(self.seed)\n        idxs = np.random.permutation(n_train)\n        batch_idxs = np.array_split(idxs, n_nets)\n        net_dataidx_map = {i: batch_idxs[i] for i in range(n_nets)}\n\n        # partitioned_datasets = []\n        for i in range(self.partitions_number):\n            # subset = torch.utils.data.Subset(dataset, net_dataidx_map[i])\n            # partitioned_datasets.append(subset)\n\n            # Print class distribution in the current partition\n            class_counts = [0] * self.num_classes\n            for idx in net_dataidx_map[i]:\n                label = dataset.targets[idx]\n                class_counts[label] += 1\n            logging.info(f\"Partition {i + 1} class distribution: {class_counts}\")\n\n        return net_dataidx_map\n\n    def balanced_iid_partition(self, dataset):\n        \"\"\"\n        Partition the dataset into balanced and IID (Independent and Identically Distributed)\n        subsets for each client.\n\n        This function divides a dataset into a specified number of subsets (federated clients),\n        where each subset has an equal class distribution. This makes the partition suitable for\n        simulating IID data scenarios in federated learning.\n\n        Args:\n            dataset (list): The dataset to partition. It should be a list of tuples where each\n                            tuple represents a data sample and its corresponding label.\n\n        Returns:\n            dict: A dictionary where keys are client IDs (ranging from 0 to partitions_number-1) and\n                    values are lists of indices corresponding to the samples assigned to each client.\n\n        The function ensures that each class is represented equally in each subset. The\n        partitioning process involves iterating over each class, shuffling the indices of that class,\n        and then splitting them equally among the clients. The function does not print the class\n        distribution in each subset.\n\n        Example usage:\n            federated_data = balanced_iid_partition(my_dataset)\n            # This creates federated data subsets with equal class distributions.\n        \"\"\"\n        num_clients = self.partitions_number\n        clients_data = {i: [] for i in range(num_clients)}\n\n        # Get the labels from the dataset\n        if isinstance(dataset.targets, np.ndarray):\n            labels = dataset.targets\n        elif hasattr(dataset.targets, \"numpy\"):  # Check if it's a tensor with .numpy() method\n            labels = dataset.targets.numpy()\n        else:  # If it's a list\n            labels = np.asarray(dataset.targets)\n\n        label_counts = np.bincount(labels)\n        min_label = label_counts.argmin()\n        min_count = label_counts[min_label]\n\n        for label in range(self.num_classes):\n            # Get the indices of the same label samples\n            label_indices = np.where(labels == label)[0]\n            np.random.seed(self.seed)\n            np.random.shuffle(label_indices)\n\n            # Split the data based on their labels\n            samples_per_client = min_count // num_clients\n\n            for i in range(num_clients):\n                start_idx = i * samples_per_client\n                end_idx = (i + 1) * samples_per_client\n                clients_data[i].extend(label_indices[start_idx:end_idx])\n\n        return clients_data\n\n    def unbalanced_iid_partition(self, dataset, imbalance_factor=2):\n        \"\"\"\n        Partition the dataset into multiple IID (Independent and Identically Distributed)\n        subsets with different size.\n\n        This function divides a dataset into a specified number of IID subsets (federated\n        clients), where each subset has a different number of samples. The number of samples\n        in each subset is determined by an imbalance factor, making the partition suitable\n        for simulating imbalanced data scenarios in federated learning.\n\n        Args:\n            dataset (list): The dataset to partition. It should be a list of tuples where\n                            each tuple represents a data sample and its corresponding label.\n            imbalance_factor (float): The factor to determine the degree of imbalance\n                                    among the subsets. A lower imbalance factor leads to more\n                                    imbalanced partitions.\n\n        Returns:\n            dict: A dictionary where keys are client IDs (ranging from 0 to partitions_number-1) and\n                    values are lists of indices corresponding to the samples assigned to each client.\n\n        The function ensures that each class is represented in each subset but with varying\n        proportions. The partitioning process involves iterating over each class, shuffling\n        the indices of that class, and then splitting them according to the calculated subset\n        sizes. The function does not print the class distribution in each subset.\n\n        Example usage:\n            federated_data = unbalanced_iid_partition(my_dataset, imbalance_factor=2)\n            # This creates federated data subsets with varying number of samples based on\n            # an imbalance factor of 2.\n        \"\"\"\n        num_clients = self.partitions_number\n        clients_data = {i: [] for i in range(num_clients)}\n\n        # Get the labels from the dataset\n        labels = np.array([dataset.targets[idx] for idx in range(len(dataset))])\n        label_counts = np.bincount(labels)\n\n        min_label = label_counts.argmin()\n        min_count = label_counts[min_label]\n\n        # Set the initial_subset_size\n        initial_subset_size = min_count // num_clients\n\n        # Calculate the number of samples for each subset based on the imbalance factor\n        subset_sizes = [initial_subset_size]\n        for i in range(1, num_clients):\n            subset_sizes.append(int(subset_sizes[i - 1] * ((imbalance_factor - 1) / imbalance_factor)))\n\n        for label in range(self.num_classes):\n            # Get the indices of the same label samples\n            label_indices = np.where(labels == label)[0]\n            np.random.seed(self.seed)\n            np.random.shuffle(label_indices)\n\n            # Split the data based on their labels\n            start = 0\n            for i in range(num_clients):\n                end = start + subset_sizes[i]\n                clients_data[i].extend(label_indices[start:end])\n                start = end\n\n        return clients_data\n\n    def percentage_partition(self, dataset, percentage=20):\n        \"\"\"\n        Partition a dataset into multiple subsets with a specified level of non-IID-ness.\n\n        Args:\n            dataset (torch.utils.data.Dataset): The dataset to partition. It should have\n                                                'data' and 'targets' attributes.\n            percentage (int): A value between 0 and 100 that specifies the desired\n                                level of non-IID-ness for the labels of the federated data.\n                                This percentage controls the imbalance in the class distribution\n                                across different subsets.\n\n        Returns:\n            dict: A dictionary where keys are subset indices (ranging from 0 to partitions_number-1)\n                and values are lists of indices corresponding to the samples in each subset.\n\n        Example usage:\n            federated_data = percentage_partition(my_dataset, percentage=20)\n            # This creates federated data subsets with varying class distributions based on\n            # a percentage of 20.\n        \"\"\"\n        if isinstance(dataset.targets, np.ndarray):\n            y_train = dataset.targets\n        elif hasattr(dataset.targets, \"numpy\"):  # Check if it's a tensor with .numpy() method\n            y_train = dataset.targets.numpy()\n        else:  # If it's a list\n            y_train = np.asarray(dataset.targets)\n\n        num_classes = self.num_classes\n        num_subsets = self.partitions_number\n        class_indices = {i: np.where(y_train == i)[0] for i in range(num_classes)}\n\n        # Get the labels from the dataset\n        labels = np.array([dataset.targets[idx] for idx in range(len(dataset))])\n        label_counts = np.bincount(labels)\n\n        min_label = label_counts.argmin()\n        min_count = label_counts[min_label]\n\n        classes_per_subset = int(num_classes * percentage / 100)\n        if classes_per_subset &lt; 1:\n            raise ValueError(\"The percentage is too low to assign at least one class to each subset.\")\n\n        subset_indices = [[] for _ in range(num_subsets)]\n        class_list = list(range(num_classes))\n        np.random.seed(self.seed)\n        np.random.shuffle(class_list)\n\n        for i in range(num_subsets):\n            for j in range(classes_per_subset):\n                # Use modulo operation to cycle through the class_list\n                class_idx = class_list[(i * classes_per_subset + j) % num_classes]\n                indices = class_indices[class_idx]\n                np.random.seed(self.seed)\n                np.random.shuffle(indices)\n                # Select approximately 50% of the indices\n                subset_indices[i].extend(indices[: min_count // 2])\n\n            class_counts = np.bincount(np.array([dataset.targets[idx] for idx in subset_indices[i]]))\n            logging.info(f\"Partition {i + 1} class distribution: {class_counts.tolist()}\")\n\n        partitioned_datasets = {i: subset_indices[i] for i in range(num_subsets)}\n\n        return partitioned_datasets\n\n    def plot_all_data_distribution(self, phase, dataset, partitions_map):\n        \"\"\"\n\n        Plot all of the data distribution of the dataset according to the partitions map provided.\n\n        Args:\n            dataset: The dataset to plot (torch.utils.data.Dataset).\n            partitions_map: The map of the dataset partitions.\n        \"\"\"\n        sns.set()\n        sns.set_style(\"whitegrid\", {\"axes.grid\": False})\n        sns.set_context(\"paper\", font_scale=1.5)\n        sns.set_palette(\"Set2\")\n\n        num_clients = len(partitions_map)\n        num_classes = self.num_classes\n\n        # Plot number of samples per class in the dataset\n        plt.figure(figsize=(12, 8))\n\n        class_counts = [0] * num_classes\n        for target in dataset.targets:\n            class_counts[target] += 1\n\n        plt.bar(range(num_classes), class_counts, tick_label=dataset.classes)\n        for j, count in enumerate(class_counts):\n            plt.text(j, count, str(count), ha=\"center\", va=\"bottom\", fontsize=12)\n        plt.title(f\"[{phase}] Number of samples per class in the dataset\")\n        plt.xlabel(\"Class\")\n        plt.ylabel(\"Number of samples\")\n        plt.tight_layout()\n\n        path_to_save_class_distribution = f\"{self.config_dir}/full_data_distribution_{'iid' if self.iid else 'non_iid'}{'_' + self.partition if not self.iid else ''}_{phase}.pdf\"\n        plt.savefig(path_to_save_class_distribution, dpi=300, bbox_inches=\"tight\")\n        plt.close()\n\n        # Plot distribution of samples across participants\n        plt.figure(figsize=(12, 8))\n\n        label_distribution = [[] for _ in range(num_classes)]\n        for c_id, idc in partitions_map.items():\n            for idx in idc:\n                label_distribution[dataset.targets[idx]].append(c_id)\n\n        plt.hist(\n            label_distribution,\n            stacked=True,\n            bins=np.arange(-0.5, num_clients + 1.5, 1),\n            label=dataset.classes,\n            rwidth=0.5,\n        )\n        plt.xticks(\n            np.arange(num_clients),\n            [\"Participant %d\" % (c_id + 1) for c_id in range(num_clients)],\n        )\n        plt.title(f\"[{phase}] Distribution of splited datasets\")\n        plt.xlabel(\"Participant\")\n        plt.ylabel(\"Number of samples\")\n        plt.xticks(range(num_clients), [f\" {i}\" for i in range(num_clients)])\n        plt.legend(loc=\"upper right\")\n        plt.tight_layout()\n\n        path_to_save = f\"{self.config_dir}/all_data_distribution_HIST_{'iid' if self.iid else 'non_iid'}{'_' + self.partition if not self.iid else ''}_{phase}.pdf\"\n        plt.savefig(path_to_save, dpi=300, bbox_inches=\"tight\")\n        plt.close()\n\n        plt.figure(figsize=(12, 8))\n        max_point_size = 1200\n        min_point_size = 0\n\n        for i in range(self.partitions_number):\n            class_counts = [0] * self.num_classes\n            indices = partitions_map[i]\n            for idx in indices:\n                label = dataset.targets[idx]\n                class_counts[label] += 1\n\n            # Normalize the point sizes for this partition, handling the case where max_samples_partition is 0\n            max_samples_partition = max(class_counts)\n            if max_samples_partition == 0:\n                logging.warning(f\"[{phase}] Participant {i} has no samples. Skipping size normalization.\")\n                sizes = [min_point_size] * self.num_classes\n            else:\n                sizes = [\n                    (size / max_samples_partition) * (max_point_size - min_point_size) + min_point_size\n                    for size in class_counts\n                ]\n            plt.scatter([i] * self.num_classes, range(self.num_classes), s=sizes, alpha=0.5)\n\n        plt.xlabel(\"Participant\")\n        plt.ylabel(\"Class\")\n        plt.xticks(range(self.partitions_number))\n        plt.yticks(range(self.num_classes))\n        plt.title(\n            f\"[{phase}] Data distribution across participants ({'IID' if self.iid else f'Non-IID - {self.partition}'} - {self.partition_parameter})\"\n        )\n        plt.tight_layout()\n\n        path_to_save = f\"{self.config_dir}/all_data_distribution_CIRCLES_{'iid' if self.iid else 'non_iid'}{'_' + self.partition if not self.iid else ''}_{phase}.pdf\"\n        plt.savefig(path_to_save, dpi=300, bbox_inches=\"tight\")\n        plt.close()\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.balanced_iid_partition","title":"<code>balanced_iid_partition(dataset)</code>","text":"<p>Partition the dataset into balanced and IID (Independent and Identically Distributed) subsets for each client.</p> <p>This function divides a dataset into a specified number of subsets (federated clients), where each subset has an equal class distribution. This makes the partition suitable for simulating IID data scenarios in federated learning.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>list</code> <p>The dataset to partition. It should be a list of tuples where each             tuple represents a data sample and its corresponding label.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary where keys are client IDs (ranging from 0 to partitions_number-1) and     values are lists of indices corresponding to the samples assigned to each client.</p> <p>The function ensures that each class is represented equally in each subset. The partitioning process involves iterating over each class, shuffling the indices of that class, and then splitting them equally among the clients. The function does not print the class distribution in each subset.</p> Example usage <p>federated_data = balanced_iid_partition(my_dataset)</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def balanced_iid_partition(self, dataset):\n    \"\"\"\n    Partition the dataset into balanced and IID (Independent and Identically Distributed)\n    subsets for each client.\n\n    This function divides a dataset into a specified number of subsets (federated clients),\n    where each subset has an equal class distribution. This makes the partition suitable for\n    simulating IID data scenarios in federated learning.\n\n    Args:\n        dataset (list): The dataset to partition. It should be a list of tuples where each\n                        tuple represents a data sample and its corresponding label.\n\n    Returns:\n        dict: A dictionary where keys are client IDs (ranging from 0 to partitions_number-1) and\n                values are lists of indices corresponding to the samples assigned to each client.\n\n    The function ensures that each class is represented equally in each subset. The\n    partitioning process involves iterating over each class, shuffling the indices of that class,\n    and then splitting them equally among the clients. The function does not print the class\n    distribution in each subset.\n\n    Example usage:\n        federated_data = balanced_iid_partition(my_dataset)\n        # This creates federated data subsets with equal class distributions.\n    \"\"\"\n    num_clients = self.partitions_number\n    clients_data = {i: [] for i in range(num_clients)}\n\n    # Get the labels from the dataset\n    if isinstance(dataset.targets, np.ndarray):\n        labels = dataset.targets\n    elif hasattr(dataset.targets, \"numpy\"):  # Check if it's a tensor with .numpy() method\n        labels = dataset.targets.numpy()\n    else:  # If it's a list\n        labels = np.asarray(dataset.targets)\n\n    label_counts = np.bincount(labels)\n    min_label = label_counts.argmin()\n    min_count = label_counts[min_label]\n\n    for label in range(self.num_classes):\n        # Get the indices of the same label samples\n        label_indices = np.where(labels == label)[0]\n        np.random.seed(self.seed)\n        np.random.shuffle(label_indices)\n\n        # Split the data based on their labels\n        samples_per_client = min_count // num_clients\n\n        for i in range(num_clients):\n            start_idx = i * samples_per_client\n            end_idx = (i + 1) * samples_per_client\n            clients_data[i].extend(label_indices[start_idx:end_idx])\n\n    return clients_data\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.balanced_iid_partition--this-creates-federated-data-subsets-with-equal-class-distributions","title":"This creates federated data subsets with equal class distributions.","text":""},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.clear","title":"<code>clear()</code>","text":"<p>Clear the dataset. This should remove or reset the dataset.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def clear(self):\n    \"\"\"\n    Clear the dataset. This should remove or reset the dataset.\n    \"\"\"\n    if self.train_set is not None and hasattr(self.train_set, \"close\"):\n        self.train_set.close()\n    if self.test_set is not None and hasattr(self.test_set, \"close\"):\n        self.test_set.close()\n\n    self.train_set = None\n    self.train_indices_map = None\n    self.test_set = None\n    self.test_indices_map = None\n    self.local_test_indices_map = None\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.data_partitioning","title":"<code>data_partitioning(plot=False)</code>","text":"<p>Perform the data partitioning.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def data_partitioning(self, plot=False):\n    \"\"\"\n    Perform the data partitioning.\n    \"\"\"\n\n    logging.info(\n        f\"Partitioning data for {self.__class__.__name__} | Partitions: {self.partitions_number} | IID: {self.iid} | Partition: {self.partition} | Partition parameter: {self.partition_parameter}\"\n    )\n\n    self.train_indices_map = (\n        self.generate_iid_map(self.train_set)\n        if self.iid\n        else self.generate_non_iid_map(self.train_set, self.partition, self.partition_parameter)\n    )\n    self.test_indices_map = self.get_test_indices_map()\n    self.local_test_indices_map = self.get_local_test_indices_map()\n\n    if plot:\n        self.plot_data_distribution(\"train\", self.train_set, self.train_indices_map)\n        self.plot_all_data_distribution(\"train\", self.train_set, self.train_indices_map)\n        self.plot_data_distribution(\"local_test\", self.test_set, self.local_test_indices_map)\n        self.plot_all_data_distribution(\"local_test\", self.test_set, self.local_test_indices_map)\n\n    self.save_partitions()\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.dirichlet_partition","title":"<code>dirichlet_partition(dataset, alpha=0.5, min_samples_size=50, balanced=False, max_iter=100, verbose=True)</code>","text":"<p>Partition the dataset among clients using a Dirichlet distribution. This function ensures each client gets at least min_samples_size samples.</p>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.dirichlet_partition--parameters","title":"Parameters","text":"<p>dataset : Dataset     The dataset to partition. Must have a 'targets' attribute. alpha : float, default=0.5     Concentration parameter for the Dirichlet distribution. min_samples_size : int, default=50     Minimum number of samples required in each partition. balanced : bool, default=False     If True, distribute each class evenly among clients.     Otherwise, allocate according to a Dirichlet distribution. max_iter : int, default=100     Maximum number of iterations to try finding a valid partition. verbose : bool, default=True     If True, print debug information per iteration.</p>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.dirichlet_partition--returns","title":"Returns","text":"<p>partitions : dict[int, list[int]]     Dictionary mapping each client index to a list of sample indices.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def dirichlet_partition(\n    self,\n    dataset: Any,\n    alpha: float = 0.5,\n    min_samples_size: int = 50,\n    balanced: bool = False,\n    max_iter: int = 100,\n    verbose: bool = True,\n) -&gt; dict[int, list[int]]:\n    \"\"\"\n    Partition the dataset among clients using a Dirichlet distribution.\n    This function ensures each client gets at least min_samples_size samples.\n\n    Parameters\n    ----------\n    dataset : Dataset\n        The dataset to partition. Must have a 'targets' attribute.\n    alpha : float, default=0.5\n        Concentration parameter for the Dirichlet distribution.\n    min_samples_size : int, default=50\n        Minimum number of samples required in each partition.\n    balanced : bool, default=False\n        If True, distribute each class evenly among clients.\n        Otherwise, allocate according to a Dirichlet distribution.\n    max_iter : int, default=100\n        Maximum number of iterations to try finding a valid partition.\n    verbose : bool, default=True\n        If True, print debug information per iteration.\n\n    Returns\n    -------\n    partitions : dict[int, list[int]]\n        Dictionary mapping each client index to a list of sample indices.\n    \"\"\"\n    # Extract targets and unique labels.\n    y_data = self._get_targets(dataset)\n    unique_labels = np.unique(y_data)\n\n    # For each class, get a shuffled list of indices.\n    class_indices = {}\n    base_rng = np.random.default_rng(self.seed)\n    for label in unique_labels:\n        idx = np.where(y_data == label)[0]\n        base_rng.shuffle(idx)\n        class_indices[label] = idx\n\n    # Prepare container for client indices.\n    indices_per_partition = [[] for _ in range(self.partitions_number)]\n\n    def allocate_for_label(label_idx: np.ndarray, rng: np.random.Generator) -&gt; np.ndarray:\n        num_label_samples = len(label_idx)\n        if balanced:\n            proportions = np.full(self.partitions_number, 1.0 / self.partitions_number)\n        else:\n            proportions = rng.dirichlet([alpha] * self.partitions_number)\n        sample_counts = (proportions * num_label_samples).astype(int)\n        remainder = num_label_samples - sample_counts.sum()\n        if remainder &gt; 0:\n            extra_indices = rng.choice(self.partitions_number, size=remainder, replace=False)\n            for idx in extra_indices:\n                sample_counts[idx] += 1\n        return sample_counts\n\n    for iteration in range(1, max_iter + 1):\n        rng = np.random.default_rng(self.seed + iteration)\n        temp_indices_per_partition = [[] for _ in range(self.partitions_number)]\n        for label in unique_labels:\n            label_idx = class_indices[label]\n            counts = allocate_for_label(label_idx, rng)\n            start = 0\n            for client_idx, count in enumerate(counts):\n                end = start + count\n                temp_indices_per_partition[client_idx].extend(label_idx[start:end])\n                start = end\n\n        client_sizes = [len(indices) for indices in temp_indices_per_partition]\n        if min(client_sizes) &gt;= min_samples_size:\n            indices_per_partition = temp_indices_per_partition\n            if verbose:\n                print(f\"Partition successful at iteration {iteration}. Client sizes: {client_sizes}\")\n            break\n        if verbose:\n            print(f\"Iteration {iteration}: client sizes {client_sizes}\")\n\n    else:\n        raise ValueError(\n            f\"Could not create partitions with at least {min_samples_size} samples per client after {max_iter} iterations.\"\n        )\n\n    initial_partition = {i: indices for i, indices in enumerate(indices_per_partition)}\n\n    final_partition = self.postprocess_partition(initial_partition, y_data)\n\n    return final_partition\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.generate_iid_map","title":"<code>generate_iid_map(dataset, plot=False)</code>  <code>abstractmethod</code>","text":"<p>Create an iid map of the dataset.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>@abstractmethod\ndef generate_iid_map(self, dataset, plot=False):\n    \"\"\"\n    Create an iid map of the dataset.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.generate_non_iid_map","title":"<code>generate_non_iid_map(dataset, partition='dirichlet', plot=False)</code>  <code>abstractmethod</code>","text":"<p>Create a non-iid map of the dataset.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>@abstractmethod\ndef generate_non_iid_map(self, dataset, partition=\"dirichlet\", plot=False):\n    \"\"\"\n    Create a non-iid map of the dataset.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.get_local_test_indices_map","title":"<code>get_local_test_indices_map()</code>","text":"<p>Get the indices of the local test set for each participant. Indices whose labels are the same as the training set are selected.</p> <p>Returns:</p> Type Description <p>A dictionary mapping participant_id to a list of indices.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def get_local_test_indices_map(self):\n    \"\"\"\n    Get the indices of the local test set for each participant.\n    Indices whose labels are the same as the training set are selected.\n\n    Returns:\n        A dictionary mapping participant_id to a list of indices.\n    \"\"\"\n    try:\n        local_test_indices_map = {}\n        test_targets = np.array(self.test_set.targets)\n        for participant_id in range(self.partitions_number):\n            train_labels = np.array([self.train_set.targets[idx] for idx in self.train_indices_map[participant_id]])\n            indices = np.where(np.isin(test_targets, train_labels))[0].tolist()\n            local_test_indices_map[participant_id] = indices\n        return local_test_indices_map\n    except Exception as e:\n        logging.exception(f\"Error in get_local_test_indices_map: {e}\")\n        raise\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.get_test_indices_map","title":"<code>get_test_indices_map()</code>","text":"<p>Get the indices of the test set for each participant.</p> <p>Returns:</p> Type Description <p>A dictionary mapping participant_id to a list of indices.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def get_test_indices_map(self):\n    \"\"\"\n    Get the indices of the test set for each participant.\n\n    Returns:\n        A dictionary mapping participant_id to a list of indices.\n    \"\"\"\n    try:\n        test_indices_map = {}\n        for participant_id in range(self.partitions_number):\n            test_indices_map[participant_id] = list(range(len(self.test_set)))\n        return test_indices_map\n    except Exception as e:\n        logging.exception(f\"Error in get_test_indices_map: {e}\")\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.homo_partition","title":"<code>homo_partition(dataset)</code>","text":"<p>Homogeneously partition the dataset into multiple subsets.</p> <p>This function divides a dataset into a specified number of subsets, where each subset is intended to have a roughly equal number of samples. This method aims to ensure a homogeneous distribution of data across all subsets. It's particularly useful in scenarios where a uniform distribution of data is desired among all federated learning clients.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>The dataset to partition. It should have                                 'data' and 'targets' attributes.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary where keys are subset indices (ranging from 0 to partitions_number-1) and values are lists of indices corresponding to the samples in each subset.</p> <p>The function randomly shuffles the entire dataset and then splits it into the number of subsets specified by <code>partitions_number</code>. It ensures that each subset has a similar number of samples. The function also prints the class distribution in each subset for reference.</p> Example usage <p>federated_data = homo_partition(my_dataset)</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def homo_partition(self, dataset):\n    \"\"\"\n    Homogeneously partition the dataset into multiple subsets.\n\n    This function divides a dataset into a specified number of subsets, where each subset\n    is intended to have a roughly equal number of samples. This method aims to ensure a\n    homogeneous distribution of data across all subsets. It's particularly useful in\n    scenarios where a uniform distribution of data is desired among all federated learning\n    clients.\n\n    Args:\n        dataset (torch.utils.data.Dataset): The dataset to partition. It should have\n                                            'data' and 'targets' attributes.\n\n    Returns:\n        dict: A dictionary where keys are subset indices (ranging from 0 to partitions_number-1)\n            and values are lists of indices corresponding to the samples in each subset.\n\n    The function randomly shuffles the entire dataset and then splits it into the number\n    of subsets specified by `partitions_number`. It ensures that each subset has a similar number\n    of samples. The function also prints the class distribution in each subset for reference.\n\n    Example usage:\n        federated_data = homo_partition(my_dataset)\n        # This creates federated data subsets with homogeneous distribution.\n    \"\"\"\n    n_nets = self.partitions_number\n\n    n_train = len(dataset.targets)\n    np.random.seed(self.seed)\n    idxs = np.random.permutation(n_train)\n    batch_idxs = np.array_split(idxs, n_nets)\n    net_dataidx_map = {i: batch_idxs[i] for i in range(n_nets)}\n\n    # partitioned_datasets = []\n    for i in range(self.partitions_number):\n        # subset = torch.utils.data.Subset(dataset, net_dataidx_map[i])\n        # partitioned_datasets.append(subset)\n\n        # Print class distribution in the current partition\n        class_counts = [0] * self.num_classes\n        for idx in net_dataidx_map[i]:\n            label = dataset.targets[idx]\n            class_counts[label] += 1\n        logging.info(f\"Partition {i + 1} class distribution: {class_counts}\")\n\n    return net_dataidx_map\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.homo_partition--this-creates-federated-data-subsets-with-homogeneous-distribution","title":"This creates federated data subsets with homogeneous distribution.","text":""},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.initialize_dataset","title":"<code>initialize_dataset()</code>  <code>abstractmethod</code>","text":"<p>Initialize the dataset. This should load or create the dataset.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>@abstractmethod\ndef initialize_dataset(self):\n    \"\"\"\n    Initialize the dataset. This should load or create the dataset.\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement this method.\")\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.percentage_partition","title":"<code>percentage_partition(dataset, percentage=20)</code>","text":"<p>Partition a dataset into multiple subsets with a specified level of non-IID-ness.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>The dataset to partition. It should have                                 'data' and 'targets' attributes.</p> required <code>percentage</code> <code>int</code> <p>A value between 0 and 100 that specifies the desired                 level of non-IID-ness for the labels of the federated data.                 This percentage controls the imbalance in the class distribution                 across different subsets.</p> <code>20</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary where keys are subset indices (ranging from 0 to partitions_number-1) and values are lists of indices corresponding to the samples in each subset.</p> Example usage <p>federated_data = percentage_partition(my_dataset, percentage=20)</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def percentage_partition(self, dataset, percentage=20):\n    \"\"\"\n    Partition a dataset into multiple subsets with a specified level of non-IID-ness.\n\n    Args:\n        dataset (torch.utils.data.Dataset): The dataset to partition. It should have\n                                            'data' and 'targets' attributes.\n        percentage (int): A value between 0 and 100 that specifies the desired\n                            level of non-IID-ness for the labels of the federated data.\n                            This percentage controls the imbalance in the class distribution\n                            across different subsets.\n\n    Returns:\n        dict: A dictionary where keys are subset indices (ranging from 0 to partitions_number-1)\n            and values are lists of indices corresponding to the samples in each subset.\n\n    Example usage:\n        federated_data = percentage_partition(my_dataset, percentage=20)\n        # This creates federated data subsets with varying class distributions based on\n        # a percentage of 20.\n    \"\"\"\n    if isinstance(dataset.targets, np.ndarray):\n        y_train = dataset.targets\n    elif hasattr(dataset.targets, \"numpy\"):  # Check if it's a tensor with .numpy() method\n        y_train = dataset.targets.numpy()\n    else:  # If it's a list\n        y_train = np.asarray(dataset.targets)\n\n    num_classes = self.num_classes\n    num_subsets = self.partitions_number\n    class_indices = {i: np.where(y_train == i)[0] for i in range(num_classes)}\n\n    # Get the labels from the dataset\n    labels = np.array([dataset.targets[idx] for idx in range(len(dataset))])\n    label_counts = np.bincount(labels)\n\n    min_label = label_counts.argmin()\n    min_count = label_counts[min_label]\n\n    classes_per_subset = int(num_classes * percentage / 100)\n    if classes_per_subset &lt; 1:\n        raise ValueError(\"The percentage is too low to assign at least one class to each subset.\")\n\n    subset_indices = [[] for _ in range(num_subsets)]\n    class_list = list(range(num_classes))\n    np.random.seed(self.seed)\n    np.random.shuffle(class_list)\n\n    for i in range(num_subsets):\n        for j in range(classes_per_subset):\n            # Use modulo operation to cycle through the class_list\n            class_idx = class_list[(i * classes_per_subset + j) % num_classes]\n            indices = class_indices[class_idx]\n            np.random.seed(self.seed)\n            np.random.shuffle(indices)\n            # Select approximately 50% of the indices\n            subset_indices[i].extend(indices[: min_count // 2])\n\n        class_counts = np.bincount(np.array([dataset.targets[idx] for idx in subset_indices[i]]))\n        logging.info(f\"Partition {i + 1} class distribution: {class_counts.tolist()}\")\n\n    partitioned_datasets = {i: subset_indices[i] for i in range(num_subsets)}\n\n    return partitioned_datasets\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.percentage_partition--this-creates-federated-data-subsets-with-varying-class-distributions-based-on","title":"This creates federated data subsets with varying class distributions based on","text":""},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.percentage_partition--a-percentage-of-20","title":"a percentage of 20.","text":""},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.plot_all_data_distribution","title":"<code>plot_all_data_distribution(phase, dataset, partitions_map)</code>","text":"<p>Plot all of the data distribution of the dataset according to the partitions map provided.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <p>The dataset to plot (torch.utils.data.Dataset).</p> required <code>partitions_map</code> <p>The map of the dataset partitions.</p> required Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def plot_all_data_distribution(self, phase, dataset, partitions_map):\n    \"\"\"\n\n    Plot all of the data distribution of the dataset according to the partitions map provided.\n\n    Args:\n        dataset: The dataset to plot (torch.utils.data.Dataset).\n        partitions_map: The map of the dataset partitions.\n    \"\"\"\n    sns.set()\n    sns.set_style(\"whitegrid\", {\"axes.grid\": False})\n    sns.set_context(\"paper\", font_scale=1.5)\n    sns.set_palette(\"Set2\")\n\n    num_clients = len(partitions_map)\n    num_classes = self.num_classes\n\n    # Plot number of samples per class in the dataset\n    plt.figure(figsize=(12, 8))\n\n    class_counts = [0] * num_classes\n    for target in dataset.targets:\n        class_counts[target] += 1\n\n    plt.bar(range(num_classes), class_counts, tick_label=dataset.classes)\n    for j, count in enumerate(class_counts):\n        plt.text(j, count, str(count), ha=\"center\", va=\"bottom\", fontsize=12)\n    plt.title(f\"[{phase}] Number of samples per class in the dataset\")\n    plt.xlabel(\"Class\")\n    plt.ylabel(\"Number of samples\")\n    plt.tight_layout()\n\n    path_to_save_class_distribution = f\"{self.config_dir}/full_data_distribution_{'iid' if self.iid else 'non_iid'}{'_' + self.partition if not self.iid else ''}_{phase}.pdf\"\n    plt.savefig(path_to_save_class_distribution, dpi=300, bbox_inches=\"tight\")\n    plt.close()\n\n    # Plot distribution of samples across participants\n    plt.figure(figsize=(12, 8))\n\n    label_distribution = [[] for _ in range(num_classes)]\n    for c_id, idc in partitions_map.items():\n        for idx in idc:\n            label_distribution[dataset.targets[idx]].append(c_id)\n\n    plt.hist(\n        label_distribution,\n        stacked=True,\n        bins=np.arange(-0.5, num_clients + 1.5, 1),\n        label=dataset.classes,\n        rwidth=0.5,\n    )\n    plt.xticks(\n        np.arange(num_clients),\n        [\"Participant %d\" % (c_id + 1) for c_id in range(num_clients)],\n    )\n    plt.title(f\"[{phase}] Distribution of splited datasets\")\n    plt.xlabel(\"Participant\")\n    plt.ylabel(\"Number of samples\")\n    plt.xticks(range(num_clients), [f\" {i}\" for i in range(num_clients)])\n    plt.legend(loc=\"upper right\")\n    plt.tight_layout()\n\n    path_to_save = f\"{self.config_dir}/all_data_distribution_HIST_{'iid' if self.iid else 'non_iid'}{'_' + self.partition if not self.iid else ''}_{phase}.pdf\"\n    plt.savefig(path_to_save, dpi=300, bbox_inches=\"tight\")\n    plt.close()\n\n    plt.figure(figsize=(12, 8))\n    max_point_size = 1200\n    min_point_size = 0\n\n    for i in range(self.partitions_number):\n        class_counts = [0] * self.num_classes\n        indices = partitions_map[i]\n        for idx in indices:\n            label = dataset.targets[idx]\n            class_counts[label] += 1\n\n        # Normalize the point sizes for this partition, handling the case where max_samples_partition is 0\n        max_samples_partition = max(class_counts)\n        if max_samples_partition == 0:\n            logging.warning(f\"[{phase}] Participant {i} has no samples. Skipping size normalization.\")\n            sizes = [min_point_size] * self.num_classes\n        else:\n            sizes = [\n                (size / max_samples_partition) * (max_point_size - min_point_size) + min_point_size\n                for size in class_counts\n            ]\n        plt.scatter([i] * self.num_classes, range(self.num_classes), s=sizes, alpha=0.5)\n\n    plt.xlabel(\"Participant\")\n    plt.ylabel(\"Class\")\n    plt.xticks(range(self.partitions_number))\n    plt.yticks(range(self.num_classes))\n    plt.title(\n        f\"[{phase}] Data distribution across participants ({'IID' if self.iid else f'Non-IID - {self.partition}'} - {self.partition_parameter})\"\n    )\n    plt.tight_layout()\n\n    path_to_save = f\"{self.config_dir}/all_data_distribution_CIRCLES_{'iid' if self.iid else 'non_iid'}{'_' + self.partition if not self.iid else ''}_{phase}.pdf\"\n    plt.savefig(path_to_save, dpi=300, bbox_inches=\"tight\")\n    plt.close()\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.plot_data_distribution","title":"<code>plot_data_distribution(phase, dataset, partitions_map)</code>","text":"<p>Plot the data distribution of the dataset.</p> <p>Plot the data distribution of the dataset according to the partitions map provided.</p> <p>Parameters:</p> Name Type Description Default <code>phase</code> <p>The phase of the dataset (train, test, local_test).</p> required <code>dataset</code> <p>The dataset to plot (torch.utils.data.Dataset).</p> required <code>partitions_map</code> <p>The map of the dataset partitions.</p> required Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def plot_data_distribution(self, phase, dataset, partitions_map):\n    \"\"\"\n    Plot the data distribution of the dataset.\n\n    Plot the data distribution of the dataset according to the partitions map provided.\n\n    Args:\n        phase: The phase of the dataset (train, test, local_test).\n        dataset: The dataset to plot (torch.utils.data.Dataset).\n        partitions_map: The map of the dataset partitions.\n    \"\"\"\n    logging_training.info(f\"Plotting data distribution for {phase} dataset\")\n    # Plot the data distribution of the dataset, one graph per partition\n    sns.set()\n    sns.set_style(\"whitegrid\", {\"axes.grid\": False})\n    sns.set_context(\"paper\", font_scale=1.5)\n    sns.set_palette(\"Set2\")\n\n    # Plot bar charts for each partition\n    partition_index = 0\n    for partition_index in range(self.partitions_number):\n        indices = partitions_map[partition_index]\n        class_counts = [0] * self.num_classes\n        for idx in indices:\n            label = dataset.targets[idx]\n            class_counts[label] += 1\n\n        logging_training.info(f\"[{phase}] Participant {partition_index} total samples: {len(indices)}\")\n        logging_training.info(f\"[{phase}] Participant {partition_index} data distribution: {class_counts}\")\n\n        plt.figure(figsize=(12, 8))\n        plt.bar(range(self.num_classes), class_counts)\n        for j, count in enumerate(class_counts):\n            plt.text(j, count, str(count), ha=\"center\", va=\"bottom\", fontsize=12)\n        plt.xlabel(\"Class\")\n        plt.ylabel(\"Number of samples\")\n        plt.xticks(range(self.num_classes))\n        plt.title(\n            f\"[{phase}] Participant {partition_index} data distribution ({'IID' if self.iid else f'Non-IID - {self.partition}'} - {self.partition_parameter})\"\n        )\n        plt.tight_layout()\n        path_to_save = f\"{self.config_dir}/participant_{partition_index}_data_distribution_{'iid' if self.iid else 'non_iid'}{'_' + self.partition if not self.iid else ''}_{phase}.pdf\"\n        logging_training.info(f\"Saving data distribution for participant {partition_index} to {path_to_save}\")\n        plt.savefig(path_to_save, dpi=300, bbox_inches=\"tight\")\n        plt.close()\n\n    if hasattr(self, \"tsne\") and self.tsne:\n        self.visualize_tsne(dataset)\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.postprocess_partition","title":"<code>postprocess_partition(partition, y_data, min_samples_per_class=10)</code>","text":"<p>Post-process a partition to remove (and reassign) classes with too few samples per client.</p> <p>For each class: - For clients with a count &gt; 0 but below min_samples_per_class, remove those samples. - Reassign the removed samples to the client that already has the maximum count for that class.</p>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.postprocess_partition--parameters","title":"Parameters","text":"<p>partition : dict[int, list[int]]     The initial partition mapping client indices to sample indices. y_data : np.ndarray     The array of labels corresponding to the dataset samples. min_samples_per_class : int, default=10     The minimum acceptable number of samples per class for each client.</p>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.postprocess_partition--returns","title":"Returns","text":"<p>new_partition : dict[int, list[int]]     The updated partition.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def postprocess_partition(\n    self, partition: dict[int, list[int]], y_data: np.ndarray, min_samples_per_class: int = 10\n) -&gt; dict[int, list[int]]:\n    \"\"\"\n    Post-process a partition to remove (and reassign) classes with too few samples per client.\n\n    For each class:\n    - For clients with a count &gt; 0 but below min_samples_per_class, remove those samples.\n    - Reassign the removed samples to the client that already has the maximum count for that class.\n\n    Parameters\n    ----------\n    partition : dict[int, list[int]]\n        The initial partition mapping client indices to sample indices.\n    y_data : np.ndarray\n        The array of labels corresponding to the dataset samples.\n    min_samples_per_class : int, default=10\n        The minimum acceptable number of samples per class for each client.\n\n    Returns\n    -------\n    new_partition : dict[int, list[int]]\n        The updated partition.\n    \"\"\"\n    # Copy partition so we can modify it.\n    new_partition = {client: list(indices) for client, indices in partition.items()}\n\n    # Iterate over each class.\n    for label in np.unique(y_data):\n        # For each client, count how many samples of this label exist.\n        client_counts = {}\n        for client, indices in new_partition.items():\n            client_counts[client] = np.sum(np.array(y_data)[indices] == label)\n\n        # Identify clients with fewer than min_samples_per_class but nonzero counts.\n        donors = [client for client, count in client_counts.items() if 0 &lt; count &lt; min_samples_per_class]\n        # Identify potential recipients: those with at least min_samples_per_class.\n        recipients = [client for client, count in client_counts.items() if count &gt;= min_samples_per_class]\n        # If no client meets the threshold, choose the one with the highest count.\n        if not recipients:\n            best_recipient = max(client_counts, key=client_counts.get)\n            recipients = [best_recipient]\n        # Choose the recipient with the maximum count.\n        best_recipient = max(recipients, key=lambda c: client_counts[c])\n\n        # For each donor, remove samples of this label and reassign them.\n        for donor in donors:\n            donor_indices = new_partition[donor]\n            # Identify indices corresponding to this label.\n            donor_label_indices = [idx for idx in donor_indices if y_data[idx] == label]\n            # Remove these from the donor.\n            new_partition[donor] = [idx for idx in donor_indices if y_data[idx] != label]\n            # Add these to the best recipient.\n            new_partition[best_recipient].extend(donor_label_indices)\n\n    return new_partition\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.save_partitions","title":"<code>save_partitions()</code>","text":"<p>Save each partition data (train, test, and local test) to separate pickle files. The controller saves one file per partition for each data split.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def save_partitions(self):\n    \"\"\"\n    Save each partition data (train, test, and local test) to separate pickle files.\n    The controller saves one file per partition for each data split.\n    \"\"\"\n    try:\n        logging.info(f\"Saving partitions data for ALL participants ({self.partitions_number}) in {self.config_dir}\")\n        path = self.config_dir\n        if not os.path.exists(path):\n            raise FileNotFoundError(f\"Path {path} does not exist\")\n        # Check that the partition maps have the expected number of partitions\n        if not (\n            len(self.train_indices_map)\n            == len(self.test_indices_map)\n            == len(self.local_test_indices_map)\n            == self.partitions_number\n        ):\n            raise ValueError(\"One of the partition maps has an unexpected length.\")\n\n        # Save global test data\n        file_name = os.path.join(path, \"global_test.h5\")\n        with h5py.File(file_name, \"w\") as f:\n            indices = list(range(len(self.test_set)))\n            test_data = [self.test_set[i] for i in indices]\n            self.save_partition(test_data, f, \"test_data\")\n            f[\"test_data\"].attrs[\"num_classes\"] = self.num_classes\n            test_targets = np.array(self.test_set.targets)\n            f.create_dataset(\"test_targets\", data=test_targets, compression=\"gzip\")\n\n        for participant in range(self.partitions_number):\n            file_name = os.path.join(path, f\"participant_{participant}_train.h5\")\n            with h5py.File(file_name, \"w\") as f:\n                logging.info(f\"Saving training data for participant {participant} in {file_name}\")\n                indices = self.train_indices_map[participant]\n                train_data = [self.train_set[i] for i in indices]\n                self.save_partition(train_data, f, \"train_data\")\n                f[\"train_data\"].attrs[\"num_classes\"] = self.num_classes\n                train_targets = np.array([self.train_set.targets[i] for i in indices])\n                f.create_dataset(\"train_targets\", data=train_targets, compression=\"gzip\")\n                logging.info(f\"Partition saved for participant {participant}.\")\n\n        logging.info(\"Successfully saved all partition files.\")\n\n    except Exception as e:\n        logging.exception(f\"Error in save_partitions: {e}\")\n\n    finally:\n        self.clear()\n        logging.info(\"Cleared dataset after saving partitions.\")\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.unbalanced_iid_partition","title":"<code>unbalanced_iid_partition(dataset, imbalance_factor=2)</code>","text":"<p>Partition the dataset into multiple IID (Independent and Identically Distributed) subsets with different size.</p> <p>This function divides a dataset into a specified number of IID subsets (federated clients), where each subset has a different number of samples. The number of samples in each subset is determined by an imbalance factor, making the partition suitable for simulating imbalanced data scenarios in federated learning.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>list</code> <p>The dataset to partition. It should be a list of tuples where             each tuple represents a data sample and its corresponding label.</p> required <code>imbalance_factor</code> <code>float</code> <p>The factor to determine the degree of imbalance                     among the subsets. A lower imbalance factor leads to more                     imbalanced partitions.</p> <code>2</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary where keys are client IDs (ranging from 0 to partitions_number-1) and     values are lists of indices corresponding to the samples assigned to each client.</p> <p>The function ensures that each class is represented in each subset but with varying proportions. The partitioning process involves iterating over each class, shuffling the indices of that class, and then splitting them according to the calculated subset sizes. The function does not print the class distribution in each subset.</p> Example usage <p>federated_data = unbalanced_iid_partition(my_dataset, imbalance_factor=2)</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def unbalanced_iid_partition(self, dataset, imbalance_factor=2):\n    \"\"\"\n    Partition the dataset into multiple IID (Independent and Identically Distributed)\n    subsets with different size.\n\n    This function divides a dataset into a specified number of IID subsets (federated\n    clients), where each subset has a different number of samples. The number of samples\n    in each subset is determined by an imbalance factor, making the partition suitable\n    for simulating imbalanced data scenarios in federated learning.\n\n    Args:\n        dataset (list): The dataset to partition. It should be a list of tuples where\n                        each tuple represents a data sample and its corresponding label.\n        imbalance_factor (float): The factor to determine the degree of imbalance\n                                among the subsets. A lower imbalance factor leads to more\n                                imbalanced partitions.\n\n    Returns:\n        dict: A dictionary where keys are client IDs (ranging from 0 to partitions_number-1) and\n                values are lists of indices corresponding to the samples assigned to each client.\n\n    The function ensures that each class is represented in each subset but with varying\n    proportions. The partitioning process involves iterating over each class, shuffling\n    the indices of that class, and then splitting them according to the calculated subset\n    sizes. The function does not print the class distribution in each subset.\n\n    Example usage:\n        federated_data = unbalanced_iid_partition(my_dataset, imbalance_factor=2)\n        # This creates federated data subsets with varying number of samples based on\n        # an imbalance factor of 2.\n    \"\"\"\n    num_clients = self.partitions_number\n    clients_data = {i: [] for i in range(num_clients)}\n\n    # Get the labels from the dataset\n    labels = np.array([dataset.targets[idx] for idx in range(len(dataset))])\n    label_counts = np.bincount(labels)\n\n    min_label = label_counts.argmin()\n    min_count = label_counts[min_label]\n\n    # Set the initial_subset_size\n    initial_subset_size = min_count // num_clients\n\n    # Calculate the number of samples for each subset based on the imbalance factor\n    subset_sizes = [initial_subset_size]\n    for i in range(1, num_clients):\n        subset_sizes.append(int(subset_sizes[i - 1] * ((imbalance_factor - 1) / imbalance_factor)))\n\n    for label in range(self.num_classes):\n        # Get the indices of the same label samples\n        label_indices = np.where(labels == label)[0]\n        np.random.seed(self.seed)\n        np.random.shuffle(label_indices)\n\n        # Split the data based on their labels\n        start = 0\n        for i in range(num_clients):\n            end = start + subset_sizes[i]\n            clients_data[i].extend(label_indices[start:end])\n            start = end\n\n    return clients_data\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.unbalanced_iid_partition--this-creates-federated-data-subsets-with-varying-number-of-samples-based-on","title":"This creates federated data subsets with varying number of samples based on","text":""},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaDataset.unbalanced_iid_partition--an-imbalance-factor-of-2","title":"an imbalance factor of 2.","text":""},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaPartition","title":"<code>NebulaPartition</code>","text":"<p>A class to handle the partitioning of datasets for federated learning.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>class NebulaPartition:\n    \"\"\"\n    A class to handle the partitioning of datasets for federated learning.\n    \"\"\"\n\n    def __init__(self, handler: NebulaPartitionHandler, config: dict[str, Any] | None = None):\n        self.handler = handler\n        self.config = config if config is not None else {}\n\n        self.train_set = None\n        self.train_indices = None\n\n        self.test_set = None\n        self.test_indices = None\n\n        self.local_test_set = None\n        self.local_test_indices = None\n\n        enable_deterministic(seed=self.config.participant[\"scenario_args\"][\"random_seed\"])\n\n    def get_train_indices(self):\n        \"\"\"\n        Get the indices of the training set based on the indices map.\n        \"\"\"\n        if self.train_indices is None:\n            return None\n        return self.train_indices\n\n    def get_test_indices(self):\n        \"\"\"\n        Get the indices of the test set based on the indices map.\n        \"\"\"\n        if self.test_indices is None:\n            return None\n        return self.test_indices\n\n    def get_local_test_indices(self):\n        \"\"\"\n        Get the indices of the local test set based on the indices map.\n        \"\"\"\n        if self.local_test_indices is None:\n            return None\n        return self.local_test_indices\n\n    def get_train_labels(self):\n        \"\"\"\n        Get the labels of the training set based on the indices map.\n        \"\"\"\n        if self.train_indices is None:\n            return None\n        return [self.train_set.targets[idx] for idx in self.train_indices]\n\n    def get_test_labels(self):\n        \"\"\"\n        Get the labels of the test set based on the indices map.\n        \"\"\"\n        if self.test_indices is None:\n            return None\n        return [self.test_set.targets[idx] for idx in self.test_indices]\n\n    def get_local_test_labels(self):\n        \"\"\"\n        Get the labels of the test set based on the indices map.\n        \"\"\"\n        if self.local_test_indices is None:\n            return None\n        return [self.test_set.targets[idx] for idx in self.local_test_indices]\n\n    def set_local_test_indices(self):\n        \"\"\"\n        Set the local test indices for the current node.\n        \"\"\"\n        test_labels = self.get_test_labels()\n        train_labels = self.get_train_labels()\n\n        if test_labels is None or train_labels is None:\n            logging_training.warning(\"Either test_labels or train_labels is None in set_local_test_indices\")\n            return []\n\n        if self.test_set is None:\n            logging_training.warning(\"test_set is None in set_local_test_indices\")\n            return []\n\n        return [idx for idx in range(len(self.test_set)) if test_labels[idx] in train_labels]\n\n    def log_partition(self):\n        logging_training.info(f\"{'=' * 10}\")\n        logging_training.info(\n            f\"LOG NEBULA PARTITION DATASET [Participant {self.config.participant['device_args']['idx']}]\"\n        )\n        logging_training.info(f\"{'=' * 10}\")\n        logging_training.info(f\"TRAIN - Train labels unique: {set(self.get_train_labels())}\")\n        logging_training.info(f\"TRAIN - Length of train indices map: {len(self.get_train_indices())}\")\n        logging_training.info(f\"{'=' * 10}\")\n        logging_training.info(f\"LOCAL - Test labels unique: {set(self.get_local_test_labels())}\")\n        logging_training.info(f\"LOCAL - Length of test indices map: {len(self.get_local_test_indices())}\")\n        logging_training.info(f\"{'=' * 10}\")\n        logging_training.info(f\"GLOBAL - Test labels unique: {set(self.get_test_labels())}\")\n        logging_training.info(f\"GLOBAL - Length of test indices map: {len(self.get_test_indices())}\")\n        logging_training.info(f\"{'=' * 10}\")\n\n    def load_partition(self):\n        \"\"\"\n        Load only the partition data corresponding to the current node.\n        The node loads its train, test, and local test partition data from HDF5 files.\n        \"\"\"\n        try:\n            p = self.config.participant[\"device_args\"][\"idx\"]\n            logging_training.info(f\"Loading partition data for participant {p}\")\n            path = self.config.participant[\"tracking_args\"][\"config_dir\"]\n\n            train_partition_file = os.path.join(path, f\"participant_{p}_train.h5\")\n            wait_for_file(train_partition_file)\n            logging_training.info(f\"Loading train data from {train_partition_file}\")\n            self.train_set = self.handler(train_partition_file, \"train\", config=self.config)\n            self.train_indices = list(range(len(self.train_set)))\n\n            test_partition_file = os.path.join(path, \"global_test.h5\")\n            wait_for_file(test_partition_file)\n            logging_training.info(f\"Loading test data from {test_partition_file}\")\n            self.test_set = self.handler(test_partition_file, \"test\", config=self.config)\n            self.test_indices = list(range(len(self.test_set)))\n\n            self.local_test_set = self.handler(\n                test_partition_file, \"local_test\", config=self.config, empty=True\n            )\n            self.local_test_set.set_data(self.test_set.data, self.test_set.targets)\n            self.local_test_indices = self.set_local_test_indices()\n\n            logging_training.info(f\"Successfully loaded partition data for participant {p}.\")\n        except Exception as e:\n            logging_training.error(f\"Error loading partition: {e}\")\n            raise\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaPartition.get_local_test_indices","title":"<code>get_local_test_indices()</code>","text":"<p>Get the indices of the local test set based on the indices map.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def get_local_test_indices(self):\n    \"\"\"\n    Get the indices of the local test set based on the indices map.\n    \"\"\"\n    if self.local_test_indices is None:\n        return None\n    return self.local_test_indices\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaPartition.get_local_test_labels","title":"<code>get_local_test_labels()</code>","text":"<p>Get the labels of the test set based on the indices map.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def get_local_test_labels(self):\n    \"\"\"\n    Get the labels of the test set based on the indices map.\n    \"\"\"\n    if self.local_test_indices is None:\n        return None\n    return [self.test_set.targets[idx] for idx in self.local_test_indices]\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaPartition.get_test_indices","title":"<code>get_test_indices()</code>","text":"<p>Get the indices of the test set based on the indices map.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def get_test_indices(self):\n    \"\"\"\n    Get the indices of the test set based on the indices map.\n    \"\"\"\n    if self.test_indices is None:\n        return None\n    return self.test_indices\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaPartition.get_test_labels","title":"<code>get_test_labels()</code>","text":"<p>Get the labels of the test set based on the indices map.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def get_test_labels(self):\n    \"\"\"\n    Get the labels of the test set based on the indices map.\n    \"\"\"\n    if self.test_indices is None:\n        return None\n    return [self.test_set.targets[idx] for idx in self.test_indices]\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaPartition.get_train_indices","title":"<code>get_train_indices()</code>","text":"<p>Get the indices of the training set based on the indices map.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def get_train_indices(self):\n    \"\"\"\n    Get the indices of the training set based on the indices map.\n    \"\"\"\n    if self.train_indices is None:\n        return None\n    return self.train_indices\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaPartition.get_train_labels","title":"<code>get_train_labels()</code>","text":"<p>Get the labels of the training set based on the indices map.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def get_train_labels(self):\n    \"\"\"\n    Get the labels of the training set based on the indices map.\n    \"\"\"\n    if self.train_indices is None:\n        return None\n    return [self.train_set.targets[idx] for idx in self.train_indices]\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaPartition.load_partition","title":"<code>load_partition()</code>","text":"<p>Load only the partition data corresponding to the current node. The node loads its train, test, and local test partition data from HDF5 files.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def load_partition(self):\n    \"\"\"\n    Load only the partition data corresponding to the current node.\n    The node loads its train, test, and local test partition data from HDF5 files.\n    \"\"\"\n    try:\n        p = self.config.participant[\"device_args\"][\"idx\"]\n        logging_training.info(f\"Loading partition data for participant {p}\")\n        path = self.config.participant[\"tracking_args\"][\"config_dir\"]\n\n        train_partition_file = os.path.join(path, f\"participant_{p}_train.h5\")\n        wait_for_file(train_partition_file)\n        logging_training.info(f\"Loading train data from {train_partition_file}\")\n        self.train_set = self.handler(train_partition_file, \"train\", config=self.config)\n        self.train_indices = list(range(len(self.train_set)))\n\n        test_partition_file = os.path.join(path, \"global_test.h5\")\n        wait_for_file(test_partition_file)\n        logging_training.info(f\"Loading test data from {test_partition_file}\")\n        self.test_set = self.handler(test_partition_file, \"test\", config=self.config)\n        self.test_indices = list(range(len(self.test_set)))\n\n        self.local_test_set = self.handler(\n            test_partition_file, \"local_test\", config=self.config, empty=True\n        )\n        self.local_test_set.set_data(self.test_set.data, self.test_set.targets)\n        self.local_test_indices = self.set_local_test_indices()\n\n        logging_training.info(f\"Successfully loaded partition data for participant {p}.\")\n    except Exception as e:\n        logging_training.error(f\"Error loading partition: {e}\")\n        raise\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaPartition.set_local_test_indices","title":"<code>set_local_test_indices()</code>","text":"<p>Set the local test indices for the current node.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def set_local_test_indices(self):\n    \"\"\"\n    Set the local test indices for the current node.\n    \"\"\"\n    test_labels = self.get_test_labels()\n    train_labels = self.get_train_labels()\n\n    if test_labels is None or train_labels is None:\n        logging_training.warning(\"Either test_labels or train_labels is None in set_local_test_indices\")\n        return []\n\n    if self.test_set is None:\n        logging_training.warning(\"test_set is None in set_local_test_indices\")\n        return []\n\n    return [idx for idx in range(len(self.test_set)) if test_labels[idx] in train_labels]\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaPartitionHandler","title":"<code>NebulaPartitionHandler</code>","text":"<p>               Bases: <code>Dataset</code>, <code>ABC</code></p> <p>A class to handle the loading of datasets from HDF5 files.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>class NebulaPartitionHandler(Dataset, ABC):\n    \"\"\"\n    A class to handle the loading of datasets from HDF5 files.\n    \"\"\"\n\n    def __init__(\n        self,\n        file_path: str,\n        prefix: str = \"train\",\n        config: dict[str, Any] | None = None,\n        empty: bool = False,\n    ):\n        self.file_path = file_path\n        self.prefix = prefix\n        self.config = config\n        self.empty = empty\n        self.transform = None\n        self.target_transform = None\n        self.file = None\n\n        self.data = None\n        self.targets = None\n        self.num_classes = None\n        self.length = None\n\n        self.load_data()\n\n    def load_data(self):\n        if self.empty:\n            logging_training.info(\n                f\"[NebulaPartitionHandler] No data loaded for {self.prefix} partition. Empty dataset.\"\n            )\n            return\n        with h5py.File(self.file_path, \"r\") as f:\n            prefix = (\n                \"test\" if self.prefix == \"local_test\" else self.prefix\n            )  # Local test uses the test prefix (same data but different split)\n            self.data = self.load_partition(f, f\"{prefix}_data\")\n            self.targets = np.array(f[f\"{prefix}_targets\"])\n            self.num_classes = f[f\"{prefix}_data\"].attrs.get(\"num_classes\", 0)\n            self.length = len(self.data)\n        logging_training.info(\n            f\"[NebulaPartitionHandler] [{self.prefix}] Loaded {self.length} samples from {self.file_path} and {self.num_classes} classes.\"\n        )\n\n    def close(self):\n        if self.file is not None:\n            self.file.close()\n            self.file = None\n            logging_training.info(f\"[NebulaPartitionHandler] Closed file {self.file_path}\")\n\n    def __del__(self):\n        self.close()\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, idx):\n        data = self.data[idx]\n        target = self.targets[idx]\n        return data, target\n\n    def set_data(self, data, targets, data_opt=None, targets_opt=None):\n        \"\"\"\n        Set the data and targets for the dataset.\n        \"\"\"\n        try:\n            # Input validation\n            if data is None or targets is None:\n                raise ValueError(\"Primary data and targets cannot be None\")\n\n            if len(data) != len(targets):\n                raise ValueError(f\"Data and targets length mismatch: {len(data)} vs {len(targets)}\")\n\n            if data_opt is None or targets_opt is None:\n                self.data = data\n                self.targets = targets\n                self.length = len(data)\n                logging_training.info(f\"[NebulaPartitionHandler] Set data with {self.length} samples.\")\n                return\n\n            if len(data_opt) != len(targets_opt):\n                raise ValueError(f\"Optional data and targets length mismatch: {len(data_opt)} vs {len(targets_opt)}\")\n\n            main_count = int(len(data) * 0.8)\n            opt_count = min(len(data_opt), int(len(data) * (1 - 0.8)))\n            if isinstance(data, np.ndarray):\n                self.data = np.concatenate((data[:main_count], data_opt[:opt_count]))\n            else:\n                self.data = data[:main_count] + data_opt[:opt_count]\n\n            if isinstance(targets, np.ndarray):\n                self.targets = np.concatenate((targets[:main_count], targets_opt[:opt_count]))\n            else:\n                self.targets = targets[:main_count] + targets_opt[:opt_count]\n            self.length = len(self.data)\n\n        except Exception as e:\n            logging_training.exception(f\"Error setting data: {e}\")\n\n    def load_partition(self, file, name):\n        item = file[name]\n        if isinstance(item, h5py.Dataset):\n            typ = item.attrs.get(\"__type__\", None)\n            if typ == \"pickle\":\n                logging_training.info(f\"Loading pickled object from {name}\")\n                return pickle.loads(item[()].tobytes())\n            else:\n                logging_training.warning(f\"[NebulaPartitionHandler] Unknown type encountered: {typ} for item {name}\")\n                return item[()]\n        else:\n            logging_training.warning(f\"[NebulaPartitionHandler] Unknown item encountered: {item} for item {name}\")\n            return item[()]\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.NebulaPartitionHandler.set_data","title":"<code>set_data(data, targets, data_opt=None, targets_opt=None)</code>","text":"<p>Set the data and targets for the dataset.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def set_data(self, data, targets, data_opt=None, targets_opt=None):\n    \"\"\"\n    Set the data and targets for the dataset.\n    \"\"\"\n    try:\n        # Input validation\n        if data is None or targets is None:\n            raise ValueError(\"Primary data and targets cannot be None\")\n\n        if len(data) != len(targets):\n            raise ValueError(f\"Data and targets length mismatch: {len(data)} vs {len(targets)}\")\n\n        if data_opt is None or targets_opt is None:\n            self.data = data\n            self.targets = targets\n            self.length = len(data)\n            logging_training.info(f\"[NebulaPartitionHandler] Set data with {self.length} samples.\")\n            return\n\n        if len(data_opt) != len(targets_opt):\n            raise ValueError(f\"Optional data and targets length mismatch: {len(data_opt)} vs {len(targets_opt)}\")\n\n        main_count = int(len(data) * 0.8)\n        opt_count = min(len(data_opt), int(len(data) * (1 - 0.8)))\n        if isinstance(data, np.ndarray):\n            self.data = np.concatenate((data[:main_count], data_opt[:opt_count]))\n        else:\n            self.data = data[:main_count] + data_opt[:opt_count]\n\n        if isinstance(targets, np.ndarray):\n            self.targets = np.concatenate((targets[:main_count], targets_opt[:opt_count]))\n        else:\n            self.targets = targets[:main_count] + targets_opt[:opt_count]\n        self.length = len(self.data)\n\n    except Exception as e:\n        logging_training.exception(f\"Error setting data: {e}\")\n</code></pre>"},{"location":"api/core/datasets/nebuladataset/#nebula.core.datasets.nebuladataset.wait_for_file","title":"<code>wait_for_file(file_path)</code>","text":"<p>Wait until the given file exists, polling every 'interval' seconds.</p> Source code in <code>nebula/core/datasets/nebuladataset.py</code> <pre><code>def wait_for_file(file_path):\n    \"\"\"Wait until the given file exists, polling every 'interval' seconds.\"\"\"\n    while not os.path.exists(file_path):\n        logging_training.info(f\"Waiting for file: {file_path}\")\n    return\n</code></pre>"},{"location":"api/core/datasets/cifar10/","title":"Documentation for Cifar10 Module","text":""},{"location":"api/core/datasets/cifar10/cifar10/","title":"Documentation for Cifar10 Module","text":""},{"location":"api/core/datasets/cifar100/","title":"Documentation for Cifar100 Module","text":""},{"location":"api/core/datasets/cifar100/cifar100/","title":"Documentation for Cifar100 Module","text":""},{"location":"api/core/datasets/emnist/","title":"Documentation for Emnist Module","text":""},{"location":"api/core/datasets/emnist/emnist/","title":"Documentation for Emnist Module","text":""},{"location":"api/core/datasets/fashionmnist/","title":"Documentation for Fashionmnist Module","text":""},{"location":"api/core/datasets/fashionmnist/fashionmnist/","title":"Documentation for Fashionmnist Module","text":""},{"location":"api/core/datasets/mnist/","title":"Documentation for Mnist Module","text":""},{"location":"api/core/datasets/mnist/mnist/","title":"Documentation for Mnist Module","text":""},{"location":"api/core/models/","title":"Documentation for Models Module","text":""},{"location":"api/core/models/nebulamodel/","title":"Documentation for Nebulamodel Module","text":""},{"location":"api/core/models/nebulamodel/#nebula.core.models.nebulamodel.NebulaModel","title":"<code>NebulaModel</code>","text":"<p>               Bases: <code>LightningModule</code>, <code>ABC</code></p> <p>Abstract class for the NEBULA model.</p> <p>This class is an abstract class that defines the interface for the NEBULA model.</p> Source code in <code>nebula/core/models/nebulamodel.py</code> <pre><code>class NebulaModel(pl.LightningModule, ABC):\n    \"\"\"\n    Abstract class for the NEBULA model.\n\n    This class is an abstract class that defines the interface for the NEBULA model.\n    \"\"\"\n\n    def process_metrics(self, phase, y_pred, y, loss=None):\n        \"\"\"\n        Calculate and log metrics for the given phase.\n        The metrics are calculated in each batch.\n        Args:\n            phase (str): One of 'Train', 'Validation', or 'Test'\n            y_pred (torch.Tensor): Model predictions\n            y (torch.Tensor): Ground truth labels\n            loss (torch.Tensor, optional): Loss value\n        \"\"\"\n\n        y_pred_classes = torch.argmax(y_pred, dim=1).detach()\n        y = y.detach()\n        if phase == \"Train\":\n            self.logger.log_data({f\"{phase}/Loss\": loss.detach()})\n            self.train_metrics.update(y_pred_classes, y)\n        elif phase == \"Validation\":\n            self.val_metrics.update(y_pred_classes, y)\n        elif phase == \"Test (Local)\":\n            self.test_metrics.update(y_pred_classes, y)\n            self.cm.update(y_pred_classes, y) if self.cm is not None else None\n        elif phase == \"Test (Global)\":\n            self.test_metrics_global.update(y_pred_classes, y)\n            self.cm_global.update(y_pred_classes, y) if self.cm_global is not None else None\n        else:\n            raise NotImplementedError\n\n        del y_pred_classes, y\n\n    def log_metrics_end(self, phase):\n        \"\"\"\n        Log metrics for the given phase.\n        Args:\n            phase (str): One of 'Train', 'Validation', 'Test (Local)', or 'Test (Global)'\n            print_cm (bool): Print confusion matrix\n            plot_cm (bool): Plot confusion matrix\n        \"\"\"\n        if phase == \"Train\":\n            output = self.train_metrics.compute()\n        elif phase == \"Validation\":\n            output = self.val_metrics.compute()\n        elif phase == \"Test (Local)\":\n            output = self.test_metrics.compute()\n        elif phase == \"Test (Global)\":\n            output = self.test_metrics_global.compute()\n        else:\n            raise NotImplementedError\n\n        output = {\n            f\"{phase}/{key.replace('Multiclass', '').split('/')[-1]}\": value.detach() for key, value in output.items()\n        }\n\n        self.logger.log_data(output, step=self.global_number[phase])\n\n        metrics_str = \"\"\n        for key, value in output.items():\n            metrics_str += f\"{key}: {value:.4f}\\n\"\n        print_msg_box(\n            metrics_str,\n            indent=2,\n            title=f\"{phase} Metrics | Epoch: {self.global_number[phase]} | Round: {self.round}\",\n            logger_name=TRAINING_LOGGER,\n        )\n\n    def generate_confusion_matrix(self, phase, print_cm=False, plot_cm=False):\n        \"\"\"\n        Generate and plot the confusion matrix for the given phase.\n        Args:\n            phase (str): One of 'Train', 'Validation', 'Test (Local)', or 'Test (Global)'\n        \"\"\"\n        if phase == \"Test (Local)\":\n            if self.cm is None:\n                raise ValueError(f\"Confusion matrix not available for {phase} phase.\")\n            cm = self.cm.compute().cpu()\n        elif phase == \"Test (Global)\":\n            if self.cm_global is None:\n                raise ValueError(f\"Confusion matrix not available for {phase} phase.\")\n            cm = self.cm_global.compute().cpu()\n        else:\n            raise NotImplementedError\n\n        if print_cm:\n            logging_training.info(f\"{phase} / Confusion Matrix:\\n{cm}\")\n\n        if plot_cm:\n            cm_numpy = cm.numpy().astype(int)\n            classes = [i for i in range(self.num_classes)]\n            fig, ax = plt.subplots(figsize=(12, 12))\n            sns.heatmap(\n                cm_numpy,\n                annot=False,\n                fmt=\"\",\n                cmap=\"Blues\",\n                ax=ax,\n                xticklabels=classes,\n                yticklabels=classes,\n                square=True,\n            )\n            ax.set_xlabel(\"Predicted labels\", fontsize=12)\n            ax.set_ylabel(\"True labels\", fontsize=12)\n            ax.set_title(f\"{phase} Confusion Matrix\", fontsize=16)\n            plt.xticks(rotation=90, fontsize=6)\n            plt.yticks(rotation=0, fontsize=6)\n            plt.tight_layout()\n            self.logger.log_figure(fig, step=self.round, name=f\"{phase}/CM\")\n            plt.close()\n\n            del cm_numpy, classes, fig, ax\n\n        # Restablecer la matriz de confusi\u00f3n\n        if phase == \"Test (Local)\":\n            self.cm.reset()\n        else:\n            self.cm_global.reset()\n\n        del cm\n\n    def __init__(\n        self,\n        input_channels=1,\n        num_classes=10,\n        learning_rate=1e-3,\n        metrics=None,\n        confusion_matrix=None,\n        seed=None,\n    ):\n        super().__init__()\n\n        self.input_channels = input_channels\n        self.num_classes = num_classes\n        self.learning_rate = learning_rate\n\n        if metrics is None:\n            metrics = MetricCollection([\n                MulticlassAccuracy(num_classes=num_classes),\n                MulticlassPrecision(num_classes=num_classes),\n                MulticlassRecall(num_classes=num_classes),\n                MulticlassF1Score(num_classes=num_classes),\n            ])\n        self.train_metrics = metrics.clone(prefix=\"Train/\")\n        self.val_metrics = metrics.clone(prefix=\"Validation/\")\n        self.test_metrics = metrics.clone(prefix=\"Test (Local)/\")\n        self.test_metrics_global = metrics.clone(prefix=\"Test (Global)/\")\n        del metrics\n        if confusion_matrix is None:\n            self.cm = MulticlassConfusionMatrix(num_classes=num_classes)\n            self.cm_global = MulticlassConfusionMatrix(num_classes=num_classes)\n        if seed is not None:\n            torch.manual_seed(seed)\n            torch.cuda.manual_seed_all(seed)\n\n        # Round counter (number of training-validation-test rounds)\n        self.round = 0\n\n        # Epochs counter\n        self.global_number = {\n            \"Train\": 0,\n            \"Validation\": 0,\n            \"Test (Local)\": 0,\n            \"Test (Global)\": 0,\n        }\n\n        # Communication manager for sending messages from the model (e.g., prototypes, gradients)\n        # Model parameters are sent by default using network.propagator\n        self.communication_manager = None\n\n        self._current_loss = -1\n        self._optimizer = None\n\n    def set_communication_manager(self, communication_manager):\n        self.communication_manager = communication_manager\n\n    def get_communication_manager(self):\n        if self.communication_manager is None:\n            raise ValueError(\"Communication manager not set.\")\n        return self.communication_manager\n\n    @abstractmethod\n    def forward(self, x):\n        \"\"\"Forward pass of the model.\"\"\"\n        pass\n\n    @abstractmethod\n    def configure_optimizers(self):\n        \"\"\"Optimizer configuration.\"\"\"\n        pass\n\n    def step(self, batch, batch_idx, phase):\n        \"\"\"Training/validation/test step.\"\"\"\n        x, y = batch\n        y_pred = self.forward(x)\n        loss = self.criterion(y_pred, y)\n        self.process_metrics(phase, y_pred, y, loss)\n\n        self._current_loss = loss\n        return loss\n\n    def get_loss(self):\n        return self._current_loss\n\n    def modify_learning_rate(self, new_lr):\n        logging.info(f\"Modifiying | learning rate, new value: {new_lr}\")\n        self.learning_rate = new_lr\n        for param_group in self._optimizer.param_groups:\n            param_group[\"lr\"] = new_lr\n\n    def show_current_learning_rate(self):\n        for param_group in self._optimizer.param_groups:\n            logging.info(f\"Showing | Learning rate current value: {param_group['lr']}\")\n\n    def set_updated_round(self, round):\n        self.round = round\n        self.global_number = {\n            \"Train\": round,\n            \"Validation\": round,\n            \"Test (Local)\": round,\n            \"Test (Global)\": round,\n        }\n\n    def training_step(self, batch, batch_idx):\n        \"\"\"\n        Training step for the model.\n        Args:\n            batch:\n            batch_id:\n\n        Returns:\n        \"\"\"\n        return self.step(batch, batch_idx=batch_idx, phase=\"Train\")\n\n    def on_train_start(self):\n        logging_training.info(f\"{'=' * 10} [Training] Started {'=' * 10}\")\n\n    def on_train_end(self):\n        logging_training.info(f\"{'=' * 10} [Training] Done {'=' * 10}\")\n\n    def on_train_epoch_end(self):\n        self.log_metrics_end(\"Train\")\n        self.train_metrics.reset()\n        self.global_number[\"Train\"] += 1\n\n    def validation_step(self, batch, batch_idx):\n        \"\"\"\n        Validation step for the model.\n        Args:\n            batch:\n            batch_idx:\n\n        Returns:\n        \"\"\"\n        return self.step(batch, batch_idx=batch_idx, phase=\"Validation\")\n\n    def on_validation_end(self):\n        pass\n\n    def on_validation_epoch_end(self):\n        # In general, the validation phase is done in one epoch\n        self.log_metrics_end(\"Validation\")\n        self.val_metrics.reset()\n        self.global_number[\"Validation\"] += 1\n\n    def test_step(self, batch, batch_idx, dataloader_idx=None):\n        \"\"\"\n        Test step for the model.\n        Args:\n            batch:\n            batch_idx:\n\n        Returns:\n        \"\"\"\n        if dataloader_idx == 0:\n            return self.step(batch, batch_idx=batch_idx, phase=\"Test (Local)\")\n        else:\n            return self.step(batch, batch_idx=batch_idx, phase=\"Test (Global)\")\n\n    def on_test_start(self):\n        logging_training.info(f\"{'=' * 10} [Testing] Started {'=' * 10}\")\n\n    def on_test_end(self):\n        logging_training.info(f\"{'=' * 10} [Testing] Done {'=' * 10}\")\n\n    def on_test_epoch_end(self):\n        # In general, the test phase is done in one epoch\n        self.log_metrics_end(\"Test (Local)\")\n        self.log_metrics_end(\"Test (Global)\")\n        self.generate_confusion_matrix(\"Test (Local)\", print_cm=True, plot_cm=True)\n        self.generate_confusion_matrix(\"Test (Global)\", print_cm=True, plot_cm=True)\n        self.test_metrics.reset()\n        self.test_metrics_global.reset()\n        self.global_number[\"Test (Local)\"] += 1\n        self.global_number[\"Test (Global)\"] += 1\n\n    def on_round_end(self):\n        self.round += 1\n</code></pre>"},{"location":"api/core/models/nebulamodel/#nebula.core.models.nebulamodel.NebulaModel.configure_optimizers","title":"<code>configure_optimizers()</code>  <code>abstractmethod</code>","text":"<p>Optimizer configuration.</p> Source code in <code>nebula/core/models/nebulamodel.py</code> <pre><code>@abstractmethod\ndef configure_optimizers(self):\n    \"\"\"Optimizer configuration.\"\"\"\n    pass\n</code></pre>"},{"location":"api/core/models/nebulamodel/#nebula.core.models.nebulamodel.NebulaModel.forward","title":"<code>forward(x)</code>  <code>abstractmethod</code>","text":"<p>Forward pass of the model.</p> Source code in <code>nebula/core/models/nebulamodel.py</code> <pre><code>@abstractmethod\ndef forward(self, x):\n    \"\"\"Forward pass of the model.\"\"\"\n    pass\n</code></pre>"},{"location":"api/core/models/nebulamodel/#nebula.core.models.nebulamodel.NebulaModel.generate_confusion_matrix","title":"<code>generate_confusion_matrix(phase, print_cm=False, plot_cm=False)</code>","text":"<p>Generate and plot the confusion matrix for the given phase. Args:     phase (str): One of 'Train', 'Validation', 'Test (Local)', or 'Test (Global)'</p> Source code in <code>nebula/core/models/nebulamodel.py</code> <pre><code>def generate_confusion_matrix(self, phase, print_cm=False, plot_cm=False):\n    \"\"\"\n    Generate and plot the confusion matrix for the given phase.\n    Args:\n        phase (str): One of 'Train', 'Validation', 'Test (Local)', or 'Test (Global)'\n    \"\"\"\n    if phase == \"Test (Local)\":\n        if self.cm is None:\n            raise ValueError(f\"Confusion matrix not available for {phase} phase.\")\n        cm = self.cm.compute().cpu()\n    elif phase == \"Test (Global)\":\n        if self.cm_global is None:\n            raise ValueError(f\"Confusion matrix not available for {phase} phase.\")\n        cm = self.cm_global.compute().cpu()\n    else:\n        raise NotImplementedError\n\n    if print_cm:\n        logging_training.info(f\"{phase} / Confusion Matrix:\\n{cm}\")\n\n    if plot_cm:\n        cm_numpy = cm.numpy().astype(int)\n        classes = [i for i in range(self.num_classes)]\n        fig, ax = plt.subplots(figsize=(12, 12))\n        sns.heatmap(\n            cm_numpy,\n            annot=False,\n            fmt=\"\",\n            cmap=\"Blues\",\n            ax=ax,\n            xticklabels=classes,\n            yticklabels=classes,\n            square=True,\n        )\n        ax.set_xlabel(\"Predicted labels\", fontsize=12)\n        ax.set_ylabel(\"True labels\", fontsize=12)\n        ax.set_title(f\"{phase} Confusion Matrix\", fontsize=16)\n        plt.xticks(rotation=90, fontsize=6)\n        plt.yticks(rotation=0, fontsize=6)\n        plt.tight_layout()\n        self.logger.log_figure(fig, step=self.round, name=f\"{phase}/CM\")\n        plt.close()\n\n        del cm_numpy, classes, fig, ax\n\n    # Restablecer la matriz de confusi\u00f3n\n    if phase == \"Test (Local)\":\n        self.cm.reset()\n    else:\n        self.cm_global.reset()\n\n    del cm\n</code></pre>"},{"location":"api/core/models/nebulamodel/#nebula.core.models.nebulamodel.NebulaModel.log_metrics_end","title":"<code>log_metrics_end(phase)</code>","text":"<p>Log metrics for the given phase. Args:     phase (str): One of 'Train', 'Validation', 'Test (Local)', or 'Test (Global)'     print_cm (bool): Print confusion matrix     plot_cm (bool): Plot confusion matrix</p> Source code in <code>nebula/core/models/nebulamodel.py</code> <pre><code>def log_metrics_end(self, phase):\n    \"\"\"\n    Log metrics for the given phase.\n    Args:\n        phase (str): One of 'Train', 'Validation', 'Test (Local)', or 'Test (Global)'\n        print_cm (bool): Print confusion matrix\n        plot_cm (bool): Plot confusion matrix\n    \"\"\"\n    if phase == \"Train\":\n        output = self.train_metrics.compute()\n    elif phase == \"Validation\":\n        output = self.val_metrics.compute()\n    elif phase == \"Test (Local)\":\n        output = self.test_metrics.compute()\n    elif phase == \"Test (Global)\":\n        output = self.test_metrics_global.compute()\n    else:\n        raise NotImplementedError\n\n    output = {\n        f\"{phase}/{key.replace('Multiclass', '').split('/')[-1]}\": value.detach() for key, value in output.items()\n    }\n\n    self.logger.log_data(output, step=self.global_number[phase])\n\n    metrics_str = \"\"\n    for key, value in output.items():\n        metrics_str += f\"{key}: {value:.4f}\\n\"\n    print_msg_box(\n        metrics_str,\n        indent=2,\n        title=f\"{phase} Metrics | Epoch: {self.global_number[phase]} | Round: {self.round}\",\n        logger_name=TRAINING_LOGGER,\n    )\n</code></pre>"},{"location":"api/core/models/nebulamodel/#nebula.core.models.nebulamodel.NebulaModel.process_metrics","title":"<code>process_metrics(phase, y_pred, y, loss=None)</code>","text":"<p>Calculate and log metrics for the given phase. The metrics are calculated in each batch. Args:     phase (str): One of 'Train', 'Validation', or 'Test'     y_pred (torch.Tensor): Model predictions     y (torch.Tensor): Ground truth labels     loss (torch.Tensor, optional): Loss value</p> Source code in <code>nebula/core/models/nebulamodel.py</code> <pre><code>def process_metrics(self, phase, y_pred, y, loss=None):\n    \"\"\"\n    Calculate and log metrics for the given phase.\n    The metrics are calculated in each batch.\n    Args:\n        phase (str): One of 'Train', 'Validation', or 'Test'\n        y_pred (torch.Tensor): Model predictions\n        y (torch.Tensor): Ground truth labels\n        loss (torch.Tensor, optional): Loss value\n    \"\"\"\n\n    y_pred_classes = torch.argmax(y_pred, dim=1).detach()\n    y = y.detach()\n    if phase == \"Train\":\n        self.logger.log_data({f\"{phase}/Loss\": loss.detach()})\n        self.train_metrics.update(y_pred_classes, y)\n    elif phase == \"Validation\":\n        self.val_metrics.update(y_pred_classes, y)\n    elif phase == \"Test (Local)\":\n        self.test_metrics.update(y_pred_classes, y)\n        self.cm.update(y_pred_classes, y) if self.cm is not None else None\n    elif phase == \"Test (Global)\":\n        self.test_metrics_global.update(y_pred_classes, y)\n        self.cm_global.update(y_pred_classes, y) if self.cm_global is not None else None\n    else:\n        raise NotImplementedError\n\n    del y_pred_classes, y\n</code></pre>"},{"location":"api/core/models/nebulamodel/#nebula.core.models.nebulamodel.NebulaModel.step","title":"<code>step(batch, batch_idx, phase)</code>","text":"<p>Training/validation/test step.</p> Source code in <code>nebula/core/models/nebulamodel.py</code> <pre><code>def step(self, batch, batch_idx, phase):\n    \"\"\"Training/validation/test step.\"\"\"\n    x, y = batch\n    y_pred = self.forward(x)\n    loss = self.criterion(y_pred, y)\n    self.process_metrics(phase, y_pred, y, loss)\n\n    self._current_loss = loss\n    return loss\n</code></pre>"},{"location":"api/core/models/nebulamodel/#nebula.core.models.nebulamodel.NebulaModel.test_step","title":"<code>test_step(batch, batch_idx, dataloader_idx=None)</code>","text":"<p>Test step for the model. Args:     batch:     batch_idx:</p> <p>Returns:</p> Source code in <code>nebula/core/models/nebulamodel.py</code> <pre><code>def test_step(self, batch, batch_idx, dataloader_idx=None):\n    \"\"\"\n    Test step for the model.\n    Args:\n        batch:\n        batch_idx:\n\n    Returns:\n    \"\"\"\n    if dataloader_idx == 0:\n        return self.step(batch, batch_idx=batch_idx, phase=\"Test (Local)\")\n    else:\n        return self.step(batch, batch_idx=batch_idx, phase=\"Test (Global)\")\n</code></pre>"},{"location":"api/core/models/nebulamodel/#nebula.core.models.nebulamodel.NebulaModel.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Training step for the model. Args:     batch:     batch_id:</p> <p>Returns:</p> Source code in <code>nebula/core/models/nebulamodel.py</code> <pre><code>def training_step(self, batch, batch_idx):\n    \"\"\"\n    Training step for the model.\n    Args:\n        batch:\n        batch_id:\n\n    Returns:\n    \"\"\"\n    return self.step(batch, batch_idx=batch_idx, phase=\"Train\")\n</code></pre>"},{"location":"api/core/models/nebulamodel/#nebula.core.models.nebulamodel.NebulaModel.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Validation step for the model. Args:     batch:     batch_idx:</p> <p>Returns:</p> Source code in <code>nebula/core/models/nebulamodel.py</code> <pre><code>def validation_step(self, batch, batch_idx):\n    \"\"\"\n    Validation step for the model.\n    Args:\n        batch:\n        batch_idx:\n\n    Returns:\n    \"\"\"\n    return self.step(batch, batch_idx=batch_idx, phase=\"Validation\")\n</code></pre>"},{"location":"api/core/models/cifar10/","title":"Documentation for Cifar10 Module","text":""},{"location":"api/core/models/cifar10/cnn/","title":"Documentation for Cnn Module","text":""},{"location":"api/core/models/cifar10/cnnV2/","title":"Documentation for Cnnv2 Module","text":""},{"location":"api/core/models/cifar10/cnnV3/","title":"Documentation for Cnnv3 Module","text":""},{"location":"api/core/models/cifar10/fastermobilenet/","title":"Documentation for Fastermobilenet Module","text":""},{"location":"api/core/models/cifar10/resnet/","title":"Documentation for Resnet Module","text":""},{"location":"api/core/models/cifar10/simplemobilenet/","title":"Documentation for Simplemobilenet Module","text":""},{"location":"api/core/models/cifar100/","title":"Documentation for Cifar100 Module","text":""},{"location":"api/core/models/cifar100/cnn/","title":"Documentation for Cnn Module","text":""},{"location":"api/core/models/emnist/","title":"Documentation for Emnist Module","text":""},{"location":"api/core/models/emnist/cnn/","title":"Documentation for Cnn Module","text":""},{"location":"api/core/models/emnist/mlp/","title":"Documentation for Mlp Module","text":""},{"location":"api/core/models/fashionmnist/","title":"Documentation for Fashionmnist Module","text":""},{"location":"api/core/models/fashionmnist/cnn/","title":"Documentation for Cnn Module","text":""},{"location":"api/core/models/fashionmnist/mlp/","title":"Documentation for Mlp Module","text":""},{"location":"api/core/models/mnist/","title":"Documentation for Mnist Module","text":""},{"location":"api/core/models/mnist/cnn/","title":"Documentation for Cnn Module","text":""},{"location":"api/core/models/mnist/mlp/","title":"Documentation for Mlp Module","text":""},{"location":"api/core/models/sentiment140/","title":"Documentation for Sentiment140 Module","text":""},{"location":"api/core/models/sentiment140/cnn/","title":"Documentation for Cnn Module","text":""},{"location":"api/core/models/sentiment140/rnn/","title":"Documentation for Rnn Module","text":""},{"location":"api/core/network/","title":"Documentation for Network Module","text":""},{"location":"api/core/network/actions/","title":"Documentation for Actions Module","text":""},{"location":"api/core/network/communications/","title":"Documentation for Communications Module","text":""},{"location":"api/core/network/connection/","title":"Documentation for Connection Module","text":""},{"location":"api/core/network/discoverer/","title":"Documentation for Discoverer Module","text":""},{"location":"api/core/network/forwarder/","title":"Documentation for Forwarder Module","text":""},{"location":"api/core/network/health/","title":"Documentation for Health Module","text":""},{"location":"api/core/network/messages/","title":"Documentation for Messages Module","text":""},{"location":"api/core/network/propagator/","title":"Documentation for Propagator Module","text":""},{"location":"api/core/pb/","title":"Documentation for Pb Module","text":""},{"location":"api/core/pb/nebula_pb2/","title":"Documentation for Nebula_pb2 Module","text":"<p>Generated protocol buffer code.</p>"},{"location":"api/core/training/","title":"Documentation for Training Module","text":""},{"location":"api/core/training/lightning/","title":"Documentation for Lightning Module","text":""},{"location":"api/core/training/lightning/#nebula.core.training.lightning.Lightning","title":"<code>Lightning</code>","text":"Source code in <code>nebula/core/training/lightning.py</code> <pre><code>class Lightning:\n    DEFAULT_MODEL_WEIGHT = 1\n    BYPASS_MODEL_WEIGHT = 0\n\n    def __init__(self, model, datamodule, config=None):\n        # self.model = torch.compile(model, mode=\"reduce-overhead\")\n        self.model = model\n        self.datamodule = datamodule\n        self.config = config\n        self._trainer = None\n        self.epochs = 1\n        self.round = 0\n        self.experiment_name = self.config.participant[\"scenario_args\"][\"name\"]\n        self.idx = self.config.participant[\"device_args\"][\"idx\"]\n        self.log_dir = os.path.join(self.config.participant[\"tracking_args\"][\"log_dir\"], self.experiment_name)\n        self._logger = None\n        self.create_logger()\n        enable_deterministic(seed=self.config.participant[\"scenario_args\"][\"random_seed\"])\n\n    @property\n    def logger(self):\n        return self._logger\n\n    def get_round(self):\n        return self.round\n\n    def set_model(self, model):\n        self.model = model\n\n    def set_datamodule(self, datamodule):\n        self.datamodule = datamodule\n\n    def create_logger(self):\n        if self.config.participant[\"tracking_args\"][\"local_tracking\"] == \"csv\":\n            nebulalogger = CSVLogger(f\"{self.log_dir}\", name=\"metrics\", version=f\"participant_{self.idx}\")\n        elif self.config.participant[\"tracking_args\"][\"local_tracking\"] == \"basic\":\n            logger_config = None\n            if self._logger is not None:\n                logger_config = self._logger.get_logger_config()\n            nebulalogger = NebulaTensorBoardLogger(\n                self.config.participant[\"scenario_args\"][\"start_time\"],\n                f\"{self.log_dir}\",\n                name=\"metrics\",\n                version=f\"participant_{self.idx}\",\n                log_graph=False,\n            )\n            # Restore logger configuration\n            nebulalogger.set_logger_config(logger_config)\n        else:\n            nebulalogger = None\n\n        self._logger = nebulalogger\n\n    def create_trainer(self):\n        # Create a new trainer and logger for each round\n        self.create_logger()\n        num_gpus = len(self.config.participant[\"device_args\"][\"gpu_id\"])\n        if self.config.participant[\"device_args\"][\"accelerator\"] == \"gpu\" and num_gpus &gt; 0:\n            # Use all available GPUs\n            if num_gpus &gt; 1:\n                gpu_index = [self.config.participant[\"device_args\"][\"idx\"] % num_gpus]\n            # Use the selected GPU\n            else:\n                gpu_index = self.config.participant[\"device_args\"][\"gpu_id\"]\n            logging_training.info(f\"Creating trainer with accelerator GPU ({gpu_index})\")\n            self._trainer = Trainer(\n                callbacks=[ModelSummary(max_depth=1), NebulaProgressBar()],\n                max_epochs=self.epochs,\n                accelerator=\"gpu\",\n                devices=gpu_index,\n                logger=self._logger,\n                enable_checkpointing=False,\n                enable_model_summary=False,\n                # deterministic=True\n            )\n        else:\n            logging_training.info(\"Creating trainer with accelerator CPU\")\n            self._trainer = Trainer(\n                callbacks=[ModelSummary(max_depth=1), NebulaProgressBar()],\n                max_epochs=self.epochs,\n                accelerator=\"cpu\",\n                devices=\"auto\",\n                logger=self._logger,\n                enable_checkpointing=False,\n                enable_model_summary=False,\n                # deterministic=True\n            )\n        logging_training.info(f\"Trainer strategy: {self._trainer.strategy}\")\n\n    def validate_neighbour_model(self, neighbour_model_param):\n        avg_loss = 0\n        running_loss = 0\n        bootstrap_dataloader = self.datamodule.bootstrap_dataloader()\n        num_samples = 0\n        neighbour_model = copy.deepcopy(self.model)\n        neighbour_model.load_state_dict(neighbour_model_param)\n\n        # enable evaluation mode, prevent memory leaks.\n        # no need to switch back to training since model is not further used.\n        if torch.cuda.is_available():\n            neighbour_model = neighbour_model.to(\"cuda\")\n        neighbour_model.eval()\n\n        # bootstrap_dataloader = bootstrap_dataloader.to('cuda')\n        with torch.no_grad():\n            for inputs, labels in bootstrap_dataloader:\n                if torch.cuda.is_available():\n                    inputs = inputs.to(\"cuda\")\n                    labels = labels.to(\"cuda\")\n                outputs = neighbour_model(inputs)\n                loss = F.cross_entropy(outputs, labels)\n                running_loss += loss.item()\n                num_samples += inputs.size(0)\n\n        avg_loss = running_loss / len(bootstrap_dataloader)\n        logging_training.info(f\"Computed neighbor loss over {num_samples} data samples\")\n        return avg_loss\n\n    def get_hash_model(self):\n        \"\"\"\n        Returns:\n            str: SHA256 hash of model parameters\n        \"\"\"\n        return hashlib.sha256(self.serialize_model(self.model)).hexdigest()\n\n    def set_epochs(self, epochs):\n        self.epochs = epochs\n\n    def set_current_round(self, round):\n        logging.info(f\"Update | current round = {round}\")\n        self.round = round\n        self.model.set_updated_round(round)\n\n    def get_current_loss(self):\n        return self.model.get_loss()\n\n    def serialize_model(self, model):\n        # From https://pytorch.org/docs/stable/notes/serialization.html\n        try:\n            buffer = io.BytesIO()\n            with gzip.GzipFile(fileobj=buffer, mode=\"wb\") as f:\n                torch.save(model, f, pickle_protocol=pickle.HIGHEST_PROTOCOL)\n            serialized_data = buffer.getvalue()\n            buffer.close()\n            del buffer\n            return serialized_data\n        except Exception as e:\n            raise ParameterSerializeError(\"Error serializing model\") from e\n\n    def deserialize_model(self, data):\n        # From https://pytorch.org/docs/stable/notes/serialization.html\n        try:\n            buffer = io.BytesIO(data)\n            with gzip.GzipFile(fileobj=buffer, mode=\"rb\") as f:\n                params_dict = torch.load(f)\n            buffer.close()\n            del buffer\n            return OrderedDict(params_dict)\n        except Exception as e:\n            raise ParameterDeserializeError(\"Error decoding parameters\") from e\n\n    def set_model_parameters(self, params, initialize=False):\n        try:\n            self.model.load_state_dict(params)\n        except Exception as e:\n            raise ParameterSettingError(\"Error setting parameters\") from e\n\n    def get_model_parameters(self, bytes=False, initialize=False):\n        if bytes:\n            return self.serialize_model(self.model.state_dict())\n        return self.model.state_dict()\n\n    async def train(self):\n        try:\n            self.create_trainer()\n            logging.info(f\"{'=' * 10} [Training] Started (check training logs for progress) {'=' * 10}\")\n            await asyncio.to_thread(self._train_sync)\n            logging.info(f\"{'=' * 10} [Training] Finished (check training logs for progress) {'=' * 10}\")\n        except Exception as e:\n            logging_training.error(f\"Error training model: {e}\")\n            logging_training.error(traceback.format_exc())\n\n    def _train_sync(self):\n        try:\n            self._trainer.fit(self.model, self.datamodule)\n        except Exception as e:\n            logging_training.error(f\"Error in _train_sync: {e}\")\n            tb = traceback.format_exc()\n            logging_training.error(f\"Traceback: {tb}\")\n            # If \"raise\", the exception will be managed by the main thread\n\n    async def test(self):\n        try:\n            self.create_trainer()\n            logging.info(f\"{'=' * 10} [Testing] Started (check training logs for progress) {'=' * 10}\")\n            await asyncio.to_thread(self._test_sync)\n            logging.info(f\"{'=' * 10} [Testing] Finished (check training logs for progress) {'=' * 10}\")\n        except Exception as e:\n            logging_training.error(f\"Error testing model: {e}\")\n            logging_training.error(traceback.format_exc())\n\n    def _test_sync(self):\n        try:\n            self._trainer.test(self.model, self.datamodule, verbose=True)\n        except Exception as e:\n            logging_training.error(f\"Error in _test_sync: {e}\")\n            tb = traceback.format_exc()\n            logging_training.error(f\"Traceback: {tb}\")\n            # If \"raise\", the exception will be managed by the main thread\n\n    def cleanup(self):\n        if self._trainer is not None:\n            self._trainer._teardown()\n            del self._trainer\n        if self.datamodule is not None:\n            self.datamodule.teardown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def get_model_weight(self):\n        weight = self.datamodule.model_weight\n        if weight is None:\n            raise ValueError(\"Model weight not set. Please call setup('fit') before requesting model weight.\")\n        return weight\n\n    def on_round_start(self):\n        self.datamodule.setup()\n        self._logger.log_data({\"A-Round\": self.round})\n        # self.reporter.enqueue_data(\"Round\", self.round)\n\n    def on_round_end(self):\n        self._logger.global_step = self._logger.global_step + self._logger.local_step\n        self._logger.local_step = 0\n        self.round += 1\n        self.model.on_round_end()\n        logging.info(\"Flushing memory cache at the end of round...\")\n        self.cleanup()\n\n    def on_learning_cycle_end(self):\n        self._logger.log_data({\"A-Round\": self.round})\n        # self.reporter.enqueue_data(\"Round\", self.round)\n\n    def update_model_learning_rate(self, new_lr):\n        self.model.modify_learning_rate(new_lr)\n\n    def show_current_learning_rate(self):\n        self.model.show_current_learning_rate()\n</code></pre>"},{"location":"api/core/training/lightning/#nebula.core.training.lightning.Lightning.get_hash_model","title":"<code>get_hash_model()</code>","text":"<p>Returns:</p> Name Type Description <code>str</code> <p>SHA256 hash of model parameters</p> Source code in <code>nebula/core/training/lightning.py</code> <pre><code>def get_hash_model(self):\n    \"\"\"\n    Returns:\n        str: SHA256 hash of model parameters\n    \"\"\"\n    return hashlib.sha256(self.serialize_model(self.model)).hexdigest()\n</code></pre>"},{"location":"api/core/training/lightning/#nebula.core.training.lightning.NebulaProgressBar","title":"<code>NebulaProgressBar</code>","text":"<p>               Bases: <code>ProgressBar</code></p> <p>Nebula progress bar for training. Logs the percentage of completion of the training process using logging.</p> Source code in <code>nebula/core/training/lightning.py</code> <pre><code>class NebulaProgressBar(ProgressBar):\n    \"\"\"Nebula progress bar for training.\n    Logs the percentage of completion of the training process using logging.\n    \"\"\"\n\n    def __init__(self, log_every_n_steps=100):\n        super().__init__()\n        self.enable = True\n        self.log_every_n_steps = log_every_n_steps\n\n    def enable(self):\n        \"\"\"Enable progress bar logging.\"\"\"\n        self.enable = True\n\n    def disable(self):\n        \"\"\"Disable the progress bar logging.\"\"\"\n        self.enable = False\n\n    def on_train_epoch_start(self, trainer, pl_module):\n        \"\"\"Called when the training epoch starts.\"\"\"\n        super().on_train_epoch_start(trainer, pl_module)\n        if self.enable:\n            logging_training.info(f\"Starting Epoch {trainer.current_epoch}\")\n\n    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n        \"\"\"Called at the end of each training batch.\"\"\"\n        super().on_train_batch_end(trainer, pl_module, outputs, batch, batch_idx)\n        if self.enable:\n            if (batch_idx + 1) % self.log_every_n_steps == 0 or (batch_idx + 1) == self.total_train_batches:\n                # Calculate percentage complete for the current epoch\n                percent = ((batch_idx + 1) / self.total_train_batches) * 100  # +1 to count current batch\n                logging_training.info(f\"Epoch {trainer.current_epoch} - {percent:.01f}% complete\")\n\n    def on_train_epoch_end(self, trainer, pl_module):\n        \"\"\"Called at the end of the training epoch.\"\"\"\n        super().on_train_epoch_end(trainer, pl_module)\n        if self.enable:\n            logging_training.info(f\"Epoch {trainer.current_epoch} finished\")\n\n    def on_validation_epoch_start(self, trainer, pl_module):\n        super().on_validation_epoch_start(trainer, pl_module)\n        if self.enable:\n            logging_training.info(f\"Starting validation for Epoch {trainer.current_epoch}\")\n\n    def on_validation_epoch_end(self, trainer, pl_module):\n        super().on_validation_epoch_end(trainer, pl_module)\n        if self.enable:\n            logging_training.info(f\"Validation for Epoch {trainer.current_epoch} finished\")\n\n    def on_test_batch_start(self, trainer, pl_module, batch, batch_idx, dataloader_idx):\n        super().on_test_batch_start(trainer, pl_module, batch, batch_idx, dataloader_idx)\n        if not self.has_dataloader_changed(dataloader_idx):\n            return\n\n    def on_test_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n        \"\"\"Called at the end of each test batch.\"\"\"\n        super().on_test_batch_end(trainer, pl_module, outputs, batch, batch_idx, dataloader_idx)\n        if self.enable:\n            total_batches = self.total_test_batches_current_dataloader\n            if total_batches == 0:\n                logging_training.warning(\n                    f\"Total test batches is 0 for dataloader {dataloader_idx}, cannot compute progress.\"\n                )\n                return\n\n            if (batch_idx + 1) % self.log_every_n_steps == 0 or (batch_idx + 1) == total_batches:\n                percent = ((batch_idx + 1) / total_batches) * 100  # +1 to count the current batch\n                logging_training.info(\n                    f\"Test Epoch {trainer.current_epoch}, Dataloader {dataloader_idx} - {percent:.01f}% complete\"\n                )\n\n    def on_test_epoch_start(self, trainer, pl_module):\n        super().on_test_epoch_start(trainer, pl_module)\n        if self.enable:\n            logging_training.info(f\"Starting testing for Epoch {trainer.current_epoch}\")\n\n    def on_test_epoch_end(self, trainer, pl_module):\n        super().on_test_epoch_end(trainer, pl_module)\n        if self.enable:\n            logging_training.info(f\"Testing for Epoch {trainer.current_epoch} finished\")\n</code></pre>"},{"location":"api/core/training/lightning/#nebula.core.training.lightning.NebulaProgressBar.disable","title":"<code>disable()</code>","text":"<p>Disable the progress bar logging.</p> Source code in <code>nebula/core/training/lightning.py</code> <pre><code>def disable(self):\n    \"\"\"Disable the progress bar logging.\"\"\"\n    self.enable = False\n</code></pre>"},{"location":"api/core/training/lightning/#nebula.core.training.lightning.NebulaProgressBar.enable","title":"<code>enable()</code>","text":"<p>Enable progress bar logging.</p> Source code in <code>nebula/core/training/lightning.py</code> <pre><code>def enable(self):\n    \"\"\"Enable progress bar logging.\"\"\"\n    self.enable = True\n</code></pre>"},{"location":"api/core/training/lightning/#nebula.core.training.lightning.NebulaProgressBar.on_test_batch_end","title":"<code>on_test_batch_end(trainer, pl_module, outputs, batch, batch_idx, dataloader_idx)</code>","text":"<p>Called at the end of each test batch.</p> Source code in <code>nebula/core/training/lightning.py</code> <pre><code>def on_test_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n    \"\"\"Called at the end of each test batch.\"\"\"\n    super().on_test_batch_end(trainer, pl_module, outputs, batch, batch_idx, dataloader_idx)\n    if self.enable:\n        total_batches = self.total_test_batches_current_dataloader\n        if total_batches == 0:\n            logging_training.warning(\n                f\"Total test batches is 0 for dataloader {dataloader_idx}, cannot compute progress.\"\n            )\n            return\n\n        if (batch_idx + 1) % self.log_every_n_steps == 0 or (batch_idx + 1) == total_batches:\n            percent = ((batch_idx + 1) / total_batches) * 100  # +1 to count the current batch\n            logging_training.info(\n                f\"Test Epoch {trainer.current_epoch}, Dataloader {dataloader_idx} - {percent:.01f}% complete\"\n            )\n</code></pre>"},{"location":"api/core/training/lightning/#nebula.core.training.lightning.NebulaProgressBar.on_train_batch_end","title":"<code>on_train_batch_end(trainer, pl_module, outputs, batch, batch_idx)</code>","text":"<p>Called at the end of each training batch.</p> Source code in <code>nebula/core/training/lightning.py</code> <pre><code>def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n    \"\"\"Called at the end of each training batch.\"\"\"\n    super().on_train_batch_end(trainer, pl_module, outputs, batch, batch_idx)\n    if self.enable:\n        if (batch_idx + 1) % self.log_every_n_steps == 0 or (batch_idx + 1) == self.total_train_batches:\n            # Calculate percentage complete for the current epoch\n            percent = ((batch_idx + 1) / self.total_train_batches) * 100  # +1 to count current batch\n            logging_training.info(f\"Epoch {trainer.current_epoch} - {percent:.01f}% complete\")\n</code></pre>"},{"location":"api/core/training/lightning/#nebula.core.training.lightning.NebulaProgressBar.on_train_epoch_end","title":"<code>on_train_epoch_end(trainer, pl_module)</code>","text":"<p>Called at the end of the training epoch.</p> Source code in <code>nebula/core/training/lightning.py</code> <pre><code>def on_train_epoch_end(self, trainer, pl_module):\n    \"\"\"Called at the end of the training epoch.\"\"\"\n    super().on_train_epoch_end(trainer, pl_module)\n    if self.enable:\n        logging_training.info(f\"Epoch {trainer.current_epoch} finished\")\n</code></pre>"},{"location":"api/core/training/lightning/#nebula.core.training.lightning.NebulaProgressBar.on_train_epoch_start","title":"<code>on_train_epoch_start(trainer, pl_module)</code>","text":"<p>Called when the training epoch starts.</p> Source code in <code>nebula/core/training/lightning.py</code> <pre><code>def on_train_epoch_start(self, trainer, pl_module):\n    \"\"\"Called when the training epoch starts.\"\"\"\n    super().on_train_epoch_start(trainer, pl_module)\n    if self.enable:\n        logging_training.info(f\"Starting Epoch {trainer.current_epoch}\")\n</code></pre>"},{"location":"api/core/training/lightning/#nebula.core.training.lightning.ParameterDeserializeError","title":"<code>ParameterDeserializeError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception for errors setting model parameters.</p> Source code in <code>nebula/core/training/lightning.py</code> <pre><code>class ParameterDeserializeError(Exception):\n    \"\"\"Custom exception for errors setting model parameters.\"\"\"\n</code></pre>"},{"location":"api/core/training/lightning/#nebula.core.training.lightning.ParameterSerializeError","title":"<code>ParameterSerializeError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception for errors setting model parameters.</p> Source code in <code>nebula/core/training/lightning.py</code> <pre><code>class ParameterSerializeError(Exception):\n    \"\"\"Custom exception for errors setting model parameters.\"\"\"\n</code></pre>"},{"location":"api/core/training/lightning/#nebula.core.training.lightning.ParameterSettingError","title":"<code>ParameterSettingError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception for errors setting model parameters.</p> Source code in <code>nebula/core/training/lightning.py</code> <pre><code>class ParameterSettingError(Exception):\n    \"\"\"Custom exception for errors setting model parameters.\"\"\"\n</code></pre>"},{"location":"api/core/training/scikit/","title":"Documentation for Scikit Module","text":""},{"location":"api/core/training/siamese/","title":"Documentation for Siamese Module","text":""},{"location":"api/core/training/siamese/#nebula.core.training.siamese.Siamese","title":"<code>Siamese</code>","text":"Source code in <code>nebula/core/training/siamese.py</code> <pre><code>class Siamese:\n    def __init__(self, model, data, config=None, logger=None):\n        # self.model = torch.compile(model, mode=\"reduce-overhead\")\n        self.model = model\n        self.data = data\n        self.config = config\n        self.logger = logger\n        self.__trainer = None\n        self.epochs = 1\n        logging.getLogger(\"lightning.pytorch\").setLevel(logging.INFO)\n        self.round = 0\n        enable_deterministic(seed=self.config.participant[\"scenario_args\"][\"random_seed\"])\n        self.logger.log_data({\"Round\": self.round}, step=self.logger.global_step)\n\n    @property\n    def logger(self):\n        return self._logger\n\n    def get_round(self):\n        return self.round\n\n    def set_model(self, model):\n        self.model = model\n\n    def set_data(self, data):\n        self.data = data\n\n    def create_trainer(self):\n        logging.info(\n            \"[Trainer] Creating trainer with accelerator: {}\".format(\n                self.config.participant[\"device_args\"][\"accelerator\"]\n            )\n        )\n        progress_bar = RichProgressBar(\n            theme=RichProgressBarTheme(\n                description=\"green_yellow\",\n                progress_bar=\"green1\",\n                progress_bar_finished=\"green1\",\n                progress_bar_pulse=\"#6206E0\",\n                batch_progress=\"green_yellow\",\n                time=\"grey82\",\n                processing_speed=\"grey82\",\n                metrics=\"grey82\",\n            ),\n            leave=True,\n        )\n        if self.config.participant[\"device_args\"][\"accelerator\"] == \"gpu\":\n            # NEBULA uses 2 GPUs (max) to distribute the nodes.\n            if self.config.participant[\"device_args\"][\"devices\"] &gt; 1:\n                # If you have more than 2 GPUs, you should specify which ones to use.\n                gpu_id = ([1] if self.config.participant[\"device_args\"][\"idx\"] % 2 == 0 else [2],)\n            else:\n                # If there is only one GPU, it will be used.\n                gpu_id = [1]\n\n            self.__trainer = Trainer(\n                callbacks=[RichModelSummary(max_depth=1), progress_bar],\n                max_epochs=self.epochs,\n                accelerator=self.config.participant[\"device_args\"][\"accelerator\"],\n                devices=gpu_id,\n                logger=self.logger,\n                log_every_n_steps=50,\n                enable_checkpointing=False,\n                enable_model_summary=False,\n                enable_progress_bar=True,\n                # deterministic=True\n            )\n        else:\n            # NEBULA uses only CPU to distribute the nodes\n            self.__trainer = Trainer(\n                callbacks=[RichModelSummary(max_depth=1), progress_bar],\n                max_epochs=self.epochs,\n                accelerator=self.config.participant[\"device_args\"][\"accelerator\"],\n                devices=\"auto\",\n                logger=self.logger,\n                log_every_n_steps=50,\n                enable_checkpointing=False,\n                enable_model_summary=False,\n                enable_progress_bar=True,\n                # deterministic=True\n            )\n\n    def get_global_model_parameters(self):\n        return self.model.get_global_model_parameters()\n\n    def set_parameter_second_aggregation(self, params):\n        try:\n            logging.info(\"Setting parameters in second aggregation...\")\n            self.model.load_state_dict(params)\n        except:\n            raise Exception(\"Error setting parameters\")\n\n    def get_model_parameters(self, bytes=False):\n        if bytes:\n            return self.serialize_model(self.model.state_dict())\n        else:\n            return self.model.state_dict()\n\n    def get_hash_model(self):\n        \"\"\"\n        Returns:\n            str: SHA256 hash of model parameters\n        \"\"\"\n        return hashlib.sha256(self.serialize_model()).hexdigest()\n\n    def set_epochs(self, epochs):\n        self.epochs = epochs\n\n    ####\n    # Model parameters serialization/deserialization\n    # From https://pytorch.org/docs/stable/notes/serialization.html\n    ####\n    def serialize_model(self, model):\n        try:\n            buffer = io.BytesIO()\n            # with gzip.GzipFile(fileobj=buffer, mode='wb') as f:\n            #    torch.save(params, f)\n            torch.save(model, buffer)\n            return buffer.getvalue()\n        except:\n            raise Exception(\"Error serializing model\")\n\n    def deserialize_model(self, data):\n        try:\n            buffer = io.BytesIO(data)\n            # with gzip.GzipFile(fileobj=buffer, mode='rb') as f:\n            #    params_dict = torch.load(f, map_location='cpu')\n            params_dict = torch.load(buffer, map_location=\"cpu\")\n            return OrderedDict(params_dict)\n        except:\n            raise Exception(\"Error decoding parameters\")\n\n    def set_model_parameters(self, params, initialize=False):\n        try:\n            if initialize:\n                self.model.load_state_dict(params)\n                self.model.global_load_state_dict(params)\n                self.model.historical_load_state_dict(params)\n            else:\n                # First aggregation\n                self.model.global_load_state_dict(params)\n        except:\n            raise Exception(\"Error setting parameters\")\n\n    def train(self):\n        try:\n            self.create_trainer()\n            # torch.autograd.set_detect_anomaly(True)\n            # TODO: It is necessary to train only the local model, save the history of the previous model and then load it, the global model is the aggregation of all the models.\n            self.__trainer.fit(self.model, self.data)\n            # Save local model as historical model (previous round)\n            # It will be compared the next round during training local model (constrantive loss)\n            # When aggregation in global model (first) and aggregation with similarities and weights (second), the historical model keeps inmutable\n            logging.info(\"Saving historical model...\")\n            self.model.save_historical_model()\n        except Exception as e:\n            logging.exception(f\"Error training model: {e}\")\n            logging.exception(traceback.format_exc())\n\n    def test(self):\n        try:\n            self.create_trainer()\n            self.__trainer.test(self.model, self.data, verbose=True)\n        except Exception as e:\n            logging.exception(f\"Error testing model: {e}\")\n            logging.exception(traceback.format_exc())\n\n    def get_model_weight(self):\n        return (\n            len(self.data.train_dataloader().dataset),\n            len(self.data.test_dataloader().dataset),\n        )\n\n    def finalize_round(self):\n        self.logger.global_step = self.logger.global_step + self.logger.local_step\n        self.logger.local_step = 0\n        self.round += 1\n        self.logger.log_data({\"Round\": self.round}, step=self.logger.global_step)\n        pass\n</code></pre>"},{"location":"api/core/training/siamese/#nebula.core.training.siamese.Siamese.get_hash_model","title":"<code>get_hash_model()</code>","text":"<p>Returns:</p> Name Type Description <code>str</code> <p>SHA256 hash of model parameters</p> Source code in <code>nebula/core/training/siamese.py</code> <pre><code>def get_hash_model(self):\n    \"\"\"\n    Returns:\n        str: SHA256 hash of model parameters\n    \"\"\"\n    return hashlib.sha256(self.serialize_model()).hexdigest()\n</code></pre>"},{"location":"api/frontend/","title":"Documentation for Frontend Module","text":""},{"location":"api/frontend/app/","title":"Documentation for App Module","text":""},{"location":"api/frontend/app/#nebula.frontend.app.attack_node_assign","title":"<code>attack_node_assign(nodes, federation, attack, poisoned_node_percent, poisoned_sample_percent, poisoned_noise_percent)</code>","text":"<p>Identify which nodes will be attacked</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>def attack_node_assign(\n    nodes,\n    federation,\n    attack,\n    poisoned_node_percent,\n    poisoned_sample_percent,\n    poisoned_noise_percent,\n):\n    \"\"\"Identify which nodes will be attacked\"\"\"\n    import math\n    import random\n\n    attack_matrix = []\n    n_nodes = len(nodes)\n    if n_nodes == 0:\n        return attack_matrix\n\n    nodes_index = []\n    # Get the nodes index\n    if federation == \"DFL\":\n        nodes_index = list(nodes.keys())\n    else:\n        for node in nodes:\n            if nodes[node][\"role\"] != \"server\":\n                nodes_index.append(node)\n\n    n_nodes = len(nodes_index)\n    # Number of attacked nodes, round up\n    num_attacked = int(math.ceil(poisoned_node_percent / 100 * n_nodes))\n    if num_attacked &gt; n_nodes:\n        num_attacked = n_nodes\n\n    # Get the index of attacked nodes\n    attacked_nodes = random.sample(nodes_index, num_attacked)\n\n    # Assign the role of each node\n    for node in nodes:\n        node_att = \"No Attack\"\n        attack_sample_persent = 0\n        poisoned_ratio = 0\n        if (node in attacked_nodes) or (nodes[node][\"malicious\"]):\n            node_att = attack\n            attack_sample_persent = poisoned_sample_percent / 100\n            poisoned_ratio = poisoned_noise_percent / 100\n        nodes[node][\"attacks\"] = node_att\n        nodes[node][\"poisoned_sample_percent\"] = attack_sample_persent\n        nodes[node][\"poisoned_ratio\"] = poisoned_ratio\n        attack_matrix.append([node, node_att, attack_sample_persent, poisoned_ratio])\n    return nodes, attack_matrix\n</code></pre>"},{"location":"api/frontend/app/#nebula.frontend.app.mobility_assign","title":"<code>mobility_assign(nodes, mobile_participants_percent)</code>","text":"<p>Assign mobility to nodes</p> Source code in <code>nebula/frontend/app.py</code> <pre><code>def mobility_assign(nodes, mobile_participants_percent):\n    \"\"\"Assign mobility to nodes\"\"\"\n    import random\n\n    # Number of mobile nodes, round down\n    num_mobile = math.floor(mobile_participants_percent / 100 * len(nodes))\n    if num_mobile &gt; len(nodes):\n        num_mobile = len(nodes)\n\n    # Get the index of mobile nodes\n    mobile_nodes = random.sample(list(nodes.keys()), num_mobile)\n\n    # Assign the role of each node\n    for node in nodes:\n        node_mob = False\n        if node in mobile_nodes:\n            node_mob = True\n        nodes[node][\"mobility\"] = node_mob\n    return nodes\n</code></pre>"},{"location":"api/frontend/database/","title":"Documentation for Database Module","text":""}]}